id,citekey,title,authors,year,month,journal,book_title,publisher,institution,volume,number,pages,note,keywords,url,code,pdf,image,thumbnail,doi,external,abstract,isbn,type_id
70.0,,A Test Case Generation Scheduling Model Based on Running Event Interval Characteristics,Y. Li; T. Qin; J. Zou; H. Li; C. -B. Yan; X. Guan,2024,,IEEE Transactions on Automation Science and Engineering,,IEEE,"MOE KLINNS Laboratory, Xi’an Jiaotong University, Xi’an, China; MOE KLINNS Laboratory, Xi’an Jiaotong University, Xi’an, China; MOE KLINNS Laboratory, Xi’an Jiaotong University, Xi’an, China; MOE KLINNS Laboratory, Xi’an Jiaotong University, Xi’an, China; Faculty of Electronic and Information Engineering, State Key Laboratory for Manufacturing Systems Engineering and the School of Automation Science and Engineering, Xi’an Jiaotong University, Xi’an, Shaanxi, China; MOE KLINNS Laboratory, Xi’an Jiaotong University, Xi’an, China",0,0,,,Testing;Fuzzing;Computer bugs;Schedules;Stability analysis;Research and development;Recording;Real-time systems;Mathematical models;Load modeling,,https://github.com/yizhenli-xjtu/CECAR,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10816224,,,10.1109/TASE.2024.3520311,False,"System testing mainly focuses on finding bugs before online deployment. However, the system’s complexity increases due to a surge in the number of users, which makes scheduling the test case generation time more difficult, impeding testing fidelity and automatic testing. In this paper, we propose a data-driven framework for scheduling the test cases and improving testing fidelity. The developed framework involves two stages: 1) mining the running event interval characteristics from logs obtained from the actual environment in the first stage, and 2) utilizing those characteristics to develop a model for the test case generation time schedule in the second stage. First, we select the running event interval, which is the interval between two adjacent running events processed by the system, as the feature for analysis. Then, we analyze the interval’s distribution characteristics, observing that it adheres to the power-law distribution from multiple aspects. Second, based on the distribution characteristics obtained, we develop a composite event chain with associated relationships (CECAR) model to schedule test case generation time. Finally, we conduct a series of experiments using logs obtained from eight regions. The results show that, compared to baseline methods, the CECAR model reduces average errors in the generated quantity, power exponent, and specific granularity intervals by approximately 1% to 9% across multiple layers, demonstrating the effectiveness of the proposed method. We also made our dataset and code publicly available at https://github.com/yizhenli-xjtu/CECAR. Note to Practitioners—System testing is a promising way to eliminate bugs and improve system stability before online deployment. If the testing method does not align with the system’s running characteristics in the actual environment, the testing fidelity and efficiency may be greatly reduced. Scheduling the test case generation time is one of the fundamental ways to address this challenge. The CECAR model developed in this paper can reproduce temporal characteristics and incorporate the concurrent relationships between different events, in turn, achieving the goal of fidelity improvement. Furthermore, based on the CECAR model, we can simulate arbitrary scenarios by combining different types of devices, which can greatly improve the testing’s flexibility and practicality.",,1
71.0,,Knowledge-Informed Machine Learning for Cancer Diagnosis and Prognosis: A Review,L. Mao; H. Wang; L. S. Hu; N. L. Tran; P. D. Canoll; K. R. Swanson; J. Li,2024,,IEEE Transactions on Automation Science and Engineering,,IEEE,"H. Milton Stewart School of Industrial and Systems Engineering, Georgia Institute of Technology, Atlanta, GA, USA; H. Milton Stewart School of Industrial and Systems Engineering, Georgia Institute of Technology, Atlanta, GA, USA; Department of Neurosurgery, Department of Radiology and the Mathematical Neuro-Oncology Laboratory, Mayo Clinic Arizona, Phoenix, AZ, USA; Department of Cancer Biology, the Department of Neurosurgery, and the Department of Radiation Oncology, Mayo Clinic Arizona, Phoenix, AZ, USA; Department of Pathology and Cell Biology, Columbia University Medical Center, New York, NY, USA; Department of Neurosurgery, Mathematical Neuro-Oncology Laboratory, Mayo Clinic Arizona, Phoenix, AZ, USA; H. Milton Stewart School of Industrial and Systems Engineering, Georgia Institute of Technology, Atlanta, GA, USA",0,0,,,Cancer;Tumors;Reviews;Biological system modeling;Medical diagnostic imaging;Prognostics and health management;Machine learning;Imaging;Automation;Medical services,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10806835,,,10.1109/TASE.2024.3515839,False,"Cancer remains one of the most challenging diseases to treat in the medical field. Machine learning (ML) has enabled in-depth analysis of complex patterns from large, diverse datasets, greatly facilitating “healthcare automation” in cancer diagnosis and prognosis. Despite these advancements, ML models face challenges stemming from limited labeled sample sizes, the intricate interplay of high-dimensionality data types, the inherent heterogeneity observed among patients and within tumors, and concerns about interpretability and consistency with existing biomedical knowledge. One approach to address these challenges is to integrate biomedical knowledge into data-driven models, which has proven potential to improve the accuracy, robustness, and interpretability of model results. Here, we review the state-of-the-art ML studies that leverage the fusion of biomedical knowledge and data, termed knowledge-informed machine learning (KIML), to advance cancer diagnosis and prognosis. We provide an overview of diverse forms of knowledge representation and current strategies of knowledge integration into machine learning pipelines with concrete examples. We conclude the review article by discussing future directions aimed at leveraging KIML to advance cancer research and healthcare automation. A live summary of the review is hosted at https://lingchm.github.io/kinformed-machine-learning-cancer/ offering an evolving resource to support research in this field. Note to Practitioners—Recognizing the challenges posed by inter-patient and intratumoral heterogeneity, constrained sample size, and interpretability requirements in cancer applications, practitioners should consider integration of existing biomedical knowledge into their modeling frameworks. This strategy holds promise for enhancing model performance, robustness, and interpretability. We review classic machine learning and deep learning models that incorporated domain knowledge in their cancer diagnosis and prognosis models spanning models that use clinical, imaging, molecular, and treatment data. Pros and cons of each integration approach are discussed. Key design questions such as which knowledge to leverage, how to represent it effectively, and how to seamlessly integrate it into their models need be examined for each case. Collaboration between modeling scientists and medical experts is essential in this endeavor.",,1
72.0,,Trajectory Progress-Based Prioritizing and Intrinsic Reward Mechanism for Robust Training of Robotic Manipulations,W. Liang; Y. Liu; J. Wang; Z. -X. Yang,2024,,IEEE Transactions on Automation Science and Engineering,,IEEE,"State Key Laboratory of Internet of Things for Smart City and Department of Electromechanical Engineering, University of Macau, Macau, China; State Key Laboratory of Internet of Things for Smart City and Department of Electromechanical Engineering, University of Macau, Macau, China; State Key Laboratory of Internet of Things for Smart City and Department of Electromechanical Engineering, University of Macau, Macau, China; State Key Laboratory of Internet of Things for Smart City and Department of Electromechanical Engineering, University of Macau, Macau, China",0,0,,,Robots;Training;Trajectory;Service robots;Space exploration;Deep reinforcement learning;Psychology;Data models;Smart manufacturing;Programming,,https://github.com/weixiang-smart/P-HER,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10792658,,,10.1109/TASE.2024.3513354,False,"Training robots by model-free deep reinforcement learning (DRL) to carry out robotic manipulation tasks without sufficient successful experiences is challenging. Hindsight experience replay (HER) is introduced to enable DRL agents to learn from failure experiences. However, the HER-enabled model-free DRL still suffers from limited training performance due to its uniform sampling strategy and scarcity of reward information in the task environment. Inspired by the progress incentive mechanism in human psychology, we propose Progress Intrinsic Motivation-based HER (P-HER) in this work to overcome these difficulties. First, the Trajectory Progress-based Prioritized Experience Replay (TPPER) module is developed to prioritize sampling valuable trajectory data thereby achieving more efficient training. Second, the Progress Intrinsic Reward (PIR) module is introduced in agent training to add extra intrinsic rewards for encouraging the agents throughout the exploration of task space. Experiments in challenging robotic manipulation tasks demonstrate that our P-HER method outperforms original HER and state-of-the-art HER-based methods in training performance. Our code of P-HER and its experimental videos in both virtual and real environments are available at https://github.com/weixiang-smart/P-HER. Note to Practitioners—This work is motivated to develop a fast and effective learning method for intelligent robotic manipulation of typical industrial tasks, including pushing, picking, and placing workpieces, which are essential and fundamental processing plan activities for accomplishing robotic machining and assembly applications towards smart manufacturing. The introduction of reinforcement learning enables robots to learn manipulation tasks autonomously, which can save the effort for engineers to teach or hard program the robot and also reduce labor costs. However, the existing HER-based reinforcement learning algorithms are with low training efficiency and performance due to the uniform sampling and scant task reward. Inspired by human learning, this work introduces a progress incentive mechanism to identify valuable trajectory data for effective training. In addition, a novel rewarding method, that applies additional intrinsic rewards for agents learning valuable trajectory space, results in fast and robust learning. The setting of important weight parameters in the rewarding method is given in the paper, which provides a practical reference for applying the proposed algorithm. The average success rate of two actual manipulation tasks in simulation and real robotic manipulation environments are 96% and 92.5%, respectively, which demonstrates that the method is effective for both environments and there is 3.5% average gap of successful rate dropping from simulation scenarios to real ones due to the inherent mismatches between simulation and reality. The high success rate demonstrated in the real Workpieces-sorting task exemplifies the potential of the trained policies for application in industrial scenarios.",,1
73.0,,"Listen, Perceive, Grasp: CLIP-Driven Attribute-Aware Network for Language-Conditioned Visual Segmentation and Grasping",J. Xie; J. Liu; S. Huang; C. Wang; F. Zhou,2024,,IEEE Transactions on Automation Science and Engineering,,IEEE,"School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China",0,0,,,Grasping;Visualization;Robots;Shape;Feature extraction;Cognition;Correlation;Linguistics;Transformers;Semantics,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10786319,,,10.1109/TASE.2024.3510777,False,"Endowing robots with the ability to understand natural language and execute grasping is a challenging task in a human-centric environment. Existing works on language-conditioned grasping achieve end-to-end grasping detection based on language. However, these works lack fine-grained visual grounding, resulting in cognitive deficits for robots. Moreover, they ignore the correlation between visual attributes of objects and grasping, leading to coarse grasp poses. To this end, we propose a CLIP-driven aTtribute-aware network (CTNet) for language-conditioned visual segmentation and grasping, enabling the robots to listen, perceive, and grasp the referred object in real-world applications. Specifically, we first employ Listen stage to understand basic linguistic and visual concepts. Subsequently, we introduce Perceive stage to mine multi-modal features and visual attribute cues (e.g., boundary and spatial location), then yield a language-conditioned segmentation mask. Further, we design Grasp stage to aggregate the perceived attribute information and refine the spatial location and grasping rectangle, generating a high-quality grasp pose. Lastly, we provide an extended large dataset Ref-OCID-Grasp to train and test our method, achieving a grasping accuracy of 97.76% and segmentation OIoU of 91.82%. The real-world robotic applications demonstrate the effectiveness of our proposed approach. The project, video, and dataset can be found at https://ctnetgrasp.github.io. Note to Practitioners—Most of the existing grasping methods focus on clearing all objects in the workspace. However, as robots integrate into human society, robots should learn to grasp the desired object by understanding human language. Therefore, language-conditioned grasping is a significant skill for human-robot collaboration. The prior works directly complete the grasp detection through the language-grasp paradigm, but they ignore the discussion on whether the robot understands the concept of vision and language expression of the object. Therefore, this paper proposed the Listen-Perceive-Grasp paradigm, in which the Listen-Perceive stage is responsible for the conception alignment of the object in language expression and visual pixels, and the Perceive-Grasp stage achieves the constraining and refining the grasp detection by the perceived visual attributes such as boundary and shape. Experiments show that this method can obtain a refiner grasp pose in cluttered environments and perform language-conditioned grasping well in the real world. In future research, we will work on 6-DoF grasping and multi-object disambiguation conditioned on language.",,1
74.0,,Multimodal Fish Feeding Intensity Assessment in Aquaculture,M. Cui; X. Liu; H. Liu; Z. Du; T. Chen; G. Lian; D. Li; W. Wang,2024,,IEEE Transactions on Automation Science and Engineering,,IEEE,"Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey, Guildford, U.K.; Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey, Guildford, U.K.; Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey, Guildford, U.K.; National Innovation Center for Digital Fishery, China Agricultural University, Beijing, China; Department of Chemical and Process Engineering, University of Surrey, Guilford, U.K.; Department of Chemical and Process Engineering, University of Surrey, Guilford, U.K.; National Innovation Center for Digital Fishery, China Agricultural University, Beijing, China; Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey, Guildford, U.K.",0,0,,,Computational modeling;Fish;Aquaculture;Visualization;Transformers;Noise;Computational efficiency;Robustness;Benchmark testing;Scholarships,,https://github.com/FishMaster93/U-FFIA,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10781433,,,10.1109/TASE.2024.3507098,False,"Fish feeding intensity assessment (FFIA) aims to evaluate fish appetite changes during feeding, which is crucial in industrial aquaculture applications. Existing FFIA methods are limited by their robustness to noise, computational complexity, and the lack of public datasets for developing the models. To address these issues, we first introduce AV-FFIA, a new dataset containing 27,000 labeled audio and video clips that capture different levels of fish feeding intensity. Then, we introduce multi-modal approaches for FFIA by leveraging the models pre-trained on individual modalities and fused with data fusion methods. We perform benchmark studies of these methods on AV-FFIA, and demonstrate the advantages of the multi-modal approach over the single-modality based approach, especially in noisy environments. However, compared to the methods developed for individual modalities, the multimodal approaches may involve higher computational costs due to the need for independent encoders for each modality. To overcome this issue, we further present a novel unified mixed-modality based method for FFIA, termed as U-FFIA. U-FFIA is a single model capable of processing audio, visual, or audio-visual modalities, by leveraging modality dropout during training and knowledge distillation using the models pre-trained with data from single modality. We demonstrate that U-FFIA can achieve performance better than or on par with the state-of-the-art modality-specific FFIA models, with significantly lower computational overhead, enabling robust and efficient FFIA for improved aquaculture management. To encourage further research, we have released the AV-FFIA dataset, the pre-trained model and codes at https://github.com/FishMaster93/U-FFIA. Note to Practitioners—Feeding is one of the most important costs in aquaculture. However, current feeding machines usually operate with fixed thresholds or human experiences, lacking the ability to automatically adjust to fish feeding intensity. FFIA can evaluate the intensity changes in fish appetite during the feeding process and optimize the control strategies of the feeding machine to avoid inadequate feeding or overfeeding, thereby reducing the feeding cost and improving the well-being of fish in industrial aquaculture. The existing methods have mainly exploited single-modality data, and have a high sensitivity to input noise. Using video and audio offers improved chances to address the challenges brought by various environments. However, compared with processing data from single modalities, using multiple modalities simultaneously often involves increased computational resources, including memory, processing power, and storage. This can impact system performance and scalability. To address these issues, we focus on the efficient unified model, which is capable of processing both multimodal and single-modal input. Our proposed model achieved state-of-the-art (SOTA) performance in FFIA with high computational efficiency.",,1
75.0,,Transmission Line Detection Through Auxiliary Feature Registration With Knowledge Distillation,Y. Wang; W. Zhou; X. Qian,2024,,IEEE Transactions on Automation Science and Engineering,,IEEE,"School of Information and Electronic Engineering, Zhejiang University of Science and Technology, Hangzhou, China; School of Information and Electronic Engineering, Zhejiang University of Science and Technology, Hangzhou, China; School of Information and Electronic Engineering, Zhejiang University of Science and Technology, Hangzhou, China",0,0,,,Performance evaluation;Computational modeling;Power transmission lines;Feature extraction;Decoding;Semantics;Mobile handsets;Knowledge engineering;Training;Drones,,https://github.com/WangYuSenn/AFRNet,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10778059,,,10.1109/TASE.2024.3506617,False,"The periodic inspection of transmission lines ensures a stable power supply across various regions. Deep learning methods, particularly those based on multimodal approaches, have made significant advancements in this field. Current multimodal fusion algorithms do not prioritize the integration of supplementary information from other modal sources. Moreover, these methods frequently depend on substantial parameterization to achieve optimal performance, severely hampering deployment on mobile devices. To enhance the supportive role of the auxiliary modality and minimize model parameters, we proposed the knowledge distillation-based auxiliary feature registration network (AFRNet-S $^\ast)$  for RGB-T transmission line detection (TLD). The method incorporates an auxiliary feature registration module during fusion, utilizing unique complementarity between primary and auxiliary feature modalities for precise feature registration. For knowledge distillation, we devised response-mimicking distillation, semantic supplementary distillation, and cross-view feature polymerization distillation tailored for the student network (AFRNet-S, without knowledge distillation) to achieve model compression. AFRNet-S can learn comprehensive final spatial, precise semantic, and complementary feature integration information from AFRNet-T through these distillation methods. Extensive experiments conducted on an industrial TLD dataset demonstrate that the proposed AFRNet-S $^\ast$  (AFRNet-S with knowledge distillation) outperforms the existing state-of-the-art methods, thereby demonstrating its generality and effectiveness. Our AFRNet-S $^\ast$ , compared to AFRNet-T, has a reduced parameter count from 44.92M to 16.86M and a decrease in FLOPs from 21.13G to 6.41G. This further optimizes the deployment of our network on edge devices, such as unmanned aerial vehicles (drones), in the future. The code and results of our approach are available at https://github.com/WangYuSenn/AFRNet. Note to Practitioners—This study introduces the AFRNet for TLD, utilizing knowledge distillation (KD) techniques. We achieved a better balance between model performance and parameter count by integrating the KD task into the TLD task, facilitating easier deployment on mobile devices. The proposed AFRNet-S $^\ast$  effectively balances model performance and parameter count, ensuring efficient performance of the TLD task. Addressing RGB-T TLD-specific challenges remains a crucial area for future research. For instance, in scenarios where a target occupies a small area within an image, removing excessive background noise is essential to enhance the efficiency of target localization. Furthermore, we aim to utilize AFRNet-S $^\ast$  on drones for transmission line inspection in the future to address practical application challenges.",,1
76.0,,LeTO: Learning Constrained Visuomotor Policy With Differentiable Trajectory Optimization,Z. Xu; Y. She,2024,,IEEE Transactions on Automation Science and Engineering,,IEEE,"Edwardson School of Industrial Engineering, Purdue University, West Lafayette, IN, USA; Edwardson School of Industrial Engineering, Purdue University, West Lafayette, IN, USA",0,0,,,Robots;Imitation learning;Trajectory optimization;Safety;Training;Recurrent neural networks;Uncertainty;Supervised learning;Robot learning;Real-time systems,,https://github.com/ZhengtongXu/LeTO,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10740461,,,10.1109/TASE.2024.3486542,False,"This paper introduces LeTO, a method for learning constrained visuomotor policy with differentiable trajectory optimization. Our approach integrates a differentiable optimization layer into the neural network. By formulating the optimization layer as a trajectory optimization problem, we enable the model to end-to-end generate actions in a safe and constraint-controlled fashion without extra modules. Our method allows for the introduction of constraint information during the training process, thereby balancing the training objectives of satisfying constraints, smoothing the trajectories, and minimizing errors with demonstrations. This “gray box” method marries optimization-based safety and interpretability with powerful representational abilities of neural networks. We quantitatively evaluate LeTO in simulation and in the real robot. The results demonstrate that LeTO performs well in both simulated and real-world tasks. In addition, it is capable of generating trajectories that are less uncertain, higher quality, and smoother compared to existing imitation learning methods. Therefore, it is shown that LeTO provides a practical example of how to achieve the integration of neural networks with trajectory optimization. We release our code at https://github.com/ZhengtongXu/LeTO. Note to Practitioners—LeTO is driven by the goal of developing an imitation learning algorithm capable of generating safe and constraint-satisfying robotic behaviors. The idea of imitation learning is to enable the robot to learn from human demonstrations of certain tasks. Subsequently, the robot is able to autonomously perform the learned tasks on its own. Thanks to the powerful representational and fitting capabilities of neural networks, imitation learning can let robots perform complex manipulation tasks. However, neural networks often exhibit a certain level of uncertainty and lack theoretical safety guarantees. For robotic systems, it is crucial that robot behaviors meet specific constraints; otherwise, the system may not be sufficiently reliable. Therefore, we introduce LeTO, an approach that integrates trajectory optimization with neural networks to generate actions that not only achieve manipulation tasks, but also comply with constraints. This improves the interpretability, safety, and reliability of robot policies acquired through imitation learning, facilitating their deployment in scenarios with high safety requirements.",,1
77.0,,Dynamic Subclass-Balancing Contrastive Learning for Long-Tail Pedestrian Trajectory Prediction With Progressive Refinement,B. Yang; K. Yan; C. Hu; H. Hu; Z. Yu; R. Ni,2024,,IEEE Transactions on Automation Science and Engineering,,IEEE,"School of Microelectronics and Control Engineering, Changzhou University, Changzhou, China; School of Microelectronics and Control Engineering, Changzhou University, Changzhou, China; School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; State Key Laboratory of Automotive Simulation and Control, Changchun, China; School of Microelectronics and Control Engineering, Changzhou University, Changzhou, China; School of Microelectronics and Control Engineering, Changzhou University, Changzhou, China",0,0,,,Trajectory;Pedestrians;Tail;Contrastive learning;Predictive models;Accuracy;Dynamics;Adaptation models;Decoding;Training,,https://github.com/YanCCZU/DSBCL-PRM,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10740479,,,10.1109/TASE.2024.3487255,False,"Pedestrian trajectory prediction is critical for understanding human behavior. The prevailing approaches employ neural networks to predict trajectories from large amounts of trajectory data. However, pedestrian trajectory data exhibits a long-tail distribution, which presents challenges in accurately predicting the future trajectories of tail samples. Previous research utilized contrastive learning and loss reweighting to tackle the long-tail distribution challenge in trajectory prediction. Although this approach enhanced the tail samples’ performance, it reduced the head samples’ performance. In order to address this limitation, we propose a trajectory prediction framework based on dynamic subclass-balancing contrastive learning in this work. Firstly, we obtain general motion patterns by clustering future trajectory data. We use the adaptive motion pattern refinement block to refine the general motion patterns, providing accurate guidance for the model and thus facilitating the recognition of tail motion patterns. Subsequently, we propose dynamic subclass-balancing contrastive learning to address the long-tail distribution issue of trajectory data on the encoder, which includes subclass-balancing clustering and dynamic dual-level contrastive learning. Subclass-balancing clustering is employed on the head trajectory data to achieve subclass balance across the dataset. Afterward, we perform dynamic dual-level contrastive learning for motion features to achieve instance balance and optimize the feature space. Finally, we use enhanced motion features to adjust the predicted trajectories through the trajectory proposal refinement block, achieving progressive refinement. This addresses the long-tail distribution issue of trajectory data on the decoder and improves the model’s generalization capability. Experimental results demonstrate that our method outperforms state-of-the-art long-tail trajectory prediction methods in addressing the long-tail distribution issue, improving the performance on both head and tail samples. The code will be released at https://github.com/YanCCZU/DSBCL-PRM. Note to Practitioners—This work aims to tackle the long-tail distribution issue of pedestrian trajectory prediction while improving the model’s generalization capability. Existing methods mitigate the impact of the long-tail distribution issue on the encoder using contrastive learning. However, their overemphasis on the tail samples through loss reweighting has reduced the head samples’ performance. This work proposes a dynamic subclass-balancing contrastive learning module, which classifies head samples into several subclasses, each with a similar sample number in the tail classes. It performs dynamic dual-level contrastive learning based on class and subclass labels to achieve subclass and instance balance, improving the performance in head and tail samples. We utilize general motion patterns from the training set to guide the prediction of future trajectories. Moreover, we propose a progressive refinement strategy consisting of two refinement blocks to mitigate the impact of the long-tail distribution issue on the decoder and improve the model’s generalization performance. First, we adaptively refine motion patterns based on the difference between observed trajectories and historical motion patterns to provide accurate guidance. Then, we adjust the original predicted trajectories using the enhanced motion features, mitigating the impact of the long-tail distribution issue on the decoder while improving the model’s generalization and adaptability in unknown scenes. Our method’s simplified and effective model design ensures excellent real-time performance. Consequently, it is well-suited for deployment of edge devices in areas such as autonomous driving, intelligent surveillance, and social robotics. The proposed method enables accurate prediction of infrequent future trajectories in various scenarios, thus supporting safer decision-making.",,1
78.0,,Latent Code Description for Unsupervised AHU Fault Detection Using Adaptive Adversarial Autoencoder,V. Tra; M. Amayri; N. Bouguila,2024,,IEEE Transactions on Automation Science and Engineering,,IEEE,"Concordia Institute for Information Systems Engineering (CIISE), Concordia University, Montreal, QC, Canada; Concordia Institute for Information Systems Engineering (CIISE), Concordia University, Montreal, QC, Canada; Concordia Institute for Information Systems Engineering (CIISE), Concordia University, Montreal, QC, Canada",0,0,,,HVAC;Training;Fault detection;Anomaly detection;Codes;Vectors;Data models;Buildings;Adaptation models;Maintenance,,https://github.com/viettra-xai/Ada-AAE,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10734249,,,10.1109/TASE.2024.3481211,False,"Timely fault detection in HVAC systems is crucial for preventing energy waste and maintaining thermal comfort in commercial buildings. This paper introduces the adaptive adversarial autoencoder (AdaAAE), an innovative anomaly detection approach designed for unsupervised fault detection within air handling units (AHUs) of HVAC systems. By combining the adversarial autoencoder (AAE) with deep support vector data description (DSVDD), AdaAAE efficiently trains the reconstruction, regularization, and compactness phases to produce a spherical and compact latent representation of the training data, enabling effective fault detection within the latent space. A key feature of AdaAAE is its ability to accurately determine the anomaly threshold at the end of the training phase, which is particularly beneficial in scenarios where there is no prior knowledge of the anomaly ratio for test instances—a common challenge in real-world applications. AdaAAE’s performance was evaluated on a real-world AHU system developed as part of the ASHRAE research project 1312 (RP-1312). Experimental results, especially with regard to AUC-ROC and AUC-PR, highlight AdaAAE’s superior fault detection capabilities across AHU datasets with varying complexities. Notably, AdaAAE achieved outstanding performance, with an average AUC-ROC of 97.5% and AUC-PR of 94.4%, significantly outperforming the baseline methods in this study. Additionally, the experimental findings demonstrate AdaAAE’s proficiency in accurately estimating the anomaly threshold without prior knowledge, with minimal performance degradation when shifting from a scenario with a known anomaly ratio to one requiring estimation. Note to Practitioners—The concept of anomaly detection through subspace learning has attracted significant interest, with a focus on achieving a compact latent distribution. In this study, we introduce a new anomaly detection technique called AdaAAE, which is designed to produce a spherical and compact latent representation of training data. This feature enhances the method’s ability to identify anomalies within the latent space. Moreover, AdaAAE offers a distinct advantage in accurately determining the anomaly threshold at the end of the training phase, which is especially useful in practical scenarios where prior information about anomaly ratios for test instances is unavailable. To ensure reproducibility and allow for future enhancements by other researchers, the entire source code for this study is available in the following repository: https://github.com/viettra-xai/Ada-AAE.",,1
79.0,,LogiCode: An LLM-Driven Framework for Logical Anomaly Detection,Y. Zhang; Y. Cao; X. Xu; W. Shen,2024,,IEEE Transactions on Automation Science and Engineering,,IEEE,"State Key Laboratory of Intelligent Manufacturing Equipment and Technology, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Intelligent Manufacturing Equipment and Technology, Huazhong University of Science and Technology, Wuhan, China; Michigan Robotics, University of Michigan, Ann Arbor, MI, USA; State Key Laboratory of Intelligent Manufacturing Equipment and Technology, Huazhong University of Science and Technology, Wuhan, China",0,0,,,Anomaly detection;Codes;Cognition;Visualization;Semantics;Logic;Image reconstruction;Automation;Accuracy;Benchmark testing,,https://github.com/22strongestme/LOCO-Annotations,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10710633,,,10.1109/TASE.2024.3468464,False,"This paper presents LogiCode, a novel framework that leverages Large Language Models (LLMs) for identifying logical anomalies in industrial settings, moving beyond the traditional focus on structural inconsistencies. By harnessing LLMs for logical reasoning, LogiCode autonomously generates Python codes to pinpoint anomalies such as incorrect component quantities or missing elements, marking a significant leap forward in anomaly detection technologies. A custom dataset “LOCO-Annotations” and a benchmark “LogiBench” are introduced to evaluate the LogiCode’s performance across various metrics including binary classification accuracy, code generation success rate, and precision in reasoning. Findings demonstrate LogiCode’s enhanced interpretability, significantly improving the accuracy of logical anomaly detection and offering detailed explanations for identified anomalies. This represents a notable shift towards more intelligent, LLM-driven approaches in industrial anomaly detection, promising substantial impacts on industry-specific applications. Our code are available at https://github.com/22strongestme/LOCO-Annotations. Note to Practitioners—This work introduces LogiCode, an innovative system leveraging Large Language Models (LLMs) for logical anomaly detection in industrial settings, shifting the paradigm from traditional visual inspection methods. LogiCode autonomously generates Python codes for logical anomaly detection, enhancing interpretability and accuracy. Our novel approach, validated through the “LOCO-Annotations” dataset and LogiBench benchmark, demonstrates superior performance in identifying logical anomalies, a challenge often encountered in complex industrial components like assembly and packaging. LogiCode provides a significant advancement in addressing the nuanced requirements of detecting logical anomalies, offering a robust and interpretable solution to practitioners seeking to enhance quality control and reduce manual inspection efforts.",,1
80.0,,SDD-DETR: Surface Defect Detection for No-Service Aero-Engine Blades With Detection Transformer,X. Sun; K. Song; X. Wen; Y. Wang; Y. Yan,2024,,IEEE Transactions on Automation Science and Engineering,,IEEE,"Ministry of Education, School of Mechanical Engineering and Automation, the National Frontiers Science Center for Industrial Intelligence and Systems Optimization, and the Key Laboratory of Data Analytics and Optimization for Smart Industry, Northeastern University, Shenyang, Liaoning, China; Ministry of Education, School of Mechanical Engineering and Automation, the National Frontiers Science Center for Industrial Intelligence and Systems Optimization, and the Key Laboratory of Data Analytics and Optimization for Smart Industry, Northeastern University, Shenyang, Liaoning, China; School of Software, Shenyang University of Technology, Shenyang, Liaoning, China; Ministry of Education, School of Mechanical Engineering and Automation, the National Frontiers Science Center for Industrial Intelligence and Systems Optimization, and the Key Laboratory of Data Analytics and Optimization for Smart Industry, Northeastern University, Shenyang, Liaoning, China; Ministry of Education, School of Mechanical Engineering and Automation, the National Frontiers Science Center for Industrial Intelligence and Systems Optimization, and the Key Laboratory of Data Analytics and Optimization for Smart Industry, Northeastern University, Shenyang, Liaoning, China",0,0,,,Blades;Aircraft propulsion;Accuracy;Transformers;Inspection;Detectors;YOLO,,https://github.com/VDT-2048/SDD-DETR,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10681441,,,10.1109/TASE.2024.3457829,False,"Vision-based surface defect detection (SDD) for no-service aero-engine blades provides a fast and effective way to monitor product quality. Most existing detection algorithms for aero-engine blades are 1) based on CNN, including artificially designed non-maximum suppression (NMS) operations, and 2) focus on improving the detection accuracy rather than improving the inference speed and even ignoring the latter. To solve the above problems, we introduce a novel object detection paradigm, DEtection TRansformer (DETR), to design a novel network (SDD-DETR) with high accuracy for the SDD of aero-engine blades. To our knowledge, the paper is the first to introduce the DETR detector to SDD of aero-engine blades. While providing high accuracy, the inference speed of DETR remained slow due to self-attention operation and feed-forward network (FFN). Therefore, two lightweight modules have been designed for SDD of aero-engine blades: a progressive feature input multi-scale deformable attention module (PFI-MSDA) and a lightweight FFN (LW-FFN). PFI-MSDA hierarchically reduces the number of tokens input to the self-attention module, thereby reducing the time complexity of the self-attention layer. LW-FFN shrinks the complexity of multilayer perceptron. In addition, no parameter sharing of the detection head is utilized to compensate for the accuracy drop caused by the lightweight. Experiments verify that our method has the same AP and F1-score as DINO (a DETR-based detector), but our approach is lighter. Compared with DINO, the FLOPs are reduced by 113.4 ${G}$ , the inference speed is increased by 42.4%, and the runtime memory usage is reduced by 5.9 ${G}$ , which allows our method to be trained on low-end GPUs with more batch size, further improving the training efficiency. The code is available at https://github.com/VDT-2048/SDD-DETR. Note to Practitioners—The motivation for this paper is to design a high-precision and high-inference speed visual detection method for the SDD of aero-engine blades. Most high-precision vision methods are based on the transformer framework. However, its high complexity and poor compatibility in deployment environments lead to slower detection speeds. Although the application object in this paper is aero-engine blades, it is also applicable in other fields of industry, such as rail detection, plate and strip steel detection, etc. However, the method proposed cannot be supported by deployment environments such as RKNN because of the deformable attention operator, so it takes a certain amount of time to be deployed and put into practical use. Currently, the frameworks of visual and language large models are based on transformers, consistent with the framework of our method, which makes extending our approach to large visual and multi-modal models more accessible.",,1
81.0,,Semi-Supervised Domain Adaptation With Dual-Adversarial Learning for Lane Detection,X. Yao; Y. Wang; L. Dai; S. Zhang; M. Dou; Y. Deng,2024,,IEEE Transactions on Automation Science and Engineering,,IEEE,"School of Computer and Information Engineering and the Anhui Engineering Research Center for Intelligent Computing and Information Innovation, Fuyang Normal University, Fuyang, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing (LIESMARS), Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing (LIESMARS), Wuhan University, Wuhan, China; Department of Civil and Environmental Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing (LIESMARS), Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing (LIESMARS), Wuhan University, Wuhan, China",0,0,,,Lane detection;Feature extraction;Entropy;Autonomous vehicles;Annotations;Adversarial machine learning;Semantics,,https://github.com/shenhuqiji/LD-SSDA,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10680388,,,10.1109/TASE.2024.3451507,False,"Lane detection approaches relying on abundant annotations have achieved great progress in autonomous driving. These approaches may suffer from performance collapse when directly applied to unseen images due to domain discrepancy. Most researches utilize unsupervised domain adaptation (UDA) to mitigate domain shifts. Despite this, there is still a large performance gap between UDA and supervised methods because lack of target supervision makes improving performance of cross-domain lane detection difficult. In this paper, we introduce semi-supervised domain adaptation (SSDA) to lane detection and develop a simple yet effective dual-adversarial learning scheme to minimize domain discrepancy. Specifically, we conduct global alignment by calculating entropy information of unlabeled target and source data to align entropy distribution and enhance feature confidence of lanes. Domain consistency adversarial learning is proposed to make full use of both source and unlabeled/labeled target data. A triplet of features is used to learn lane-invariant representations by distinguishing whether the feature belongs to same domain. This is conducive to promote consistency of lanes information in cross domain. In two datasets, our method obtains a better performance of 42.5% (54.9%) and 45.8% (64.1%) based on a small amount of target supervision. Experimental results demonstrate effectiveness of our method as well as its superiority to other methods in terms of reducing performance gap with supervised methods (https://github.com/shenhuqiji/LD-SSDA) Note to Practitioners—This paper focuses on the cross domain lane detection task to narrow domain discrepancy in lane detection among different scenarios for autonomous driving. It aims to reduce extra label annotations and retraining costs. Considering unsupervised domain adaptation method is hard to further improve detection performance when a large domain shift occurs in adaptation scenarios, we construct an effective dual-adversarial learning scheme based on semi-supervised domain adaptation for lane detection. This method takes as input a triplet of source data, unlabeled and labeled target data to effectively enhance cross domain detection accuracy, which is contributed to the perception tasks in driving scenarios, such as path and motion planning, lane departure warning, lane changing decisions. Our method has a better generalization ability in face of various scenarios and can be easily integrated in software platforms, which greatly facilitates the applications on intelligent systems in autonomous vehicles.",,1
82.0,,Probabilistic Data-Driven Modeling of a Melt Pool in Laser Powder Bed Fusion Additive Manufacturing,Q. Fang; G. Xiong; M. Zhao; T. S. Tamir; Z. Shen; C. -B. Yan; F. -Y. Wang,2024,,IEEE Transactions on Automation Science and Engineering,,IEEE,"Institute of Automation, State Key Laboratory of Multimodal Artificial Intelligence Systems, Chinese Academy of Sciences, Beijing, China; Beijing Engineering Research Center of Intelligent Systems and Technology, Institute of Automation, State Key Laboratory of Multimodal Artificial Intelligence Systems, Chinese Academy of Sciences, Beijing, China; Institute of Automation, State Key Laboratory of Multimodal Artificial Intelligence Systems, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Precision Electronic Manufacturing Technology and Equipment, Guangdong University of Technology, Guangzhou, China; Beijing Engineering Research Center of Intelligent Systems and Technology, Institute of Automation, State Key Laboratory of Multimodal Artificial Intelligence Systems, Chinese Academy of Sciences, Beijing, China; Faculty of Electronic and Information Engineering, State Key Laboratory for Manufacturing Systems Engineering and the School of Automation Science and Engineering, Xi’an Jiaotong University, Xi’an, Shaanxi, China; Institute of Automation, State Key Laboratory for Management and Control of Complex Systems, Chinese Academy of Sciences, Beijing, China",0,0,,,Numerical models;Data models;Uncertainty;Powders;Predictive models;Systematics;Reliability,,https://github.com/qihangGH/probabilistic_melt_pool_model,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10632066,,,10.1109/TASE.2024.3412431,False,"The widespread adoption of laser powder bed fusion (LPBF) additive manufacturing is hampered by process unreliability problems. Modeling the melt pool behavior in LPBF is crucial to develop process control methods. While data-driven models linking melt pool dynamics to specific process parameters have shown appreciable advancements, existing models often oversimplify these relationships as deterministic, failing to account for the inherent instability of LPBF processes. Such simplifications can lead to overconfident and unreliable predictions, potentially resulting in erroneous process decisions. To address this critical issue, we propose a probabilistic data-driven approach to melt pool modeling that incorporates process noise and uncertainty. Our framework formulates a problem that includes distribution approximation and uncertainty quantification. Specifically, the Gaussian distribution with higher order priors, aided with variational inference and importance sampling, is used to approximate the probability distribution of melt pool characteristics. The uncertainty inherent in both LPBF process data and the modeling approach itself are then decomposed and approximated by using Monte Carlo sampling. The melt pool model is improved further by using a novel grid-based representation for the neighborhood of a fusion point, and a neural network architecture designed for effective feature fusion. This approach not only refines the accuracy of the model but also quantifies the uncertainty of the predictions, thereby enabling more informed decision-making with reduced risk. Two potential applications, including LPBF process planning and anomaly detection, are discussed. The implementation of our model is available at https://github.com/qihangGH/probabilistic_melt_pool_model. Note to Practitioners—Modeling the melt pool behavior in laser powder bed fusion (LPBF) processes is pivotal for enhancing its quality control. However, a problem is that most existing data-driven melt pool models learn melt pool behavior with a deterministic function, which predicts the same outputs if its inputs are the same. This deviates from the reality and neglects the uncertainty in LPBF processes. As a consequence, the quality control methods based on such melt pool models lack required reliability. In response to these challenges, this work proposes to model melt pool behavior by using probability distributions with deep learning techniques, which can quantify the uncertainty in both LPBF process data and data-driven models. Aided with an elegantly designed representation for the neighborhood of a fusion point as model input, and a neural network architecture that fuses multi-modal data, the proposed model achieves accurate melt pool size prediction results. More importantly, this work quantifies and decomposes the prediction uncertainty. By accounting for noise and parameter variations, the probabilistic modeling models developed herein offer a more robust foundation for LPBF quality control than the existing ones. They can be readily applied by practitioners to perform improved process planning, defect prognosis, and real-time anomaly detection tasks.",,1
83.0,,Decision Making for Multi-Robot Fixture Planning Using Multi-Agent Reinforcement Learning,E. Canzini; M. Auledas-Noguera; S. Pope; A. Tiwari,2024,,IEEE Transactions on Automation Science and Engineering,,IEEE,"Department of Automatic Control and Systems Engineering, The University of Sheffield, Sheffield, U.K.; Department of Automatic Control and Systems Engineering, The University of Sheffield, Sheffield, U.K.; Department of Automatic Control and Systems Engineering, The University of Sheffield, Sheffield, U.K.; Department of Automatic Control and Systems Engineering, The University of Sheffield, Sheffield, U.K.",0,0,,,Fixtures;Planning;Task analysis;Manufacturing;Reinforcement learning;Robots;Layout,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10597641,,,10.1109/TASE.2024.3424677,False,"Within the realm of flexible manufacturing, fixture layout planning allows manufacturers to rapidly deploy optimal fixturing plans that can reduce surface deformation that leads to crack propagation in components during manufacturing tasks. The role of fixture layout planning has evolved from being performed by experienced engineers to computational methods due to the number of possible configurations for components. Current optimisation methods commonly fall into sub-optimal positions due to the existence of local optima, with data-driven machine learning techniques relying on costly to collect labelled training data. In this paper, we present a framework for multi-agent reinforcement learning with team decision theory to find optimal fixturing plans for manufacturing tasks. We demonstrate our approach on two representative aerospace components with complex geometries across a set of drilling tasks, illustrating the capabilities of our method; we will compare this against state of the art methods to showcase our method’s improvement at finding optimal fixturing plans with 3 times the improvement in deformation control within tolerance bounds. Note to Practitioners—Fixture layout planning is one of the most fundamental manufacturing tasks that must be carried out before production cycles can begin, to ensure that deformation is within tolerances to avoid crack propagation and component damage. However, reliance on humans to generate these plans has led to sub-optimal solutions, leading to manufacturers incurring losses and wanting to explore analytical methods for fixture planning. In this vein, there may be the temptation to find a single solution that can be applied to all problems regardless of complexity. However, in the age of flexible manufacturing, the benefits of building tailored solutions within a framework -referred to as “freedom within a framework” -become more apparent. This paper outlines the framework for multi-agent reinforcement learning for fixture layout planning, and demonstrates the capabilities of this framework to outperform current state of the art methods with a simple algorithm through extensive experiments on representative aerospace components. We provide code implementations of our work on GitHub (Multi-Agent Fixture Planner on GitHub).",,1
84.0,,DGNR: Density-Guided Neural Point Rendering of Large Driving Scenes,Z. Li; C. Wu; L. Zhang; J. Zhu,2024,,IEEE Transactions on Automation Science and Engineering,,IEEE,"College of Software, Zhejiang University, Hangzhou, China; Robotics and Autonomous Driving Laboratory (RAL), Baidu Research, Beijing, China; Robotics and Autonomous Driving Laboratory (RAL), Baidu Research, Beijing, China; College of Computer Science, Zhejiang University, Hangzhou, China",0,0,,,Rendering (computer graphics);Real-time systems;Three-dimensional displays;Trajectory;Laser radar;Image color analysis;Geometry,,https://github.com/JOP-Lee/DGNR-Rendering,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10570249,,,10.1109/TASE.2024.3410891,False,"Despite the recent success of Neural Radiance Field (NeRF), it is still challenging to render large-scale driving scenes with long trajectories, particularly when the rendering quality and efficiency are in high demand. Existing methods for such scenes usually involve with spatial warping, geometric supervision from zero-shot normal or depth estimation, or scene division strategies, where the synthesized views are often blurry or fail to meet the requirement of efficient rendering. To address the above challenges, this paper presents a novel framework that learns a density space from the scenes to guide the construction of a point-based renderer, dubbed as DGNR (Density-Guided Neural Rendering). In DGNR, geometric priors are no longer needed, which can be intrinsically learned from the density space through volumetric rendering. Specifically, we make use of a differentiable renderer to synthesize images from the neural density features obtained from the learned density space. A density-based fusion module and geometric regularization are proposed to optimize the density space. By conducting experiments on a widely used autonomous driving dataset, we have validated the effectiveness of DGNR in synthesizing photorealistic driving scenes and achieving real-time capable rendering. Our project page is available at https://github.com/JOP-Lee/DGNR-Rendering. Note to Practitioners—While Neural Radiance Field (NeRF) has been gaining attraction, it is still challenging to create highly detailed, efficient renderings of large driving scenes. Current methods often resort to spatial warping, geometric guidance from tools like zero-shot normal or depth estimates, or dividing the scene into smaller parts. Unfortunately, these techniques can result in blurred images or fail to meet efficiency needs. To solve these challenges, we introduce a learned density space to build a point-based renderer, termed Density-Guided Neural Rendering (DGNR). With DGNR, we no longer need geometric priors because the density space can inherently learn them through volume rendering. Specifically, we use a flexible renderer to create images from the neural density features derived from the learned density space. We have also proposed a density-based fusion module and geometric regularization to optimize the density space. We evaluated DGNR on a popular autonomous driving dataset and found it to be effective in creating realistic driving scenes and capable of real-time rendering. Project page: https://github.com/JOP-Lee/DGNR-Rendering.",,1
85.0,,MVoxTi-DNeRF: Explicit Multi-Scale Voxel Interpolation and Temporal Encoding Network for Efficient Dynamic Neural Radiance Field,W. Yan; Y. Chen; W. Zhou; R. Cong,2024,,IEEE Transactions on Automation Science and Engineering,,IEEE,"School of Computer and Control Engineering, Yantai University, Yantai, China; School of Computer and Control Engineering, Yantai University, Yantai, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore; School of Control Science and Engineering, Shandong University, Jinan, China",0,0,,,Rendering (computer graphics);Three-dimensional displays;Deformation;Estimation;Training;Interpolation;Image color analysis,,https://github.com/CHenYYff/MVoxTi-DNeRF,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10570380,,,10.1109/TASE.2024.3416533,False,"Neural radiance fields have revolutionized the field of novel view synthesis, achieving remarkable results. However, traditional approaches based on implicit representations, particularly those built upon NeRF, suffer from slow rendering speeds due to the need for numerous MLP evaluations. Recently, there has been a promising shift towards explicit representations using voxel grids, which has significantly improved reconstruction times for static scenes. Nonetheless, extending these methods from static scenes to dynamic scenes is a non-trivial task as it requires accounting for the changing geometry and appearance of the scene over time. In this paper, we propose an efficient dynamic neural radiance field with multi-scale explicit voxel interpolation and temporal encoding. We leverage an explicit voxel structure to store the 3D dynamic features, while employing a lightweight MLP to estimate the displacement, thereby significantly enhancing the reconstruction speed. In our canonical module, we incorporated temporal information encoding in density estimation and color estimation to rectify the error estimation of the displacement in deformation module. In addition, a multi-scale voxel interpolation is designed to accommodate large-scale motions while meticulously capturing intricate details in small-scale motions in the density estimation module. In experiment, we evaluate our MVoxTi-DNeRF method on both synthetic and real scenes, where it achieves superior or comparable rendering quality compared to state of the art methods, while remaining computationally efficient (more than 60 $\times$  faster than the original DNeRF). More experiment results and test code are available at https://github.com/CHenYYff/MVoxTi-DNeRF Note to Practitioners—Neural Radiance Fields(NeRF) can create highly detailed and realistic 3D models from 2D images via a neural network. It is particularly popular for its capacity to generate novel views of a scene, enabling it to synthesize images from previously unobserved viewpoints. NeRF has found applications in a wide range of fields, including virtual reality, augmented reality, gaming, and so on. In this study, we introduces an efficient dynamic NeRF method. First, our network leverages an optimized explicit voxel grid to store 3D dynamic features and employs a lightweight MLP to decode these deformation features, significantly accelerating the training process. Second, to correct the error estimation related to deformation displacement, we introduce encoding of temporal information into density and color estimation in our canonical module, which fortifies the canonical field’s perception of temporal information. Third, we utilize multi-scale voxel interpolation to capture different-scale motion in the density estimation module, where minor motions are modeled using nearby voxels, while motion within a broader range is captured through more distant voxels. The consideration of multi-scale voxel features diminishes the detrimental impact caused by inaccurate displacement estimation.",,1
86.0,,Deep Reinforcement Learning With Multiple Unrelated Rewards for AGV Mapless Navigation,B. Cai; C. Wei; Z. Ji,2024,,IEEE Transactions on Automation Science and Engineering,,IEEE,"College of Mechanical and Electrical Engineering, Hohai University, Changzhou, China; College of Mechanical and Electrical Engineering, Hohai University, Changzhou, China; School of Engineering, Cardiff University, Cardiff, U.K.",0,0,,,Robots;Navigation;Collision avoidance;Task analysis;Servers;Heuristic algorithms;Vehicle dynamics,,https://github.com/dornenkrone/MFPG,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555525,,,10.1109/TASE.2024.3410162,False,"Mapless navigation for Automated Guided Vehicles (AGV) via Deep Reinforcement Learning (DRL) algorithms has attracted significantly rising attention in recent years. Collision avoidance from dynamic obstacles in unstructured environments, such as pedestrians and other vehicles, is one of the key challenges for mapless navigation. Autonomous navigation requires a policy to make decisions to optimize the path distance towards the goal but also to reduce the probability of collisions with obstacles. Mostly, the reward for AGV navigation is calculated by combining multiple reward functions for different purposes, such as encouraging the robot to move towards the goal or avoiding collisions, as a state-conditioned function. The combined reward, however, may lead to biased behaviours due to the empirically chosen weights when multiple rewards are combined and dangerous situations are misjudged. Therefore, this paper proposes a learning-based method with multiple unrelated rewards, which represent the evaluation of different behaviours respectively. The policy network, named Multi-Feature Policy Gradients (MFPG), is conducted by two separate Q networks that are constructed by two individual rewards, corresponding to goal distance shortening and collision avoidance, respectively. In addition, we also propose an auto-tuning method, named Ada-MFPG, that allows the MFPG algorithm to automatically adjust the weights for the two separate policy gradients. For collision avoidance, we present a new social norm-oriented continuous biased reward for performing specific social norm so as to reduce the probabilities of AGV collisions. By adding an offset gain to one of the reward functions, vehicles conducted by the proposed algorithm exhibited the predetermined features. The work was tested in different simulation environments under multiple scenarios with a single robot or multiple robots. The proposed MFPG method is compared with standard Deep Deterministic Policy Gradient (DDPG), the modified DDPG, SAC and TD3 with a social norm mechanism. MFPG significantly increases the success rate in robot navigation tasks compared with the DDPG. Besides, among all the benchmarking algorithms, the MFPG-based algorithms have the optimal task completion duration and lower variance compared with the baselines. The work has also been tested on real robots. Experiments on the real robots demonstrate the viability of the trained model for the real world scenarios. The learned model can be used for multi-robot mapless navigation in complex environments, such as a warehouse, that need multi-robot cooperation. Our source code and supplementary material is available at https://github.com/dornenkrone/MFPG Note to Practitioners—Autonomous navigation for AGVs in complex and large-scale environments, such as factories and warehouses, is challenging. AGVs are usually centrally controlled and depend on reliable communications. However, centralized control is not always reliable due to poor signal strengths or crashes of the server, and hence unsuitable due to the requirements of accurate information of the dynamic environments and fast responses of decision making. Therefore, it is necessary for the vehicles to perform reliable decision making based on only onboard sensors and processors, for efficient and safe autonomous navigation. Existing methods, such as simultaneous localization and mapping (SLAM) and motion planning algorithms, have been widely used. However, they are neither flexible nor generalizable enough. This paper proposes a method for autonomous navigation based on reinforcement learning (RL), which allows vehicles to gain experience through cumulative rewards by continuously interacting with the environment. The RL-based controller is designed for optimising its performance in two independent aspects, namely collision avoidance and navigation, which are quantified as separate rewards. Instead of carefully hand-crafting a combined reward, our proposed approach trains the agent using the two rewards separately to obtain one optimal policy. It is clearly easier and more practical to design the individual rewards than manually combining them. Besides, the algorithm includes a mechanism for incorporating social norms to encourage the vehicles to follow the right-hand rule, such that they can avoid pedestrians or other vehicles in a socially acceptable manner. This is achieved by adding a continuous bias on the collision avoidance reward. Experiments using simulation environments and real robots suggest that the method is generalizable to multi-robot systems, while guaranteeing safety. In future research, we will focus on incorporating uncertainties of sensor readings for safe and reliable autonomous navigation.",,1
87.0,,Effective Multi-Agent Deep Reinforcement Learning Control With Relative Entropy Regularization,C. Miao; Y. Cui; H. Li; X. Wu,2024,,IEEE Transactions on Automation Science and Engineering,,IEEE,"University of Chinese Academy of Sciences, Beijing, China; Shenzhen Institute of Advanced Technology, Guangdong-Hong Kong-Macao Joint Laboratory of Human-Machine Intelligence-Synergy Systems, Chinese Academy of Sciences, Shenzhen, China; Shenzhen Institute of Advanced Technology, Guangdong-Hong Kong-Macao Joint Laboratory of Human-Machine Intelligence-Synergy Systems, Chinese Academy of Sciences, Shenzhen, China; Shenzhen Institute of Advanced Technology, Guangdong-Hong Kong-Macao Joint Laboratory of Human-Machine Intelligence-Synergy Systems, Chinese Academy of Sciences, Shenzhen, China",0,0,,,Task analysis;Entropy;Games;Training;Chemicals;Optimization;Large-scale systems,,https://github.com/AdrienLin1/MACDPP,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10530635,,,10.1109/TASE.2024.3398712,False,"This paper focused on developing an effective Multi-Agent Reinforcement Learning (MARL) approach that quickly explores optimal control policies of multiple agents through interactions with unknown environments. Multi-Agent Continuous Dynamic Policy Gradient (MACDPP) was proposed to tackle the issues of limited capability and sample efficiency in the current MARL approaches. It alleviates the inconsistency of multiple agents’ policy updates by introducing the relative entropy regularization to the Centralized Training with Decentralized Execution (CTDE) framework with the Actor-Critic (AC) structure. Evaluated by multi-agent cooperation and competition tasks and traditional control tasks including OpenAI benchmarks and robot arm manipulation, MACDPP demonstrates its significant superiority in learning capability and sample efficiency compared with both related multi-agent and widely implemented signal-agent baselines. It converges to  $62\%$  higher average return and uses  $38\%$  fewer samples compared with the suboptimal baseline over all tasks, indicating the potential of MARL in challenging control scenarios, especially when the number of interactions is limited. The open source code of MACDPP is available at https://github.com/AdrienLin1/MACDPP. Note to Practitioners—Learning proper cooperation strategy over multiple agents in complicated systems has been a challenge in the domain of Reinforcement Learning. Our work extends the traditional MARL approach FKDPP that has been successfully implemented in the real-world chemical plant by Yokogawa to the CTDE framework and AC structure that supports continuous actions. This extension significantly expands its range of applications from cooperative/competitive tasks to the joint control of one complex system while maintaining its effectiveness.",,1
88.0,,GTSCalib: Generalized Target Segmentation for Target-Based Extrinsic Calibration of Non-Repetitive Scanning LiDAR and Camera,H. Huang; M. Zhang; L. Li; J. Hu; H. Wang,2024,,IEEE Transactions on Automation Science and Engineering,,IEEE,"School of Cyber Science and Engineering, Xi’an Jiaotong University, Xi’an, China; School of Cyber Science and Engineering, Xi’an Jiaotong University, Xi’an, China; School of Electronic Information Engineering, Xi’an Technological University, Xi’an, China; School of Cyber Science and Engineering, Xi’an Jiaotong University, Xi’an, China; Department of Automation, the Key Laboratory of System Control and Information Processing of Ministry of Education, and Shanghai Engineering Research Center of Intelligent Control and Management, Shanghai Jiao Tong University, Shanghai, China",0,0,,,Calibration;Three-dimensional displays;Laser radar;Point cloud compression;Image segmentation;Tuning;Feature extraction,,https://github.com/Natsu-Akatsuki/GTSCalib,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10529216,,,10.1109/TASE.2024.3398623,False,"Existing target segmentation methods are typically considered easy to implement and yield satisfactory results. However, they generally cannot adapt to a new environment without tiring parameter tuning, which leads to poor performance, including issues such as over-segmentation, under-segmentation, missing segmentation, and false positives. To avoid the wrong segmentation in a parameter-tuning-free and user-friendly fashion, we propose a generalized target segmentation (GTS) method based on the image-view representation of point clouds. Specifically, the method avoids devising a Euclidean space-based algorithm that is sensitive to surrounding objects and to the varied point cloud density and intensity in a new environment. The target segment produced by GTS can be used with any target-based extrinsic calibration architecture, based on which this paper further proposes a generalized target-based (in this case, chessboard) extrinsic calibration framework called GTSCalib for a non-repetitive scanning LiDAR and a camera. GTSCalib additionally introduces a novel intensity threshold method based on kernel density estimation (KDE) for 3D corner detection and the SQPnP solver for optimization to achieve more generalized and robust performance. Extensive simulations and experiments demonstrate that GTSCalib has high generalization ability, robustness, and accuracy. The code is released at https://github.com/Natsu-Akatsuki/GTSCalib. Note to Practitioners—Calibration is necessary for many non-repetitive scanning LiDAR-camera systems to enable sensor fusion in the fields of mapping, localization, and perception. Unfortunately, existing target-based (in this case, chessboard) calibration methods are weakly adaptable to the surrounding environment with variable density or intensity of point clouds, resulting in unstable performance, particularly for the target segmentation submodule. To solve this problem, we introduce a new target segmentation approach, GTS, and a more generalized and robust extrinsic calibration framework, GTSCalib. The proposed GTSCalib is very suitable for practitioners looking for a robust and accurate target-based calibration without limits on the target’s pose or its surrounding environment and without the need for time-consuming parameter tuning.",,1
89.0,,PyRoboCOP: Python-Based Robotic Control and Optimization Package for Manipulation and Collision Avoidance,A. U. Raghunathan; D. K. Jha; D. Romeres,2024,,IEEE Transactions on Automation Science and Engineering,,IEEE,"Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA, USA; Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA, USA; Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA, USA",0,0,,,Optimization;Collision avoidance;Robots;Finite element analysis;Trajectory optimization;Task analysis;Optimal control,,https://github.com/merlresearch/PyRoboCOP,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10440590,,,10.1109/TASE.2024.3365637,False,"Contacts are central to most manipulation tasks as they provide additional dexterity to robots to perform challenging tasks. However, frictional contacts leads to complex complementarity constraints. Planning in the presence of contacts requires robust handling of these constraints to find feasible solutions. This paper presents PyRoboCOP which is a lightweight Python-based package for control and optimization of robotic systems described by nonlinear Differential Algebraic Equations (DAEs). In particular, the proposed optimization package can handle systems with contacts that are described by complementarity constraints. We also present a general framework for specifying obstacle avoidance constraints using complementarity constraints. The package performs direct transcription of the DAEs into a set of nonlinear equations by performing orthogonal collocation on finite elements. The resulting optimization problem belongs to the class of Mathematical Programs with Complementarity Constraints (MPCCs). MPCCs fail to satisfy commonly assumed constraint qualifications and require special handling of the complementarity constraints in order for NonLinear Program (NLP) solvers to solve them effectively. PyRoboCOP provides automatic reformulation of the complementarity constraints that enables NLP solvers to perform optimization of robotic systems. The package is interfaced with for obtaining sparse derivatives by automatic differentiation and for performing optimization. We provide extensive numerical examples for various different robotic systems with collision avoidance as well as contact constraints represented using complementarity constraints. We provide comparisons with other open source optimization packages like and . The code is open sourced and available at https://github.com/merlresearch/PyRoboCOP. Note to Practitioners—PyRoboCOP is intended to be an easy-to-use software package written in Python which can be used for optimization, estimation and control for a large class of robotic systems. Including, in particular, contact-rich applications to deal with complex scenarios that arise when making and breaking contacts during a task. Typical problems that can be solved with our work are trajectory and control sequence optimization, parameter estimation. To make the proposed software package easier for practitioners, the paper provides access to the package and a large number of example problems. Furthermore, the package also provides a guide describing the details of all the methods a user might have to implement for their own system. Compared to some of the other packages, PyRoboCOP works with NumPy object arrays which is the native computing package in Python. We believe that this will make it much easier to learn and use compared to some of the other optimal control packages.",,1
90.0,,Fast Adaptation Trajectory Prediction Method Based on Online Multisource Transfer Learning,B. Yang; J. Zhu; Z. Yu; F. Fan; X. Liu; R. Ni,2024,,IEEE Transactions on Automation Science and Engineering,,IEEE,"School of Microelectronics and Control Engineering, Changzhou University, Changzhou, China; School of Computer Science and Artificial Intelligence, Changzhou University, Changzhou, China; School of Microelectronics and Control Engineering, Changzhou University, Changzhou, China; School of Computer Science and Artificial Intelligence, Changzhou University, Changzhou, China; College of IoT, Hohai University, Nanjing, China; School of Microelectronics and Control Engineering, Changzhou University, Changzhou, China",0,0,,,Trajectory;Predictive models;Pedestrians;Hidden Markov models;Adaptation models;Data models;Uncertainty,,https://github.com/zjrcczu/OMTL-PTP,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10433100,,,10.1109/TASE.2024.3362980,False,"Conventional deep learning-based trajectory prediction methods always adopt offline training based on trajectory data collected in known scenes. Despite its high prediction accuracy, it is unable to process trajectory data acquired in real-time, making it non-trivial to adapt to unknown scenes. To mitigate the above problem, an online multi-source transfer learning-based pedestrian trajectory predictor, dubbed OMTL-PTP, is proposed to achieve fast adaptation of trajectory prediction. OMTL-PTP resorts to online transfer learning to transfer trajectory knowledge from multiple source domains to the target domain, enabling the model to learn from the new scene and continuously improve its trajectory prediction ability. Concretely, we propose several base learners with external memory modules to preserve source domain trajectory knowledge for online knowledge transfer. A multi-hop attention mechanism is introduced in each learner to handle the future uncertainty of generated trajectories. To fully utilize the knowledge from multiple source domains, OMTL-PTP leverages ensemble learning to transfer knowledge from multiple base learners in the source domains to the online learner and fine-tunes the online learner in the target domain. Specifically, all base learners are combined to update the online learner, improving its ability to process future arriving samples and adapt to unknown scenes quickly. Qualitative and quantitative evaluations on ETH/UCY indicate the effectiveness of OMTL-PTP in online learning, which is beneficial for deploying trajectory prediction methods on intelligent edge devices. The code will be released at https://github.com/zjrcczu/OMTL-PTP after acceptance. Note to Practitioners—This paper is motivated by the challenge of online sustained trajectory prediction for unmanned autonomous agents, but it also applies to other trajectory prediction tasks, such as intelligent monitoring. Existing approaches always collect trajectory data from different scenes for training, making the model generalize to other scenarios. However, they may suffer from performance degradation since they cannot learn trajectory knowledge from unknown scenes. This paper suggests a new approach by transferring trajectory knowledge from known scenes to unknown scenes and gradually learning from unknown scenes, inspired by online transfer learning. In this paper, we propose a trajectory predictor based on a memory network and introduce the multi-hop attention mechanism to mitigate future uncertainty of trajectory prediction. We then show how the external memory can preserve trajectory knowledge, which facilitates transferring knowledge from source domains to the target domain. Afterward, we train an online trajectory predictor based on online multi-source transfer learning, improving the generalization and adaptability of trajectory prediction models in unknown scenes and facilitating deploying trajectory prediction models in edge devices. This method also applies to other neural network-based regression tasks that require online sustained learning. In future research, we will improve the trajectory prediction performance while maintaining the online learning ability.",,1
91.0,,When Is It Likely to Fail? Performance Monitor for Black-Box Trajectory Prediction Model,W. Shao; B. Li; W. Yu; J. Xu; H. Wang,2024,,IEEE Transactions on Automation Science and Engineering,,IEEE,"School of Vehicle and Mobility, Tsinghua University, Beijing, China; Department of Civil and Environmental Engineering, University of Michigan, Ann Arbor, MI, USA; School of Vehicle and Mobility, Tsinghua University, Beijing, China; School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China; School of Vehicle and Mobility, Tsinghua University, Beijing, China",0,0,,,Predictive models;Monitoring;Trajectory;Closed box;Autonomous vehicles;Reliability;Safety,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10415388,,,10.1109/TASE.2024.3356070,False,"Accurate trajectory prediction is vital for various applications, including autonomous vehicles. However, the complexity and limited transparency of many prediction algorithms often result in black-box models, making it challenging to understand their limitations and anticipate potential failures. This further raises potential risks for systems based on these prediction models. This study introduces the performance monitor for black-box trajectory prediction model (PMBP) to address this challenge. The PMBP estimates the performance of black-box trajectory prediction models online, enabling informed decision-making. The study explores various methods’ applicability to the PMBP, including anomaly detection, machine learning, deep learning, and ensemble, with specific monitors designed for each method to provide online output representing prediction performance. Comprehensive experiments validate the PMBP’s effectiveness, comparing different monitoring methods. Results show that the PMBP effectively achieves promising monitoring performance, particularly excelling in deep learning-based monitoring. It achieves improvement scores of 0.81 and 0.79 for average prediction error and final prediction error monitoring, respectively, outperforming previous white-box and gray-box methods. Furthermore, the PMBP’s applicability is validated on different datasets and prediction models, while ablation studies confirm the effectiveness of the proposed mechanism. Hybrid prediction and autonomous driving planning experiments further show the PMBP’s value from an application perspective. Project page: https://swb19.github.io/PMBP/. Note to Practitioners—This research presents PMBP, a valuable tool for practitioners in the automation industry. The PMBP enables online monitoring of black-box trajectory prediction models, enhancing system reliability and facilitating informed decision-making. The practical application of PMBP lies in improving safety and reliability in critical domains, especially in the context of autonomous vehicles. Black-box trajectory prediction models commonly used in these domains may exhibit unexpected deficiencies, potentially leading to risks. By monitoring the prediction performance online, systems can proactively identify potential insufficiencies and make informed decisions to ensure safer and more reliable operations. The PMBP offers practitioners different monitoring solutions based on various approaches, addressing their specific needs effectively. While the PMBP has shown promising outcomes, further exploration and testing are necessary to fully harness and apply its monitoring results in automated systems. Practitioners are encouraged to adopt the PMBP as an essential monitoring mechanism to enhance the reliability of their trajectory prediction models and achieve safer and more efficient automation in their domains.",,1
92.0,,Automatically Prepare Training Data for YOLO Using Robotic In-Hand Observation and Synthesis,H. Chen; W. Wan; M. Matsushita; T. Kotaka; K. Harada,2024,,IEEE Transactions on Automation Science and Engineering,,IEEE,"Department of System Innovation, Graduate School of Engineering Science, Osaka University, Osaka, Japan; Department of System Innovation, Graduate School of Engineering Science, Osaka University, Osaka, Japan; H.U. Group Research Institute G.K., Akiruno, Japan; H.U. Group Research Institute G.K., Akiruno, Japan; Department of System Innovation, Graduate School of Engineering Science, Osaka University, Osaka, Japan",0,0,,,Robots;Electron tubes;Training data;Robot sensing systems;Image segmentation;Visualization;Training,,https://github.com/wrslab/tubedet,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10239212,,,10.1109/TASE.2023.3304420,False,"Deep learning methods have recently exhibited impressive performance in object detection. However, such methods needed much training data to achieve high recognition accuracy, which was time-consuming and required considerable manual work like labeling images. In this paper, we automatically prepare training data using robots. Considering the low efficiency and high energy consumption in robot motion, we proposed combining robotic in-hand observation and data synthesis to enlarge the limited data set collected by the robot. We first used a robot with a depth sensor to collect images of objects held in the robot’s hands and segment the object pictures. Then, we used a copy-paste method to synthesize the segmented objects with rack backgrounds. The collected and synthetic images are combined to train a deep detection neural network. We conducted experiments to compare YOLOv5x detectors trained with images collected using the proposed method and several other methods. The results showed that combined observation and synthetic images led to comparable performance to manual data preparation. They provided a good guide on optimizing data configurations and parameter settings for training detectors. The proposed method required only a single process and was a low-cost way to produce the combined data. Interested readers may find the data sets and trained models from the following GitHub repository: github.com/wrslab/tubedet Note to Practitioners—The background of this study is a requirement in lab automation – Using robots to arrange randomly placed tubes automatically. Before sending test tubes to an examination machine for gradient tests, humans need to categorize and organize the tubes into specific patterns to fit the machine’s internal design. Employing humans is difficult as the tube arrangement requirements are time-varying. A preferred solution is using robots to replace humans. The robots should have a vision system to detect the tubes and a manipulation system to perform physical arranging actions. They will be used in busy seasons while deployed for other tasks in leisure time. Deep neural networks like YOLO are effective for the tube detection task. However, preparing the training data is challenging and unsuitable for lab end users. Pre-trained neural networks are options but have limited tube detection ability and cannot deal with newly included tube types. The method developed in this work helps solve the training data preparation problem. With its support, the robot can automatically prepare training data that has comparable quality to manually labeled ones in a single-process and low-cost way.",,1
93.0,,Layout Optimization for Photovoltaic Panels in Solar Power Plants via a MINLP Approach,N. Mignoni; R. Carli; M. Dotoli,2023,,IEEE Transactions on Automation Science and Engineering,,IEEE,"Department of Electrical and Information Engineering, Polytechnic of Bari, Bari, Italy; Department of Electrical and Information Engineering, Polytechnic of Bari, Bari, Italy; Department of Electrical and Information Engineering, Polytechnic of Bari, Bari, Italy",0,0,,,Optimization;Layout;Sun;Observers;Mathematical models;Azimuth;Photovoltaic systems,,https://github.com/nicomignoni/pvlayout,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10368589,,,10.1109/TASE.2023.3322786,False,"Photovoltaic (PV) technology is one of the most popular means of renewable generation, whose applications range from commercial and residential buildings to industrial facilities and grid infrastructures. The problem of determining a suitable layout for the PV arrays, on a given deployment region, is generally non-trivial and has a crucial importance in the planning phase of solar plants design and development. In this paper, we provide a mixed integer non-linear programming formulation of the PV arrays’ layout problem. First, we define the astronomical and geometrical models, considering crucial factors such as self-shadowing and irradiance variability, depending on the geographical position of the solar plant and yearly time window. Subsequently, we formalize the mathematical optimization problem, whose constraints’ set is characterized by non-convexities. In order to propose a computationally tractable approach, we provide a tight parametrized convex relaxation. The resulting optimization resolution procedure is tested numerically, using realistic data, and benchmarked against the traditional global resolution approach, showing that the proposed methodology yields near-optimal solutions in lower computational time. Note to Practitioners—The paper is motivated by the need for efficient algorithmic procedures which can yield near-optimal solutions to the PV arrays layout problem. Due to the strong non-convexity of even simple instances, the existing methods heavily rely on global or stochastic solvers, which are computationally demanding, both in terms of resources and run-time. Our approach acts as a baseline, from which practitioners can derive more elaborate instances, by suitably modifying both the objective function and/or the constraints. In fact, we focus on the minimum set of necessary geometrical (e.g., arrays position model), astronomical (e.g., irradiance variation), and operational (e.g., power requirements) constraints which make the overall problem hard. The Appendices provide a guideline for suitably choosing the optimization parameters. All data and simulation code are available on a public repository at: https://github.com/nicomignoni/pvlayout.git.",,1
94.0,,ArduCode: Predictive Framework for Automation Engineering,A. Canedo; P. Goyal; D. Huang; A. Pandey; G. Quiros,2021,,IEEE Transactions on Automation Science and Engineering,,IEEE,"Siemens Corporate Technology, Princeton, NJ, USA; Information Sciences Institute, University of Southern California (USC), Los Angeles, CA, USA; Information Sciences Institute, University of Southern California (USC), Los Angeles, CA, USA; Siemens Corporate Technology, Princeton, NJ, USA; Siemens Corporate Technology, Princeton, NJ, USA",0,0,,,Automation;Machine learning;Software engineering;Artificial intelligence,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9145653,,,10.1109/TASE.2020.3008055,False,"Automation engineering is the task of integrating, via software, various sensors, actuators, and controls to automate a real-world process. Today, automation engineering is supported by a suite of software tools, including integrated development environments (IDEs), hardware configurators, compilers, and runtimes. These tools focus on the automation code itself but leave the automation engineer unassisted in their decision-making. This can lead to longer software development cycles due to the imperfections in the decision-making, which arise when integrating software and hardware. To address this problem, this article addresses multiple challenges often faced in automation engineering and proposes machine learning-based solutions to assist engineers tackle these challenges. We show that machine learning can be leveraged to assist the automation engineer in classifying automation code, finding similar code snippets, and reasoning about the hardware selection of sensors and actuators. We validate our architecture on two real data sets consisting of 2927 Arduino projects and 683 programmable logic controller (PLC) projects. Our results show that paragraph embedding techniques can be utilized to classify automation using code snippets with precision close to human annotation, giving an  $F_{1}$ -score of 72%. Furthermore, we show that such embedding techniques can help us find similar code snippets with high accuracy. Finally, we use autoencoder models for hardware recommendation and achieve a  $p\text{@}3$  of 0.79 and  $p\text{@}5$  of 0.95. We also present the implementation of ArduCode in a proof-of-concept user interface integrated into an existing automation engineering system platform. Note to Practitioners—This article is motivated by the use of artificial intelligence methods to improve the efficiency and quality of the automation engineering software development process. Our goal is to develop and integrate intelligent assistants in existing automation engineering development tools to minimally disrupt existing workflows. Practitioners should be able to adapt our framework to other tools and data. Our contributions address important practical problems: 1) we address the lack of realistic data sets in automation engineering with two publicly available data sources; 2) we make the reference implementation of our algorithms publicly available on GitHub for other practitioners to have a starting point for future research; and 3) we demonstrate the integration of our framework as an add-on to an existing automation engineering toolchain.",,1
95.0,,Automatic Composition and Optimization of Multicomponent Predictive Systems With an Extended Auto-WEKA,M. Martin Salvador; M. Budka; B. Gabrys,2019,,IEEE Transactions on Automation Science and Engineering,,IEEE,"Blink Technologies, Palo Alto, CA, USA; Department of Computing and Informatics, Bournemouth University, Poole, U.K.; Advanced Analytics Institute, University of Technology Sydney, Sydney, NSW, Australia",0,0,,,Optimization;Predictive models;Computational modeling;Task analysis;Tools;Data models;Petri nets,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8550732,,,10.1109/TASE.2018.2876430,False,"Composition and parameterization of multicomponent predictive systems (MCPSs) consisting of chains of data transformation steps are a challenging task. Auto-WEKA is a tool to automate the combined algorithm selection and hyperparameter (CASH) optimization problem. In this paper, we extend the CASH problem and Auto-WEKA to support the MCPS, including preprocessing steps for both classification and regression tasks. We define the optimization problem in which the search space consists of suitably parameterized Petri nets forming the sought MCPS solutions. In the experimental analysis, we focus on examining the impact of considerably extending the search space (from approximately 22000 to 812 billion possible combinations of methods and categorical hyperparameters). In a range of extensive experiments, three different optimization strategies are used to automatically compose MCPSs for 21 publicly available data sets. The diversity of the composed MCPSs found is an indication that fully and automatically exploiting different combinations of data cleaning and preprocessing techniques is possible and highly beneficial for different predictive models. We also present the results on seven data sets from real chemical production processes. Our findings can have a major impact on the development of high-quality predictive models as well as their maintenance and scalability aspects needed in modern applications and deployment scenarios. Note to Practitioners—The extension of Auto-WEKA to compose and optimize multicomponent predictive systems (MCPSs) developed as part of this paper is freely available on GitHub under GPL license, and we encourage practitioners to use it on a broad variety of classification and regression problems. The software can either be used as a blackbox—where search space is made of all possible WEKA filters, predictors, and metapredictors (e.g., ensembles)—or as an optimization tool on a subset of preselected machine learning methods. The application has a graphical user interface, but it can also run from command line and can be embedded in any project as a Java library. There are three main outputs once an Auto-WEKA run has finished: 1) the trained MCPS ready to make predictions on unseen data; 2) the WEKA configuration (i.e., parameterized components); and 3) the Petri net in a Petri Net Markup Language format that can be analyzed using any tool supporting this standard language. There are, however, some practical considerations affecting the quality of the results that must be taken into consideration, such as the CPU time budget or the search starting point. These are extensively discussed in this paper.",,1
96.0,,Synthesis of Energy-Bounded Planar Caging Grasps Using Persistent Homology,J. Mahler; F. T. Pokorny; S. Niyaz; K. Goldberg,2018,,IEEE Transactions on Automation Science and Engineering,,IEEE,"Department of Electrical Engineering and Computer Science, University of California at Berkeley, Berkeley, CA, USA; RPL, School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden; Department of Electrical Engineering and Computer Science, University of California at Berkeley, Berkeley, CA, USA; Department of Industrial Engineering and Operations Research, University of California at Berkeley, Berkeley, CA, USA",0,0,,,Robots;Grippers;Gravity;Robustness;Planning;Friction,,https://github.com/BerkeleyAutomation/caging/zipball/master,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8360035,,,10.1109/TASE.2018.2831724,False,"For applications such as manufacturing, caging grasps restrict object motion without requiring complete immobilization, providing a robust alternative to forceand form-closure grasps. Energy-bounded cages are a new class of caging grasps that relax the requirement of complete caging in the presence of external forces such as gravity or constant velocity pushing in the horizontal plane with Coulomb friction. We address the problem of synthesizing planar energy-bounded cages by identifying gripper and force-direction configurations that maximize the energy required for the object to escape. We present Energy-BoundedCage-Synthesis-2-D (EBCS-2-D), a sampling-based algorithm that uses persistent homology, a recently-developed multiscale approach for topological analysis, to efficiently compute candidate rigid configurations of obstacles that form energy-bounded cages of an object from an α-shape approximation to the configuration space. If a synthesized configuration has infinite escape energy then the object is completely caged. EBCS-2-D runs in O(s3 + sn2) time, where s is the number of samples and n is the number of object and obstacle vertices, where typically n ≪ s. We observe runtimes closer to O(s) for fixed n. We implement EBCS-2-D using the persistent homology algorithms toolbox and study performance on a set of seven planar objects and four gripper types. Experiments suggest that EBCS-2-D takes 2-3 min on a 6 core processor with 200000 pose samples. We also confirm that an rapidly-exploring random tree* motion planner is unable to find escape paths with lower energy. Physical experiments on a five degree of freedom Zymark Zymate and ABB YuMi suggest that push grasps synthesized by EBCS-2-D are robust to perturbations. Data and code are available at http://berkeleyautomation.github.io/caging/.",,1
