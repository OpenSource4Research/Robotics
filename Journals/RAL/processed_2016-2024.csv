index,Document Title,Authors,Author Affiliations,Publication Title,Date Added To Xplore,Publication Year,Volume,Issue,Start Page,End Page,Abstract,ISSN,ISBNs,DOI,Funding Information,PDF Link,Author Keywords,IEEE Terms,Mesh_Terms,Article Citation Count,Patent Citation Count,Reference Count,License,Online Date,Issue Date,Meeting Date,Publisher,Document Identifier,github_url,github_code
0,Depth-Based Efficient PnP: A Rapid and Accurate Method for Camera Pose Estimation,X. Xie; D. Zou,"Dalian University of Technology, Dalian, China; Harbin Institute of Technology, Harbin, China",IEEE Robotics and Automation Letters,19 Sep 2024,2024,9,11,9287,9294,"This letter presents a novel approach, DEPnP (Depth-based Efficient PnP), addressing the Perspective-n-Point (PnP) problem crucial in vision-based navigation and SLAM (Simultaneous Localization and Mapping) in robotics and automation, which estimates the pose of a calibrated camera by observing the 2D projections of known 3D points onto the camera image plane. The method employs eight variables to control the depth of control points and orientation of camera, formulating camera pose estimation as an optimization task. By optimizing these variables utilizing mean-subtracted rotation equations, rapid and accurate camera pose estimation is achieved. Notably, the careful selection of variables and objective function simplifies the computation of the Jacobian matrix, ensuring computational efficiency. DEPnP demonstrates robustness against noise and inlier disturbances, consistently delivering accurate camera pose estimation. Experimental evaluations validate the effectiveness and accuracy of DEPnP, positioning it as a competitive solution for real-time applications requiring precise camera pose estimation in robotics and automation. Our code has been open-sourced on GitHub.",2377-3766,,10.1109/LRA.2024.3438037,National Natural Science Foundation of China(grant numbers:62171075); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10621448,Optimization;perspective-n-point (PnP);real-time pose estimation;SLAM;vision-based navigation,Cameras;Accuracy;Uncertainty;Three-dimensional displays;Computational efficiency;Pose estimation;Robustness,,,,27,IEEE,2 Aug 2024,,,IEEE,IEEE Journals,,
1,Graph-Based SLAM-Aware Exploration With Prior Topo-Metric Information,R. Bai; H. Guo; W. -Y. Yau; L. Xie,"School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Institute for Infocomm Research, Agency for Science, Technology and Research, Singapore; Institute for Infocomm Research, Agency for Science, Technology and Research, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",IEEE Robotics and Automation Letters,22 Jul 2024,2024,9,9,7597,7604,"Autonomous exploration requires a robot to explore an unknown environment while constructing an accurate map using Simultaneous Localization and Mapping (SLAM) techniques. Without prior information, the exploration performance is usually conservative due to the limited planning horizon. This letter exploits prior information about the environment, represented as a topo-metric graph, to benefit both the exploration efficiency and the pose graph reliability in SLAM. Based on the relationship between pose graph reliability and graph topology, we formulate a SLAM-aware path planning problem over the prior graph, which finds a fast exploration path enhanced with the globally informative loop-closing actions to stabilize the SLAM pose graph. A greedy algorithm is proposed to solve the problem, where theoretical thresholds are derived to significantly prune non-optimal loop-closing actions, without affecting the potential informative ones. Furthermore, we incorporate the proposed planner into a hierarchical exploration framework, with flexible features including path replanning, and online prior graph update that adds additional information to the prior graph. Simulation and real-world experiments indicate that the proposed method can reliably achieve higher mapping accuracy than compared methods when exploring environments with rich topologies, while maintaining comparable exploration efficiency. Our method has been open-sourced on GitHub.",2377-3766,,10.1109/LRA.2024.3420817,"National Research Foundation; Medium Sized Center for Advanced Robotics Technology Innovation(grant numbers:C221518004); Robotics HTCO, Agency for Science, Technology and Research; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10577228,Autonomous exploration;planning under uncertainty;Simultaneous Localization and Mapping (SLAM),Simultaneous localization and mapping;Reliability;Laplace equations;Uncertainty;Robot kinematics;Covariance matrices;Autonomous robots,,1.0,,26,IEEE,28 Jun 2024,,,IEEE,IEEE Journals,,
2,Sample-Efficient Learning-Based Dynamic Environment Navigation With Transferring Experience From Optimization-Based Planner,L. Huajian; D. Wei; M. Shouren; W. Chao; G. Yongzhuo,"State Key Laboratory of Robotics and System, Harbin Institute of Technology, Heilongjiang, China; State Key Laboratory of Robotics and System, Harbin Institute of Technology, Heilongjiang, China; State Key Laboratory of Robotics and System, Harbin Institute of Technology, Heilongjiang, China; State Key Laboratory of Robotics and System, Harbin Institute of Technology, Heilongjiang, China; State Key Laboratory of Robotics and System, Harbin Institute of Technology, Heilongjiang, China",IEEE Robotics and Automation Letters,28 Jun 2024,2024,9,8,7055,7062,"This letter presents a sample-efficient deep reinforcement learning (DRL) based method to address the intricate task of autonomous navigation in dynamic, constrained environments. The proposed method leverages a graph neural network with feature-wise linear modulation modules to effectively extract features from observations modeled as heterogeneous graph. By transferring offline model predictive control (MPC) experience data, which can be generated fully in parallel, the training process can be jump-started to cope with the sparse rewards. Simulation results demonstrate a 98% navigation success rate, surpassing baselines by at least 6%, while halving the training steps. Furthermore, we successfully deploy our policy network on a real robot and contribute our code to the community through open-sourcing: github.com/TIB-K330/drl_planner.",2377-3766,,10.1109/LRA.2024.3412610,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10552894,Autonomous agents;autonomous vehicle naviga- tion;collision avoidance;reinforcement learning,Feature extraction;Navigation;Collision avoidance;Training;Robot kinematics;Vehicle dynamics;Autonomous agents;Autonomous vehicles;Reinforcement learning,,,,28,IEEE,11 Jun 2024,,,IEEE,IEEE Journals,https://github.com/TIB-K330/drl_planner,https://github.com/TIB-K330/drl_planner
3,LIV-GaussMap: LiDAR-Inertial-Visual Fusion for Real-Time 3D Radiance Field Map Rendering,S. Hong; J. He; X. Zheng; C. Zheng,"Department of Electronic Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, SAR, China; System Hub, The Hong Kong University of Science and Technology, Guangzhou, China; System Hub, The Hong Kong University of Science and Technology, Guangzhou, China; Department of Mechanical Engineering, The University of Hong Kong, Hong Kong, SAR, China",IEEE Robotics and Automation Letters,2 Oct 2024,2024,9,11,9765,9772,"We introduce an integrated precise LiDAR, Inertial, and Visual (LIV) multimodal sensor fused mapping system that builds on the differentiable Gaussians to improve the mapping fidelity, quality, and structural accuracy. Notably, this is also a novel form of tightly coupled map for LiDAR-visual-inertial sensor fusion. This system leverages the complementary characteristics of LiDAR and visual data to capture the geometric structures of large-scale 3D scenes and restore their visual surface information with high fidelity. The initialization for the scene's surface Gaussians and the sensor's poses of each frame are obtained using a LiDAR-inertial system with the feature of size-adaptive voxels. Then, we optimized and refined the Gaussians using visual-derived photometric gradients to optimize their quality and density. Our method is compatible with various types of LiDAR, including solid-state and mechanical LiDAR, supporting both repetitive and non-repetitive scanning modes. Bolstering structure construction through LiDAR and facilitating real-time generation of photorealistic renderings across diverse LIV datasets. It showcases notable resilience and versatility in generating real-time photorealistic scenes potentially for digital twins and virtual reality, while also holding potential applicability in real-time SLAM and robotics domains. We release our software and hardware and self-collected datasets on Github to benefit the community.",2377-3766,,10.1109/LRA.2024.3400149,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10529285,LiDAR;multi-sensor fusion;mapping;radiance field;3D gaussian splatting,Three-dimensional displays;Laser radar;Point cloud compression;Harmonic analysis;Visualization;Cloud computing;Simultaneous localization and mapping,,5.0,,28,IEEE,13 May 2024,,,IEEE,IEEE Journals,,
4,Understanding URDF: A Dataset and Analysis,D. Tola; P. Corke,"Department of Electrical and Computer Engineering, Aarhus University, Aarhus, Denmark; Centre for Robotics, Queensland University of Technology, Brisbane, QLD, Australia",IEEE Robotics and Automation Letters,3 Apr 2024,2024,9,5,4479,4486,"The complexity of robot systems is rising which makes it increasingly effective to simulate before deployment. To do this, a model of the robot's kinematics or dynamics is required, and the most commonly used format is the Unified Robot Description Format (URDF). This article presents the first dataset of URDF files with metadata and analysis. The dataset contains 322 URDF files from various industrial and research organizations, and the metadata describes each robot, its type, manufacturer, and the source of the model. The files correspond to 195 unique robot models – the excess URDFs correspond to either multiple definitions across sources, or URDF variants of the same robot. We analyze the files in the dataset and provide information on how they were generated, which mesh file types are most commonly used, and compare models of multiply-defined robots. The purpose of this article is to create foundational knowledge about URDF files, how they are created and used, and generate insight into the current state of the URDF format. Publishing the dataset, analysis, and the scripts and tools used enables others using, researching or developing URDFs to easily access this data and use it in their own work.",2377-3766,,10.1109/LRA.2024.3381482,Innovation Foundation Denmark; MADE FAST; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10478618,Simulation and animation;kinematics;software tools for benchmarking and reproducibility,Mathematical models;Collision avoidance;Visualization;Matlab;Kinematics;Service robots;Animation;Simulation;Robot kinematics;Benchmark testing;Reproducibility of results,,6.0,,25,IEEE,25 Mar 2024,,,IEEE,IEEE Journals,,
5,iG-LIO: An Incremental GICP-Based Tightly-Coupled LiDAR-Inertial Odometry,Z. Chen; Y. Xu; S. Yuan; L. Xie,"Provincial Key Laboratory of Intelligent Decision and Cooperative Control, School of Automation, Guangdong University of Technology, Guangzhou, China; Provincial Key Laboratory of Intelligent Decision and Cooperative Control, School of Automation, Guangdong University of Technology, Guangzhou, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",IEEE Robotics and Automation Letters,16 Jan 2024,2024,9,2,1883,1890,"This work proposes an incremental Generalized Iterative Closest Point (GICP) based tightly-coupled LiDAR-inertial odometry (LIO), iG-LIO, which integrates the GICP constraints and inertial constraints into a unified estimation framework. iG-LIO uses a voxel-based surface covariance estimator to estimate the surface covariances of scans, and utilizes an incremental voxel map to represent the probabilistic models of surrounding environments. These methods successfully reduce the time consumption of the covariance estimation, nearest neighbor search, and map management. Extensive datasets collected from mechanical LiDARs and solid-state LiDARs are employed to evaluate the efficiency and accuracy of the proposed LIO. Even though iG-LIO keeps identical parameters across all datasets, the results show that it is more efficient than Faster-LIO while maintaining comparable accuracy with state-of-the-art LIO systems. The source code for iG-LIO has been open-sourced on GitHub: https://github.com/zijiechenrobotics/ig_lio.",2377-3766,,10.1109/LRA.2024.3349915,"National Natural Science Foundation of China(grant numbers:62121004); Natural Science Foundation of Guangdong Province, China(grant numbers:2021B1515420008); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10380742,SLAM;sensor fusion;LiDAR-inertial odometry,Odometry;Estimation;Laser radar;Measurement;Probabilistic logic;Indexes;Surface treatment,,6.0,,35,IEEE,4 Jan 2024,,,IEEE,IEEE Journals,https://github.com/zijiechenrobotics/ig_lio,https://github.com/zijiechenrobotics/ig_lio
6,Onboard Dynamic-Object Detection and Tracking for Autonomous Robot Navigation With RGB-D Camera,Z. Xu; X. Zhan; Y. Xiu; C. Suzuki; K. Shimada,"Department of Mechanical Engineering, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Mechanical Engineering, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Mechanical Engineering, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Mechanical Engineering, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Mechanical Engineering, Carnegie Mellon University, Pittsburgh, PA, USA",IEEE Robotics and Automation Letters,4 Dec 2023,2024,9,1,651,658,"Deploying autonomous robots in crowded indoor environments usually requires them to have accurate dynamic obstacle perception. Although plenty of previous works in the autonomous driving field have investigated the 3D object detection problem, the usage of dense point clouds from a heavy Light Detection and Ranging (LiDAR) sensor and their high computation cost for learning-based data processing make those methods not applicable to small robots, such as vision-based UAVs with small onboard computers. To address this issue, we propose a lightweight 3D dynamic obstacle detection and tracking (DODT) method based on an RGB-D camera, which is designed for low-power robots with limited computing power. Our method adopts a novel ensemble detection strategy, combining multiple computationally efficient but low-accuracy detectors to achieve real-time high-accuracy obstacle detection. Besides, we introduce a new feature-based data association and tracking method to prevent mismatches utilizing point clouds' statistical features. In addition, our system includes an optional and auxiliary learning-based module to enhance the obstacle detection range and dynamic obstacle identification. The proposed method is implemented in a small quadcopter, and the results show that our method can achieve the lowest position error (0.11 m) and a comparable velocity error (0.23 m/s) across the benchmarking algorithms running on the robot's onboard computer. The flight experiments prove that the tracking results from the proposed method can make the robot efficiently alter its trajectory for navigating dynamic environments. Our software is available on GitHub1 as an open-source ROS package.",2377-3766,,10.1109/LRA.2023.3334683,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10323166,RGB-D perception;vision-based navigation;visual tracking;3D object detection;collision avoidance,Detectors;Cameras;Three-dimensional displays;Point cloud compression;Robot vision systems;Heuristic algorithms;Robot vision systems;Collision avoidance,,10.0,,29,IEEE,20 Nov 2023,,,IEEE,IEEE Journals,,
7,VoxelMap++: Mergeable Voxel Mapping Method for Online LiDAR(-Inertial) Odometry,C. Wu; Y. You; Y. Yuan; X. Kong; Y. Zhang; Q. Li; K. Zhao,"Institute of Information and Communication Engineering, University of Electronic Science and Technology of China (UESTC), Chengdu, China; XGRIDS, Shenzhen, China; Institute of Information and Communication Engineering, University of Electronic Science and Technology of China (UESTC), Chengdu, China; Institute of Information and Communication Engineering, University of Electronic Science and Technology of China (UESTC), Chengdu, China; Institute of Information and Communication Engineering, University of Electronic Science and Technology of China (UESTC), Chengdu, China; Institute of Information and Communication Engineering, University of Electronic Science and Technology of China (UESTC), Chengdu, China; XGRIDS, Shenzhen, China",IEEE Robotics and Automation Letters,27 Nov 2023,2024,9,1,427,434,"This letter presents VoxelMap++: a voxel mapping method with plane merging which can effectively improve the accuracy and efficiency of LiDAR(-inertial) based simultaneous localization and mapping (SLAM). This map is a collection of voxels that contains one plane feature with 3DOF representation and corresponding covariance estimation. Considering map will contain a large number of coplanar features (kid planes), these kid planes' can be regarded as the measurements with covariance of a larger plane (father plane). Thus, we have designed a plane merging module based on the union-find. This merging module is capable of distinguishing co-plane relationship within various voxels, then merge these kid planes to estimate the father plane by minimizing the trace of covariance. After merging, the father plane exhibits more accurate compare to kids plane, with decreasing of uncertainty, which improve the accuracy of LiDAR(-inertial) odometry. Experiments on different environments demonstrate the superior of VoxelMap++ compared with other state-of-the-art methods (see our attached video). Our implementation is open-sourced on GitHub which is applicable for both non-repetitive scanning LiDARs and traditional scanning LiDAR.",2377-3766,,10.1109/LRA.2023.3333736,Science and Technology Project Fund of Sichuan Province(grant numbers:2018sz0364); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10321658,Mapping;union-find;localization;simultaneous localization and mapping (SLAM),Laser radar;Odometry;Merging;Estimation;Point cloud compression;Fitting;Robustness,,6.0,,25,IEEE,17 Nov 2023,,,IEEE,IEEE Journals,,
8,Integrated Planning and Control for Quadrotor Navigation in Presence of Suddenly Appearing Objects and Disturbances,W. Liu; Y. Ren; F. Zhang,"Department of Mechanical Engineering, University of Hong Kong, Hong Kong, Hong Kong; Department of Mechanical Engineering, University of Hong Kong, Hong Kong, Hong Kong; Department of Mechanical Engineering, University of Hong Kong, Hong Kong, Hong Kong",IEEE Robotics and Automation Letters,14 Dec 2023,2024,9,1,899,906,"Autonomous flight for quadrotors in environments with suddenly appearing objects and disturbances still faces significant challenges. In this work, we propose an integrated planning and control framework called IPC. Specifically, we design a framework consisting of a lightweight frontend and an MPC backend. On the frontend, we employ the A* algorithm to generate the reference path on a local map. On the backend, we model the trajectory planning and control problem as a linear model predictive control (MPC) problem. In the MPC formulation, the quadrotor is modeled as a high-order integral system (a linear system) to follow the reference path from the frontend. We use a series of convex polyhedrons (i.e., Safe Flight Corridor, SFC) to represent the free space in the environment and employ the multiple hyperplanes of the polyhedrons as a linear inequality constraint of the MPC problem to ensure flight safety. In this way, the linear MPC generates control actions that strictly meet the safety constraints in a short time ($2 \,\mathrm{ms}$-$3.5 \,\mathrm{ms}$). Then, the control actions of the linear MPC (i.e., jerk) are transformed to the actual control commands (i.e., angular velocity and throttle) through the differential flatness of the quadrotor. Since the MPC computes the control actions directly according to the obstacles and quadrotor's state at a rather high frequency (i.e., $100 \,\mathrm{Hz}$), it improves the quadrotor's response speed to dynamic obstacles and disturbance rejection ability to external disturbances. In simulation experiments involving avoiding a suddenly appearing object, our method outperforms state-of-the-art baselines in terms of success rate. Furthermore, we validate our method in real-world environments with dynamic objects and disturbances using a fully autonomous LiDAR-based quadrotor system, achieving autonomous navigation at velocities up to 5.86 m/s in dense forests. Our IPC is released as a ROS package on GitHub as open source software.",2377-3766,,10.1109/LRA.2023.3311358,Information Science Academy of China Electronics Technology Group Corporation(grant numbers:200010756); University Grants Committee of Hong Kong General Research Fund(grant numbers:17204523); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10238764,Aerial systems: Applications;collision avoidance;integrated planning and control,Quadrotors;Planning;Navigation;Collision avoidance;Laser radar;Autonomous aerial vehicles;Trajectory optimization,,5.0,,25,IEEE,4 Sep 2023,,,IEEE,IEEE Journals,,
9,PBACalib: Targetless Extrinsic Calibration for High-Resolution LiDAR-Camera System Based on Plane-Constrained Bundle Adjustment,F. Chen; L. Li; S. Zhang; J. Wu; L. Wang,"Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, SAR, China; Department of Mechanical Engineering, The University of Hong Kong, Hong Kong, SAR, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, SAR, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, SAR, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, SAR, China",IEEE Robotics and Automation Letters,8 Dec 2022,2023,8,1,304,311,"The strategy of fusing multi-model data especially from cameras, light detection and ranging sensors (LiDAR), is frequently considered in robotics to enhance the performance of the perception and navigation tasks. Extrinsic calibration, which spatially aligns different sources into a unified coordinate representation, directly determines the performance of the combined data. In this letter, we propose PBACalib, a novel targetless extrinsic calibration algorithm aiming at the dense LiDAR-camera system based on the plane-constrained bundle adjustment (PBA). The proposed method utilizes the feature points derived from a prominent plane in the scene and iteratively minimizes the reprojection error. A maximum likelihood estimator (MLE) is designed by considering the uncertainty information of the measurements. Furthermore, we explore the distribution of collected data and characterize the robustness and solvability of the extrinsic estimates using a confidence factor. Simulation and real-world experiments both qualitatively and quantitatively demonstrate the robustness and accuracy of our method. The comparison experiments show that the proposed method outperforms another targetless method. To benefit the community, Matlab code has been publicly released on Github.",2377-3766,,10.1109/LRA.2022.3226026,Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2021B1515120032); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9968080,Bundle adjustment;high-resolution LiDAR;LiDAR-camera calibration;targetless extrinsic calibration,Calibration;Laser radar;Cameras;Sensors;Maximum likelihood estimation;Feature extraction;Bundle adjustment,,11.0,,25,IEEE,1 Dec 2022,,,IEEE,IEEE Journals,,
10,Arena-Bench: A Benchmarking Suite for Obstacle Avoidance Approaches in Highly Dynamic Environments,L. Kästner; T. Bhuiyan; T. A. Le; E. Treis; J. Cox; B. Meinardus; J. Kmiecik; R. Carstens; D. Pichel; B. Fatloun; N. Khorsandi; J. Lambrecht,"Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany",IEEE Robotics and Automation Letters,25 Jul 2022,2022,7,4,9477,9484,"The ability to autonomously navigate safely, especially within dynamic environments, is paramount for mobile robotics. In recent years, DRL approaches have shown superior performance in dynamic obstacle avoidance. However, these learning-based approaches are often developed in specially designed simulation environments and are hard to test against conventional planning approaches. Furthermore, the integration and deployment of these approaches into real robotic platforms are not yet completely solved. In this letter, we present Arena-bench, a benchmark suite to train, test, and evaluate navigation planners on different robotic platforms within 3D environments. It provides tools to design and generate highly dynamic evaluation worlds, scenarios, and tasks for autonomous navigation and is fully integrated into the robot operating system. To demonstrate the functionalities of our suite, we trained a DRL agent on our platform and compared it against a variety of existing different model-based and learning-based navigation approaches on a variety of relevant metrics. Finally, we deployed the approaches towards real robots and demonstrated the reproducibility of the results. The code is publicly available at github.com/ignc-research/arena-bench",2377-3766,,10.1109/LRA.2022.3190086,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9827557,Software tools for benchmarking and reproducibility;motion and path planning;collision avoidance;reinforcement learning,Navigation;Benchmark testing;Robots;Task analysis;Measurement;Three-dimensional displays;Collision avoidance,,25.0,,30,IEEE,12 Jul 2022,,,IEEE,IEEE Journals,https://github.com/ignc-research/arena-bench,https://github.com/ignc-research/arena-bench
11,Visibility Maximization Controller for Robotic Manipulation,K. He; R. Newbury; T. Tran; J. Haviland; B. Burgess-Limerick; D. Kulić; P. Corke; A. Cosgun,"Monash University, Clayton, VIC, Australia; Monash University, Clayton, VIC, Australia; Monash University, Clayton, VIC, Australia; Queensland University of Technology Centre for Robotics, Brisbane, QLD, Australia; Queensland University of Technology Centre for Robotics, Brisbane, QLD, Australia; Monash University, Clayton, VIC, Australia; Queensland University of Technology Centre for Robotics, Brisbane, QLD, Australia; Monash University, Clayton, VIC, Australia",IEEE Robotics and Automation Letters,12 Jul 2022,2022,7,3,8479,8486,"Occlusions caused by a robot’s own body is a common problem for closed-loop control methods employed in eye-to-hand camera setups. We propose an optimization-based reactive controller that minimizes self-occlusions while achieving a desired goal pose. The approach allows coordinated control between the robot’s base, arm and head by encoding the line-of-sight visibility to the target as a soft constraint along with other task-related constraints, and solving for feasible joint and base velocities. The generalizability of the approach is demonstrated in simulated and real-world experiments, on robots with fixed or mobile bases, with moving or fixed objects, and multiple objects. The experiments revealed a trade-off between occlusion rates and other task metrics. While a planning-based baseline achieved lower occlusion rates than the proposed controller, it came at the expense of highly inefficient paths and a significant drop in the task success. On the other hand, the proposed controller is shown to improve visibility to the target object(s) without sacrificing too much from the task success and efficiency. Videos and code can be found at: rhys-newbury.github.io/projects/vmc/.",2377-3766,,10.1109/LRA.2022.3188430,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9815144,Visual servoing;sensor-based control;mobile ma-nipulation,Robots;Cameras;Task analysis;Robot kinematics;Robot vision systems;End effectors;Shock absorbers,,11.0,,26,IEEE,4 Jul 2022,,,IEEE,IEEE Journals,https://rhys-newbury.github.io/projects/vmc,https://github.com/alshedivat/al-folio
12,Efficient and Probabilistic Adaptive Voxel Mapping for Accurate Online LiDAR Odometry,C. Yuan; W. Xu; X. Liu; X. Hong; F. Zhang,"Department of Mechanical Engineering, The University of Hong Kong, Hong Kong Special Administrative Region, Hong Kong; Department of Mechanical Engineering, The University of Hong Kong, Hong Kong Special Administrative Region, Hong Kong; Department of Mechanical Engineering, The University of Hong Kong, Hong Kong Special Administrative Region, Hong Kong; School of System Design and Intelligent Manufacturing, Southern University of Science and Technology, Shenzhen, China; Department of Mechanical Engineering, The University of Hong Kong, Hong Kong Special Administrative Region, Hong Kong",IEEE Robotics and Automation Letters,13 Jul 2022,2022,7,3,8518,8525,This letter proposes an efficient and probabilistic adaptive voxel mapping method for LiDAR odometry. The map is a collection of voxels; each contains one plane feature that enables the probabilistic representation of the environment and accurate registration of a new LiDAR scan. We further analyze the need for coarse-to-fine voxel mapping and then use a novel voxel map organized by a Hash table and octrees to build and update the map efficiently. We apply the proposed voxel map to an iterated extended Kalman filter and construct a maximum a posteriori probability problem for pose estimation. Experiments on the open KITTI dataset show the high accuracy and efficiency of our method compared to other state-of-the-art methods. Experiments on indoor and unstructured outdoor environments with solid-state LiDAR and non-repetitive scanning LiDAR further verify the adaptability of our mapping method to different environments and LiDAR scanning patterns (see our attached video1). Our codes and dataset are open-sourced on Github2,2377-3766,,10.1109/LRA.2022.3187250,University Grants Committee of Hong Kong General Research Fund(grant numbers:17206421); SUSTech startup Fund(grant numbers:Y01966105); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9813516,Mapping;Localization;SLAM,Laser radar;Uncertainty;Noise measurement;Probabilistic logic;Measurement uncertainty;Three-dimensional displays;Point cloud compression,,69.0,,35,IEEE,1 Jul 2022,,,IEEE,IEEE Journals,,
13,Point Cloud Registration Leveraging Structural Regularity in Manhattan World,J. Liu; Y. Liu; Z. Meng,"Department of Precision Instrument, Tsinghua University, Beijing, China; Department of Precision Instrument, Tsinghua University, Beijing, China; Department of Precision Instrument, Tsinghua University, Beijing, China",IEEE Robotics and Automation Letters,4 Jul 2022,2022,7,3,7888,7895,"Point cloud registration is an essential technique for many tasks including autonomous navigation, augmented/virtual reality and scene reconstruction. In man-made environments, abundant plane features with structural regularities are commonly seen, which is actually beneficial for point cloud registration. This paper presents an accurate and robust registration method that formulates the registration problem as a maximum likelihood estimation (MLE) problem where the useful structure information is also leveraged. To align two point sets efficiently, one point set is first represented as a Gaussian mixture models (GMM) tree, and then the Manhattan frame (MF) with dominant planes is estimated and the degenerated nodes corresponding to the dominant planes in the GMM tree are clustered to embody the structure information. The proposed MLE problem is solved by an Expectation Maximization (EM) framework. In the E step, the probabilistic data association between the GMM and another point set is first estimated and then refined by aligning the MF and planes. In the M step, an optimization problem using Mahalanobis distance is solved. Experimental results on both indoor room and hybrid man-made environments show that the proposed method not only achieves a good balance between accuracy and speed, but also provides robust and accurate performance in sequence registration process. The source code is available on our Github1.",2377-3766,,10.1109/LRA.2022.3185782,"National Natural Science Foundation of China(grant numbers:U19B2029,61873140); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9806054,Localization;probability;statistical methods,Point cloud compression;Probabilistic logic;Measurement;Maximum likelihood estimation;Three-dimensional displays;Frequency modulation;Feature extraction,,5.0,,35,IEEE,24 Jun 2022,,,IEEE,IEEE Journals,,
14,An Algorithm for the SE(3)-Transformation on Neural Implicit Maps for Remapping Functions,Y. Yuan; A. Nüchter,"Informatics VII: Robotics and Telematics, University of Würzburg, Wuerzburg, Germany; Informatics VII: Robotics and Telematics, University of Würzburg, Wuerzburg, Germany",IEEE Robotics and Automation Letters,1 Jul 2022,2022,7,3,7763,7770,"Implicit representations are widely used for object reconstruction due to their efficiency and flexibility. In 2021, a novel structure named neural implicit map has been invented for incremental reconstruction. A neural implicit map alleviates the problem of inefficient memory cost of previous online 3D dense reconstruction while producing better quality. However, the neural implicit map suffers the limitation that it does not support remapping as the frames of scans are encoded into a deep prior after generating the neural implicit map. This means, that neither this generation process is invertible, nor a deep prior is transformable. The non-remappable property makes it not possible to apply loop-closure techniques. We present a neural implicit map based transformation algorithm to fill this gap. As our neural implicit map is transformable, our model supports remapping for this special map of latent features. Experiments show that our remapping module is capable to well-transform neural implicit maps to new poses. Embedded into a SLAM framework, our mapping model is able to tackle the remapping of loop closures and demonstrates high-quality surface reconstruction. Our implementation is available at github for the research community.",2377-3766,,10.1109/LRA.2022.3185383,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9804847,Mapping;SLAM,Point cloud compression;Three-dimensional displays;Simultaneous localization and mapping;Solid modeling;Shape;Pipelines;Geometry,,5.0,,39,IEEE,23 Jun 2022,,,IEEE,IEEE Journals,,
15,Indirect Point Cloud Registration: Aligning Distance Fields Using a Pseudo Third Point Set,Y. Yuan; A. Nüchter,"Informatics VII: Robotics and Telematics, University of Würzburg, Würzburg, Germany; Informatics VII: Robotics and Telematics, University of Würzburg, Würzburg, Germany",IEEE Robotics and Automation Letters,14 Jun 2022,2022,7,3,7075,7082,"In recent years, implicit functions have drawn attention in the field of 3D reconstruction and have successfully been applied with Deep Learning. However, for incremental reconstruction, implicit function-based registrations have been rarely explored. Inspired by the high precision of deep learning global feature registration, we propose to combine this with distance fields. We generalize the algorithm to a non-Deep Learning setting while retaining the accuracy. Our algorithm is more accurate than conventional models while, without any training, it achieves a competitive performance and faster speed, compared to Deep Learning-based registration models. The implementation is available on github1 for the research community.",2377-3766,,10.1109/LRA.2022.3181356,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9793674,Localization;mapping;SLAM,Point cloud compression;Jacobian matrices;Measurement;Three-dimensional displays;Feature extraction;Registers;Training,,5.0,,30,IEEE,10 Jun 2022,,,IEEE,IEEE Journals,,
16,Object-Aware Monocular Depth Prediction With Instance Convolutions,E. Simsar; E. P. Örnek; F. Manhardt; H. Dhamo; N. Navab; F. Tombari,"Technical University of Munich, München, Germany; Technical University of Munich, München, Germany; Google Inc., Garching, Germany; Technical University of Munich, München, Germany; Technical University of Munich, München, Germany; Technical University of Munich, München, Germany",IEEE Robotics and Automation Letters,14 Mar 2022,2022,7,2,5389,5396,"With the advent of deep learning, estimating depth from a single RGB image has recently received a lot of attention, being capable of empowering many different applications ranging from path planning for robotics to computational cinematography. Nevertheless,while the depth maps are in their entirety fairly reliable, the estimates around object discontinuities are still far from satisfactory. This can beattributed to the fact that the convolutional operator naturally aggregates features across object discontinuities, resulting in smooth transitions rather than clear boundaries. Therefore, in order to circumvent this issue, we propose a novel convolutional operator which is explicitly tailored to avoid feature aggregation of different object parts. In particular, our method is based on estimating per-part depth values by means of super-pixels. The proposed convolutional operator, which we dub “Instance Convolution,” then only considers each object part individually on the basis of the estimated super-pixels. Our evaluation with respect to the NYUv2, iBims and KITTI datasets demonstrate the advantages of Instance Convolutions over the classical convolution at estimating depth around occlusion boundaries, while producing comparable results elsewhere. Our code is available at github.com/enisimsar/instance-conv.",2377-3766,,10.1109/LRA.2022.3155823,German Federal Ministry of Education and Research(grant numbers:16SV8088); Google; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9726910,Deep learning for visual perception;RGB-D perception,Feature extraction;Convolutional codes;Kernel;Three-dimensional displays;Image segmentation;Robots;Deep learning,,1.0,,54,IEEE,3 Mar 2022,,,IEEE,IEEE Journals,https://github.com/enisimsar/instance-conv,https://github.com/enisimsar/instance-conv
17,Global-Reasoned Multi-Task Learning Model for Surgical Scene Understanding,L. Seenivasan; S. Mitheran; M. Islam; H. Ren,"Department of Biomedical Engineering, National University of Singapore, Singapore; Department of Electronics and Communication Engineering, National Institute of Technology, Tiruchirappalli, India; Biomedical Image Analysis Group, Imperial College London, White City, London, U.K.; Department of Biomedical Engineering, National University of Singapore, Singapore",IEEE Robotics and Automation Letters,16 Feb 2022,2022,7,2,3858,3865,"Global and local relational reasoning enable scene understanding models to perform human-like scene analysis and understanding. Scene understanding enables better semantic segmentation and object-to-object interaction detection. In the medical domain, a robust surgical scene understanding model allows the automation of surgical skill evaluation, real-time monitoring of surgeon’s performance and post-surgical analysis. This letter introduces a globally-reasoned multi-task surgical scene understanding model capable of performing instrument segmentation and tool-tissue interaction detection. Here, we incorporate global relational reasoning in the latent interaction space and introduce multi-scale local (neighborhood) reasoning in the coordinate space to improve segmentation. Utilizing the multi-task model setup, the performance of the visual-semantic graph attention network in interaction detection is further enhanced through global reasoning. The global interaction space features from the segmentation module are introduced into the graph network, allowing it to detect interactions based on both node-to-node and global interaction reasoning. Our model reduces the computation cost compared to running two independent single-task models by sharing common modules, which is indispensable for practical applications. Using a sequential optimization technique, the proposed multi-task model outperforms other state-of-the-art single-task models on the MICCAI endoscopic vision challenge 2018 dataset. Additionally, we also observe the performance of the multi-task model when trained using the knowledge distillation technique. The official code implementation is made available in GitHub.",2377-3766,,10.1109/LRA.2022.3146544,"National Key R&D Program of China(grant numbers:2018YFB1307700,2018YFB1307703); Ministry of Science and Technology (MOST) of China; Hong Kong Research Grants Council (RGC) Collaborative Research Fund(grant numbers:CRF C4026-21GF); Shun Hing Institute of Advanced Engineering(grant numbers:BME-p1-21/8115064); Chinese University of Hong Kong; Singapore Academic Research Fund(grant numbers:R397000353114); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9695281,Computer vision for medical robotics;deep learning methods;semantic scene understanding;medical robots and systems,Cognition;Feature extraction;Instruments;Decoding;Computational modeling;Multitasking;Semantics,,22.0,,29,IEEE,27 Jan 2022,,,IEEE,IEEE Journals,,
18,ACD-EDMD: Analytical Construction for Dictionaries of Lifting Functions in Koopman Operator-Based Nonlinear Robotic Systems,L. Shi; K. Karydis,"Department of Electrical and Computer Engineering, University of California, Riverside, CA, USA; Department of Electrical and Computer Engineering, University of California, Riverside, CA, USA",IEEE Robotics and Automation Letters,24 Dec 2021,2022,7,2,906,913,"Koopman operator theory has been gaining momentum for model extraction, planning, and control of data-driven robotic systems. The Koopman operator’s ability to extract dynamics from data depends heavily on the selection of an appropriate dictionary of lifting functions. In this letter, we propose ACD-EDMD, a new method for Analytical Construction of Dictionaries of appropriate lifting functions for a range of data-driven Koopman operator based nonlinear robotic systems. The key insight of this work is that information about fundamental topological spaces of the nonlinear system (such as its configuration space and workspace) can be exploited to steer the construction of Hermite polynomial-based lifting functions. We show that the proposed method leads to dictionaries that are simple to implement while enjoying provable completeness and convergence guarantees when observables are weighted bounded. We evaluate ACD-EDMD using a range of diverse nonlinear robotic systems in both simulated and physical hardware experimentation (a wheeled mobile robot, a two-revolute-joint robotic arm, and a soft robotic leg). Results reveal that our method leads to dictionaries that enable high-accuracy prediction and that can generalize to diverse validation sets. The associated GitHub repository of our algorithm can be accessed at https://github.com/UCR-Robotics/ACD-EDMD.",2377-3766,,10.1109/LRA.2021.3133001,"NSF(grant numbers:IIS-1910087,CMMI-2046270,CMMI-2133084); ONR(grant numbers:N00014-19-1-2264); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9640554,Calibration and identification;model learning for control;machine learning for robot control;koopman operator;extended dynamic mode decomposition,Robots;Dictionaries;Soft robotics;Nonlinear dynamical systems;Convergence;Legged locomotion;Vehicle dynamics,,14.0,,36,IEEE,7 Dec 2021,,,IEEE,IEEE Journals,https://github.com/UCR-Robotics/ACD-EDMD,https://github.com/UCR-Robotics/ACD-EDMD
19,Avoiding Dynamic Small Obstacles With Onboard Sensing and Computation on Aerial Robots,F. Kong; W. Xu; Y. Cai; F. Zhang,"Department of Mechanical Engineering, University of Hong Kong, Hong Kong; Department of Mechanical Engineering, University of Hong Kong, Hong Kong; Department of Mechanical Engineering, University of Hong Kong, Hong Kong; Department of Mechanical Engineering, University of Hong Kong, Hong Kong",IEEE Robotics and Automation Letters,17 Aug 2021,2021,6,4,7869,7876,"In practical applications, autonomous quadrotors are still facing significant challenges, such as the detection and avoidance of very small and even dynamic obstacles (e.g., tree branches, power lines). In this paper, we propose a compact, integrated, and fully autonomous quadrotor system, which can fly safely in cluttered environments while avoiding dynamic small obstacles. Our quadrotor platform is equipped with a forward-looking three-dimensional (3D) light detection and ranging (lidar) sensor to perceive the environment and an onboard embedded computer to perform all the estimation, mapping, and planning tasks. Specifically, the computer estimates the current pose of the UAV, maintains a local map (time-accumulated point clouds KD-Trees), and computes a safe trajectory using kinodynamic A* search to the goal point. The whole perception and planning system can run onboard at 50 Hz. Various indoor and outdoor experiments show that the system can avoid dynamic small obstacles (down to 9 mm diameter bar) while flying at 2 m/s in cluttered environments. High-speed experiments are also carried out, with a maximum speed of 5.5 m/s. Our codes are open-sourced on Github1.",2377-3766,,10.1109/LRA.2021.3101877,DJI(grant numbers:200009538); GRF(grant numbers:17206920); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9507303,Aerial systems;perception and autonomy;motion and path planning;collision avoidance,Robot sensing systems;Laser radar;Trajectory;Three-dimensional displays;Navigation;Vehicle dynamics;Planning,,36.0,,29,IEEE,4 Aug 2021,,,IEEE,IEEE Journals,,
20,SRH-Net: Stacked Recurrent Hourglass Network for Stereo Matching,H. Du; Y. Li; Y. Sun; J. Zhu; F. Tombari,"State Key Laboratory of Precision Measuring Technology and Instruments, Tianjin University, Tianjin, China; Department of Informatic, Technical University of Munich, Munich, Germany; State Key Laboratory of Precision Measuring Technology and Instruments, Tianjin University, Tianjin, China; State Key Laboratory of Precision Measuring Technology and Instruments, Tianjin University, Tianjin, China; Department of Informatic, Technical University of Munich, Munich, Germany",IEEE Robotics and Automation Letters,20 Aug 2021,2021,6,4,8005,8012,"The cost aggregation strategy shows a crucialrole in learning-based stereo matching tasks, where 3D convolutional filters obtain state of the art but require intensive computation resources, while 2D operations need less GPU memory but are sensitive to domain shift. In this letter, we decouple the 4D cubic cost volume used by 3D convolutional filters into sequential cost maps along the direction of disparity instead of dealing with it at once by exploiting a recurrent cost aggregation strategy. Furthermore, a novel recurrent module, Stacked Recurrent Hourglass (SRH), is proposed to process each cost map. Our hourglass network is constructed based on Gated Recurrent Units (GRUs) and down/upsampling layers, which provides GRUs larger receptive fields. Then two hourglass networks are stacked together, while multi-scale information is processed by skip connections to enhance the performance of the pipeline in textureless areas. The proposed architecture is implemented in an end-to-end pipeline and evaluated on public datasets, which reduces GPU memory consumption by up to 56.1% compared with PSMNet using stacked hourglass 3D CNNs without the degradation of accuracy. Then, we further demonstrate the scalability of the proposed method on several high-resolution pairs, while previously learned approaches often fail due to the memory constraint. The code is released at https://github.com/hongzhidu/SRHNet.",2377-3766,,10.1109/LRA.2021.3101523,"National Natural Science Foundation of China(grant numbers:52075382,51721003); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9506870,Computer vision for automation;deep learning in robotics and automation,Three-dimensional displays;Pipelines;Feature extraction;Decoding;Graphics processing units;Task analysis;Convolutional codes,,8.0,,31,IEEE,4 Aug 2021,,,IEEE,IEEE Journals,https://github.com/hongzhidu/SRHNet,https://github.com/hongzhidu/SRHNet
21,Pixel-Level Extrinsic Self Calibration of High Resolution LiDAR and Camera in Targetless Environments,C. Yuan; X. Liu; X. Hong; F. Zhang,"Department of Mechanical Engineering, University of Hong Kong, Hong Kong, China; Department of Mechanical Engineering, University of Hong Kong, Hong Kong, China; School of System Design and Intelligent Manufacturing, Southern University of Science and Technology, Shenzhen, China; Department of Mechanical Engineering, University of Hong Kong, Hong Kong, China",IEEE Robotics and Automation Letters,16 Aug 2021,2021,6,4,7517,7524,"In this letter, we present a novel method for automatic extrinsic calibration of high-resolution LiDARs and RGB cameras in targetless environments. Our approach does not require checkerboards but can achieve pixel-level accuracy by aligning natural edge features in the two sensors. On the theory level, we analyze the constraints imposed by edge features and the sensitivity of calibration accuracy with respect to edge distribution in the scene. On the implementation level, we carefully investigate the physical measuring principles of LiDARs and propose an efficient and accurate LiDAR edge extraction method based on point cloud voxel cutting and plane fitting. Due to the edges' richness in natural scenes, we have carried out experiments in many indoor and outdoor scenes. The results show that this method has high robustness, accuracy, and consistency. It can promote the research and application of the fusion between LiDAR and camera. We have open sourced our code on GitHub1 to benefit the community.",2377-3766,,10.1109/LRA.2021.3098923,DJI(grant numbers:200009538); SUSTech Startup Fund; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9495137,Calibration and identification;sensor fusion;mapping,Image edge detection;Laser radar;Feature extraction;Calibration;Three-dimensional displays;Cameras;Laser beams,,167.0,,22,IEEE,26 Jul 2021,,,IEEE,IEEE Journals,,
22,MLPD: Multi-Label Pedestrian Detector in Multispectral Domain,J. Kim; H. Kim; T. Kim; N. Kim; Y. Choi,"School of Intelligent Mechatronics Engineering, Sejong University, Seoul, South Korea; School of Intelligent Mechatronics Engineering, Sejong University, Seoul, South Korea; School of Intelligent Mechatronics Engineering, Sejong University, Seoul, South Korea; NAVER LABS, Gyeonggi-do, South Korea; School of Intelligent Mechatronics Engineering, Sejong University, Seoul, South Korea",IEEE Robotics and Automation Letters,17 Aug 2021,2021,6,4,7846,7853,"Multispectral pedestrian detection has been actively studied as a promising multi-modality solution to handle illumination and weather changes. Most multi-modality approaches carry the assumption that all inputs are fully-overlapped. However, these kinds of data pairs are not common in practical applications due to the complexity of the existing sensor configuration. In this letter, we tackle multispectral pedestrian detection, where all input data are not paired. To this end, we propose a novel single-stage detection framework that leverages multi-label learning to learn input state-aware features by assigning a separate label according to the given state of the input image pair. We also present a novel augmentation strategy by applying geometric transformations to synthesize the unpaired multispectral images. In extensive experiments, we demonstrate the efficacy of the proposed method under various real-world conditions, such as fully-overlapped images and partially-overlapped images, in stereo-vision. Code and a demonstration video are available at https://github.com/sejong-rcv/MLPD-Multi-Label-Pedestrian-Detection.",2377-3766,,10.1109/LRA.2021.3099870,"National Research Foundation of Korea(grant numbers:NRF-2020M3F6A1109603,NRF-2020R1F1A1076987); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9496129,Deep learning for visual perception;object detection;multispectral pedestrian detection,Feature extraction;Head;Robot sensing systems;Detectors;Training;Lighting;Task analysis,,42.0,,22,IEEE,26 Jul 2021,,,IEEE,IEEE Journals,https://github.com/sejong-rcv/MLPD-Multi-Label-Pedestrian-Detection,https://github.com/sejong-rcv/MLPD-Multi-Label-Pedestrian-Detection
23,Human Joint Torque Modelling With MMG and EMG During Lower Limb Human-Exoskeleton Interaction,C. Caulcrick; W. Huo; W. Hoult; R. Vaidyanathan,"Department of Mechanical Engineering, and DRI-CRT, Imperial College London, London, U.K.; Department of Mechanical Engineering, and DRI-CRT, Imperial College London, London, U.K.; McLaren Applied, McLaren Technology Centre, Woking, U.K.; Department of Mechanical Engineering, and DRI-CRT, Imperial College London, London, U.K.",IEEE Robotics and Automation Letters,28 Jul 2021,2021,6,4,7185,7192,"Human-robot cooperation is vital for optimising powered assist of lower limb exoskeletons (LLEs). Robotic capacity to intelligently adapt to human force, however, demands a fusion of data from exoskeleton and user state for smooth human-robot synergy. Muscle activity, mapped through electromyography (EMG) or mechanomyography (MMG) is widely acknowledged as usable sensor input that precedes the onset of human joint torque. However, competing and complementary information between such physiological feedback is yet to be exploited, or even assessed, for predictive LLE control. We investigate complementary and competing benefits of EMG and MMG sensing modalities as a means of calculating human torque input for assist-as-needed (AAN) LLE control. Three biomechanically agnostic machine learning approaches, linear regression, polynomial regression, and neural networks, are implemented for joint torque prediction during human-exoskeleton interaction experiments. Results demonstrate MMG predicts human joint torque with slightly lower accuracy than EMG for isometric human-exoskeleton interaction. Performance is comparable for dynamic exercise. Neural network models achieve the best performance for both MMG and EMG (94.8±0.7% with MMG and 97.6±0.8% with EMG (Mean ± SD)) at the expense of training time and implementation complexity. This investigation represents the first MMG human joint torque models for LLEs and their first comparison with EMG. We provide our implementations for future investigations (https://github.com/cic12/ieee_appx).",2377-3766,,10.1109/LRA.2021.3097832,U.K. EPSRC; Dementia Research Institute Care Research Technology Centre(grant numbers:UKDRI-7003); UK-India Education and Research Initiative(grant numbers:2016-17-103); McLaren Applied Technologies; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9490364,Physical human-robot interaction;exoskeletons;rehabilitation robotics,Torque;Muscles;Electromyography;Exoskeletons;Robot sensing systems;Robots;Knee,,34.0,,36,IEEE,19 Jul 2021,,,IEEE,IEEE Journals,https://github.com/cic12/ieee_appx,https://github.com/cic12/ieee_appx
24,Topo-Boundary: A Benchmark Dataset on Topological Road-Boundary Detection Using Aerial Images for Autonomous Driving,Z. Xu; Y. Sun; M. Liu,"Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong; Department of Mechanical Engineering, The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong",IEEE Robotics and Automation Letters,29 Jul 2021,2021,6,4,7248,7255,"Road-boundary detection is important for autonomous driving. It can be used to constrain autonomous vehicles running on road areas to ensure driving safety. Compared with online road-boundary detection using on-vehicle cameras/Lidars, offline detection using aerial images could alleviate the severe occlusion issue. Moreover, the offline detection results can be directly employed to annotate high-definition (HD) maps. In recent years, deep-learning technologies have been used in offline detection. But there still lacks a publicly available dataset for this task, which hinders the research progress in this area. So in this letter, we propose a new benchmark dataset, named Topo-boundary, for offline topological road-boundary detection. The dataset contains 25,295 1000×1000-sized 4-channel aerial images. Each image is provided with 8 training labels for different sub-tasks. We also design a new entropy-based metric for connectivity evaluation, which could better handle noises or outliers. We implement and evaluate 3 segmentation-based baselines and 5 graph-based baselines using the dataset. We also propose a new imitation-learning-based baseline which is enhanced from our previous work. The superiority of our enhancement is demonstrated from the comparison. The dataset and our-implemented code for the baselines are available at https://tonyxuqaq.github.io/Topo-boundary/.",2377-3766,,10.1109/LRA.2021.3097512,Collaborative Research Fund; Research Grants Council Hong Kong(grant numbers:C4063-18G); Department of Science and Technology of Guangdong Province Fund(grant numbers:GDST20EG54); Zhongshan Municipal Science and Technology Bureau Fund(grant numbers:ZSST21EG06); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9488209,Road-boundary detection;imitation learning;large-scale dataset;autonomous driving,Roads;Measurement;Task analysis;Object detection;Image edge detection;Semantics;Databases,,31.0,,25,IEEE,16 Jul 2021,,,IEEE,IEEE Journals,https://tonyxuqaq.github.io/Topo-boundary,https://github.com/TonyXuQAQ/Topo-boundary
25,"R $^2$ LIVE: A Robust, Real-Time, LiDAR-Inertial-Visual Tightly-Coupled State Estimator and Mapping",J. Lin; C. Zheng; W. Xu; F. Zhang,"Department of Mechanical Engineering, The University of Hong Kong, Hong Kong SAR, China; Department of Mechanical Engineering, The University of Hong Kong, Hong Kong SAR, China; Department of Mechanical Engineering, The University of Hong Kong, Hong Kong SAR, China; Department of Mechanical Engineering, The University of Hong Kong, Hong Kong SAR, China",IEEE Robotics and Automation Letters,3 Aug 2021,2021,6,4,7469,7476,"In this letter, we propose a robust, real-time tightly-coupled multi-sensor fusion framework, which fuses measurements from LiDAR, inertial sensor, and visual camera to achieve robust and accurate state estimation. Our proposed framework is composed of two parts: the filter-based odometry and factor graph optimization. To guarantee real-time performance, we estimate the state within the framework of error-state iterated Kalman-filter, and further improve the overall precision with our factor graph optimization. Taking advantage of measurements from all individual sensors, our algorithm is robust enough to various visual failure, LiDAR-degenerated scenarios, and is able to run in real time on an on-board computation platform, as shown by extensive experiments conducted in indoor, outdoor, and mixed environments of different scale (see attached video). Moreover, the results show that our proposed framework can improve the accuracy of state-of-the-art LiDAR-inertial or visual-inertial odometry. To share our findings and to make contributions to the community, we open source our codes on our Github.",2377-3766,,10.1109/LRA.2021.3095515,DJI(grant numbers:200009538); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9478206,SLAM;mapping;sensor fusion,Laser radar;Visualization;Cameras;Optimization;Real-time systems;Kalman filters;Simultaneous localization and mapping,,160.0,,30,IEEE,8 Jul 2021,,,IEEE,IEEE Journals,,
26,LiDARTag: A Real-Time Fiducial Tag System for Point Clouds,J. -K. Huang; S. Wang; M. Ghaffari; J. W. Grizzle,"Robotics Institute, University of Michigan, Ann Arbor, Michigan, MI, USA; Robotics Institute, University of Michigan, Ann Arbor, Michigan, MI, USA; Robotics Institute, University of Michigan, Ann Arbor, Michigan, MI, USA; Robotics Institute, University of Michigan, Ann Arbor, Michigan, MI, USA",IEEE Robotics and Automation Letters,20 Apr 2021,2021,6,3,4875,4882,"Image-based fiducial markers are useful in problems such as object tracking in cluttered or textureless environments, camera (and multi-sensor) calibration tasks, and vision-based simultaneous localization and mapping (SLAM). The state-of-the-art fiducial marker detection algorithms rely on the consistency of the ambient lighting. This article introduces LiDARTag, a novel fiducial tag design and detection algorithm suitable for light detection and ranging (LiDAR) point clouds. The proposed method runs in real-time and can process data at 100 Hz, which is faster than the currently available LiDAR sensor frequencies. Because of the LiDAR sensors' nature, rapidly changing ambient lighting will not affect the detection of a LiDARTag; hence, the proposed fiducial marker can operate in a completely dark environment. In addition, the LiDARTag nicely complements and is compatible with existing visual fiducial markers, such as AprilTags, allowing for efficient multi-sensor fusion and calibration tasks. We further propose a concept of minimizing a fitting error between a point cloud and the marker's template to estimate the marker's pose. The proposed method achieves millimeter error in translation and a few degrees in rotation. Due to LiDAR returns' sparsity, the point cloud is lifted to a continuous function in a reproducing kernel Hilbert space where the inner product can be used to determine a marker's ID. The experimental results, verified by a motion capture system, confirm that the proposed method can reliably provide a tag's pose and unique ID code. The rejection of false positives is validated on the Google Cartographer indoor dataset and the Honda H3D outdoor dataset. All implementations are coded in C++ and are available at https://github.com/UMich-BipedLab/LiDARTag.",2377-3766,,10.1109/LRA.2021.3070302,Toyota Research Institute; NSF(grant numbers:1808051); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9392337,Range sensing;visual tracking;object detection;segmentation and categorization;computer vision for automation,Laser radar;Three-dimensional displays;Cameras;Payloads;Lighting;Detectors;Visualization,,22.0,,51,IEEE,31 Mar 2021,,,IEEE,IEEE Journals,https://github.com/UMich-BipedLab/LiDARTag,https://github.com/UMich-BipedLab/LiDARTag
27,A Surface Geometry Model for LiDAR Depth Completion,Y. Zhao; L. Bai; Z. Zhang; X. Huang,"Department of Electrical and Computer Engineering, Worcester Polytechnic Institute, Worcester, MA, USA; Department of Electrical and Computer Engineering, Worcester Polytechnic Institute, Worcester, MA, USA; Department of Electrical and Computer Engineering, Worcester Polytechnic Institute, Worcester, MA, USA; Department of Electrical and Computer Engineering, Worcester Polytechnic Institute, Worcester, MA, USA",IEEE Robotics and Automation Letters,14 Apr 2021,2021,6,3,4457,4464,"LiDAR depth completion is a task that predicts depth values for every pixel on the corresponding camera frame, although only sparse LiDAR points are available. Most of the existing state-of-the-art solutions are based on deep neural networks, which need a large amount of data and heavy computations for training the models. In this letter, a novel non-learning depth completion method is proposed by exploiting the local surface geometry that is enhanced by an outlier removal algorithm. The proposed surface geometry model is inspired by the observation that most pixels with unknown depth have a nearby LiDAR point. Therefore, it is assumed those pixels share the same surface with the nearest LiDAR point, and their respective depth can be estimated as the nearest LiDAR depth value plus a residual error. The residual error is calculated by using a derived equation with several physical parameters as input, including the known camera intrinsic parameters, estimated normal vector, and offset distance on the image plane. The proposed method is further enhanced by an outlier removal algorithm that is designed to remove incorrectly mapped LiDAR points from occluded regions. On KITTI dataset, the proposed solution achieves the best error performance among all existing non-learning methods and is comparable to the best self-supervised learning method and some supervised learning methods. Moreover, since outlier points from occluded regions is a commonly existing problem, the proposed outlier removal algorithm is a general preprocessing step that is applicable to many robotic systems with both camera and LiDAR sensors. The code has been published at https://github.com/placeforyiming/RAL_Non-Learning_DepthCompletion.",2377-3766,,10.1109/LRA.2021.3068885,U.S. NSF(grant numbers:CCF-2006738); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9387169,Computer vision for transportation;range sensing;sensor fusion,Laser radar;Cameras;Task analysis;Geometry;Sensors;Three-dimensional displays;Neural networks,,30.0,,43,IEEE,25 Mar 2021,,,IEEE,IEEE Journals,https://github.com/placeforyiming/RAL_Non-Learning_DepthCompletion,https://github.com/placeforyiming/RAL_Non-Learning_DepthCompletion
28,Attentional Graph Neural Network for Parking-Slot Detection,C. Min; J. Xu; L. Xiao; D. Zhao; Y. Nie; B. Dai,"Unmanned Systems Research Center, National Innovation Institute of Defense Technology, Beijing, China; Unmanned Systems Research Center, National Innovation Institute of Defense Technology, Beijing, China; Unmanned Systems Research Center, National Innovation Institute of Defense Technology, Beijing, China; Unmanned Systems Research Center, National Innovation Institute of Defense Technology, Beijing, China; Unmanned Systems Research Center, National Innovation Institute of Defense Technology, Beijing, China; Unmanned Systems Research Center, National Innovation Institute of Defense Technology, Beijing, China",IEEE Robotics and Automation Letters,24 Mar 2021,2021,6,2,3445,3450,"Deep learning has recently demonstrated its promising performance for vision-based parking-slot detection. However, very few existing methods explicitly take into account learning the link information of the marking-points, resulting in complex post-processing and erroneous detection. In this letter, we propose an attentional graph neural network based parking-slot detection method, which refers the marking-points in an around-view image as graph-structured data and utilize graph neural network to aggregate the neighboring information between marking-points. Without any manually designed post-processing, the proposed method is end-to-end trainable. Extensive experiments have been conducted on public benchmark dataset, where the proposed method achieves state-of-the-art accuracy. Code is publicly available at https://github.com/Jiaolong/gcn-parking-slot.",2377-3766,,10.1109/LRA.2021.3064270,"National Natural Science Foundation of China(grant numbers:61803380,61790565); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9372817,Deep learning for visual perception;graph neural network;parking-slot detection;recognition,Feature extraction;Graph neural networks;Detectors;Manuals;Aggregates;Training;Shape,,21.0,,26,IEEE,8 Mar 2021,,,IEEE,IEEE Journals,https://github.com/Jiaolong/gcn-parking-slot,https://github.com/Jiaolong/gcn-parking-slot
29,"FAST-LIO: A Fast, Robust LiDAR-Inertial Odometry Package by Tightly-Coupled Iterated Kalman Filter",W. Xu; F. Zhang,"Mechatronics and Robotic Systems (MaRS) Laboratory, Department of Mechanical Engineering, University of Hong Kong, Hong Kong; Mechatronics and Robotic Systems (MaRS) Laboratory, Department of Mechanical Engineering, University of Hong Kong, Hong Kong",IEEE Robotics and Automation Letters,24 Mar 2021,2021,6,2,3317,3324,"This letter presents a computationally efficient and robust LiDAR-inertial odometry framework. We fuse LiDAR feature points with IMU data using a tightly-coupled iterated extended Kalman filter to allow robust navigation in fast-motion, noisy or cluttered environments where degeneration occurs. To lower the computation load in the presence of a large number of measurements, we present a new formula to compute the Kalman gain. The new formula has computation load depending on the state dimension instead of the measurement dimension. The proposed method and its implementation are tested in various indoor and outdoor environments. In all tests, our method produces reliable navigation results in real-time: running on a quadrotor onboard computer, it fuses more than 1200 effective feature points in a scan and completes all iterations of an iEKF step within 25 ms. Our codes are open-sourced on Github.1",2377-3766,,10.1109/LRA.2021.3064227,DJI(grant numbers:200009538); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9372856,Aerial systems;localization;perception and autonomy;sensor fusion,Laser radar;Kalman filters;Fuses;Feature extraction;Distortion measurement;Three-dimensional displays;Noise measurement,,442.0,,28,IEEE,8 Mar 2021,,,IEEE,IEEE Journals,,
30,BALM: Bundle Adjustment for Lidar Mapping,Z. Liu; F. Zhang,"Department of Mechanical Engineering, University of Hong Kong, Hong Kong, China; Department of Mechanical Engineering, University of Hong Kong, Hong Kong, China",IEEE Robotics and Automation Letters,22 Mar 2021,2021,6,2,3184,3191,"A local Bundle Adjustment (BA) on a sliding window of keyframes has been widely used in visual SLAM and proved to be very effective in lowering the drift. But in lidar SLAM, BA method is hardly used because the sparse feature points (e.g., edge and plane) make the exact point matching impossible. In this letter, we formulate the lidar BA as minimizing the distance from a feature point to its matched edge or plane. Unlike the visual SLAM (and prior plane adjustment method in lidar SLAM) where the feature has to be co-determined along with the pose, we show that the feature can be analytically solved and removed from the BA, the resultant BA is only dependent on the scan poses. This greatly reduces the optimization scale and allows large-scale dense plane and edge features to be used. To speedup the optimization, we derive the analytical derivatives of the cost function, up to second order, in closed form. Moreover, we propose a novel adaptive voxelization method to search feature correspondence efficiently. The proposed formulations are incorporated into a LOAM back-end for map refinement. Results show that, although as a back-end, the local BA can be solved very efficiently, even in real-time at 10 Hz when optimizing 20 scans of point-cloud. The local BA also considerably lowers the LOAM drift. Our implementation of the BA optimization and LOAM are open-sourced to benefit the community.11https://github.com/hku-mars/BALM.",2377-3766,,10.1109/LRA.2021.3062815,DJI(grant numbers:200009538); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9366383,Bundle adujustment;lidar;localization;mapping;SLAM,Laser radar;Visualization;Simultaneous localization and mapping;Cost function;Bundle adjustment;Eigenvalues and eigenfunctions;Navigation,,127.0,,39,IEEE,1 Mar 2021,,,IEEE,IEEE Journals,https://github.com/hku-mars/BALM,https://github.com/hku-mars/BALM
31,Learning Multi-Object Dense Descriptor for Autonomous Goal-Conditioned Grasping,S. Yang; W. Zhang; R. Song; J. Cheng; Y. Li,"School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China",IEEE Robotics and Automation Letters,6 Apr 2021,2021,6,2,4109,4116,"In a goal-conditioned grasping task, a robot is asked to grasp the objects designated by a user. Existing methods for goal-conditioned grasping either can only handle relatively simple scenes or require extra user annotations. This letter proposes an autonomous method to enable the grasping of target object in a challenging yet general scene that contains multiple objects of different classes. It can effectively learn a dense descriptor and integrate it with a newly designed grasp affordance model. The proposed method is a self-supervised pipeline trained without any human supervision or robotic sampling. We validate our method via both simulated and real-world experiments while the training relies only on a variety of synthetic data, demonstrating a good generalization capability. Supplementary video demonstrations and material are available at https://vsislab.github.io/agcg/.",2377-3766,,10.1109/LRA.2021.3062300,"National Key Research and Development Plan of China(grant numbers:2018AAA0102504); National Natural Science Foundation of China(grant numbers:U1913204,61991411); Natural Science Foundation of Shandong Province(grant numbers:ZR2020JQ29); Shandong Major Scientific and Technological Innovation(grant numbers:2018CXGC1503); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9363610,Perception for Grasping manipulation;representation learning,Grasping;Affordances;Robots;Task analysis;Predictive models;Annotations;Visualization,,15.0,,34,IEEE,25 Feb 2021,,,IEEE,IEEE Journals,https://vsislab.github.io/agcg,
32,Robust LiDAR Feature Localization for Autonomous Vehicles Using Geometric Fingerprinting on Open Datasets,N. Steinke; C. -N. Ritter; D. Goehring; R. Rojas,"Dahlem Center for Machine Learning and Robotics (DCMLR), Department of Mathematics and Computer Science, Freie Universität Berlin, Berlin, Germany; Dahlem Center for Machine Learning and Robotics (DCMLR), Department of Mathematics and Computer Science, Freie Universität Berlin, Berlin, Germany; Dahlem Center for Machine Learning and Robotics (DCMLR), Department of Mathematics and Computer Science, Freie Universität Berlin, Berlin, Germany; Dahlem Center for Machine Learning and Robotics (DCMLR), Department of Mathematics and Computer Science, Freie Universität Berlin, Berlin, Germany",IEEE Robotics and Automation Letters,18 Mar 2021,2021,6,2,2761,2767,"Localization is a key task for autonomous vehicles. It is often solved with GNSS but due to multipath the performance is often not sufficient. Feature localization systems using LiDAR can deliver an accurate localization but the creation of the necessary feature maps is an effortful task. With digitization of urban planning processes a lot of street level data is being generated and increasingly becomes openly available. We propose a novel feature localization system which utilizes geometric fingerprinting to robustly associate features to a feature map generated from this open data from the city of Berlin. With this association, we perform a precise localization of a vehicle in areas spanning over several square kilometers using an optional IMU, the vehicle's CAN-odometry and an initial pose estimate. We evaluated our system with our autonomous car in real world scenarios and achieved a centimeter precision localization accuracy outperforming a high-cost GNSS. The source code will be published at https://github.com/dcmlr/fingerprint-localization.",2377-3766,,10.1109/LRA.2021.3062354,Bundesministerium für Verkehr und Digitale Infrastruktur; Automatisiertes und Vernetztes Fahren auf digitalen Testfeldern in Deutschland; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9363614,Localization;mapping;recognition,Location awareness;Feature extraction;Laser radar;Robot sensing systems;Three-dimensional displays;Detectors;Automobiles,,12.0,,16,IEEE,25 Feb 2021,,,IEEE,IEEE Journals,https://github.com/dcmlr/fingerprint-localization,https://github.com/dcmlr/fingerprint-localization
33,"Cross-Modal Contrastive Learning of Representations for Navigation Using Lightweight, Low-Cost Millimeter Wave Radar for Adverse Environmental Conditions",J. -T. Huang; C. -L. Lu; P. -K. Chang; C. -I. Huang; C. -C. Hsu; Z. L. Ewe; P. -J. Huang; H. -C. Wang,"Institute of Electrical and Control Engineering, National Yang Ming Chiao Tung University (NYCU), Hsinchu, Taiwan; Institute of Electrical and Control Engineering, National Yang Ming Chiao Tung University (NYCU), Hsinchu, Taiwan; Institute of Electrical and Control Engineering, National Yang Ming Chiao Tung University (NYCU), Hsinchu, Taiwan; Institute of Electrical and Control Engineering, National Yang Ming Chiao Tung University (NYCU), Hsinchu, Taiwan; Institute of Electrical and Control Engineering, National Yang Ming Chiao Tung University (NYCU), Hsinchu, Taiwan; Institute of Electrical and Control Engineering, National Yang Ming Chiao Tung University (NYCU), Hsinchu, Taiwan; Department of Electrical and Computer Engineering, National Chiao Tung University (NCTU), Hsinchu, Taiwan; Department of Electrical and Computer Engineering, National Chiao Tung University (NCTU), also with the Institute of Electrical and Control Engineering, and National Yang Ming Chiao Tung University (NYCU), and also with the Pervasive Artificial Intelligence Research (PAIR) Labs, Hsinchu, Taiwan",IEEE Robotics and Automation Letters,24 Mar 2021,2021,6,2,3333,3340,"Deep reinforcement learning (RL), where the agent learns from mistakes, has been successfully applied to a variety of tasks. With the aim of learning collision-free policies for unmanned vehicles, deep RL has been used for training with various types of data, such as colored images, depth images, and LiDAR point clouds, without the use of classic map-localize-plan approaches. However, existing methods are limited by their reliance on cameras and LiDAR devices, which have degraded sensing under adverse environmental conditions (e.g., smoky environments). In response, we propose the use of single-chip millimeter-wave (mmWave) radar, which is lightweight and inexpensive, for learning-based autonomous navigation. However, because mmWave radar signals are often noisy and sparse, we propose a cross-modal contrastive learning of representations (CM-CLR) method that maximizes the agreement between mmWave radar data and LiDAR data in the training stage. We evaluated our method in real-world robot compared with 1) a method with two separate networks using cross-modal generative reconstruction and an RL policy and 2) a baseline RL policy without cross-modal representations. Our proposed end-to-end deep RL policy with contrastive learning successfully navigated the robot through smoke-filled maze environments and achieved better performance compared with generative reconstruction methods, in which noisy artifact walls or obstacles were produced. All pretrained models and hardware settings are open access for reproducing this study and can be obtained at https://arg-nctu.github.io/projects/deeprl-mmWave.html.",2377-3766,,10.1109/LRA.2021.3062011,"Ministry of Science and Technology, Taiwan(grant numbers:MOST 109-2634-F-009-019,MOST 110-2634-F-009-022,MOST 109-2634-F-009-027); Pervasive Artificial Intelligence Research(grant numbers:MOST 109-2321-B-009-006,MOST 109- 2224-E-007-004,MOST 109-2221-E-009-074); Qualcomm Taiwan University; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9362209,Representation learning;search and rescue robots;collision avoidance,Laser radar;Radar;Navigation;Sensors;Training;Robot sensing systems;Robots,,16.0,,31,IEEE,24 Feb 2021,,,IEEE,IEEE Journals,https://arg-nctu.github.io/projects/deeprl-mmWave,https://github.com/ganymede42/h5pyViewer
34,PREGAN: Pose Randomization and Estimation for Weakly Paired Image Style Translation,Z. Chen; J. Guo; X. Xu; Y. Wang; H. Huang; Y. Wang; R. Xiong,"State Key Laboratory of Industrial Control Technology, and Institute of Cyber-Systems and Control, Zhejiang University, Zhejiang, China; State Key Laboratory of Industrial Control Technology, and Institute of Cyber-Systems and Control, Zhejiang University, Zhejiang, China; State Key Laboratory of Industrial Control Technology, and Institute of Cyber-Systems and Control, Zhejiang University, Zhejiang, China; State Key Laboratory of Industrial Control Technology, and Institute of Cyber-Systems and Control, Zhejiang University, Zhejiang, China; Beijing Institute of Control Engineering, Beijing, China; State Key Laboratory of Industrial Control Technology, and Institute of Cyber-Systems and Control, Zhejiang University, Zhejiang, China; State Key Laboratory of Industrial Control Technology, and Institute of Cyber-Systems and Control, Zhejiang University, Zhejiang, China",IEEE Robotics and Automation Letters,11 Mar 2021,2021,6,2,2209,2216,"Utilizing the trained model under different conditions without data annotation is attractive for robot applications. Towards this goal, one class of methods is to translate the image style from another environment to the one on which models are trained. In this letter, we propose a weakly-paired setting for the style translation, where the content in the two images is aligned with errors in poses. These images could be acquired by different sensors in different conditions that share an overlapping region, e.g., with LiDAR or stereo cameras, from sunny days or foggy nights. We consider this setting to be more practical with: (i) easier labeling than the paired data; (ii) better interpretability and detail retrieval than the unpaired data. To translate across such images, we propose PREGAN to train a style translator by intentionally transforming the two images with a random pose, and to estimate the given random pose by differentiable non-trainable pose estimator given that the more aligned in style, the better the estimated result is. Such adversarial training enforces the network to learn the style translation, avoiding being entangled with other variations. Finally, PREGAN is validated on both simulated and real-world collected data to show the effectiveness. Results on down-stream tasks, classification, road segmentation, object detection, and feature matching show its potential for real applications. https://github.com/wrld/PRoGAN.",2377-3766,,10.1109/LRA.2021.3061359,National Key Research and Development Program of China(grant numbers:2018AAA0102700); National Natural Science Foundation of China(grant numbers:61903332); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9361304,Transfer learning;computer vision for automation;domain randomization,Task analysis;Gallium nitride;Robots;Training;Generative adversarial networks;Data models;Sensors,,2.0,,28,IEEE,23 Feb 2021,,,IEEE,IEEE Journals,https://github.com/wrld/PRoGAN,https://github.com/wrld/PRoGAN
35,Deep Samplable Observation Model for Global Localization and Kidnapping,R. Chen; H. Yin; Y. Jiao; G. Dissanayake; Y. Wang; R. Xiong,"Zhejiang University, Zhejiang, China; Zhejiang University, Zhejiang, China; Zhejiang University, Zhejiang, China; Center for Autonomous System, University of Technology Sydney, Ultimo, NSW, Australia; Zhejiang University, Zhejiang, China; Zhejiang University, Zhejiang, China",IEEE Robotics and Automation Letters,12 Mar 2021,2021,6,2,2296,2303,"Global localization and kidnapping are two challenging problems in robot localization. The popular method, Monte Carlo Localization (MCL) addresses the problem by iteratively updating a set of particles with a “sampling-weighting” loop. Sampling is decisive to the performance of MCL [1]. However, traditional MCL can only sample from a uniform distribution over the state space. Although variants of MCL propose different sampling models, they fail to provide an accurate distribution or generalize across scenes. To better deal with these problems, we present a distribution proposal model named Deep Samplable Observation Model (DSOM). DSOM takes a map and a 2D laser scan as inputs and outputs a conditional multimodal probability distribution of the pose, making the samples more focusing on the regions with higher likelihood. With such samples, the convergence is expected to be more effective and efficient. Considering that the learning-based sampling model may fail to capture the accurate pose sometimes, we furthermore propose the Adaptive Mixture MCL (AdaM MCL), which deploys a trusty mechanism to adaptively select updating mode for each particle to tolerate this situation. Equipped with DSOM, AdaM MCL can achieve more accurate estimation, faster convergence and better scalability than previous methods in both synthetic and real scenes. Even in real environments with long-term changes, AdaM MCL is able to localize the robot using DSOM trained only by simulation observations from a SLAM map or a blueprint map. Source code for this paper is available here: https://github.com/Runjian-Chen/AdaM_MCL.",2377-3766,,10.1109/LRA.2021.3061339,National Natural Science Foundation of China(grant numbers:61903332); Zhejiang Province Public Welfare Technology Application Research Project(grant numbers:LGG21F030012); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9361285,Global localization;multimodal;samplable observation model,Robots;Location awareness;Proposals;Probability distribution;Adaptation models;Feature extraction;Two dimensional displays,,12.0,,27,IEEE,23 Feb 2021,,,IEEE,IEEE Journals,https://github.com/Runjian-Chen/AdaM_MCL,https://github.com/Runjian-Chen/AdaM_MCL
36,Adversarial Inverse Reinforcement Learning With Self-Attention Dynamics Model,J. Sun; L. Yu; P. Dong; B. Lu; B. Zhou,"Chinese University of Hong Kong, Hong Kong, China; Computer Science Department, Stanford University, Mountain View, CA, USA; State Key Laboratory of Digital Manufacturing Equipment and Technology, Huazhong University of Science and Technology, Wuhan, China; Chinese University of Hong Kong, Hong Kong, China; Chinese University of Hong Kong, Hong Kong, China",IEEE Robotics and Automation Letters,9 Mar 2021,2021,6,2,1880,1886,"In many real-world applications where specifying a proper reward function is difficult, it is desirable to learn policies from expert demonstrations. Adversarial Inverse Reinforcement Learning (AIRL) is one of the most common approaches for learning from demonstrations. However, due to the stochastic policy, current computation graph of AIRL is no longer end-to-end differentiable like Generative Adversarial Networks (GANs), resulting in the need for high-variance gradient estimation methods and large sample size. In this work, we propose the Model-based Adversarial Inverse Reinforcement Learning (MAIRL), an end-to-end model-based policy optimization method with self-attention. By adopting the self-attention dynamics model to make the computation graph end-to-end differentiable, MAIRL has the low variance for policy optimization. We evaluate our approach thoroughly on various control tasks. The experimental results show that our approach not only learns near-optimal rewards and policies that match expert behavior but also outperforms previous inverse reinforcement learning algorithms in real robot experiments. Code is available at https://decisionforce.github.io/MAIRL/.",2377-3766,,10.1109/LRA.2021.3061397,InnoHK CPII; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9361118,Imitation learning;learning from demonstration,Computational modeling;Atmospheric modeling;Reinforcement learning;Training;Trajectory;Stochastic processes;Task analysis,,14.0,,27,IEEE,23 Feb 2021,,,IEEE,IEEE Journals,https://decisionforce.github.io/MAIRL,
37,Imitation Learning of Hierarchical Driving Model: From Continuous Intention to Continuous Trajectory,Y. Wang; D. Zhang; J. Wang; Z. Chen; Y. Li; Y. Wang; R. Xiong,"State Key Laboratory of Industrial Control Technology, and Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; State Key Laboratory of Industrial Control Technology, and Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; State Key Laboratory of Industrial Control Technology, and Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; State Key Laboratory of Industrial Control Technology, and Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Intelligent Space Robotics Joint Research Laboratory, Zhejiang Lab, Hangzhou, China; State Key Laboratory of Industrial Control Technology, and Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; State Key Laboratory of Industrial Control Technology, and Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China",IEEE Robotics and Automation Letters,15 Mar 2021,2021,6,2,2477,2484,"One of the challenges to reduce the gap between the machine and the human level driving is how to endow the system with the learning capacity to deal with the coupled complexity of environments, intentions, and dynamics. In this letter, we propose a hierarchical driving model with explicit models of continuous intention and continuous dynamics, which decouples the complexity in the observation-to-action reasoning in the human driving data. Specifically, the continuous intention module takes perception to generate a potential map encoded with obstacles and intentions. Then, the potential map is regarded as a condition, together with the current dynamics, to generate a continuous trajectory as output by a continuous function approximator network, whose derivatives can be used for supervision without additional parameters. Finally, our method is validated by both datasets and stimulation, demonstrating that our method has higher prediction accuracy of displacement and velocity and generates smoother trajectories. Our method is also deployed on the real vehicle with loop latency, validating its effectiveness. To the best of our knowledge, this is the first work to produce the driving trajectory using a continuous function approximator network. Our code is available at https://github.com/ZJU-Robotics-Lab/CICT.",2377-3766,,10.1109/LRA.2021.3061336,"National Natural Science Foundation of China(grant numbers:61903332); State Administration of Science, Technology, and Industry for National Defence Grant(grant numbers:HTKJ2019KL502005); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9361054,Imitation learning;motion and path planning;vision-based navigation,Trajectory;Planning;Vehicle dynamics;Data models;Routing;Robot kinematics;Cognition,,12.0,,29,IEEE,23 Feb 2021,,,IEEE,IEEE Journals,https://github.com/ZJU-Robotics-Lab/CICT,https://github.com/ZJU-Robotics-Lab/CICT
38,OV$^{2}$SLAM: A Fully Online and Versatile Visual SLAM for Real-Time Applications,M. Ferrera; A. Eudes; J. Moras; M. Sanfourche; G. Le Besnerais,"DTIS, ONERA, Université Paris-Saclay, Palaiseau, France; DTIS, ONERA, Université Paris-Saclay, Palaiseau, France; DTIS, ONERA, Université Paris-Saclay, Palaiseau, France; DTIS, ONERA, Université Paris-Saclay, Palaiseau, France; DTIS, ONERA, Université Paris-Saclay, Palaiseau, France",IEEE Robotics and Automation Letters,23 Feb 2021,2021,6,2,1399,1406,"Many applications of Visual SLAM, such as augmented reality, virtual reality, robotics or autonomous driving, require versatile, robust and precise solutions, most often with real-time capability. In this work, we describe OV2SLAM, a fully online algorithm, handling both monocular and stereo camera setups, various map scales and frame-rates ranging from a few Hertz up to several hundreds. It combines numerous recent contributions in visual localization within an efficient multi-threaded architecture. Extensive comparisons with competing algorithms shows the state-of-the-art accuracy and real-time performance of the resulting algorithm. For the benefit of the community, we release the source code: https://github.com/ov2slam/ov2slam.",2377-3766,,10.1109/LRA.2021.3058069,ANR/DGA; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9351614,SLAM;localization;mapping;field robotics,Simultaneous localization and mapping;Cameras;Three-dimensional displays;Feature extraction;Tracking;Visualization;Real-time systems,,55.0,,36,IEEE,9 Feb 2021,,,IEEE,IEEE Journals,https://github.com/ov2slam/ov2slam,https://github.com/ov2slam/ov2slam
39,iCurb: Imitation Learning-Based Detection of Road Curbs Using Aerial Images for Autonomous Driving,Z. Xu; Y. Sun; M. Liu,"Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong; Department of Mechanical Engineering, The Hong Kong Polytechnic University, Kowloon, Hung Hom, Hong Kong; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong",IEEE Robotics and Automation Letters,15 Feb 2021,2021,6,2,1097,1104,"Detection of road curbs is an essential capability for autonomous driving. It can be used for autonomous vehicles to determine drivable areas on roads. Usually, road curbs are detected on-line using vehicle-mounted sensors, such as video cameras and 3-D Lidars. However, on-line detection using video cameras may suffer from challenging illumination conditions, and Lidar-based approaches may be difficult to detect far-away road curbs due to the sparsity issue of point clouds. In recent years, aerial images are becoming more and more worldwide available. We find that the visual appearances between road areas and off-road areas are usually different in aerial images, so we propose a novel solution to detect road curbs off-line using aerial images. The input to our method is an aerial image, and the output is directly a graph (i.e., vertices and edges) representing road curbs. To this end, we formulate the problem as an imitation learning problem, and design a novel network and an innovative training strategy to train an agent to iteratively find the road-curb graph. The experimental results on a public dataset confirm the effectiveness and superiority of our method. This work is accompanied with a demonstration video and a supplementary document at https://tonyxuqaq.github.io/iCurb/.",2377-3766,,10.1109/LRA.2021.3056344,National Natural Science Foundation of China(grant numbers:U1713211); Collaborative Research Fund by Research Grants Council Hong Kong(grant numbers:C4063-18G); HKUST-SJTU Joint Research Collaboration Fund(grant numbers:SJTU20EG03); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9345473,Road-curb Detection;Graph Representation;Imitation Learning;Autonomous Driving,Roads;Training;Head;Visualization;Image segmentation;Feature extraction;Autonomous vehicles,,29.0,,26,IEEE,2 Feb 2021,,,IEEE,IEEE Journals,https://tonyxuqaq.github.io/iCurb,https://github.com/TonyXuQAQ/iCurb
40,BiTraP: Bi-Directional Pedestrian Trajectory Prediction With Multi-Modal Goal Estimation,Y. Yao; E. Atkins; M. Johnson-Roberson; R. Vasudevan; X. Du,"Robotics Institute, University of Michigan, Ann Arbor, MI, USA; Aerospace Engineering Department, University of Michigan, Ann Arbor, MI, USA; Department of Naval Architecture and Marine Engineering, University of Michigan, Ann Arbor, MI, USA; Department of Mechanical Engineering, University of Michigan, Ann Arbor, MI, USA; Department of Naval Architecture and Marine Engineering, University of Michigan, Ann Arbor, MI, USA",IEEE Robotics and Automation Letters,25 Feb 2021,2021,6,2,1463,1470,"Pedestrian trajectory prediction is an essential task in robotic applications such as autonomous driving and robot navigation. State-of-the-art trajectory predictors use a conditional variational autoencoder (CVAE) with recurrent neural networks (RNNs) to encode observed trajectories and decode multi-modal future trajectories. This process can suffer from accumulated errors over long prediction horizons (≥2 seconds). This letter presents BiTraP, a goal-conditioned bi-directional multi-modal trajectory prediction method based on the CVAE. BiTraP estimates the goal (end-point) of trajectories and introduces a novel bidirectional decoder to improve longer-term trajectory prediction accuracy. Extensive experiments show that BiTraP generalizes to both first-person view (FPV) and bird's-eye view (BEV) scenarios and outperforms state-of-the-art results by ~10-50%. We also show that different choices of non-parametric versus parametric target models in the CVAE directly influence the predicted multimodal trajectory distributions. These results provide guidance on trajectory predictor design for robotic applications such as collision avoidance and navigation systems. Our code is available at: https://github.com/umautobots/bidireaction-trajectory-prediction.",2377-3766,,10.1109/LRA.2021.3056339,Ford Motor Company; Ford-UM Alliance(grant numbers:N028603); Federal Highway Administration(grant numbers:693JJ319000009); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9345445,Computer vision for automation;human and humanoid motion analysis and synthesis;deep learning methods;multi-modal trajectory prediction;goal-conditioned prediction,Trajectory;Bidirectional control;Predictive models;Decoding;Estimation;Robots;Collision avoidance,,106.0,,45,IEEE,2 Feb 2021,,,IEEE,IEEE Journals,https://github.com/umautobots/bidireaction-trajectory-prediction,https://github.com/umautobots/bidireaction-trajectory-prediction
41,Do We Need to Compensate for Motion Distortion and Doppler Effects in Spinning Radar Navigation?,K. Burnett; A. P. Schoellig; T. D. Barfoot,"University of Toronto Institute for Aerospace Studies (UTIAS), Toronto, Ontario, Canada; University of Toronto Institute for Aerospace Studies (UTIAS), Toronto, Ontario, Canada; University of Toronto Institute for Aerospace Studies (UTIAS), Toronto, Ontario, Canada",IEEE Robotics and Automation Letters,1 Feb 2021,2021,6,2,771,778,"In order to tackle the challenge of unfavorable weather conditions such as rain and snow, radar is being revisited as a parallel sensing modality to vision and lidar. Recent works have made tremendous progress in applying spinning radar to odometry and place recognition. However, these works have so far ignored the impact of motion distortion and Doppler effects on spinning-radar-based navigation, which may be significant in the self-driving car domain where speeds can be high. In this work, we demonstrate the effect of these distortions on radar odometry using the Oxford Radar RobotCar Dataset and metric localization using our own data-taking platform. We revisit a lightweight estimator that can recover the motion between a pair of radar scans while accounting for both effects. Our conclusion is that both motion distortion and the Doppler effect are significant in different aspects of spinning radar navigation, with the former more prominent than the latter. Code for this project can be found at: https://github.com/keenan-burnett/yeti_radar_odometry.",2377-3766,,10.1109/LRA.2021.3052439,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9327473,Localization;range sensing;intelligent transportation systems,Radar;Doppler radar;Distortion;Spaceborne radar;Robot sensing systems;Laser radar;Navigation,,54.0,,40,IEEE,18 Jan 2021,,,IEEE,IEEE Journals,https://github.com/keenan-burnett/yeti_radar_odometry,https://github.com/keenan-burnett/yeti_radar_odometry
42,Reinforcement Learning-Based Visual Navigation With Information-Theoretic Regularization,Q. Wu; K. Xu; J. Wang; M. Xu; X. Gong; D. Manocha,"College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; School of Computer Science, National University of Defense Technology, Changsha, China; College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Zhengzhou University, China; College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Department of Computer Science, the University of Maryland, College Park, MD, USA",IEEE Robotics and Automation Letters,25 Jan 2021,2021,6,2,731,738,"To enhance the cross-target and cross-scene generalization of target-driven visual navigation based on deep reinforcement learning (RL), we introduce an information-theoretic regularization term into the RL objective. The regularization maximizes the mutual information between navigation actions and visual observation transforms of an agent, thus promoting more informed navigation decisions. This way, the agent models the action-observation dynamics by learning a variational generative model. Based on the model, the agent generates (imagines) the next observation from its current observation and navigation target. This way, the agent learns to understand the causality between navigation actions and the changes in its observations, which allows the agent to predict the next action for navigation by comparing the current and the imagined next observations. Cross-target and cross-scene evaluations on the AI2-THOR framework show that our method attains at least 10% improvement of average success rate over some state-of-the-art models. We further evaluate our model in two real-world settings: navigation in unseen indoor scenes from a discrete Active Vision Dataset (AVD) and continuous real-world environments with a TurtleBot. We demonstrate that our navigation model is able to successfully achieve navigation tasks in these scenarios.11[Online]. Available: https://github.com/wqynew/RL-based-navigation.",2377-3766,,10.1109/LRA.2020.3048668,"National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2020YFB2010702,2019YFB1707504,2018AAA0102200); National Natural Science Foundation of China(grant numbers:61902419,61772267,61622212,61532003); Aeronautical Science Foundation of China(grant numbers:2019ZE052008); National Science Foundation of Jiangsu Province(grant numbers:BK20190016); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9312496,Model learning for robot control;reinforcement learning;visual-based navigation,Navigation;Visualization;Task analysis;Training;Robots;Robot sensing systems;Robot kinematics,,20.0,,34,IEEE,1 Jan 2021,,,IEEE,IEEE Journals,https://github.com/wqynew/RL-based-navigation,https://github.com/wqynew/RL-based-navigation
43,MoGaze: A Dataset of Full-Body Motions that Includes Workspace Geometry and Eye-Gaze,P. Kratzer; S. Bihlmaier; N. B. Midlagajni; R. Prakash; M. Toussaint; J. Mainprice,"Machine Learning and Robotics Lab, University of Stuttgart, Germany; Humans to Robots Motions Research Group, University of Stuttgart, Stuttgart, Germany; Humans to Robots Motions Research Group, University of Stuttgart, Stuttgart, Germany; Humans to Robots Motions Research Group, University of Stuttgart, Stuttgart, Germany; Learning and Intelligent Systems lab, Berlin, TU, Germany; Machine Learning and Robotics Lab, University of Stuttgart, Germany",IEEE Robotics and Automation Letters,22 Dec 2020,2021,6,2,367,373,"As robots become more present in open human environments, it will become crucial for robotic systems to understand and predict human motion. Such capabilities depend heavily on the quality and availability of motion capture data. However, existing datasets of full-body motion rarely include 1) long sequences of manipulation tasks, 2) the 3D model of the workspace geometry, and 3) eye-gaze, which are all important when a robot needs to predict the movements of humans in close proximity. Hence, in this letter, we present a novel dataset of full-body motion for everyday manipulation tasks, which includes the above. The motion data was captured using a traditional motion capture system based on reflective markers. We additionally captured eye-gaze using a wearable pupil-tracking device. As we show in experiments, the dataset can be used for the design and evaluation of full-body motion prediction algorithms. Furthermore, our experiments show eye-gaze as a powerful predictor of human intent. The dataset includes 180 min of motion capture data with 1627 pick and place actions being performed. It is available at https://humans-to-robots-motion.github.io/mogaze/MoGaze, Dataset and is planned to be extended to collaborative tasks with two humans in the near future.",2377-3766,,10.1109/LRA.2020.3043167,"German Federal Ministry for Science, Research and Arts; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9286421,Datasets for human motion;human-centered robotics;modeling and simulating humans,Robots;Cameras;Task analysis;Tracking;Three-dimensional displays;Pupils;Linux,,21.0,,26,IEEE,8 Dec 2020,,,IEEE,IEEE Journals,https://humans-to-robots-motion.github.io/mogaze/MoGaze,
44,LiPMatch: LiDAR Point Cloud Plane Based Loop-Closure,J. Jiang; J. Wang; P. Wang; P. Bao; Z. Chen,"Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automatic Control and Systems Engineering, The University of Sheffield, Sheffield, U.K.; Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China",IEEE Robotics and Automation Letters,15 Sep 2020,2020,5,4,6861,6868,"This letter presents a point cloud based loop-closure method to correct long-term drifts in Light Detection and Ranging based Simultaneous Localization and Mapping systems. In the method, we formulate each keyframe as a fully-connected graph with nodes representing planes. To detect loop-closures, the proposed method employs geometric restrictions to define a similarity metric to match current keyframe and those in the map. After similarity assessment, the candidate keyframes which comply with the geometric restrictions are further checked out successively by normal constraints of planes, and validated by an improved Iterative Closest Point method. The latter also provides relative pose transformation estimation between the current keyframe and the matched keyframe in the global reference frame. Experimental results demonstrate that the proposed method is able to fulfill fast and reliable loop-closure. To benefit the community by serving a benchmark for loop-closure, the entire system is made open source on GitHub3.",2377-3766,,10.1109/LRA.2020.3021374,National Natural Science Foundation of China(grant numbers:91848111); National Natural Science Foundation of China(grant numbers:61703387); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9185044,Localization;mapping;range sensing;slam,Three-dimensional displays;Laser radar;Simultaneous localization and mapping;Iterative closest point algorithm;Reliability;Feature extraction,,20.0,,28,IEEE,2 Sep 2020,,,IEEE,IEEE Journals,,
45,IDDA: A Large-Scale Multi-Domain Dataset for Autonomous Driving,E. Alberti; A. Tavera; C. Masone; B. Caputo,"Department of Control and Computer Engineering, Politecnico di Torino, Turin, Italy; Department of Control and Computer Engineering, Politecnico di Torino, Turin, Italy; Italdesign Giugiaro S.p.A., Turin, Italy; Department of Control and Computer Engineering, Politecnico di Torino, Turin, Italy",IEEE Robotics and Automation Letters,20 Jul 2020,2020,5,4,5526,5533,"Semantic segmentation is key in autonomous driving. Using deep visual learning architectures is not trivial in this context, because of the challenges in creating suitable large scale annotated datasets. This issue has been traditionally circumvented through the use of synthetic datasets, that have become a popular resource in this field. They have been released with the need to develop semantic segmentation algorithms able to close the visual domain shift between the training and test data. Although exacerbated by the use of artificial data, the problem is extremely relevant in this field even when training on real data. Indeed, weather conditions, viewpoint changes and variations in the city appearances can vary considerably from car to car, and even at test time for a single, specific vehicle. How to deal with domain adaptation in semantic segmentation, and how to leverage effectively several different data distributions (source domains) are important research questions in this field. To support work in this direction, this letter contributes a new large scale, synthetic dataset for semantic segmentation with more than 100 different source visual domains. The dataset has been created to explicitly address the challenges of domain shift between training and test data in various weather and view point conditions, in seven different city types. Extensive benchmark experiments assess the dataset, showcasing open challenges for the current state of the art. The dataset will be available at: https://idda-dataset.github.io/home/.",2377-3766,,10.1109/LRA.2020.3009075,Italdesign Giugiaro S.p.A; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9140347,Semantic scene understanding;deep learning for visual perception;computer vision for transportation,Semantics;Meteorology;Autonomous vehicles;Sensors;Engines;Benchmark testing;Training,,34.0,,26,IEEE,14 Jul 2020,,,IEEE,IEEE Journals,https://idda-dataset.github.io/home,
46,Unsupervised Domain Adaptation Through Inter-Modal Rotation for RGB-D Object Recognition,M. R. Loghmani; L. Robbiano; M. Planamente; K. Park; B. Caputo; M. Vincze,"ACIN, Vision4Robotics Group, Vienna, Austria; VANDAL Laboratory, Politecnico di Torino and Italian Institute of Technology, Torino, Italy; VANDAL Laboratory, Politecnico di Torino and Italian Institute of Technology, Torino, Italy; ACIN, Vision4Robotics Group, Vienna, Austria; VANDAL Laboratory, Politecnico di Torino and Italian Institute of Technology, Torino, Italy; ACIN, Vision4Robotics Group, Vienna, Austria",IEEE Robotics and Automation Letters,24 Aug 2020,2020,5,4,6631,6638,"Unsupervised Domain Adaptation (DA) exploits the supervision of a label-rich source dataset to make predictions on an unlabeled target dataset by aligning the two data distributions. In robotics, DA is used to take advantage of automatically generated synthetic data, that come with “free” annotation, to make effective predictions on real data. However, existing DA methods are not designed to cope with the multi-modal nature of RGB-D data, which are widely used in robotic vision. We propose a novel RGB-D DA method that reduces the synthetic-to-real domain shift by exploiting the inter-modal relation between the RGB and depth image. Our method consists of training a convolutional neural network to solve, in addition to the main recognition task, the pretext task of predicting the relative rotation between the RGB and depth image. To evaluate our method and encourage further research in this area, we define two benchmark datasets for object categorization and instance recognition. With extensive experiments, we show the benefits of leveraging the inter-modal relations for RGB-D DA. The code is available at: “https://github.com/MRLoghmani/relative-rotation”.",2377-3766,,10.1109/LRA.2020.3007092,EU H2020 research and innovation program; Marie Skłodowska-Curie Grant(grant numbers:676157); ERC(grant numbers:637076); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9133308,RGB-D perception;recognition;visual learning,Task analysis;Solid modeling;Three-dimensional displays;Object recognition;Robots;Training;Annotations,,20.0,,38,IEEE,3 Jul 2020,,,IEEE,IEEE Journals,https://github.com/MRLoghmani/relative-rotation,https://github.com/MRLoghmani/relative-rotation
47,Delta Descriptors: Change-Based Place Representation for Robust Visual Localization,S. Garg; B. Harwood; G. Anand; M. Milford,"School of Electrical Engineering and Robotics, Queensland University of Technology, Brisbane, Australia; Department of Electrical and Computer Systems Engineering, Monash University, Clayton, Australia; School of Computer Science, Queensland University of Technology, Brisbane, Australia; School of Electrical Engineering and Robotics, Queensland University of Technology, Brisbane, Australia",IEEE Robotics and Automation Letters,13 Jul 2020,2020,5,4,5120,5127,"Visual place recognition is challenging because there are so many factors that can cause the appearance of a place to change, from day-night cycles to seasonal change to atmospheric conditions. In recent years a large range of approaches have been developed to address this challenge including deep-learnt image descriptors, domain translation, and sequential filtering, all with shortcomings including generality and velocity-sensitivity. In this letter we propose a novel descriptor derived from tracking changes in any learned global descriptor over time, dubbed Delta Descriptors. Delta Descriptors mitigate the offsets induced in the original descriptor matching space in an unsupervised manner by considering temporal differences across places observed along a route. Like all other approaches, Delta Descriptors have a shortcoming - volatility on a frame to frame basis - which can be overcome by combining them with sequential filtering methods. Using two benchmark datasets, we first demonstrate the high performance of Delta Descriptors in isolation, before showing new state-of-the-art performance when combined with sequence-based matching. We also present results demonstrating the approach working with four different underlying descriptor types, and two other beneficial properties of Delta Descriptors in comparison to existing techniques: their increased inherent robustness to variations in camera motion and a reduced rate of performance degradation as dimensional reduction is applied. Source code is made available at https://github.com/oravus/DeltaDescriptors.",2377-3766,,10.1109/LRA.2020.3005627,AOARD(grant numbers:FA2386-19-1-4079); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9128035,Localization;deep learning for visual perception;recognition;computer vision for automation;autonomous vehicle navigation,Visualization;Robots;Cameras;Deep learning;Australia;Face recognition;Pattern matching,,28.0,,55,IEEE,29 Jun 2020,,,IEEE,IEEE Journals,https://github.com/oravus/DeltaDescriptors,https://github.com/oravus/DeltaDescriptors
48,Denoising IMU Gyroscopes With Deep Learning for Open-Loop Attitude Estimation,M. Brossard; S. Bonnabel; A. Barrau,"MINES ParisTech, Centre for Robotics, PSL Research University, Paris, France; MINES ParisTech, Centre for Robotics, PSL Research University, Paris, France; MINES ParisTech, Centre for Robotics, PSL Research University, Paris, France",IEEE Robotics and Automation Letters,1 Jul 2020,2020,5,3,4796,4803,"This article proposes a learning method for denoising gyroscopes of Inertial Measurement Units (IMUs) using ground truth data, and estimating in real time the orientation (attitude) of a robot in dead reckoning. The obtained algorithm outperforms the state-of-the-art on the (unseen) test sequences. The obtained performances are achieved, thanks to a well-chosen model, a proper loss function for orientation increments, and through the identification of key points when training with high-frequency inertial data. Our approach builds upon a neural network based on dilated convolutions, without requiring any recurrent neural network. We demonstrate how efficient our strategy is for 3D attitude estimation on the EuRoC and TUM-VI datasets. Interestingly, we observe our dead reckoning algorithm manages to beat top-ranked visual-inertial odometry systems in terms of attitude estimation although it does not use vision sensors. We believe this article offers new perspectives for visual-inertial localization and constitutes a step toward more efficient learning methods involving IMUs. Our open-source implementation is available at https://github.com/mbrossar/denoise-imu-gyro.",2377-3766,,10.1109/LRA.2020.3003256,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9119813,Localization;deep learning in robotics and automation;autonomous systems navigation,Accelerometers;Training;Learning systems;Noise measurement;Convolutional neural networks;Deep learning,,71.0,,34,IEEE,17 Jun 2020,,,IEEE,IEEE Journals,https://github.com/mbrossar/denoise-imu-gyro,https://github.com/mbrossar/denoise-imu-gyro
49,Next-Best-Sense: A Multi-Criteria Robotic Exploration Strategy for RFID Tags Discovery,R. Polvara; M. Fernandez-Carmona; G. Neumann; M. Hanheide,"School of Computer Science, Lincoln Center for Autonomous Systems (L-CAS), University of Lincoln, Lincoln, U.K.; School of Computer Science, Lincoln Center for Autonomous Systems (L-CAS), University of Lincoln, Lincoln, U.K.; School of Computer Science, Lincoln Center for Autonomous Systems (L-CAS), University of Lincoln, Lincoln, U.K.; School of Computer Science, Lincoln Center for Autonomous Systems (L-CAS), University of Lincoln, Lincoln, U.K.",IEEE Robotics and Automation Letters,19 Jun 2020,2020,5,3,4477,4484,"Automated exploration is one of the most relevant applications for autonomous robots. In this letter, we propose a novel online coverage algorithm called Next-Best-Sense (NBS), an extension of the Next-Best-View class of exploration algorithms which optimizes the exploration task balancing multiple criteria. NBS is applied to the problem of localizing all Radio Frequency Identification (RFID) tags with a mobile robot. We cast this problem as a coverage planning problem by defining a basic sensing operation - a scan with the RFID reader - as the field of “view” of the sensor. NBS evaluates candidate locations with a global utility function which combines utility values for travel distance, information gain, sensing time, battery status and RFID information gain, generalizing the use of Multi-Criteria Decision Making. We developed an RFID reader and tag model in the Gazebo simulator for validation. Experiments performed both in simulation and with a robot suggest that our NBS approach can successfully localize all the RFID tags while minimizing navigation metrics, such sensing operations, total traveling distance and battery consumption. The code developed is publicly available on the authors' repository.11https://github.com/LCAS/nbs.",2377-3766,,10.1109/LRA.2020.3001539,EPSRC(grant numbers:EP/R02572X/1); National Center for Nuclear Robotics; EU's H2020 research and innovation program(grant numbers:732737); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9113679,Reactive and sensor-based planning;surveillance systems;service robots;RFID;next-best-view,Robot sensing systems;NIST;RFID tags;Planning,,7.0,,27,IEEE,10 Jun 2020,,,IEEE,IEEE Journals,https://github.com/LCAS/nbs,https://github.com/LCAS/nbs
50,EGAD! An Evolved Grasping Analysis Dataset for Diversity and Reproducibility in Robotic Manipulation,D. Morrison; P. Corke; J. Leitner,"Australian Centre for Robotic Vision (ACRV), Queensland University of Technology (QUT), Brisbane, Australia; Australian Centre for Robotic Vision (ACRV), Queensland University of Technology (QUT), Brisbane, Australia; Australian Centre for Robotic Vision (ACRV), Queensland University of Technology (QUT), Brisbane, Australia",IEEE Robotics and Automation Letters,5 Jun 2020,2020,5,3,4368,4375,"We present the Evolved Grasping Analysis Dataset (EGAD), comprising over 2000 generated objects aimed at training and evaluating robotic visual grasp detection algorithms. The objects in EGAD are geometrically diverse, filling a space ranging from simple to complex shapes and from easy to difficult to grasp, compared to other datasets for robotic grasping, which may be limited in size or contain only a small number of object classes. Additionally, we specify a set of 49 diverse 3D-printable evaluation objects to encourage reproducible testing of robotic grasping systems across a range of complexity and difficulty. The dataset, code and videos can be found at https://dougsm.github.io/egad/.",2377-3766,,10.1109/LRA.2020.2992195,Australian Research Council(grant numbers:CE140100016); QUT Centre for Robotics; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9085936,Grasping;performance evaluation and benchmarking;deep learning in grasping and manipulation,Robots;Grasping;Shape;Complexity theory;Three-dimensional displays;Visualization;Search problems,,73.0,,51,IEEE,4 May 2020,,,IEEE,IEEE Journals,https://dougsm.github.io/egad,https://github.com/dougsm/egad
51,Real-Time Soft Body 3D Proprioception via Deep Vision-Based Sensing,R. Wang; S. Wang; S. Du; E. Xiao; W. Yuan; C. Feng,"Tandon School of Engineering, New York University, Brooklyn, USA; Tandon School of Engineering, New York University, Brooklyn, USA; Tandon School of Engineering, New York University, Brooklyn, USA; Tandon School of Engineering, New York University, Brooklyn, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, USA; Tandon School of Engineering, New York University, Brooklyn, USA",IEEE Robotics and Automation Letters,9 Mar 2020,2020,5,2,3382,3389,"Soft bodies made from flexible and deformable materials are popular in many robotics applications, but their proprioceptive sensing has been a long-standing challenge. In other words, there has hardly been a method to measure and model the high-dimensional 3D shapes of soft bodies with internal sensors. We propose a framework to measure the high-resolution 3D shapes of soft bodies in real-time with embedded cameras. The cameras capture visual patterns inside a soft body, and a convolutional neural network (CNN) produces a latent code representing the deformation state, which can then be used to reconstruct the body's 3D shape using another neural network. We test the framework on various soft bodies, such as a Baymax-shaped toy, a latex balloon, and some soft robot fingers, and achieve real-time computation (≤2.5 ms/frame) for robust shape estimation with high precision (≤1% relative error) and high resolution. We believe the method could be applied to soft robotics and human-robot interaction for proprioceptive shape sensing. Our code is available at: https: //ai4ce.github.io/DeepSoRo/.",2377-3766,,10.1109/LRA.2020.2975709,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9006921,"Modeling, control, and learning for soft robots;deep learning in robotics and automation;3D deep learning",Three-dimensional displays;Shape;Robot sensing systems;Soft robotics;Strain,,30.0,,33,IEEE,21 Feb 2020,,,IEEE,IEEE Journals,https://ai4ce.github.io/DeepSoRo,https://github.com/ai4ce/DeepSoRo
52,Fast Underwater Image Enhancement for Improved Visual Perception,M. J. Islam; Y. Xia; J. Sattar,"Interactive Robotics and Vision Laboratory, Department of Computer Science and Engineering, University of Minnesota, Twin Cities, USA; Interactive Robotics and Vision Laboratory, Department of Computer Science and Engineering, University of Minnesota, Twin Cities, USA; Interactive Robotics and Vision Laboratory, Department of Computer Science and Engineering, University of Minnesota, Twin Cities, USA",IEEE Robotics and Automation Letters,6 Mar 2020,2020,5,2,3227,3234,"In this letter, we present a conditional generative adversarial network-based model for real-time underwater image enhancement. To supervise the adversarial training, we formulate an objective function that evaluates the perceptual image quality based on its global content, color, local texture, and style information. We also present EUVP,a large-scale dataset of a paired and an unpaired collection of underwater images (of `poor' and `good' quality) that are captured using seven different cameras over various visibility conditions during oceanic explorations and human-robot collaborative experiments. In addition, we perform several qualitative and quantitative evaluations which suggest that the proposed model can learn to enhance underwater image quality from both paired and unpaired training. More importantly, the enhanced images provide improved performances of standard models for underwater object detection, human pose estimation, and saliency prediction. These results validate that it is suitable for real-time preprocessing in the autonomy pipeline by visually-guided underwater robots. The model and associated training pipelines are available athttps://github.com/xahidbuffon/funie-gan.",2377-3766,,10.1109/LRA.2020.2974710,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9001231,Marine robotics;deep learning in robotics and automation;computer vision for automation,Image enhancement;Training;Gallium nitride;Real-time systems;Generators;Optical distortion;Visual perception,,721.0,,47,IEEE,18 Feb 2020,,,IEEE,IEEE Journals,https://github.com/xahidbuffon/funie-gan,https://github.com/xahidbuffon/funie-gan
53,Low to High Dimensional Modality Hallucination Using Aggregated Fields of View,K. Gunasekar; Q. Qiu; Y. Yang,"Arizona State University, Tempe, USA; Duke University, Durham, USA; Arizona State University, Tempe, USA",IEEE Robotics and Automation Letters,13 Feb 2020,2020,5,2,1983,1990,"Real-world robotics systems deal with data from a multitude of modalities, especially for tasks such as navigation and recognition. The performance of those systems can drastically degrade when one or more modalities become inaccessible, due to factors such as sensors' malfunctions or adverse environments. Here, we argue modality hallucination as one effective way to ensure consistent modality availability and thereby reduce unfavorable consequences. While hallucinating data from a modality with richer information, e.g., RGB to depth, has been researched extensively, we investigate the more challenging low-to-high modality hallucination with interesting use cases in robotics and autonomous systems. We present a novel hallucination architecture that aggregates information from multiple fields of view of the local neighborhood to recover the lost information from the extant modality. The process is implemented by capturing a non-linear mapping between the data modalities and the learned mapping is used to aid the extant modality to mitigate the risk posed to the system in the adverse scenarios which involve modality loss. We also conduct extensive classification and segmentation experiments on UWRGBD and NYUD datasets and demonstrate that hallucination allays the negative effects of the modality loss. Implementation and models: https://github.com/kausic94/Hallucination.",2377-3766,,10.1109/LRA.2020.2970679,National Science Foundation(grant numbers:#1750082); National Robotics Initiative program(grant numbers:#1925403); AWS MLRA; GPU donations from NVIDIA; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8977350,Deep learning in robotics and automation;robot safety;modality hallucination;computer vision for other robotic applications,Kernel;Task analysis;Robot sensing systems;Computer architecture;Semantics,,2.0,,28,IEEE,31 Jan 2020,,,IEEE,IEEE Journals,https://github.com/kausic94/Hallucination,https://github.com/kausic94/Hallucination
54,Semantic Foreground Inpainting From Weak Supervision,C. Lu; G. Dubbelman,"Mobile Perception Systems research lab of the SPS/VCA group, Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, Netherlands; Mobile Perception Systems research lab of the SPS/VCA group, Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, Netherlands",IEEE Robotics and Automation Letters,3 Feb 2020,2020,5,2,1334,1341,"Semantic scene understanding is an essential task for self-driving vehicles and mobile robots. In our work, we aim to estimate a semantic segmentation map, in which the foreground objects are removed and semantically inpainted with background classes, from a single RGB image. This semantic foreground inpainting task is performed by a single-stage convolutional neural network (CNN) that contains our novel max-pooling as inpainting (MPI) module, which is trained with weak supervision, i.e., it does not require manual background annotations for the foreground regions to be inpainted. Our approach is inherently more efficient than the previous two-stage state-of-the-art method, and outperforms it by a margin of 3% IoU for the inpainted foreground regions on Cityscapes. The performance margin increases to 6% IoU, when tested on the unseen KITTI dataset. The code and the manually annotated datasets for testing are shared with the research community at https://github.com/Chenyang-Lu/semantic-foregroundinpainting.",2377-3766,,10.1109/LRA.2020.2967712,Netherlands Organization for Scientific Research; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8963753,Semantic scene understanding;computer vision for transportation;semantic inpainting;weak supervision,Semantics;Task analysis;Image segmentation;Training;Roads;Cognition;Pipelines,,8.0,,27,IEEE,20 Jan 2020,,,IEEE,IEEE Journals,https://github.com/Chenyang-Lu/semantic-foregroundinpainting,https://github.com/Chenyang-Lu/semantic-foregroundinpainting
55,GCNv2: Efficient Correspondence Prediction for Real-Time SLAM,J. Tang; L. Ericson; J. Folkesson; P. Jensfelt,"Centre for Autonomous Systems, KTH Royal Institute of Technology, Stockholm, Sweden; Centre for Autonomous Systems, KTH Royal Institute of Technology, Stockholm, Sweden; Centre for Autonomous Systems, KTH Royal Institute of Technology, Stockholm, Sweden; Centre for Autonomous Systems, KTH Royal Institute of Technology, Stockholm, Sweden",IEEE Robotics and Automation Letters,24 Jul 2019,2019,4,4,3505,3512,"In this letter, we present a deep learning-based network, GCNv2, for generation of keypoints and descriptors. GCNv2 is built on our previous method, GCN, a network trained for 3D projective geometry. GCNv2 is designed with a binary descriptor vector as the ORB feature so that it can easily replace ORB in systems such as ORB-SLAM2. GCNv2 significantly improves the computational efficiency over GCN that was only able to run on desktop hardware. We show how a modified version of ORBSLAM2 using GCNv2 features runs on a Jetson TX2, an embedded low-power platform. Experimental results show that GCNv2 retains comparable accuracy as GCN and that it is robust enough to use for control of a flying drone. Source code is available at: https://github.com/jiexiong2016/GCNv2_SLAM.",2377-3766,,10.1109/LRA.2019.2927954,Wallenberg AI; Autonomous Systems and Software Program (WASP)(grant numbers:XPLORE3D); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8758836,Visual-based navigation;SLAM;deep learning in robotics and automation,Simultaneous localization and mapping;Drones;Feature extraction;Real-time systems;Motion estimation;Training;Image resolution,,85.0,,45,IEEE,10 Jul 2019,,,IEEE,IEEE Journals,https://github.com/jiexiong2016/GCNv2_SLAM,https://github.com/jiexiong2016/GCNv2_SLAM
56,Volumetric Instance-Aware Semantic Mapping and 3D Object Discovery,M. Grinvald; F. Furrer; T. Novkovic; J. J. Chung; C. Cadena; R. Siegwart; J. Nieto,"Autonomous Systems Lab, ETH Zurich, Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Zurich, Switzerland",IEEE Robotics and Automation Letters,3 Jul 2019,2019,4,3,3037,3044,"To autonomously navigate and plan interactions in real-world environments, robots require the ability to robustly perceive and map complex, unstructured surrounding scenes. Besides building an internal representation of the observed scene geometry, the key insight toward a truly functional understanding of the environment is the usage of higher level entities during mapping, such as individual object instances. This work presents an approach to incrementally build volumetric object-centric maps during online scanning with a localized RGB-D camera. First, a per-frame segmentation scheme combines an unsupervised geometric approach with instance-aware semantic predictions to detect both recognized scene elements as well as previously unseen objects. Next, a data association step tracks the predicted instances across the different frames. Finally, a map integration strategy fuses information about their 3D shape, location, and, if available, semantic class into a global volume. Evaluation on a publicly available dataset shows that the proposed approach for building instance-level semantic maps is competitive with state-of-theart methods, while additionally able to discover objects of unseen categories. The system is further evaluated within a real-world robotic mapping setup, for which qualitative results highlight the online nature of the method. Code is available athttps://github.com/ ethz-asl/voxblox-plusplus.",2377-3766,,10.1109/LRA.2019.2923960,ABB Corporate Research; Amazon Research Awards program; Swiss National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8741085,RGB-D perception;object detection;segmentation and categorization;mapping,Semantics;Three-dimensional displays;Robots;Shape;Task analysis;Image segmentation;Planning,,172.0,,26,IEEE,19 Jun 2019,,,IEEE,IEEE Journals,,
57,Normalization in Training U-Net for 2-D Biomedical Semantic Segmentation,X. -Y. Zhou; G. -Z. Yang,"Hamlyn Centre for Robotic Surgery, Imperial College London, London, U.K.; Hamlyn Centre for Robotic Surgery, Imperial College London, London, U.K.",IEEE Robotics and Automation Letters,24 Feb 2019,2019,4,2,1792,1799,"Two-dimensional (2-D) biomedical semantic segmentation is important for robotic vision in surgery. Segmentation methods based on deep convolutional neural network (DCNN) can out-perform conventional methods in terms of both accuracy and levels of automation. One common issue in training a DCNN for biomedical semantic segmentation is the internal covariate shift where the training of convolutional kernels is encumbered by the distribution change of input features, hence both the training speed and performance are decreased. Batch normalization (BN) is the first proposed method for addressing internal covariate shift and is widely used. Instance normalization (IN) and layer normalization (LN) have also been proposed. Group normalization (GN) is proposed more recently and has not yet been applied to 2-D biomedical semantic segmentation (GN was used in 3-D biomedical semantic segmentation in [P.-Y. Kao, T. Ngo, A. Zhang, J. Chen, and B. Manjunath, Brain tumor segmentation and tractographic feature extraction from structural MR images for overall survival prediction 2018, arXiv:1807.07716], however, no specific validations on GN were given). Most DCNNs for biomedical semantic segmentation adopt BN as the normalization method by default, without reviewing its performance. In this letter, four normalization methods-BN, IN, LN, and GN are compared in details, specifically for 2-D biomedical semantic segmentation. U-Net is adopted as the basic DCNN structure. Three datasets regarding the right ventricle, aorta, and left ventricle are used for the validation. The results show that detailed subdivision of the feature map, i.e., GN with a large group number or IN, achieves higher accuracy. This accuracy improvement mainly comes from better model generalization. Codes are uploaded and maintained at Xiao-Yun Zhou's Github.",2377-3766,,10.1109/LRA.2019.2896518,EP(grant numbers:EP/L020688/1); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8630007,Computer vision for medical robotics;deep learning in robotics and automation;surgical robotics: planning;visual-based navigation,Semantics;Image segmentation;Feature extraction;Training;Two dimensional displays;Robots;Three-dimensional displays,,61.0,,29,IEEE,30 Jan 2019,,,IEEE,IEEE Journals,,
58,On-Policy Dataset Synthesis for Learning Robot Grasping Policies Using Fully Convolutional Deep Networks,V. Satish; J. Mahler; K. Goldberg,"Department of Electrical Engineering and Computer Science, University of California, Berkeley, Berkeley, USA; Department of Electrical Engineering and Computer Science and Department of Industrial Operations and Engineering Research, University of California, Berkeley, Berkeley, USA; Department of Electrical Engineering and Computer Science and Department of Industrial Operations and Engineering Research, University of California, Berkeley, Berkeley, USA",IEEE Robotics and Automation Letters,18 Feb 2019,2019,4,2,1357,1364,"Rapid and reliable robot grasping for a diverse set of objects has applications from warehouse automation to home decluttering. One promising approach is to learn deep policies from synthetic training datasets of point clouds, grasps, and rewards sampled using analytic models with stochastic noise models for domain randomization. In this letter, we explore how the distribution of synthetic training examples affects the rate and reliability of the learned robot policy. We propose a synthetic data sampling distribution that combines grasps sampled from the policy action set with guiding samples from a robust grasping supervisor that has full state knowledge. We use this to train a robot policy based on a fully convolutional network architecture that evaluates millions of grasp candidates in 4-DOF (3-D position and planar orientation). Physical robot experiments suggest that a policy based on fully convolutional grasp quality CNNs (FC-GQ-CNNs) can plan grasps in 0.625 s, considering 5000x more grasps than our prior policy based on iterative grasp sampling and evaluation. This computational efficiency improves rate and reliability, achieving 296 mean picks per hour (MPPH) compared to 250 MPPH for iterative policies. Sensitivity experiments explore the effect of supervisor guidance level and granularity of the policy action space. Code, datasets, videos, and supplementary material can be found at http://berkeleyautomation.github.io/fcgqcnn.",2377-3766,,10.1109/LRA.2019.2895878,"AUTOLAB at UC Berkeley in affiliation with the Berkeley AI Research (BAIR) Lab; Berkeley Deep Drive (BDD); Real-Time Intelligent Secure Execution (RISE) Lab; CITRIS “People and Robots” (CPAR) Initiative; Siemens, Google, Amazon Robotics, Toyota Research Institute, Autodesk, ABB, Samsung, Knapp, Loccioni, Honda, Intel, Comcast, Cisco, and Hewlett-Packard; PhotoNeo, Nvidia, and Intuitive Surgical; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8629272,Grasping;Deep Learning in Robotics and Automation,Grasping;Training;Three-dimensional displays;Robot sensing systems;Reliability;Planning,,93.0,,38,IEEE,29 Jan 2019,,,IEEE,IEEE Journals,http://berkeleyautomation.github.io/fcgqcnn,https://github.com/BerkeleyAutomation/fcgqcnn/raw/gh-pages/docs/on_policy_dataset_synth_fcgqcnn.pdf
59,Unsupervised Deep Homography: A Fast and Robust Homography Estimation Model,T. Nguyen; S. W. Chen; S. S. Shivakumar; C. J. Taylor; V. Kumar,"General Robotics, Automation, Sensing, and Perception Laboratory, University of Pennsylvania, Philadelphia, PA, USA; General Robotics, Automation, Sensing, and Perception Laboratory, University of Pennsylvania, Philadelphia, PA, USA; General Robotics, Automation, Sensing, and Perception Laboratory, University of Pennsylvania, Philadelphia, PA, USA; General Robotics, Automation, Sensing, and Perception Laboratory, University of Pennsylvania, Philadelphia, PA, USA; General Robotics, Automation, Sensing, and Perception Laboratory, University of Pennsylvania, Philadelphia, PA, USA",IEEE Robotics and Automation Letters,28 Mar 2018,2018,3,3,2346,2353,"Homography estimation between multiple aerial images can provide relative pose estimation for collaborative autonomous exploration and monitoring. The usage on a robotic system requires a fast and robust homography estimation algorithm. In this letter, we propose an unsupervised learning algorithm that trains a deep convolutional neural network to estimate planar homographies. We compare the proposed algorithm to traditional-feature-based and direct methods, as well as a corresponding supervised learning algorithm. Our empirical results demonstrate that compared to traditional approaches, the unsupervised algorithm achieves faster inference speed, while maintaining comparable or better accuracy and robustness to illumination variation. In addition, our unsupervised method has superior adaptability and performance compared to the corresponding supervised deep learning method. Our image dataset and a Tensorflow implementation of our work are available at https://github.com/tynguyen/unsupervisedDeepHomographyRAL2018.",2377-3766,,10.1109/LRA.2018.2809549,"Applied Research Laboratory(grant numbers:W911NF-08-2-0004,W911NF-10-2-0016); Army Research Laboratory(grant numbers:W911NF-13-1-0350,N00014-14-1-0510,N00014-09-1-1051,N00014-11-1-0725,N00014-15-1-2115,N00014-09-1-103); Defense Advanced Research Projects Agency(grant numbers:HR001151626,HR0011516850); USDA(grant numbers:2015-67021-23857); National Science Foundation(grant numbers:IIS-1138847,IIS-1426840,CNS-1446592,CNS-1521617,IIS-1328805); Qualcomm Research, United Technologies, and TerraSwarm, one of six centers of STARnet, a Semiconductor Research Corporation program; MARCO and Defense Advanced Research Projects Agency; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8302515,Computer vision for automation;deep learning in robotics and automation;computer vision for other robotic applications;image alignment;deep homography,Estimation;Machine learning;Lighting;Robots;Supervised learning;Training;Robustness,,244.0,,29,IEEE,26 Feb 2018,,,IEEE,IEEE Journals,https://github.com/tynguyen/unsupervisedDeepHomographyRAL2018,https://github.com/tynguyen/unsupervisedDeepHomographyRAL2018
60,Incremental-Segment-Based Localization in 3-D Point Clouds,R. Dubé; M. G. Gollub; H. Sommer; I. Gilitschenski; R. Siegwart; C. Cadena; J. Nieto,"Autonomous Systems Lab, ETH Zürich, Zürich, Switzerland; Autonomous Systems Lab, ETH Zürich, Zürich, Switzerland; Autonomous Systems Lab, ETH Zürich, Zürich, Switzerland; Autonomous Systems Lab, ETH Zürich, Zürich, Switzerland; Autonomous Systems Lab, ETH Zürich, Zürich, Switzerland; Autonomous Systems Lab, ETH Zürich, Zürich, Switzerland; Autonomous Systems Lab, ETH Zürich, Zürich, Switzerland",IEEE Robotics and Automation Letters,12 Mar 2018,2018,3,3,1832,1839,"Localization in 3-D point clouds is a highly challenging task due to the complexity associated with extracting information from 3-D data. This letter proposes an incremental approach addressing this problem efficiently. The presented method first accumulates the measurements in a dynamic voxel grid and selectively updates the point normals affected by the insertion. An incremental segmentation algorithm, based on region growing, tracks the evolution of single segments, which enables an efficient recognition strategy using partitioning and caching of geometric consistencies. We show that the incremental method can perform global localization at 10 Hz in an urban driving environment, a speedup of ×7.1 over the compared batch solution. The efficiency of the method makes it suitable for applications where real-time localization is required and enables its usage on cheaper low-energy systems. Our implementation is available open source along with instructions for running the system. (The implementation is available at https://github.com/ethz-asl/segmatch and a video demonstration is available at https://youtu.be/cHfs3HLzc2Y).",2377-3766,,10.1109/LRA.2018.2803213,European Union's Seventh Framework Program; TRADR(grant numbers:FP7-ICT-609763); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8283725,Localization;range sensing;recognition;SLAM,Three-dimensional displays;Indexes;Simultaneous localization and mapping;Trajectory;Estimation;Covariance matrices,,45.0,,16,IEEE,7 Feb 2018,,,IEEE,IEEE Journals,https://github.com/ethz-asl/segmatch,https://github.com/ethz-asl/segmatch
61,Robust Stereo Visual Inertial Odometry for Fast Autonomous Flight,K. Sun; K. Mohta; B. Pfrommer; M. Watterson; S. Liu; Y. Mulgaonkar; C. J. Taylor; V. Kumar,"GRASP Lab, University of Pennsylvania, Philadelphia,, USA; GRASP Lab, University of Pennsylvania, Philadelphia,, USA; GRASP Lab, University of Pennsylvania, Philadelphia,, USA; GRASP Lab, University of Pennsylvania, Philadelphia,, USA; GRASP Lab, University of Pennsylvania, Philadelphia,, USA; GRASP Lab, University of Pennsylvania, Philadelphia,, USA; GRASP Lab, University of Pennsylvania, Philadelphia,, USA; GRASP Lab, University of Pennsylvania, Philadelphia,, USA",IEEE Robotics and Automation Letters,2 Feb 2018,2018,3,2,965,972,"In recent years, vision-aided inertial odometry for state estimation has matured significantly. However, we still encounter challenges in terms of improving the computational efficiency and robustness of the underlying algorithms for applications in autonomous flight with microaerial vehicles, in which it is difficult to use high-quality sensors and powerful processors because of constraints on size and weight. In this letter, we present a filter-based stereo visual inertial odometry that uses the multistate constraint Kalman filter. Previous work on the stereo visual inertial odometry has resulted in solutions that are computationally expensive. We demonstrate that our stereo multistate constraint Kalman filter (S-MSCKF) is comparable to state-of-the-art monocular solutions in terms of computational cost, while providing significantly greater robustness. We evaluate our S-MSCKF algorithm and compare it with state-of-the-art methods including OKVIS, ROVIO, and VINS-MONO on both the EuRoC dataset and our own experimental datasets demonstrating fast autonomous flight with a maximum speed of 17.5 m/s in indoor and outdoor environments. Our implementation of the S-MSCKF is available at https://github.com/KumarRobotics/msckf_vio.",2377-3766,,10.1109/LRA.2018.2793349,"Defense Advanced Research Projects Agency(grant numbers:HR001151626,HR0011516850); Army Research Laboratory(grant numbers:W911NF-08-2-0004); NASA Space Technology Research Fellowship; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258858,Localization;aerial systems: perception and autonomy;SLAM,Cameras;Robustness;Visualization;Quaternions;Covariance matrices;Kalman filters;Real-time systems,,376.0,,36,IEEE,15 Jan 2018,,,IEEE,IEEE Journals,https://github.com/KumarRobotics/msckf_vio,https://github.com/KumarRobotics/msckf_vio
62,Energy-Bounded Caging: Formal Definition and 2-D Energy Lower Bound Algorithm Based on Weighted Alpha Shapes,J. Mahler; F. T. Pokorny; Z. McCarthy; A. F. van der Stappen; K. Goldberg,"Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, CA, USA; Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, CA, USA; Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, CA, USA; Department of Information and Computing Sciences, Utrecht University, Utrecht, The Netherlands; Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA, USA",IEEE Robotics and Automation Letters,20 May 2017,2016,1,1,508,515,"Caging grasps are valuable as they can be robust to bounded variations in object shape and pose, do not depend on friction, and enable transport of an object without full immobilization. Complete caging of an object is useful but may not be necessary in cases where forces such as gravity are present. This letter extends caging theory by defining energy-bounded cages with respect to an energy field such as gravity. This letter also introduces energy-bounded-cage-analysis-2-D (EBCA-2-D), a sampling-based algorithm for planar analysis that takes as input an energy function over poses, a polygonal object, and a configuration of rigid fixed polygonal obstacles, e.g., a gripper, and returns a lower bound on the minimum escape energy. In the special case when the object is completely caged, our approach is independent of the energy and can provably verify the cage. EBCA-2-D builds on recent results in collision detection and the computational geometric theory of weighted α-shapes and runs in O(s2 + sn2) time where s is the number of samples, n is the total number of object and obstacle vertices, and typically n <;<; s. We implemented EBCA-2-D and evaluated it with nine parallel-jaw gripper configurations and four nonconvex obstacle configurations across six nonconvex polygonal objects. We found that the lower bounds returned by EBCA-2-D are consistent with intuition, and we verified the algorithm experimentally with Box2-D simulations and RRT* motion planning experiments that were unable to find escape paths with lower energy. EBCA2-D required an average of 3 min per problem on a single-core processor but has potential to be parallelized in a cloud-based implementation. Additional proofs, data, and code are available at: http://berkeleyautomation.github.io/caging/.",2377-3766,,10.1109/LRA.2016.2519145,U.S. National Science Foundation(grant numbers:IIS-1227536); Department of Defense (DoD); National Defense Science and Engineering Graduate Fellowship (NDSEG) Program; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7384720,Grasping;Computational Geometry;Motion and Path Planning;Grasping;computational geometry;motion and path planning,Grippers;Computational geometry;Path planning;Robustness;Robots;Approximation algorithms,,37.0,1.0,48,IEEE,18 Jan 2016,,,IEEE,IEEE Journals,http://berkeleyautomation.github.io/caging,https://github.com/BerkeleyAutomation/caging/zipball/master
