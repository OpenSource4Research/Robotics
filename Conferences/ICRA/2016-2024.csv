"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Dex-Net 1.0: A cloud-based network of 3D objects for robust grasp planning using a Multi-Armed Bandit model with correlated rewards","J. Mahler; F. T. Pokorny; B. Hou; M. Roderick; M. Laskey; M. Aubry; K. Kohlhoff; T. Kröger; J. Kuffner; K. Goldberg","University of California, Berkeley, USA; University of California, Berkeley, USA; University of California, Berkeley, USA; University of California, Berkeley, USA; University of California, Berkeley, USA; University of California, Berkeley, USA; Google Inc., Mountain View, USA; Google Inc., Mountain View, USA; Google Inc., Mountain View, USA; Google Inc., Mountain View, USA",2016 IEEE International Conference on Robotics and Automation (ICRA),"9 Jun 2016","2016","","","1957","1964","This paper presents the Dexterity Network (Dex-Net) 1.0, a dataset of 3D object models and a sampling-based planning algorithm to explore how Cloud Robotics can be used for robust grasp planning. The algorithm uses a Multi- Armed Bandit model with correlated rewards to leverage prior grasps and 3D object models in a growing dataset that currently includes over 10,000 unique 3D object models and 2.5 million parallel-jaw grasps. Each grasp includes an estimate of the probability of force closure under uncertainty in object and gripper pose and friction. Dex-Net 1.0 uses Multi-View Convolutional Neural Networks (MV-CNNs), a new deep learning method for 3D object classification, to provide a similarity metric between objects, and the Google Cloud Platform to simultaneously run up to 1,500 virtual cores, reducing experiment runtime by up to three orders of magnitude. Experiments suggest that correlated bandit techniques can use a cloud-based network of object models to significantly reduce the number of samples required for robust grasp planning. We report on system sensitivity to variations in similarity metrics and in uncertainty in pose and friction. Code and updated information is available at http://berkeleyautomation.github.io/dex-net/.","","978-1-4673-8026-3","10.1109/ICRA.2016.7487342","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7487342","","Three-dimensional displays;Solid modeling;Robustness;Planning;Measurement;Force;Friction","","234","10","47","IEEE","9 Jun 2016","","","IEEE","IEEE Conferences"
"Automating multi-throw multilateral surgical suturing with a mechanical needle guide and sequential convex optimization","S. Sen; A. Garg; D. V. Gealy; S. McKinley; Y. Jen; K. Goldberg","EECS, University of California, Berkeley, CA, USA; IEOR & EECS, University of California, Berkeley, CA, USA; Mechanical Engineering, University of California, Berkeley, CA, USA; Mechanical Engineering, University of California, Berkeley, CA, USA; EECS, University of California, Berkeley, CA, USA; IEOR & EECS, University of California, Berkeley, CA, USA",2016 IEEE International Conference on Robotics and Automation (ICRA),"9 Jun 2016","2016","","","4178","4185","For supervised automation of multi-throw suturing in Robot-Assisted Minimally Invasive Surgery, we present a novel mechanical needle guide and a framework for optimizing needle size, trajectory, and control parameters using sequential convex programming. The Suture Needle Angular Positioner (SNAP) results in a 3x error reduction in the needle pose estimate in comparison with the standard actuator. We evaluate the algorithm and SNAP on a da Vinci Research Kit using tissue phantoms and compare completion time with that of humans from the JIGSAWS dataset [5]. Initial results suggest that the dVRK can perform suturing at 30% of human speed while completing 86% suture throws attempted. Videos and data are available at: berkeleyautomation.github.io/amts","","978-1-4673-8026-3","10.1109/ICRA.2016.7487611","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7487611","","Needles;Trajectory;Surgery;Actuators;Planning;Automation;Robots","","134","","36","IEEE","9 Jun 2016","","","IEEE","IEEE Conferences"
"TSC-DL: Unsupervised trajectory segmentation of multi-modal surgical demonstrations with Deep Learning","A. Murali; A. Garg; S. Krishnan; F. T. Pokorny; P. Abbeel; T. Darrell; K. Goldberg","EECS & IEOR, University of California, Berkeley, CA, USA; EECS & IEOR, University of California, Berkeley, CA, USA; EECS & IEOR, University of California, Berkeley, CA, USA; EECS & IEOR, University of California, Berkeley, CA, USA; EECS & IEOR, University of California, Berkeley, CA, USA; EECS & IEOR, University of California, Berkeley, CA, USA; EECS & IEOR, University of California, Berkeley, CA, USA",2016 IEEE International Conference on Robotics and Automation (ICRA),"9 Jun 2016","2016","","","4150","4157","The growth of robot-assisted minimally invasive surgery has led to sizable datasets of fixed-camera video and kinematic recordings of surgical subtasks. Segmentation of these trajectories into locally-similar contiguous sections can facilitate learning from demonstrations, skill assessment, and salvaging good segments from otherwise inconsistent demonstrations. Manual, or supervised, segmentation can be prone to error and impractical for large datasets. We present Transition State Clustering with Deep Learning (TSC-DL), a new unsupervised algorithm that leverages video and kinematic data for task-level segmentation, and finds regions of the visual feature space that correlate with transition events using features constructed from layers of pre-trained image classification Deep Convolutional Neural Networks (CNNs). We report results on three datasets comparing Deep Learning architectures (AlexNet and VGG), choice of convolutional layer, dimensionality reduction techniques, visual encoding, and the use of Scale Invariant Feature Transforms (SIFT). We find that the deep architectures extract features that result in up-to a 30.4% improvement in Silhouette Score (a measure of cluster tightness) over the traditional “shallow” features from SIFT. We also present cases where TSC-DL discovers human annotator omissions. Supplementary material, data and code is available at: http://berkeleyautomation.github.io/tsc-dl/","","978-1-4673-8026-3","10.1109/ICRA.2016.7487607","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7487607","","Visualization;Motion segmentation;Feature extraction;Hidden Markov models;Kinematics;Machine learning","","43","1","29","IEEE","9 Jun 2016","","","IEEE","IEEE Conferences"
"Reducing drift in visual odometry by inferring sun direction using a Bayesian Convolutional Neural Network","V. Peretroukhin; L. Clement; J. Kelly","Space & Terrestrial Autonomous Robotic Systems (STARS) laboratory at the University of Toronto Institute for Aerospace Studies (UTIAS), Canada; Space & Terrestrial Autonomous Robotic Systems (STARS) laboratory at the University of Toronto Institute for Aerospace Studies (UTIAS), Canada; Space & Terrestrial Autonomous Robotic Systems (STARS) laboratory at the University of Toronto Institute for Aerospace Studies (UTIAS), Canada",2017 IEEE International Conference on Robotics and Automation (ICRA),"24 Jul 2017","2017","","","2035","2042","We present a method to incorporate global orientation information from the sun into a visual odometry pipeline using only the existing image stream, where the sun is typically not visible. We leverage recent advances in Bayesian Convolutional Neural Networks to train and implement a sun detection model that infers a three-dimensional sun direction vector from a single RGB image. Crucially, our method also computes a principled uncertainty associated with each prediction, using a Monte Carlo dropout scheme. We incorporate this uncertainty into a sliding window stereo visual odometry pipeline where accurate uncertainty estimates are critical for optimal data fusion. Our Bayesian sun detection model achieves a median error of approximately 12 degrees on the KITTI odometry benchmark training set, and yields improvements of up to 42% in translational ARMSE and 32% in rotational ARMSE compared to standard VO. An open source implementation of our Bayesian CNN sun estimator (Sun-BCNN) using Caffe is available at https://github.com/utiasSTARS/sun-bcnn-vo.","","978-1-5090-4633-1","10.1109/ICRA.2017.7989235","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7989235","","Sun;Bayes methods;Neural networks;Visualization;Pipelines;Training","","16","","31","IEEE","24 Jul 2017","","","IEEE","IEEE Conferences"
"PennCOSYVIO: A challenging Visual Inertial Odometry benchmark","B. Pfrommer; N. Sanket; K. Daniilidis; J. Cleveland","University of Pennsylvania School of Engineering and Applied Science; University of Pennsylvania School of Engineering and Applied Science; University of Pennsylvania School of Engineering and Applied Science; Cognitive Operational Systems, LLC",2017 IEEE International Conference on Robotics and Automation (ICRA),"24 Jul 2017","2017","","","3847","3854","We present PennCOSYVIO, a new challenging Visual Inertial Odometry (VIO) benchmark with synchronized data from a VI-sensor (stereo camera and IMU), two Project Tango hand-held devices, and three GoPro Hero 4 cameras. Recorded at UPenn's Singh center, the 150m long path of the hand-held rig crosses from outdoors to indoors and includes rapid rotations, thereby testing the abilities of VIO and Simultaneous Localization and Mapping (SLAM) algorithms to handle changes in lighting, different textures, repetitive structures, and large glass surfaces. All sensors are synchronized and intrinsically and extrinsically calibrated. We demonstrate the accuracy with which ground-truth poses can be obtained via optic localization off of fiducial markers. The data set can be found at https://daniilidis-group.github.io/penncosyvio/.","","978-1-5090-4633-1","10.1109/ICRA.2017.7989443","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7989443","","Cameras;Benchmark testing;Visualization;Simultaneous localization and mapping;Global Positioning System;Calibration","","54","","32","IEEE","24 Jul 2017","","","IEEE","IEEE Conferences"
"Beyond Pixels: Leveraging Geometry and Shape Cues for Online Multi-Object Tracking","S. Sharma; J. A. Ansari; J. Krishna Murthy; K. Madhava Krishna","Robotics Research Center, KCIS, IIIT Hyderabad, India; Robotics Research Center, KCIS, IIIT Hyderabad, India; Robotics Research Center, KCIS, IIIT Hyderabad, India; Robotics Research Center, KCIS, IIIT Hyderabad, India",2018 IEEE International Conference on Robotics and Automation (ICRA),"13 Sep 2018","2018","","","3508","3515","This paper introduces geometry and object shape and pose costs for multi-object tracking in urban driving scenarios. Using images from a monocular camera alone, we devise pairwise costs for object tracks, based on several 3D cues such as object pose, shape, and motion. The proposed costs are agnostic to the data association method and can be incorporated into any optimization framework to output the pairwise data associations. These costs are easy to implement, can be computed in real-time, and complement each other to account for possible errors in a tracking-by-detection framework. We perform an extensive analysis of the designed costs and empirically demonstrate consistent improvement over the state-of-the-art under varying conditions that employ a range of object detectors, exhibit a variety in camera and object motions, and, more importantly, are not reliant on the choice of the association framework. We also show that, by using the simplest of associations frameworks (two-frame Hungarian assignment), we surpass the state-of-the-art in multi-object-tracking on road scenes. More qualitative and quantitative results can be found at https://junaidcs032.github.io/Geometry_ObjectShape_MOT/.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8461018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8461018","","Three-dimensional displays;Shape;Target tracking;Roads;Trajectory;Cameras","","121","","26","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Relocalization, Global Optimization and Map Merging for Monocular Visual-Inertial SLAM","T. Qin; P. Li; S. Shen","Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, China; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, China; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, China",2018 IEEE International Conference on Robotics and Automation (ICRA),"13 Sep 2018","2018","","","1197","1204","The monocular visual-inertial system (VINS), which consists one camera and one low-cost inertial measurement unit (IMU), is a popular approach to achieve accurate 6-DOF state estimation. However, such locally accurate visual-inertial odometry is prone to drift and cannot provide absolute pose estimation. Leveraging history information to relocalize and correct drift has become a hot topic. In this paper, we propose a monocular visual-inertial SLAM system, which can relocalize camera and get the absolute pose in a previous-built map. Then 4-DOF pose graph optimization is performed to correct drifts and achieve global consistent. The 4-DOF contains x, y, z, and yaw angle, which is the actual drifted direction in the visual-inertial system. Furthermore, the proposed system can reuse a map by saving and loading it in an efficient way. Current map and previous map can be merged together by the global pose graph optimization. We validate the accuracy of our system on public datasets and compare against other state-of-the-art algorithms. We also evaluate the map merging ability of our system in the large-scale outdoor environment. The source code of map reuse is integrated into our public code, VINS-Monol11https://github.com/HKUST-Aerial-Robotics/VINS-Mono.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8460780","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8460780","","Cameras;Optimization;Visualization;Feature extraction;Microsoft Windows;Simultaneous localization and mapping;Real-time systems","","43","","34","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Robust Dense Mapping for Large-Scale Dynamic Environments","I. A. Barsan; P. Liu; M. Pollefeys; A. Geiger","Computer Vision and Geometry Group, ETH Zurich, Switzerland; Computer Vision and Geometry Group, ETH Zurich, Switzerland; Computer Vision and Geometry Group, ETH Zurich, Switzerland; Computer Vision and Geometry Group, ETH Zurich, Switzerland",2018 IEEE International Conference on Robotics and Automation (ICRA),"13 Sep 2018","2018","","","7510","7517","We present a stereo-based dense mapping algorithm for large-scale dynamic urban environments. In contrast to other existing methods, we simultaneously reconstruct the static background, the moving objects, and the potentially moving but currently stationary objects separately, which is desirable for high-level mobile robotic tasks such as path planning in crowded environments. We use both instance-aware semantic segmentation and sparse scene flow to classify objects as either background, moving, or potentially moving, thereby ensuring that the system is able to model objects with the potential to transition from static to dynamic, such as parked cars. Given camera poses estimated from visual odometry, both the background and the (potentially) moving objects are reconstructed separately by fusing the depth maps computed from the stereo input. In addition to visual odometry, sparse scene flow is also used to estimate the 3D motions of the detected moving objects, in order to reconstruct them accurately. A map pruning technique is further developed to improve reconstruction accuracy and reduce memory consumption, leading to increased scalability. We evaluate our system thoroughly on the well-known KITTI dataset. Our system is capable of running on a PC at approximately 2.5Hz, with the primary bottleneck being the instance-aware semantic segmentation, which is a limitation we hope to address in future work. The source code is available from the project websiteaahttp://andreibarsan.github.io/dynslam.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8462974","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8462974","","Three-dimensional displays;Cameras;Semantics;Vehicle dynamics;Dynamics;Real-time systems;Heuristic algorithms","","100","2","30","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Data-Efficient Decentralized Visual SLAM","T. Cieslewski; S. Choudhary; D. Scaramuzza","Dep. of Informatics, University of Zurich and ETH Zurich, Switzerland; College of Computing, Georgia Institute of Technology, Atlanta, GA, USA; Dep. of Informatics, University of Zurich and ETH Zurich, Switzerland",2018 IEEE International Conference on Robotics and Automation (ICRA),"13 Sep 2018","2018","","","2466","2473","Decentralized visual simultaneous localization and mapping (SLAM) is a powerful tool for multi-robot applications in environments where absolute positioning is not available. Being visual, it relies on cheap, lightweight and versatile cameras, and, being decentralized, it does not rely on communication to a central entity. In this work, we integrate state-of-the-art decentralized SLAM components into a new, complete decentralized visual SLAM system. To allow for data association and optimization, existing decentralized visual SLAM systems exchange the full map data among all robots, incurring large data transfers at a complexity that scales quadratically with the robot count. In contrast, our method performs efficient data association in two stages: first, a compact full-image descriptor is deterministically sent to only one robot. Then, only if the first stage succeeded, the data required for relative pose estimation is sent, again to only one robot. Thus, data association scales linearly with the robot count and uses highly compact place representations. For optimization, a state-of-the-art decentralized pose-graph optimization method is used. It exchanges a minimum amount of data which is linear with trajectory overlap. We characterize the resulting system and identify bottlenecks in its components. The system is evaluated on publicly available datasets and we provide open access to the code. Supplementary Material Data and code are at: https://github.com/uzh-rpg/dslam_open.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8461155","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8461155","","Simultaneous localization and mapping;Visualization;Optimization;Pose estimation;Trajectory;Bandwidth","","122","","44","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Time-Contrastive Networks: Self-Supervised Learning from Video","P. Sermanet; C. Lynch; Y. Chebotar; J. Hsu; E. Jang; S. Schaal; S. Levine; G. Brain",Google Brain; Google Brain; University of Southern California; Google Brain; Google Brain; University of Southern California; Google Brain; Google Brain,2018 IEEE International Conference on Robotics and Automation (ICRA),"13 Sep 2018","2018","","","1134","1141","We propose a self-supervised approach for learning representations and robotic behaviors entirely from unlabeled videos recorded from multiple viewpoints, and study how this representation can be used in two robotic imitation settings: imitating object interactions from videos of humans, and imitating human poses. Imitation of human behavior requires a viewpoint-invariant representation that captures the relationships between end-effectors (hands or robot grippers) and the environment, object attributes, and body pose. We train our representations using a triplet loss, where multiple simultaneous viewpoints of the same observation are attracted in the embedding space, while being repelled from temporal neighbors which are often visually similar but functionally different. This signal causes our model to discover attributes that do not change across viewpoint, but do change across time, while ignoring nuisance variables such as occlusions, motion blur, lighting and background. We demonstrate that this representation can be used by a robot to directly mimic human poses without an explicit correspondence, and that it can be used as a reward function within a reinforcement learning algorithm. While representations are learned from an unlabeled collection of task-related videos, robot behaviors such as pouring are learned by watching a single 3rd-person demonstration by a human. Reward functions obtained by following the human demonstrations under the learned representation enable efficient reinforcement learning that is practical for real-world robotic systems. Video results, open-source code and dataset are available at sermanet.github.io/imitate.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8462891","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8462891","","Robots;Task analysis;Visualization;Learning (artificial intelligence);Training;Liquids;Lighting","","324","1","44","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Neural Task Programming: Learning to Generalize Across Hierarchical Tasks","D. Xu; S. Nair; Y. Zhu; J. Gao; A. Garg; L. Fei-Fei; S. Savarese","Stanford Vision & Learning Lab; CS, Caltech; Stanford Vision & Learning Lab; Stanford Vision & Learning Lab; Stanford Vision & Learning Lab; Stanford Vision & Learning Lab; Stanford Vision & Learning Lab",2018 IEEE International Conference on Robotics and Automation (ICRA),"13 Sep 2018","2018","","","3795","3802","In this work, we propose a novel robot learning framework called Neural Task Programming (NTP), which bridges the idea of few-shot learning from demonstration and neural program induction. NTP takes as input a task specification (e.g., video demonstration of a task) and recursively decomposes it into finer sub-task specifications. These specifications are fed to a hierarchical neural program, where bottom-level programs are callable subroutines that interact with the environment. We validate our method in three robot manipulation tasks. NTP achieves strong generalization across sequential tasks that exhibit hierarchal and compositional structures. The experimental results show that NTP learns to generalize well towards unseen tasks with increasing lengths, variable topologies, and changing objectives.stanfordvl.github.io/ntp/.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8460689","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8460689","","Task analysis;Programming;Robots;Sorting;Semantics;Topology;Data models","","71","","33","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Dex-Net 3.0: Computing Robust Vacuum Suction Grasp Targets in Point Clouds Using a New Analytic Model and Deep Learning","J. Mahler; M. Matl; X. Liu; A. Li; D. Gealy; K. Goldberg","Dept. of Electrical Engineering and Computer Science; Dept. of Electrical Engineering and Computer Science; Dept. of Electrical Engineering and Computer Science; Dept. of Electrical Engineering and Computer Science; Dept. of Electrical Engineering and Computer Science; Dept. of Industrial Operations and Engineering Research, AUTOLAB University of California, Berkeley, USA",2018 IEEE International Conference on Robotics and Automation (ICRA),"13 Sep 2018","2018","","","5620","5627","Vacuum-based end effectors are widely used in industry and are often preferred over parallel-jaw and multifinger grippers due to their ability to lift objects with a single point of contact. Suction grasp planners often target planar surfaces on point clouds near the estimated centroid of an object. In this paper, we propose a compliant suction contact model that computes the quality of the seal between the suction cup and local target surface and a measure of the ability of the suction grasp to resist an external gravity wrench. To characterize grasps, we estimate robustness to perturbations in end-effector and object pose, material properties, and external wrenches. We analyze grasps across 1,500 3D object models to generate Dex-Net 3.0, a dataset of 2.8 million point clouds, suction grasps, and grasp robustness labels. We use Dex-Net 3.0 to train a Grasp Quality Convolutional Neural Network (GQ-CNN) to classify robust suction targets in point clouds containing a single object. We evaluate the resulting system in 350 physical trials on an ABB YuMi fitted with a pneumatic suction gripper. When evaluated on novel objects that we categorize as Basic (prismatic or cylindrical), Typical (more complex geometry), and Adversarial (with few available suction-grasp points) Dex-Net 3.0 achieves success rates of 98%, 82%, and 58% respectively, improving to 81% in the latter case when the training set includes only adversarial objects. Code, datasets, and supplemental material can be found at http://berkeleyautomation.github.io/dex-net.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8460887","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8460887","","Three-dimensional displays;Robustness;Robots;Analytical models;Seals;Computational modeling;Planning","","189","1","35","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"AffordanceNet: An End-to-End Deep Learning Approach for Object Affordance Detection","T. -T. Do; A. Nguyen; I. Reid","Australian Centre for Robotic Vision (ACRV), University of Adelaide; Department of Advanced Robotics, IIT, Italy; Australian Centre for Robotic Vision (ACRV), University of Adelaide",2018 IEEE International Conference on Robotics and Automation (ICRA),"13 Sep 2018","2018","","","5882","5889","We propose AffordanceNet, a new deep learning approach to simultaneously detect multiple objects and their affordances from RGB images. Our AffordanceNet has two branches: an object detection branch to localize and classify the object, and an affordance detection branch to assign each pixel in the object to its most probable affordance label. The proposed framework employs three key components for effectively handling the multiclass problem in the affordance mask: a sequence of deconvolutional layers, a robust resizing strategy, and a multi-task loss function. The experimental results on the public datasets show that our AffordanceNet outperforms recent state-of-the-art methods by a fair margin, while its end-to-end architecture allows the inference at the speed of 150ms per image. This makes our AffordanceNet well suitable for real-time robotic applications. Furthermore, we demonstrate the effectiveness of AffordanceNet in different testing environments and in real robotic applications. The source code is available at https://github.com/nqanh/affordance-net.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8460902","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8460902","","Feature extraction;Robots;Computer architecture;Object detection;Training;Image segmentation;Machine learning","","146","","31","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Self-Supervised Deep Reinforcement Learning with Generalized Computation Graphs for Robot Navigation","G. Kahn; A. Villaflor; B. Ding; P. Abbeel; S. Levine","Berkeley AI Research (BAIR), University of California, Berkeley; Berkeley AI Research (BAIR), University of California, Berkeley; Berkeley AI Research (BAIR), University of California, Berkeley; Berkeley AI Research (BAIR), University of California, Berkeley; Berkeley AI Research (BAIR), University of California, Berkeley",2018 IEEE International Conference on Robotics and Automation (ICRA),"13 Sep 2018","2018","","","5129","5136","Enabling robots to autonomously navigate complex environments is essential for real-world deployment. Prior methods approach this problem by having the robot maintain an internal map of the world, and then use a localization and planning method to navigate through the internal map. However, these approaches often include a variety of assumptions, are computationally intensive, and do not learn from failures. In contrast, learning-based methods improve as the robot acts in the environment, but are difficult to deploy in the real-world due to their high sample complexity. To address the need to learn complex policies with few samples, we propose a generalized computation graph that subsumes value-based model-free methods and model-based methods, with specific instantiations interpolating between model-free and model-based. We then instantiate this graph to form a navigation model that learns from raw images and is sample efficient. Our simulated car experiments explore the design decisions of our navigation model, and show our approach outperforms single-step and N-step double Q-learning. We also evaluate our approach on a real-world RC car and show it can learn to navigate through a complex indoor environment with a few hours of fully autonomous, self-supervised training. Videos of the experiments and code can be found at github.com/gkahn13/gcg.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8460655","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8460655","","Computational modeling;Navigation;Learning (artificial intelligence);Robots;Task analysis;Prediction algorithms;Planning","","174","","32","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Sparse-to-Dense: Depth Prediction from Sparse Depth Samples and a Single Image","F. Ma; S. Karaman","The Laboratory for Information & Decision Systems, Massachusetts Institute of Technology, Cambridge, MA, USA; The Laboratory for Information & Decision Systems, Massachusetts Institute of Technology, Cambridge, MA, USA",2018 IEEE International Conference on Robotics and Automation (ICRA),"13 Sep 2018","2018","","","4796","4803","We consider the problem of dense depth prediction from a sparse set of depth measurements and a single RGB image. Since depth estimation from monocular images alone is inherently ambiguous and unreliable, to attain a higher level of robustness and accuracy, we introduce additional sparse depth samples, which are either acquired with a low-resolution depth sensor or computed via visual Simultaneous Localization and Mapping (SLAM) algorithms. We propose the use of a single deep regression network to learn directly from the RGB-D raw data, and explore the impact of number of depth samples on prediction accuracy. Our experiments show that, compared to using only RGB images, the addition of 100 spatially random depth samples reduces the prediction root-mean-square error by 50% on the NYU-Depth-v2 indoor dataset. It also boosts the percentage of reliable prediction from 59 % to 92 % on the KITTI dataset. We demonstrate two applications of the proposed algorithm: a plug-in module in SLAM to convert sparse maps to dense maps, and super-resolution for LiDARs. Software22https://github.com/fangchangma/sparse-to-dense and video demonstration33https://www.youtube.com/watch?v=vNIIT_M7×7Y are publicly available.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8460184","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8460184","","Training;Laser radar;Image reconstruction;Estimation;Prediction algorithms;Simultaneous localization and mapping","","380","3","30","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Leveraging Temporal Reasoning for Policy Selection in Learning from Demonstration","E. Carpio; M. Clark-Turner; P. Gesel; M. Begum","Cognitive Assistive Robotics Laboratory, University of New Hampshire, Durham, NH, USA; Cognitive Assistive Robotics Laboratory, University of New Hampshire, Durham, NH, USA; Cognitive Assistive Robotics Laboratory, University of New Hampshire, Durham, NH, USA; Cognitive Assistive Robotics Laboratory, University of New Hampshire, Durham, NH, USA",2019 International Conference on Robotics and Automation (ICRA),"12 Aug 2019","2019","","","7798","7804","High-level human activities often have rich temporal structures that determine the order in which atomic actions are executed. We propose the Temporal Context Graph (TCG), a temporal reasoning model that integrates probabilistic inference with Allen's interval algebra, to capture these temporal structures. TCGs are capable of modeling tasks with cyclical atomic actions and consisting of sequential and parallel temporal relations. We present Learning from Demonstration as the application domain where the use of TCGs can improve policy selection and address the problem of perceptual aliasing. Experiments validating the model are presented for learning two tasks from demonstration that involve structured human-robot interactions. The source code for this implementation is available at https://github.com/AssistiveRoboticsUNH/TCG.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8794461","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794461","","Task analysis;Hidden Markov models;Cognition;Context modeling;Robots;Algebra;Probabilistic logic","","1","","26","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"Anytime Stereo Image Depth Estimation on Mobile Devices","Y. Wang; Z. Lai; G. Huang; B. H. Wang; L. van der Maaten; M. Campbell; K. Q. Weinberger",Cornell University; University of Oxford; Cornell University; Cornell University; Facebook AI Research; Cornell University; Cornell University,2019 International Conference on Robotics and Automation (ICRA),"12 Aug 2019","2019","","","5893","5900","Many applications of stereo depth estimation in robotics require the generation of accurate disparity maps in real time under significant computational constraints. Current state-of-the-art algorithms force a choice between either generating accurate mappings at a slow pace, or quickly generating inaccurate ones, and additionally these methods typically require far too many parameters to be usable on power- or memory-constrained devices. Motivated by these shortcomings, we propose a novel approach for disparity prediction in the anytime setting. In contrast to prior work, our end-to-end learned approach can trade off computation and accuracy at inference time. Depth estimation is performed in stages, during which the model can be queried at any time to output its current best estimate. Our final model can process 1242×375 resolution images within a range of 10-35 FPS on an NVIDIA Jetson TX2 module with only marginal increases in error - using two orders of magnitude fewer parameters than the most competitive baseline. The source code is available at https://github.com/mileyan/AnyNet.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8794003","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794003","","Estimation;Image resolution;Feature extraction;Computational modeling;Three-dimensional displays;Cameras;Predictive models","","129","1","54","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"Quasi-Direct Drive for Low-Cost Compliant Robotic Manipulation","D. V. Gealy; S. McKinley; B. Yi; P. Wu; P. R. Downey; G. Balke; A. Zhao; M. Guo; R. Thomasson; A. Sinclair; P. Cuellar; Z. McCarthy; P. Abbeel","Mechanical Engineering, University of California, Berkeley; Industrial Engineering & Operations Research, University of California, Berkeley; Electrical Engineering & Computer Science, University of California, Berkeley; Electrical Engineering & Computer Science, University of California, Berkeley; Mechanical Engineering, University of California, Berkeley; Electrical Engineering & Computer Science, University of California, Berkeley; Electrical Engineering & Computer Science, University of California, Berkeley; Mechanical Engineering, University of California, Berkeley; Mechanical Engineering, University of California, Berkeley; Mechanical Engineering, University of California, Berkeley; Mechanical Engineering, University of California, Berkeley; Electrical Engineering & Computer Science, University of California, Berkeley; Electrical Engineering & Computer Science, University of California, Berkeley",2019 International Conference on Robotics and Automation (ICRA),"12 Aug 2019","2019","","","437","443","Robots must cost less and be force-controlled to enable widespread, safe deployment in unconstrained human environments. We propose Quasi-Direct Drive actuation as a capable paradigm for robotic force-controlled manipulation in human environments at low-cost. Our prototype - Blue - is a human scale 7 Degree of Freedom arm with 2kg payload. Blue can cost less than $5000. We show that Blue has dynamic properties that meet or exceed the needs of human operators: the robot has a nominal position-control bandwidth of 7.5Hz and repeatability within 4mm. We demonstrate a Virtual Reality based interface that can be used as a method for telepresence and collecting robot training demonstrations. Manufacturability, scaling, and potential use-cases for the Blue system are also addressed. Videos and additional information can be found online at berkeleyopenarms.github.io.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8794236","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794236","","Manipulators;Payloads;Bandwidth;Robot sensing systems;Task analysis;Belts","","59","","29","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"CNN-SVO: Improving the Mapping in Semi-Direct Visual Odometry Using Single-Image Depth Prediction","S. Y. Loo; A. J. Amiri; S. Mashohor; S. H. Tang; H. Zhang","Faculty of Engineering, Universiti Putra Malaysia, Malaysia; Department of Computing Science, University of Alberta, Canada; Faculty of Engineering, Universiti Putra Malaysia, Malaysia; Faculty of Engineering, Universiti Putra Malaysia, Malaysia; Department of Computing Science, University of Alberta, Canada",2019 International Conference on Robotics and Automation (ICRA),"12 Aug 2019","2019","","","5218","5223","Reliable feature correspondence between frames is a critical step in visual odometry (VO) and visual simultaneous localization and mapping (V-SLAM) algorithms. In comparison with existing VO and V-SLAM algorithms, semi-direct visual odometry (SVO) has two main advantages that lead to state-of-the-art frame rate camera motion estimation: direct pixel correspondence and efficient implementation of probabilistic mapping method. This paper improves the SVO mapping by initializing the mean and the variance of the depth at a feature location according to the depth prediction from a single-image depth prediction network. By significantly reducing the depth uncertainty of the initialized map point (i.e., small variance centred about the depth prediction), the benefits are twofold: reliable feature correspondence between views and fast convergence to the true depth in order to create new map points. We evaluate our method with two outdoor datasets: KITTI dataset and Oxford Robotcar dataset. The experimental results indicate that improved SVO mapping results in increased robustness and camera tracking accuracy. The implementation of this work is available at https: //github.com/yan99033/CNN-SVO","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8794425","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794425","","Uncertainty;Cameras;Visual odometry;Feature extraction;Reliability;Estimation;Motion estimation","","52","","19","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"Generalization through Simulation: Integrating Simulated and Real Data into Deep Reinforcement Learning for Vision-Based Autonomous Flight","K. Kang; S. Belkhale; G. Kahn; P. Abbeel; S. Levine","Berkeley AI Research (BAIR), University of California, Berkeley; Berkeley AI Research (BAIR), University of California, Berkeley; Berkeley AI Research (BAIR), University of California, Berkeley; Berkeley AI Research (BAIR), University of California, Berkeley; Berkeley AI Research (BAIR), University of California, Berkeley",2019 International Conference on Robotics and Automation (ICRA),"12 Aug 2019","2019","","","6008","6014","Deep reinforcement learning provides a promising approach for vision-based control of real-world robots. However, the generalization of such models depends critically on the quantity and variety of data available for training. This data can be difficult to obtain for some types of robotic systems, such as fragile, small-scale quadrotors. Simulated rendering and physics can provide for much larger datasets, but such data is inherently of lower quality: many of the phenomena that make the real-world autonomous flight problem challenging, such as complex physics and air currents, are modeled poorly or not at all, and the systematic differences between simulation and the real world are typically impossible to eliminate. In this work, we investigate how data from both simulation and the real world can be combined in a hybrid deep reinforcement learning algorithm. Our method uses real-world data to learn about the dynamics of the system, and simulated data to learn a generalizable perception system that can enable the robot to avoid collisions using only a monocular camera. We demonstrate our approach on a real-world nano aerial vehicle collision avoidance task, showing that with only an hour of real-world data, the quadrotor can avoid collisions in new environments with various lighting conditions and geometry. Code, instructions for building the aerial vehicles, and videos of the experiments can be found at github.com/gkahn13/GtS.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8793735","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8793735","","Data models;Robots;Task analysis;Predictive models;Neural networks;Reinforcement learning;Collision avoidance","","69","","45","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"Beyond Photometric Loss for Self-Supervised Ego-Motion Estimation","T. Shen; Z. Luo; L. Zhou; H. Deng; R. Zhang; T. Fang; L. Quan","Department of Computer Science and Engineering, Hong Kong University of Science and Technology; Department of Computer Science and Engineering, Hong Kong University of Science and Technology; Department of Computer Science and Engineering, Hong Kong University of Science and Technology; Department of Computer Science and Engineering, Hong Kong University of Science and Technology; Tencent YouTu Lab, Shenzhen; Everest Innovation Technology (Altizure), Hong Kong; Department of Computer Science and Engineering, Hong Kong University of Science and Technology",2019 International Conference on Robotics and Automation (ICRA),"12 Aug 2019","2019","","","6359","6365","Accurate relative pose is one of the key components in visual odometry (VO) and simultaneous localization and mapping (SLAM). Recently, the self-supervised learning framework that jointly optimizes the relative pose and target image depth has attracted the attention of the community. Previous works rely on the photometric error generated from depths and poses between adjacent frames, which contains large systematic error under realistic scenes due to reflective surfaces and occlusions. In this paper, we bridge the gap between geometric loss and photometric loss by introducing the matching loss constrained by epipolar geometry in a self-supervised framework. Evaluated on the KITTI dataset, our method outperforms the state-of-the-art unsupervised egomotion estimation methods by a large margin. The code and data are available at https://github.com/hlzz/DeepMatchVO.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8793479","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8793479","","Simultaneous localization and mapping;Estimation;Geometry;Cameras;Motion estimation;Visualization;Visual odometry","","57","","52","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"BLVD: Building A Large-scale 5D Semantics Benchmark for Autonomous Driving","J. Xue; J. Fang; T. Li; B. Zhang; P. Zhang; Z. Ye; J. Dou","Institute of Artificial Intelligence and Robotics, Xian Jiaotong University, Xi’an, China; Institute of Artificial Intelligence and Robotics, Xian Jiaotong University, Xi’an, China; Institute of Artificial Intelligence and Robotics, Xian Jiaotong University, Xi’an, China; Institute of Artificial Intelligence and Robotics, Xian Jiaotong University, Xi’an, China; Institute of Artificial Intelligence and Robotics, Xian Jiaotong University, Xi’an, China; Institute of Artificial Intelligence and Robotics, Xian Jiaotong University, Xi’an, China; Institute of Artificial Intelligence and Robotics, Xian Jiaotong University, Xi’an, China",2019 International Conference on Robotics and Automation (ICRA),"12 Aug 2019","2019","","","6685","6691","In autonomous driving community, numerous benchmarks have been established to assist the tasks of 3D/2D object detection, stereo vision, semantic/instance segmentation. However, the more meaningful dynamic evolution of the surrounding objects of ego-vehicle is rarely exploited, and lacks a large-scale dataset platform. To address this, we introduce BLVD, a large-scale 5D semantics benchmark which does not concentrate on the static detection or semantic/instance segmentation tasks tackled adequately before. Instead, BLVD aims to provide a platform for the tasks of dynamic 4D (3D+temporal) tracking, 5D (4D+interactive) interactive event recognition and intention prediction. This benchmark will boost the deeper understanding of traffic scenes than ever before. We totally yield 249, 129 3D annotations, 4, 902 independent individuals for tracking with the length of overall 214, 922 points, 6, 004 valid fragments for 5D interactive event recognition, and 4, 900 individuals for 5D intention prediction. These tasks are contained in four kinds of scenarios depending on the object density (low and high) and light conditions (daytime and nighttime). The benchmark can be downloaded from our project site https://github.com/VCCIV/BLVD/.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8793523","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8793523","","Three-dimensional displays;Benchmark testing;Trajectory;Roads;Task analysis;Semantics;Legged locomotion","","38","","28","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"SqueezeSegV2: Improved Model Structure and Unsupervised Domain Adaptation for Road-Object Segmentation from a LiDAR Point Cloud","B. Wu; X. Zhou; S. Zhao; X. Yue; K. Keutzer","UC, Berkeley; UC, Berkeley; UC, Berkeley; UC, Berkeley; UC, Berkeley",2019 International Conference on Robotics and Automation (ICRA),"12 Aug 2019","2019","","","4376","4382","Earlier work demonstrates the promise of deep-learning-based approaches for point cloud segmentation; however, these approaches need to be improved to be practically useful. To this end, we introduce a new model SqueezeSegV2. With an improved model structure, SqueezeSetV2 is more robust against dropout noises in LiDAR point cloud and therefore achieves significant accuracy improvement. Training models for point cloud segmentation requires large amounts of labeled data, which is expensive to obtain. To sidestep the cost of data collection and annotation, simulators such as GTA-V can be used to create unlimited amounts of labeled, synthetic data. However, due to domain shift, models trained on synthetic data often do not generalize well to the real world. Existing domain-adaptation methods mainly focus on images and most of them cannot be directly applied to point clouds. We address this problem with a domain-adaptation training pipeline consisting of three major components: 1) learned intensity rendering, 2) geodesic correlation alignment, and 3) progressive domain calibration. When trained on real data, our new model exhibits segmentation accuracy improvements of 6.0-8.6% over the original SqueezeSeg. When training our new model on synthetic data using the proposed domain adaptation pipeline, we nearly double test accuracy on real-world data, from 29.0% to 57.4%. Our source code and synthetic dataset are open sourced11https://github.com/xuanyuzhou98/SqueezeSegV2","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8793495","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8793495","","Three-dimensional displays;Laser radar;Training;Adaptation models;Data models;Pipelines;Sensors","","422","","35","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"PointNetGPD: Detecting Grasp Configurations from Point Sets","H. Liang; X. Ma; S. Li; M. Görner; S. Tang; B. Fang; F. Sun; J. Zhang","TAMS (Technical Aspects of Multimodal Systems), Universität Hamburg; Tsinghua National Laboratory for Information Science and Technology (TNList), State Key Lab on Intelligent Technology and Systems, Tsinghua University; TAMS (Technical Aspects of Multimodal Systems), Universität Hamburg; TAMS (Technical Aspects of Multimodal Systems), Universität Hamburg; TAMS (Technical Aspects of Multimodal Systems), Universität Hamburg; Tsinghua National Laboratory for Information Science and Technology (TNList), State Key Lab on Intelligent Technology and Systems, Tsinghua University; Tsinghua National Laboratory for Information Science and Technology (TNList), State Key Lab on Intelligent Technology and Systems, Tsinghua University; TAMS (Technical Aspects of Multimodal Systems), Universität Hamburg",2019 International Conference on Robotics and Automation (ICRA),"12 Aug 2019","2019","","","3629","3635","In this paper, we propose an end-to-end grasp evaluation model to address the challenging problem of localizing robot grasp configurations directly from the point cloud. Compared to recent grasp evaluation metrics that are based on handcrafted depth features and a convolutional neural network (CNN), our proposed PointNetGPD is lightweight and can directly process the 3D point cloud that locates within the gripper for grasp evaluation. Taking the raw point cloud as input, our proposed grasp evaluation network can capture the complex geometric structure of the contact area between the gripper and the object even if the point cloud is very sparse. To further improve our proposed model, we generate a large-scale grasp dataset with 350k real point cloud and grasps with the YCB object set for training. The performance of the proposed model is quantitatively measured both in simulation and on robotic hardware. Experiments on object grasping and clutter removal show that our proposed model generalizes well to novel objects and outperforms state-of-the-art methods. Code and video are available at https://lianghongzhuo.github.io/PointNetGPD.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8794435","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794435","","Three-dimensional displays;Robot sensing systems;Measurement;Grippers;Grasping;Solid modeling;Geometry","","230","","28","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"Multi-View Picking: Next-best-view Reaching for Improved Grasping in Clutter","D. Morrison; P. Corke; J. Leitner","Australian Centre for Robotic Vision (ACRV), Queensland University of Technology (QUT), Brisbane, Australia; Australian Centre for Robotic Vision (ACRV), Queensland University of Technology (QUT), Brisbane, Australia; Australian Centre for Robotic Vision (ACRV), Queensland University of Technology (QUT), Brisbane, Australia",2019 International Conference on Robotics and Automation (ICRA),"12 Aug 2019","2019","","","8762","8768","Camera viewpoint selection is an important aspect of visual grasp detection, especially in clutter where many occlusions are present. Where other approaches use a static camera position or fixed data collection routines, our Multi-View Picking (MVP) controller uses an active perception approach to choose informative viewpoints based directly on a distribution of grasp pose estimates in real time, reducing uncertainty in the grasp poses caused by clutter and occlusions. In trials of grasping 20 objects from clutter, our MVP controller achieves 80% grasp success, outperforming a single-viewpoint grasp detector by 12%. We also show that our approach is both more accurate and more efficient than approaches which consider multiple fixed viewpoints. Code is available at https://github.com/dougsm/mvp_grasp.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8793805","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8793805","","Visualization;Entropy;Cameras;Grasping;Clutter;Robot vision systems","","46","","28","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"MoveIt! Task Constructor for Task-Level Motion Planning","M. Görner; R. Haschke; H. Ritter; J. Zhang","SynTouch, Los Angeles, CA; SynTouch, Los Angeles, CA; SynTouch, Los Angeles, CA; Berke Prosthetics, San Mateo, CA",2019 International Conference on Robotics and Automation (ICRA),"12 Aug 2019","2019","","","190","196","A lot of motion planning research in robotics focuses on efficient means to find trajectories between individual start and goal regions, but it remains challenging to specify and plan robotic manipulation actions which consist of multiple interdependent subtasks. The Task Constructor framework we present in this work provides a flexible and transparent way to define and plan such actions, enhancing the capabilities of the popular robotic manipulation framework MoveIt!.11The Task Constructor framework is publicly available at https://github.com/ros-planning/moveit_task_constructor Subproblems are solved in isolation in black-box planning stages and a common interface is used to pass solution hypotheses between stages. The framework enables the hierarchical organization of basic stages using containers, allowing for sequential as well as parallel compositions. The flexibility of the framework is illustrated in multiple scenarios performed on various robot platforms, including bimanual ones.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8793898","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8793898","","Planning;Task analysis;Robots;Trajectory;Containers;Generators;Collision avoidance","","60","","18","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"A Code for Unscented Kalman Filtering on Manifolds (UKF-M)","M. Brossard; A. Barrau; S. Bonnabel","Centre for Robotics, MINES ParisTech, PSL Research University, Paris, France; Safran Tech, Groupe Safran, Rue des Jeunes Bois-Châteaufort, Magny Les Hameaux Cedex, France; Centre for Robotics, MINES ParisTech, PSL Research University, Paris, France",2020 IEEE International Conference on Robotics and Automation (ICRA),"15 Sep 2020","2020","","","5701","5708","The present paper introduces a novel methodology for Unscented Kalman Filtering (UKF) on manifolds that extends previous work by the authors on UKF on Lie groups. Beyond filtering performance, the main interests of the approach are its versatility, as the method applies to numerous state estimation problems, and its simplicity of implementation for practitioners not being necessarily familiar with manifolds and Lie groups. We have developed the method on two independent open-source Python and Matlab frameworks we call UKF-M, for quickly implementing and testing the approach. The online repositories contain tutorials, documentation, and various relevant robotics examples that the user can readily reproduce and then adapt, for fast prototyping and benchmarking. The code is available at https://github.com/CAOR-MINES-ParisTech/ukfm.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197489","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197489","","Manifolds;Kalman filters;Two dimensional displays;Dispersion;Robots;Covariance matrices","","32","","54","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Fast, Compact and Highly Scalable Visual Place Recognition through Sequence-based Matching of Overloaded Representations","S. Garg; M. Milford","QUT Centre for Robotics, School of Electrical Engineering and Robotics at the Queensland University of Technology; QUT Centre for Robotics, School of Electrical Engineering and Robotics at the Queensland University of Technology",2020 IEEE International Conference on Robotics and Automation (ICRA),"15 Sep 2020","2020","","","3341","3348","Visual place recognition algorithms trade off three key characteristics: their storage footprint, their computational requirements, and their resultant performance, often expressed in terms of recall rate. Significant prior work has investigated highly compact place representations, sub-linear computational scaling and sub-linear storage scaling techniques, but have always involved a significant compromise in one or more of these regards, and have only been demonstrated on relatively small datasets. In this paper we present a novel place recognition system which enables for the first time the combination of ultra-compact place representations, near sub-linear storage scaling and extremely lightweight compute requirements. Our approach exploits the inherently sequential nature of much spatial data in the robotics domain and inverts the typical target criteria, through intentionally coarse scalar quantization-based hashing that leads to more collisions but is resolved by sequence-based matching. For the first time, we show how effective place recognition rates can be achieved on a new very large 10 million place dataset, requiring only 8 bytes of storage per place and 37K unitary operations to achieve over 50% recall for matching a sequence of 100 frames, where a conventional stateof-the-art approach both consumes 1300 times more compute and fails catastrophically. We present analysis investigating the effectiveness of our hashing overload approach under varying sizes of quantized vector length, comparison of near miss matches with the actual match selections and characterise the effect of variance re-scaling of data on quantization. Resource link: https://github.com/oravus/CoarseHash.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9196827","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196827","","Quantization (signal);Visualization;Indexes;Robots;Principal component analysis;Benchmark testing;Image recognition","","10","","58","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Brno Urban Dataset - The New Data for Self-Driving Agents and Mapping Tasks","A. Ligocki; A. Jelinek; L. Zalud","Central European Institute of Technology (CEITEC), Cybernetics in Material Science research group, Brno University of Technology, BrnoKralovo Pole, Czechia; Central European Institute of Technology (CEITEC), Cybernetics in Material Science research group, Brno University of Technology, BrnoKralovo Pole, Czechia; Central European Institute of Technology (CEITEC), Cybernetics in Material Science research group, Brno University of Technology, BrnoKralovo Pole, Czechia",2020 IEEE International Conference on Robotics and Automation (ICRA),"15 Sep 2020","2020","","","3284","3290","Autonomous driving is a dynamically growing field of research, where quality and amount of experimental data is critical. Although several rich datasets are available these days, the demands of researchers and technical possibilities are evolving. Through this paper, we bring a new dataset recorded in Brno - Czech Republic. It offers data from four WUXGA cameras, two 3D LiDARs, inertial measurement unit, infrared camera and especially differential RTK GNSS receiver with centimetre accuracy which, to the best knowledge of the authors, is not available from any other public dataset so far. In addition, all the data are precisely timestamped with submillisecond precision to allow wider range of applications. At the time of publishing of this paper, recordings of more than 350 km of rides in varying environment are shared at: https://github.com/RoboticsBUT/Brno-Urban-Dataset.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197277","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197277","","Cameras;Sensors;Global Positioning System;Global navigation satellite system;Receivers;Laser radar;Synchronization","","23","","37","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"CAGE: Context-Aware Grasping Engine","W. Liu; A. Daruna; S. Chernova","Georgia Institute of Technology, Atlanta, Georgia, United States; Georgia Institute of Technology, Atlanta, Georgia, United States; Georgia Institute of Technology, Atlanta, Georgia, United States",2020 IEEE International Conference on Robotics and Automation (ICRA),"15 Sep 2020","2020","","","2550","2556","Semantic grasping is the problem of selecting stable grasps that are functionally suitable for specific object manipulation tasks. In order for robots to effectively perform object manipulation, a broad sense of contexts, including object and task constraints, needs to be accounted for. We introduce the Context-Aware Grasping Engine, which combines a novel semantic representation of grasp contexts with a neural network structure based on the Wide & Deep model, capable of capturing complex reasoning patterns. We quantitatively validate our approach against three prior methods on a novel dataset consisting of 14,000 semantic grasps for 44 objects, 7 tasks, and 6 different object states. Our approach outperformed all baselines by statistically significant margins, producing new insights into the importance of balancing memorization and generalization of contexts for semantic grasping. We further demonstrate the effectiveness of our approach on robot experiments in which the presented model successfully achieved 31 of 32 suitable grasps. The code and data are available at: https://github.com/wliu88/railsemanticgrasping.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197289","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197289","","Semantics;Task analysis;Grasping;Feature extraction;Robots;Cognition;Context modeling","","21","","37","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"AU-AIR: A Multi-modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance","I. Bozcan; E. Kayacan","Department of Engineering, Aarhus University, Aarhus C, Denmark; Department of Engineering, Aarhus University, Aarhus C, Denmark",2020 IEEE International Conference on Robotics and Automation (ICRA),"15 Sep 2020","2020","","","8504","8510","Unmanned aerial vehicles (UAVs) with mounted cameras have the advantage of capturing aerial (bird-view) images. The availability of aerial visual data and the recent advances in object detection algorithms led the computer vision community to focus on object detection tasks on aerial images. As a result of this, several aerial datasets have been introduced, including visual data with object annotations. UAVs are used solely as flying-cameras in these datasets, discarding different data types regarding the flight (e.g., time, location, internal sensors). In this work, we propose a multi-purpose aerial dataset (AU-AIR) that has multi-modal sensor data (i.e., visual, time, location, altitude, IMU, velocity) collected in real-world outdoor environments. The AU-AIR dataset includes meta-data for extracted frames (i.e., bounding box annotations for traffic-related object category) from recorded RGB videos. Moreover, we emphasize the differences between natural and aerial images in the context of object detection task. For this end, we train and test mobile object detectors (including YOLOv3-Tiny and MobileNetv2-SSDLite) on the AU-AIR dataset, which are applicable for real-time object detection using on-board computers with UAVs. Since our dataset has diversity in recorded data types, it contributes to filling the gap between computer vision and robotics. The dataset is available at https://bozcani.github.io/auairdataset.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9196845","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196845","","Object detection;Videos;Visualization;Cameras;Detectors;Surveillance","","92","","29","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"3DCFS: Fast and Robust Joint 3D Semantic-Instance Segmentation via Coupled Feature Selection","L. Du; J. Tan; X. Xue; L. Chen; H. Wen; J. Feng; J. Li; X. Zhang","Institute of Science and Technology for Brain-Inspired Intelligence, Fudan University, China; Chinese Academy of Sciences, Shanghai Institute of Microsystem and Information Technology, Shanghai, China; School of Computer Science, Fudan University, China; Chinese Academy of Sciences, Shanghai Institute of Microsystem and Information Technology, Shanghai, China; Institute of Science and Technology for Brain-Inspired Intelligence, Fudan University, China; Institute of Science and Technology for Brain-Inspired Intelligence, Fudan University, China; Chinese Academy of Sciences, Shanghai Institute of Microsystem and Information Technology, Shanghai, China; Chinese Academy of Sciences, Shanghai Institute of Microsystem and Information Technology, Shanghai, China",2020 IEEE International Conference on Robotics and Automation (ICRA),"15 Sep 2020","2020","","","6868","6875","We propose a novel fast and robust 3D point clouds segmentation framework via coupled feature selection, named 3DCFS, that jointly performs semantic and instance segmentation. Inspired by the human scene perception process, we design a novel coupled feature selection module, named CFSM, that adaptively selects and fuses the reciprocal semantic and instance features from two tasks in a coupled manner. To further boost the performance of the instance segmentation task in our 3DCFS, we investigate a loss function that helps the model learn to balance the magnitudes of the output embedding dimensions during training, which makes calculating the Euclidean distance more reliable and enhances the generalizability of the model. Extensive experiments demonstrate that our 3DCFS outperforms state-of-the-art methods on benchmark datasets in terms of accuracy, speed and computational cost. Codes are available at: https://github.com/Biotan/3DCFS.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197242","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197242","","Semantics;Task analysis;Three-dimensional displays;Feature extraction;Logic gates;Training;Euclidean distance","","9","","48","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Learning Camera Miscalibration Detection","A. Cramariuc; A. Petrov; R. Suri; M. Mittal; R. Siegwart; C. Cadena","ETH, Autonomous Systems Lab, Zurich, Switzerland; ETH, Autonomous Systems Lab, Zurich, Switzerland; ETH, Autonomous Systems Lab, Zurich, Switzerland; ETH, Autonomous Systems Lab, Zurich, Switzerland; ETH, Autonomous Systems Lab, Zurich, Switzerland; ETH, Autonomous Systems Lab, Zurich, Switzerland",2020 IEEE International Conference on Robotics and Automation (ICRA),"15 Sep 2020","2020","","","4997","5003","Self-diagnosis and self-repair are some of the key challenges in deploying robotic platforms for long-term real-world applications. One of the issues that can occur to a robot is miscalibration of its sensors due to aging, environmental transients, or external disturbances. Precise calibration lies at the core of a variety of applications, due to the need to accurately perceive the world. However, while a lot of work has focused on calibrating the sensors, not much has been done towards identifying when a sensor needs to be recalibrated. This paper focuses on a data-driven approach to learn the detection of miscalibration in vision sensors, specifically RGB cameras. Our contributions include a proposed miscalibration metric for RGB cameras and a novel semi-synthetic dataset generation pipeline based on this metric. Additionally, by training a deep convolutional neural network, we demonstrate the effectiveness of our pipeline to identify whether a recalibration of the camera’s intrinsic parameters is required or not. The code is available at http://github.com/ethz-asl/camera_miscalib_detection.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197378","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197378","","Cameras;Calibration;Robots;Training;Sensor systems","","8","","32","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Instance Segmentation of LiDAR Point Clouds","F. Zhang; C. Guan; J. Fang; S. Bai; R. Yang; P. H. S. Torr; V. Prisacariu",University of Oxford; Baidu Research; Baidu Research; University of Oxford; Baidu Research; University of Oxford; University of Oxford,2020 IEEE International Conference on Robotics and Automation (ICRA),"15 Sep 2020","2020","","","9448","9455","We propose a robust baseline method for instance segmentation which are specially designed for large-scale outdoor LiDAR point clouds. Our method includes a novel dense feature encoding technique, allowing the localization and segmentation of small, far-away objects, a simple but effective solution for single-shot instance prediction and effective strategies for handling severe class imbalances. Since there is no public dataset for the study of LiDAR instance segmentation, we also build a new publicly available LiDAR point cloud dataset to include both precise 3D bounding box and point-wise labels for instance segmentation, while still being about 3~20 times as large as other existing LiDAR datasets. The dataset will be published at https://github.com/feihuzhang/LiDARSeg.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9196622","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196622","","Three-dimensional displays;Laser radar;Image segmentation;Two dimensional displays;Encoding;Semantics;Feature extraction","","39","","60","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Robust 6D Object Pose Estimation by Learning RGB-D Features","M. Tian; L. Pan; M. H. Ang; G. Hee Lee","Department of Mechanical Engineering, National University of Singapore; Department of Mechanical Engineering, National University of Singapore; Department of Mechanical Engineering, National University of Singapore; Computer Vision and Robotic Perception Lab, Department of Computer Science, School of Computing, National University of Singapore",2020 IEEE International Conference on Robotics and Automation (ICRA),"15 Sep 2020","2020","","","6218","6224","Accurate 6D object pose estimation is fundamental to robotic manipulation and grasping. Previous methods follow a local optimization approach which minimizes the distance between closest point pairs to handle the rotation ambiguity of symmetric objects. In this work, we propose a novel discrete- continuous formulation for rotation regression to resolve this local-optimum problem. We uniformly sample rotation anchors in SO(3), and predict a constrained deviation from each anchor to the target, as well as uncertainty scores for selecting the best prediction. Additionally, the object location is detected by aggregating point-wise vectors pointing to the 3D center. Experiments on two benchmarks: LINEMOD and YCB-Video, show that the proposed method outperforms state-of-the-art approaches. Our code is available at https://github.com/mentian/object-posenet.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197555","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197555","","Feature extraction;Pose estimation;Three-dimensional displays;Robustness;Uncertainty;Image segmentation;Robots","","30","","41","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"BayesOD: A Bayesian Approach for Uncertainty Estimation in Deep Object Detectors","A. Harakeh; M. Smart; S. L. Waslander","Institute For Aerospace Studies (UTIAS), University of Toronto, Toronto, Canada; Department of Mechanical and Mechatronics Engineering, University of Waterloo, Waterloo, Canada; Institute For Aerospace Studies (UTIAS), University of Toronto, Toronto, Canada",2020 IEEE International Conference on Robotics and Automation (ICRA),"15 Sep 2020","2020","","","87","93","When incorporating deep neural networks into robotic systems, a major challenge is the lack of uncertainty measures associated with their output predictions. Methods for uncertainty estimation in the output of deep object detectors (DNNs) have been proposed in recent works, but have had limited success due to 1) information loss at the detectors nonmaximum suppression (NMS) stage, and 2) failure to take into account the multitask, many-to-one nature of anchor-based object detection. To that end, we introduce BayesOD, an uncertainty estimation approach that reformulates the standard object detector inference and Non-Maximum suppression components from a Bayesian perspective. Experiments performed on four common object detection datasets show that BayesOD provides uncertainty estimates that are better correlated with the accuracy of detections, manifesting as a significant reduction of 9.77%-13.13% on the minimum Gaussian uncertainty error metric and a reduction of 1.63%-5.23% on the minimum Categorical uncertainty error metric. Code will be released at https://github.com/asharakeh/bayes-od-rc.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9196544","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196544","","Uncertainty;Detectors;Neural networks;Bayes methods;Estimation;Object detection;Measurement uncertainty","","74","","27","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"DeepRacer: Autonomous Racing Platform for Experimentation with Sim2Real Reinforcement Learning","B. Balaji; S. Mallya; S. Genc; S. Gupta; L. Dirac; V. Khare; G. Roy; T. Sun; Y. Tao; B. Townsend; E. Calleja; S. Muralidhara; D. Karuppasamy",Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services,2020 IEEE International Conference on Robotics and Automation (ICRA),"15 Sep 2020","2020","","","2746","2754","DeepRacer is a platform for end-to-end experimentation with RL and can be used to systematically investigate the key challenges in developing intelligent control systems. Using the platform, we demonstrate how a 1/18th scale car can learn to drive autonomously using RL with a monocular camera. It is trained in simulation with no additional tuning in the physical world and demonstrates: 1) formulation and solution of a robust reinforcement learning algorithm, 2) narrowing the reality gap through joint perception and dynamics, 3) distributed on-demand compute architecture for training optimal policies, and 4) a robust evaluation method to identify when to stop training. It is the first successful large-scale deployment of deep reinforcement learning on a robotic control agent that uses only raw camera images as observations and a model-free learning method to perform robust path planning. We open source our code and video demo on GitHub2.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197465","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197465","","Training;Automobiles;Robots;Computational modeling;Cameras;Robustness;Navigation","","59","","91","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"360SD-Net: 360° Stereo Depth Estimation with Learnable Cost Volume","N. -H. Wang; B. Solarte; Y. -H. Tsai; W. -C. Chiu; M. Sun",National Tsing Hua University; National Tsing Hua University; NEC Labs America; National Chiao Tung University; National Tsing Hua University,2020 IEEE International Conference on Robotics and Automation (ICRA),"15 Sep 2020","2020","","","582","588","Recently, end-to-end trainable deep neural networks have significantly improved stereo depth estimation for perspective images. However, 360° images captured under equirectangular projection cannot benefit from directly adopting existing methods due to distortion introduced (i.e., lines in 3D are not projected onto lines in 2D). To tackle this issue, we present a novel architecture specifically designed for spherical disparity using the setting of top-bottom 360° camera pairs. Moreover, we propose to mitigate the distortion issue by (1) an additional input branch capturing the position and relation of each pixel in the spherical coordinate, and (2) a cost volume built upon a learnable shifting filter. Due to the lack of 360° stereo data, we collect two 360° stereo datasets from Matterport3D and Stanford3D for training and evaluation. Extensive experiments and ablation study are provided to validate our method against existing algorithms. Finally, we show promising results on real-world environments capturing images with two consumer-level cameras. Our project page is at https://albert100121.github.io/360SD-Net-Project-Page.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9196975","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196975","","Cameras;Estimation;Three-dimensional displays;Distortion;Feature extraction;Convolution;Training","","42","","34","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Are We Ready for Service Robots? The OpenLORIS-Scene Datasets for Lifelong SLAM","X. Shi; D. Li; P. Zhao; Q. Tian; Y. Tian; Q. Long; C. Zhu; J. Song; F. Qiao; L. Song; Y. Guo; Z. Wang; Y. Zhang; B. Qin; W. Yang; F. Wang; R. H. M. Chan; Q. She","Intel Labs China, Beijing, China; Department of Electronic Engineering and BNRist, Tsinghua University, Beijing, China; Intel Labs China, Beijing, China; Department of Electronic Engineering and BNRist, Tsinghua University, Beijing, China; Department of Electronic Engineering and BNRist, Tsinghua University, Beijing, China; Department of Electronic Engineering and BNRist, Tsinghua University, Beijing, China; Department of Electronic Engineering and BNRist, Tsinghua University, Beijing, China; Department of Electronic Engineering and BNRist, Tsinghua University, Beijing, China; Department of Electronic Engineering and BNRist, Tsinghua University, Beijing, China; Gaussian Robotics, Shanghai, China; Gaussian Robotics, Shanghai, China; Intel Labs China, Beijing, China; Intel Labs China, Beijing, China; Gaussian Robotics, Shanghai, China; Beijing Jiaotong University, Beijing, China; Beijing Jiaotong University, Beijing, China; City University of Hong Kong, Hong Kong, China; Intel Labs China, Beijing, China",2020 IEEE International Conference on Robotics and Automation (ICRA),"15 Sep 2020","2020","","","3139","3145","Service robots should be able to operate autonomously in dynamic and daily changing environments over an extended period of time. While Simultaneous Localization And Mapping (SLAM) is one of the most fundamental problems for robotic autonomy, most existing SLAM works are evaluated with data sequences that are recorded in a short period of time. In real-world deployment, there can be out-of-sight scene changes caused by both natural factors and human activities. For example, in home scenarios, most objects may be movable, replaceable or deformable, and the visual features of the same place may be significantly different in some successive days. Such out-of-sight dynamics pose great challenges to the robustness of pose estimation, and hence a robot’s long-term deployment and operation. To differentiate the forementioned problem from the conventional works which are usually evaluated in a static setting in a single run, the term lifelong SLAM is used here to address SLAM problems in an ever-changing environment over a long period of time. To accelerate lifelong SLAM research, we release the OpenLORIS-Scene datasets. The data are collected in real-world indoor scenes, for multiple times in each place to include scene changes in real life. We also design benchmarking metrics for lifelong SLAM, with which the robustness and accuracy of pose estimation are evaluated separately. The datasets and benchmark are available online at lifelong-robotic-vision.github.io/dataset/scene.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9196638","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196638","","Simultaneous localization and mapping;Robot kinematics;Cameras;Synchronization;Trajectory","","87","","25","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"AC/DCC : Accurate Calibration of Dynamic Camera Clusters for Visual SLAM","J. Rebello; A. Fung; S. L. Waslander","TRAILab, University of Toronto Institute for Aerospace Studies (UTIAS); MIE department, University of Toronto; University of Toronto Institute for Aerospace Studies (UTIAS)",2020 IEEE International Conference on Robotics and Automation (ICRA),"15 Sep 2020","2020","","","6035","6041","In order to relate information across cameras in a Dynamic Camera Cluster (DCC), an accurate time-varying set of extrinsic calibration transformations need to be determined. Previous calibration approaches rely solely on collecting measurements from a known fiducial target which limits calibration accuracy as insufficient excitation of the gimbal is achieved. In this paper, we improve DCC calibration accuracy by collecting measurements over the entire configuration space of the gimbal and achieve a 10X improvement in pixel re-projection error. We perform a joint optimization over the calibration parameters between any number of cameras and unknown joint angles using a pose-loop error optimization approach, thereby avoiding the need for overlapping fields-of-view. We test our method in simulation and provide a calibration sensitivity analysis for different levels of camera intrinsic and joint angle noise. In addition, we provide a novel analysis of the degenerate parameters in the calibration when joint angle values are unknown, which avoids situations in which the calibration cannot be uniquely recovered. The calibration code will be made available at https://github.com/TRAILab/AC-DCC.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197217","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197217","","Cameras;Calibration;Robot vision systems;Simultaneous localization and mapping;Vehicle dynamics;Optimization;Measurement uncertainty","","1","","22","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Form2Fit: Learning Shape Priors for Generalizable Assembly from Disassembly","K. Zakka; A. Zeng; J. Lee; S. Song",Stanford University; Google; Google; Google,2020 IEEE International Conference on Robotics and Automation (ICRA),"15 Sep 2020","2020","","","9404","9410","Is it possible to learn policies for robotic assembly that can generalize to new objects? We explore this idea in the context of the kit assembly task. Since classic methods rely heavily on object pose estimation, they often struggle to generalize to new objects without 3D CAD models or task-specific training data. In this work, we propose to formulate the kit assembly task as a shape matching problem, where the goal is to learn a shape descriptor that establishes geometric correspondences between object surfaces and their target placement locations from visual input. This formulation enables the model to acquire a broader understanding of how shapes and surfaces fit together for assembly - allowing it to generalize to new objects and kits. To obtain training data for our model, we present a self-supervised data-collection pipeline that obtains ground truth object-to-placement correspondences by disassembling complete kits. Our resulting real-world system, Form2Fit, learns effective pick and place strategies for assembling objects into a variety of kits - achieving 90% average success rates under different initial conditions (e.g. varying object and kit poses), 94% success under new configurations of multiple kits, and over 86% success with completely new objects and kits. Code, videos, and supplemental material are available at https://form2fit.github.io.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9196733","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196733","","Task analysis;Shape;Visualization;Training data;Three-dimensional displays;Solid modeling;Training","","62","","36","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Integrated Motion Planner for Real-time Aerial Videography with a Drone in a Dense Environment","B. Jeon; Y. Lee; H. J. Kim","Department of mechanical and aerospace engineering, Seoul national university, South Korea; Department of mechanical and aerospace engineering, Seoul national university, South Korea; Department of mechanical and aerospace engineering, Seoul national university, South Korea",2020 IEEE International Conference on Robotics and Automation (ICRA),"15 Sep 2020","2020","","","1243","1249","This work suggests an integrated approach for a drone (or multirotor) to perform an autonomous videography task in a 3-D obstacle environment by following a moving object. The proposed system includes 1) a target motion prediction module which can be applied to dense environments and 2) a hierarchical chasing planner. Leveraging covariant optimization, the prediction module estimates the future motion of the target assuming it efforts to avoid the obstacles. The other module, chasing planner, is in a bi-level structure composed of preplanner and smooth planner. In the first phase, we exploit a graph-search method to plan a chasing corridor which incorporates safety and visibility of target. In the subsequent phase, we generate a smooth and dynamically feasible trajectory within the corridor using quadratic programming (QP). We validate our approach with multiple complex scenarios and actual experiments. The source code and the experiment video can be found in https://github.com/icsl-Jeon/traj_gen_vis and https://www.youtube.com/watch?v=_JSwXBwYRl8.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9196703","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196703","","Drones;Trajectory;Safety;Optimization;Measurement;Shape;Real-time systems","","33","","17","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"OpenLORIS-Object: A Robotic Vision Dataset and Benchmark for Lifelong Deep Learning","Q. She; F. Feng; X. Hao; Q. Yang; C. Lan; V. Lomonaco; X. Shi; Z. Wang; Y. Guo; Y. Zhang; F. Qiao; R. H. M. Chan","Intel Labs, Robot Innovation Lab, Beijing, China; Department of Electrical Engineering, City University of Hong Kong, China; Department of Electronic Engineering, Tsinghua University, China; Department of Electrical Engineering, City University of Hong Kong, China; Department of Electrical Engineering, City University of Hong Kong, China; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Intel Labs, Robot Innovation Lab, Beijing, China; Insight Centre for Data Analytics, Dublin City University, Ireland; Imperial College, The Hamlyn Centre for Robotic Surgery, London, UK; Intel Labs, Robot Innovation Lab, Beijing, China; Department of Electronic Engineering, Tsinghua University, China; Department of Electrical Engineering, City University of Hong Kong, China",2020 IEEE International Conference on Robotics and Automation (ICRA),"15 Sep 2020","2020","","","4767","4773","The recent breakthroughs in computer vision have benefited from the availability of large representative datasets (e.g. ImageNet and COCO) for training. Yet, robotic vision poses unique challenges for applying visual algorithms developed from these standard computer vision datasets due to their implicit assumption over non-varying distributions for a fixed set of tasks. Fully retraining models each time a new task becomes available is infeasible due to computational, storage and sometimes privacy issues, while naïve incremental strategies have been shown to suffer from catastrophic forgetting. It is crucial for the robots to operate continuously under open-set and detrimental conditions with adaptive visual perceptual systems, where lifelong learning is a fundamental capability. However, very few datasets and benchmarks are available to evaluate and compare emerging techniques. To fill this gap, we provide a new lifelong robotic vision dataset (""OpenLORIS-Object"") collected via RGB-D cameras. The dataset embeds the challenges faced by a robot in the real-life application and provides new benchmarks for validating lifelong object recognition algorithms. Moreover, we have provided a testbed of 9 state-of-the-art lifelong learning algorithms. Each of them involves 48 tasks with 4 evaluation metrics over the OpenLORIS-Object dataset. The results demonstrate that the object recognition task in the ever-changing difficulty environments is far from being solved and the bottlenecks are at the forward/backward transfer designs. Our dataset and benchmark are publicly available at https://lifelong-robotic-vision.github.io/dataset/object.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9196887","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196887","","Task analysis;Lighting;Object recognition;Cameras;Robot vision systems;Clutter","","41","","44","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Weakly Supervised Silhouette-based Semantic Scene Change Detection","K. Sakurada; M. Shibuya; W. Wang","Artificial Intelligence Research Center, National Institute of Advanced Industrial Science and Technology, Tokyo, Japan; Artificial Intelligence Research Center, National Institute of Advanced Industrial Science and Technology, Tokyo, Japan; Artificial Intelligence Research Center, National Institute of Advanced Industrial Science and Technology, Tokyo, Japan",2020 IEEE International Conference on Robotics and Automation (ICRA),"15 Sep 2020","2020","","","6861","6867","This paper presents a novel semantic scene change detection scheme with only weak supervision. A straightforward approach for this task is to train a semantic change detection network directly from a large-scale dataset in an end-to-end manner. However, a specific dataset for this task, which is usually labor-intensive and time-consuming, becomes indispensable. To avoid this problem, we propose to train this kind of network from existing datasets by dividing this task into change detection and semantic extraction. On the other hand, the difference in camera viewpoints, for example, images of the same scene captured from a vehicle-mounted camera at different time points, usually brings a challenge to the change detection task. To address this challenge, we propose a new siamese network structure with the introduction of correlation layer. In addition, we create a publicly available dataset for semantic change detection to evaluate the proposed method. The experimental results verified both the robustness to viewpoint difference in change detection task and the effectiveness for semantic change detection of the proposed networks. Our code and dataset are available at https://github.com/xdspacelab/sscdnet.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9196985","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196985","","Semantics;Image segmentation;Cameras;Training;Task analysis;Satellites;Estimation","","34","","45","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Robust, Occlusion-aware Pose Estimation for Objects Grasped by Adaptive Hands","B. Wen; C. Mitash; S. Soorian; A. Kimmel; A. Sintov; K. E. Bekris","Computer Science Dept. of Rutgers, Univ. in NJ, USA; Computer Science Dept. of Rutgers, Univ. in NJ, USA; Computer Science Dept. of Rutgers, Univ. in NJ, USA; Computer Science Dept. of Rutgers, Univ. in NJ, USA; Computer Science Dept. of Rutgers, Univ. in NJ, USA; Computer Science Dept. of Rutgers, Univ. in NJ, USA",2020 IEEE International Conference on Robotics and Automation (ICRA),"15 Sep 2020","2020","","","6210","6217","Many manipulation tasks, such as placement or within-hand manipulation, require the object's pose relative to a robot hand. The task is difficult when the hand significantly occludes the object. It is especially hard for adaptive hands, for which it is not easy to detect the finger's configuration. In addition, RGB-only approaches face issues with texture-less objects or when the hand and the object look similar. This paper presents a depth-based framework, which aims for robust pose estimation and short response times. The approach detects the adaptive hand's state via efficient parallel search given the highest overlap between the hand's model and the point cloud. The hand's point cloud is pruned and robust global registration is performed to generate object pose hypotheses, which are clustered. False hypotheses are pruned via physical reasoning. The remaining poses' quality is evaluated given agreement with observed data. Extensive evaluation on synthetic and real data demonstrates the accuracy and computational efficiency of the framework when applied on challenging, highly-occluded scenarios for different object types. An ablation study identifies how the framework's components help in performance. This work also provides a dataset for in-hand 6D object pose estimation. Code and dataset are available at: https://github.com/wenbowen123/icra20-hand-object-pose.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197350","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197350","","Three-dimensional displays;Pose estimation;Robot sensing systems;Robustness;Computational modeling;Solid modeling","","27","","60","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Dynamic Anchor Selection for Improving Object Localization","P. Shyam; K. -J. Yoon; K. -S. Kim","Department of Mechanical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea; Department of Mechanical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea; Department of Mechanical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea",2020 IEEE International Conference on Robotics and Automation (ICRA),"15 Sep 2020","2020","","","9477","9483","Anchor boxes act as potential object localization candidates allow single-stage detectors to achieve real-time performance, at the cost of localization accuracy when compared to state-of-the-art two-stage detectors. Therefore, correct selection of the scale and aspect ratio associated with an anchor box is crucial for detector performance. In this work, we propose a novel architecture called DANet for improving the localization performance of single-stage object detectors, while maintaining real-time inference. The proposed network achieves this by predicting (1) the combination of aspect ratio and scale per feature map based on object density and (2) localization confidence per anchor box. We evaluate the proposed network using the benchmark dataset. On the MS COCO dataset, DANet achieves 30.9% AP at 51.8 fps using ResNet-18 and 45.3% AP at 7.4 fps using ResNeXt-101. The code and models will be available at https://github.com/PS06/AnchorNet.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197076","","Feature extraction;Detectors;Computer architecture;Task analysis;Spatial resolution;Head;Object detection","","4","","46","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Real-time Data Driven Precision Estimator for RAVEN-II Surgical Robot End Effector Position","H. Peng; X. Yang; Y. -H. Su; B. Hannaford","Department of Electrical and Computer Engineering, University of Washington, Seattle, WA, USA; Department of Electrical and Computer Engineering, University of Washington, Seattle, WA, USA; Department of Electrical and Computer Engineering, University of Washington, Seattle, WA, USA; Department of Electrical and Computer Engineering, University of Washington, Seattle, WA, USA",2020 IEEE International Conference on Robotics and Automation (ICRA),"15 Sep 2020","2020","","","350","356","Surgical robots have been introduced to operating rooms over the past few decades due to their high sensitivity, small size, and remote controllability. The cable-driven nature of many surgical robots allows the systems to be dexterous and lightweight, with diameters as low as 5mm. However, due to the slack and stretch of the cables and the backlash of the gears, inevitable uncertainties are brought into the kinematics calcu-lation [1]. Since the reported end effector position of surgical robots like RAVEN-II [2] is directly calculated using the motor encoder measurements and forward kinematics, it may contain relatively large error up to 10mm, whereas semi-autonomous functions being introduced into abdominal surgeries require position inaccuracy of at most 1mm. To resolve the problem, a cost-effective, real-time and data-driven pipeline for robot end effector position precision estimation is proposed and tested on RAVEN-II. Analysis shows an improved end effector position error of around 1mm RMS traversing through the entire robot workspace without high-resolution motion tracker. The open source code, data sets, videos, and user guide can be found at //github.com/HaonanPeng/RAVEN Neural Network Estimator.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9196915","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196915","","End effectors;Cameras;Medical robotics;Robot sensing systems;Image edge detection;Surgery","","16","","26","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"FADNet: A Fast and Accurate Network for Disparity Estimation","Q. Wang; S. Shi; S. Zheng; K. Zhao; X. Chu","Department of Computer Science, Hong Kong Baptist University; Department of Computer Science, Hong Kong Baptist University; Department of Computer Science, Hong Kong Baptist University; Department of Computer Science, Hong Kong Baptist University; Department of Computer Science, Hong Kong Baptist University",2020 IEEE International Conference on Robotics and Automation (ICRA),"15 Sep 2020","2020","","","101","107","Deep neural networks (DNNs) have achieved great success in the area of computer vision. The disparity estimation problem tends to be addressed by DNNs which achieve much better prediction accuracy in stereo matching than traditional hand-crafted feature based methods. On one hand, however, the designed DNNs require significant memory and computation resources to accurately predict the disparity, especially for those 3D convolution based networks, which makes it difficult for deployment in real-time applications. On the other hand, existing computation-efficient networks lack expression capability in large-scale datasets so that they cannot make an accurate prediction in many scenarios. To this end, we propose an efficient and accurate deep network for disparity estimation named FADNet with three main features: 1) It exploits efficient 2D based correlation layers with stacked blocks to preserve fast computation; 2) It combines the residual structures to make the deeper model easier to learn; 3) It contains multi-scale predictions so as to exploit a multi-scale weight scheduling training technique to improve the accuracy. We conduct experiments to demonstrate the effectiveness of FADNet on two popular datasets, Scene Flow and KITTI 2015. Experimental results show that FADNet achieves state-of-the-art prediction accuracy, and runs at a significant order of magnitude faster speed than existing 3D models. The codes of FADNet are available at https://github.com/HKBU-HPML/FADNet.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197031","","Estimation;Convolution;Correlation;Three-dimensional displays;Training;Feature extraction;Computer architecture","","58","","26","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Loam livox: A fast, robust, high-precision LiDAR odometry and mapping package for LiDARs of small FoV","J. Lin; F. Zhang","Department of Mechanical Engineering, Hong Kong University, Hong Kong SAR., China; Department of Mechanical Engineering, Hong Kong University, Hong Kong SAR., China",2020 IEEE International Conference on Robotics and Automation (ICRA),"15 Sep 2020","2020","","","3126","3131","LiDAR odometry and mapping (LOAM) has been playing an important role in autonomous vehicles, due to its ability to simultaneously localize the robot’s pose and build high-precision, high-resolution maps of the surrounding environment. This enables autonomous navigation and safe path planning of autonomous vehicles. In this paper, we present a robust, real-time LOAM algorithm for LiDARs with small FoV and irregular samplings. By taking effort on both frontend and back-end, we address several fundamental challenges arising from such LiDARs, and achieve better performance in both precision and efficiency compared to existing baselines. To share our findings and to make contributions to the community, we open source our codes on Github1.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197440","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197440","","Laser radar;Feature extraction;Three-dimensional displays;Measurement by laser beam;Laser noise;Real-time systems;Spinning","","228","","20","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Exploring Dynamic Context for Multi-path Trajectory Prediction","H. Cheng; W. Liao; X. Tang; M. Y. Yang; M. Sester; B. Rosenhahn","Institute of Cartography and Geoinformatics, Leibniz University Hannover, Germany; Institute of Information Processing, Leibniz University Hannover, Germany; Institute of Information Processing, Leibniz University Hannover, Germany; Scene Understanding Group, University of Twente, The Netherlands; Institute of Cartography and Geoinformatics, Leibniz University Hannover, Germany; Institute of Information Processing, Leibniz University Hannover, Germany",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","12795","12801","To accurately predict future positions of different agents in traffic scenarios is crucial for safely deploying intelligent autonomous systems in the real-world environment. However, it remains a challenge due to the behavior of a target agent being affected by other agents dynamically and there being more than one socially possible paths the agent could take. In this paper, we propose a novel framework, named Dynamic Context Encoder Network (DCENet). In our framework, first, the spatial context between agents is explored by using self-attention architectures. Then, the two-stream encoders are trained to learn temporal context between steps by taking the respective observed trajectories and the extracted dynamic spatial context as input. The spatial-temporal context is encoded into a latent space using a Conditional Variational Auto-Encoder (CVAE) module. Finally, a set of future trajectories for each agent is predicted conditioned on the learned spatial-temporal context by sampling from the latent space, repeatedly. DCENet is evaluated on one of the most popular challenging benchmarks for trajectory forecasting Trajnet and reports a new state-of-the-art performance. It also demonstrates superior performance evaluated on the benchmark inD for mixed traffic at intersections. A series of ablation studies is conducted to validate the effectiveness of each proposed module. Our code is available at https://github.com/wtliao/DCENet.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9562034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9562034","","Codes;Automation;Autonomous systems;Conferences;Layout;Benchmark testing;Trajectory","","23","","51","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"RGB-D SLAM with Structural Regularities","Y. Li; R. Yunus; N. Brasch; N. Navab; F. Tombari","Technical University of Munich, Germany; Technical University of Munich, Germany; Technical University of Munich, Germany; Technical University of Munich, Germany; Technical University of Munich, Germany",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","11581","11587","This work proposes a RGB-D SLAM system specifically designed for structured environments and aimed at improved tracking and mapping accuracy by relying on geometric features that are extracted from the surrounding. Structured environments offer, in addition to points, also an abundance of geometrical features such as lines and planes, which we exploit to design both the tracking and mapping components of our SLAM system. For the tracking part, we explore geometric relationships between these features based on the assumption of a Manhattan World (MW). We propose a decoupling-refinement method based on points, lines, and planes, as well as the use of Manhattan relationships in an additional pose refinement module. For the mapping part, different levels of maps from sparse to dense are reconstructed at a low computational cost. We propose an instance-wise meshing strategy to build a dense map by meshing plane instances independently. The overall performance in terms of pose estimation and reconstruction is evaluated on public benchmarks and shows improved performance compared to state-of-the-art methods. The code is released at https://github.com/yanyan-li/PlanarSLAM.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561560","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561560","","Simultaneous localization and mapping;Codes;Automation;Conferences;Pose estimation;Benchmark testing;Feature extraction","","54","","38","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Adaptive Sampling using POMDPs with Domain-Specific Considerations","G. Salhotra; C. E. Denniston; D. A. Caron; G. S. Sukhatme",University of Southern California; University of Southern California; University of Southern California; University of Southern California,2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","2385","2391","We investigate improving Monte Carlo Tree Search based solvers for Partially Observable Markov Decision Processes (POMDPs), when applied to adaptive sampling problems. We propose improvements in rollout allocation, the action exploration algorithm, and plan commitment. The first allocates a different number of rollouts depending on how many actions the agent has taken in an episode. We find that rollouts are more valuable after some initial information is gained about the environment. Thus, a linear increase in the number of rollouts, i.e. allocating a fixed number at each step, is not appropriate for adaptive sampling tasks. The second alters which actions the agent chooses to explore when building the planning tree. We find that by using knowledge of the number of rollouts allocated, the agent can more effectively choose actions to explore. The third improvement is in determining how many actions the agent should take from one plan. Typically, an agent will plan to take the first action from the planning tree and then call the planner again from the new state. Using statistical techniques, we show that it is possible to greatly reduce the number of rollouts by increasing the number of actions taken from a single planning tree without affecting the agent’s final reward. Finally, we demonstrate experimentally, on simulated and real aquatic data from an underwater robot, that these improvements can be combined, leading to better adaptive sampling. The code for this work is available at https://github.com/uscresl/AdaptiveSamplingPOMCP.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561319","California State Water Resources Control Board; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561319","","Monte Carlo methods;Codes;Conferences;Markov processes;Unmanned underwater vehicles;Search problems;Planning","","3","","22","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Unsupervised Learning of 3D Scene Flow from Monocular Camera","G. Wang; X. Tian; R. Ding; H. Wang","Department of Automation, Institute of Medical Robotics, Shanghai Engineering Research Center of Intelligent Control and Management, Shanghai Jiao Tong University, Key Laboratory of System Control and Information Processing of Ministry of Education, Key Laboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai, China; Department of Automation, Institute of Medical Robotics, Shanghai Engineering Research Center of Intelligent Control and Management, Shanghai Jiao Tong University, Key Laboratory of System Control and Information Processing of Ministry of Education, Key Laboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai, China; Department of Automation, Institute of Medical Robotics, Shanghai Engineering Research Center of Intelligent Control and Management, Shanghai Jiao Tong University, Key Laboratory of System Control and Information Processing of Ministry of Education, Key Laboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai, China; Department of Automation, Institute of Medical Robotics, Shanghai Engineering Research Center of Intelligent Control and Management, Shanghai Jiao Tong University, Key Laboratory of System Control and Information Processing of Ministry of Education, Key Laboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai, China",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","4325","4331","Scene flow represents the motion of points in the 3D space, which is the counterpart of the optical flow that represents the motion of pixels in the 2D image. However, it is difficult to obtain the ground truth of scene flow in the real scenes, and recent studies are based on synthetic data for training. Therefore, how to train a scene flow network with unsupervised methods based on real-world data shows crucial significance. A novel unsupervised learning method for scene flow is proposed in this paper, which utilizes the images of two consecutive frames taken by monocular camera without the ground truth of scene flow for training. Our method realizes the goal that training scene flow network with real-world data, which bridges the gap between training data and test data and broadens the scope of available data for training. Unsupervised learning of scene flow in this paper mainly consists of two parts: (i) depth estimation and camera pose estimation, and (ii) scene flow estimation based on four different loss functions. Depth estimation and camera pose estimation obtain the depth maps and camera pose between two consecutive frames, which provide further information for the next scene flow estimation. After that, we used depth consistency loss, dynamic-static consistency loss, Chamfer loss, and Laplacian regularization loss to carry out unsupervised training of the scene flow network. To our knowledge, this is the first paper that realizes the unsupervised learning of 3D scene flow from monocular camera. The experiment results on KITTI show that our method for unsupervised learning of scene flow meets great performance compared to traditional methods Iterative Closest Point (ICP) and Fast Global Registration (FGR). The source code is available at: https://github.com/IRMVLab/3DUnMonoFlow.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561572","Shanghai Municipal Education Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561572","","Training;Three-dimensional displays;Laser radar;Laplace equations;Pose estimation;Training data;Cameras","","12","","51","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"AcinoSet: A 3D Pose Estimation Dataset and Baseline Models for Cheetahs in the Wild","D. Joska; L. Clark; N. Muramatsu; R. Jericevich; F. Nicolls; A. Mathis; M. W. Mathis; A. Patel","African Robotics Unit (ARU), University of Cape Town, South Africa; African Robotics Unit (ARU), University of Cape Town, South Africa; University of Tsukuba, Japan; African Robotics Unit (ARU), University of Cape Town, South Africa; African Robotics Unit (ARU), University of Cape Town, South Africa; École Polytechnique Fédérale de Lausanne, Switzerland; École Polytechnique Fédérale de Lausanne, Switzerland; African Robotics Unit (ARU), University of Cape Town, South Africa",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","13901","13908","Animals are capable of extreme agility, yet understanding their complex dynamics, which have ecological, biomechanical and evolutionary implications, remains challenging. Being able to study this incredible agility will be critical for the development of next-generation autonomous legged robots. In particular, the cheetah (acinonyx jubatus) is supremely fast and maneuverable, yet quantifying its wholebody 3D kinematic data during locomotion in the wild remains a challenge, even with new deep learning-based methods. In this work we present an extensive dataset of free-running cheetahs in the wild, called AcinoSet, that contains 119, 490 frames of multi-view synchronized high-speed video footage, camera calibration files and 7, 588 human-annotated frames. We utilize markerless animal pose estimation to provide 2D keypoints. Then, we use three methods that serve as strong baselines for 3D pose estimation tool development: traditional sparse bundle adjustment, an Extended Kalman Filter, and a trajectory optimization-based method we call Full Trajectory Estimation. The resulting 3D trajectories, human-checked 3D ground truth, and an interactive tool to inspect the data is also provided. We believe this dataset will be useful for a diverse range of fields such as ecology, neuroscience, robotics, biomechanics as well as computer vision. Code and data can be found at: https://github.com/African-Robotics-Unit/AcinoSet.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561338","National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561338","","Biomechanics;Legged locomotion;Solid modeling;Neuroscience;Animals;Pose estimation;Tools","","36","","50","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Locus: LiDAR-based Place Recognition using Spatiotemporal Higher-Order Pooling","K. Vidanapathirana; P. Moghadam; B. Harwood; M. Zhao; S. Sridharan; C. Fookes","DATA61, CSIRO, Robotics and Autonomous Systems Group, Brisbane, QLD, Australia; DATA61, CSIRO, Robotics and Autonomous Systems Group, Brisbane, QLD, Australia; DATA61, CSIRO, Robotics and Autonomous Systems Group, Brisbane, QLD, Australia; DATA61, CSIRO, Robotics and Autonomous Systems Group, Brisbane, QLD, Australia; School of Electrical Engineering and Robotics, Queensland University of Technology (QUT), Brisbane, Australia; School of Electrical Engineering and Robotics, Queensland University of Technology (QUT), Brisbane, Australia",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","5075","5081","Place Recognition enables the estimation of a globally consistent map and trajectory by providing non-local constraints in Simultaneous Localisation and Mapping (SLAM). This paper presents Locus, a novel place recognition method using 3D LiDAR point clouds in large-scale environments. We propose a method for extracting and encoding topological and temporal information related to components in a scene and demonstrate how the inclusion of this auxiliary information in place description leads to more robust and discriminative scene representations. Second-order pooling along with a non- linear transform is used to aggregate these multi-level features to generate a fixed-length global descriptor, which is invariant to the permutation of input features. The proposed method outperforms state-of-the-art methods on the KITTI dataset. Furthermore, Locus is demonstrated to be robust across several challenging situations such as occlusions and viewpoint changes in 3D LiDAR point clouds. The open-source implementation is available at: https://github.com/csiro-robotics/locus.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9560915","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9560915","","Three-dimensional displays;Laser radar;Simultaneous localization and mapping;Estimation;Transforms;Feature extraction;Robustness","","53","","29","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Graph-SIM: A Graph-based Spatiotemporal Interaction Modelling for Pedestrian Action Prediction","T. Yau; S. Malekmohammadi; A. Rasouli; P. Lakner; M. Rohani; J. Luo","Noah’s Ark Lab, Huawei, Canada; Noah’s Ark Lab, Huawei, Canada; Noah’s Ark Lab, Huawei, Canada; Noah’s Ark Lab, Huawei, Canada; Noah’s Ark Lab, Huawei, Canada; Noah’s Ark Lab, Huawei, Canada",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","8580","8586","One of the most crucial yet challenging tasks for autonomous vehicles in urban environments is predicting the future behaviour of nearby pedestrians, especially at points of crossing. Predicting behaviour depends on many social and environmental factors, particularly interactions between road users. Capturing such interactions requires a global view of the scene and dynamics of the road users in three-dimensional space. This information, however, is missing from the current pedestrian behaviour benchmark datasets. Motivated by these challenges, we propose 1) a novel graph-based model for predicting pedestrian crossing action. Our method models pedestrians’ interactions with nearby road users through clustering and relative importance weighting of interactions using features obtained from the bird’s-eye-view. 2) We introduce a new dataset that provides 3D bounding box and pedestrian behavioural annotations for the existing nuScenes dataset. On the new data, our approach achieves state-of-the-art performance by improving on various metrics by more than 15% in comparison to existing methods. The dataset is available at https://github.com/huawei-noah/datasets/PePScenes.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561107","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561107","","Measurement;Solid modeling;Three-dimensional displays;Annotations;Roads;Urban areas;Predictive models","","19","","37","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Inverse Dynamics Control of Compliant Hybrid Zero Dynamic Walking","J. Reher; A. D. Ames","Department of Mechanical and Civil Engineering, California Institute of Technology, Pasadena, CA; Department of Mechanical and Civil Engineering, California Institute of Technology, Pasadena, CA",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","2040","2047","We present a trajectory planning and control architecture for bipedal locomotion at a variety of speeds on a highly underactuated and compliant bipedal robot. A library of compliant walking trajectories are planned offline, and stored as compact arrays of polynomial coefficients for tracking online. The control implementation uses a floating-base inverse dynamics controller which generates dynamically consistent feedforward torques to realize walking using information obtained from the trajectory optimization. The effectiveness of the controller is demonstrated in simulation and on hardware for walking both indoors on flat terrain and over unplanned disturbances outdoors. Additionally, both the controller and optimization source code are made available on GitHub.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9560906","Disney Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9560906","","Legged locomotion;Codes;Automation;Trajectory planning;Conferences;Libraries;Hardware","","12","","29","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"No-frills Dynamic Planning using Static Planners","M. Levy; V. Ayyagari; A. Shrivastava","University of Maryland, College Park; University of Maryland, College Park; University of Maryland, College Park",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","2005","2011","In this paper, we address the task of interacting with dynamic environments where the changes in the environment are independent of the agent. We study this through the context of trapping a moving ball with a UR5 robotic arm. Our key contribution is an approach to utilize a static planner for dynamic tasks using a Dynamic Planning add-on; that is, if we can successfully solve a task with a static target, then our approach can solve the same task when the target is moving. Our approach has three key components: an off-the-shelf static planner, a trajectory forecasting network, and a network to predict robot’s estimated time of arrival at any location. We demonstrate the generalization of our approach across environments. More information and videos at https://mlevy2525.github.io/DynamicAddOn/.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9560762","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9560762","","Automation;Conferences;Manipulators;Planning;Trajectory;Synchronization;Noise measurement","","","","24","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Conquering Textureless with RF-referenced Monocular Vision for MAV State Estimation","S. Zhang; S. Tang; W. Wang; T. Jiang; Q. Zhang","School of Electronic Information and Communications, HUST, Wuhan, China; School of Electronic Information and Communications, HUST, Wuhan, China; School of Electronic Information and Communications, HUST, Wuhan, China; School of Electronic Information and Communications, HUST, Wuhan, China; Department of Computer Science and Engineering, HKUST, Hong Kong, SAR China",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","146","152","The versatile nature of agile micro aerial vehicles (MAVs) poses fundamental challenges to the design of robust state estimation in various complex environments. Achieving high-quality performance in textureless scenes is one of the missing pieces in the puzzle. Previously proposed solutions either seek a remedy with visual loop closure or leverage RF localizability with inferior accuracy. None of them support accurate MAV state estimation in textureless scenes. This paper presents RFSift, a new state estimator that conquers the textureless challenge with RF-referenced monocular vision, achieving centimeter-level accuracy in textureless scenes. Our key observation is that RF and visual measurements are tied up with pose constraints. Mapping RF to feature quality and sift well-matched ones significantly improves accuracy. RFSift consists of 1) an RF-sifting algorithm that maps 3D UWB measurements to 2D visual features for sifting the best features; 2) an RF-visual-inertial sensor fusion algorithm that enables robust state estimation by leveraging multiple sensors with complementary advantages. We implement the prototype with off-the-shelf products and conduct large-scale experiments. The results demonstrate that RFSift is robust in textureless scenes, 10x more accurate than the state-of-the-art monocular vision system. The code of RFSift is available at https://github.com/weisgroup/RFSift.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9560744","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9560744","","Radio frequency;Visualization;Three-dimensional displays;Navigation;Machine vision;Conferences;Robot vision systems","","1","","40","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"MDANet: Multi-Modal Deep Aggregation Network for Depth Completion","Y. Ke; K. Li; W. Yang; Z. Xu; D. Hao; L. Huang; G. Wang","University of Science and Technology of China; Damo Academy, Alibaba Group; University of Science and Technology of China; University of Science and Technology of China; Damo Academy, Alibaba Group; University of Science and Technology of China; Damo Academy, Alibaba Group",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","4288","4294","Depth completion aims to recover the dense depth map from sparse depth data and RGB image respectively. However, due to the huge difference between the multi-modal signal input, vanilla convolutional neural network and simple fusion strategy cannot extract features from sparse data and aggregate multi-modal information effectively. To tackle this problem, we design a novel network architecture that takes full advantage of multi-modal features for depth completion. An effective Pre-completion algorithm is first put forward to increase the density of the input depth map and to provide distribution priors. Moreover, to effectively fuse the image features and the depth features, we propose a multi-modal deep aggregation block that consists of multiple connection and aggregation pathways for deeper fusion. Furthermore, based on the intuition that semantic image features are beneficial for accurate contour, we introduce the deformable guided fusion layer to guide the generation of the dense depth map. The resulting architecture, called MDANet, outperforms all the stateof-the-art methods on the popular KITTI Depth Completion Benchmark, meanwhile with fewer parameters than recent methods. The code of this work will be available at https://github.com/USTC-Keyanjie/MDANet_ICRA2021.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561490","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561490","","Image resolution;Fuses;Semantics;Network architecture;Data aggregation;Feature extraction;Data mining","","9","","43","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Deep Balanced Learning for Long-tailed Facial Expressions Recognition","H. Gao; S. An; J. Li; C. Liu","State Key Laboratory of Bioelectronics, School of Instrument Science and Engineering, Southeast University, Nanjing, China; Tech & Data Center, JD.COM Inc., Beijing, China; State Key Laboratory of Bioelectronics, School of Instrument Science and Engineering, Southeast University, Nanjing, China; State Key Laboratory of Bioelectronics, School of Instrument Science and Engineering, Southeast University, Nanjing, China",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","11147","11153","The analysis of facial expression is a very complex and challenging problem. Most researches for automated Facial Expression Recognition (FER) are mainly based on deep learning networks, rarely considering data imbalance. This paper commits to addressing the long-tail distribution problems among large-scale datasets in wild. Inspired by the continual learning method, we reconstruct multi-subsets first by randomly selecting from head classes and up-sampling tail classes. A pre-trained backbone is then introduced to learn general weights in a repeatedly train-prune fashion. Hereafter, our approach creatively trains a new classifier based on union parameters previously preserved and achieves an outperformance without extra parameters added in, using the gradual-prune technique. The results show that the independent training of classifiers has been a contributing factor. We successfully conduct this experiment with several classic networks, prove its effectiveness in training a deep network on imbalanced dataset. In the face of the poor performance in current FER, we find that domain knowledge is somehow affecting the accuracy of recognition by further exploring the obstacles from the image itself.Code available at https://github.com/Epicghx/FER","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561155","National Natural Science Foundation of China; Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561155","","Training;Deep learning;Emotion recognition;Image recognition;Head;Automation;Face recognition","","3","","39","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"YolactEdge: Real-time Instance Segmentation on the Edge","H. Liu; R. A. Rivera Soto; F. Xiao; Y. Jae Lee","Amazon Web Services, Inc., University of California, Davis; Amazon Web Services, Inc., University of California, Davis; Amazon Web Services, Inc., University of California, Davis; Amazon Web Services, Inc., University of California, Davis",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","9579","9585","We propose YolactEdge, the first competitive instance segmentation approach that runs on small edge devices at real-time speeds. Specifically, YolactEdge runs at up to 30.8 FPS on a Jetson AGX Xavier (and 172.7 FPS on an RTX 2080 Ti) with a ResNet-101 backbone on 550x550 resolution images. To achieve this, we make two improvements to the state-of-the-art image-based real-time method YOLACT [1]: (1) applying TensorRT optimization while carefully trading off speed and accuracy, and (2) a novel feature warping module to exploit temporal redundancy in videos. Experiments on the YouTube VIS and MS COCO datasets demonstrate that YolactEdge produces a 3-5x speed up over existing real-time methods while producing competitive mask and box detection accuracy. We also conduct ablation studies to dissect our design choices and modules. Code and models are available at https://github.com/haotian-liu/yolact_edge.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561858","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561858","","Image segmentation;Image resolution;Codes;Automation;Image edge detection;Conferences;Redundancy","","45","","25","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"“What’s This?” - Learning to Segment Unknown Objects from Manipulation Sequences","W. Boerdijk; M. Sundermeyer; M. Durner; R. Triebel","German Aerospace Center (DLR), Institute of Robotics and Mechatronics, Wessling, Germany; German Aerospace Center (DLR), Institute of Robotics and Mechatronics, Wessling, Germany; German Aerospace Center (DLR), Institute of Robotics and Mechatronics, Wessling, Germany; German Aerospace Center (DLR), Institute of Robotics and Mechatronics, Wessling, Germany",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","10160","10167","We present a novel framework for self-supervised grasped object segmentation with a robotic manipulator. Our method successively learns an agnostic foreground segmentation followed by a distinction between manipulator and object solely by observing the motion between consecutive RGB frames. In contrast to previous approaches, we propose a single, end-to-end trainable architecture which jointly incorporates motion cues and semantic knowledge. Furthermore, while the motion of the manipulator and the object are substantial cues for our algorithm, we present means to robustly deal with distraction objects moving in the background, as well as with completely static scenes. Our method neither depends on any visual registration of a kinematic robot or 3D object models, nor on precise hand-eye calibration or any additional sensor data. By extensive experimental evaluation we demonstrate the superiority of our framework and provide detailed insights on its capability of dealing with the aforementioned extreme cases of motion. We also show that training a semantic segmentation network with the automatically labeled data achieves results on par with manually annotated training data. Code and pretrained model are available at https://github.com/DLR-RM/DistinctNet.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9560806","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9560806","","Training;Visualization;Three-dimensional displays;Motion segmentation;Semantics;Training data;Object segmentation","","3","","52","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"CamVox: A Low-cost and Accurate Lidar-assisted Visual SLAM System","Y. Zhu; C. Zheng; C. Yuan; X. Huang; X. Hong","School of System Design and Intelligent Manufacturing, Southern University of Science and Technology, China; School of System Design and Intelligent Manufacturing, Southern University of Science and Technology, China; School of System Design and Intelligent Manufacturing, Southern University of Science and Technology, China; School of System Design and Intelligent Manufacturing, Southern University of Science and Technology, China; School of System Design and Intelligent Manufacturing, Southern University of Science and Technology, China",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","5049","5055","Combining lidar in camera-based simultaneous localization and mapping (SLAM) is an effective method in improving overall accuracy, especially at outdoor large scale scenes. Recent development of low-cost lidars (e.g. Livox lidar) enable us to explore such SLAM systems with lower budget and higher performance. In this paper we propose CamVox by adapting Livox lidars into visual SLAM (ORB-SLAM2) by exploring the lidars’ unique features. Based on the unique scan pattern of Livox lidars, we propose an automatic lidar-camera calibration method that will work in uncontrolled scenes. The long depth detection range also benefit a more accurate mapping. Comparison of CamVox with visual SLAM (VINS-mono) and lidar SLAM (LOAM) are evaluated on the same dataset to demonstrate the performance. We open sourced our hardware, code and dataset on GitHub1.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561149","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561149","","Visualization;Simultaneous localization and mapping;Laser radar;Codes;Automation;Conferences;Hardware","","66","","22","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Learning Spatial Context with Graph Neural Network for Multi-Person Pose Grouping","J. Jiahao Lin; G. Hee Lee","Department of Computer Science, National University of Singapore; Department of Computer Science, National University of Singapore",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","4230","4236","Bottom-up approaches for image-based multi-person pose estimation consist of two stages: (1) keypoint detection and (2) grouping of the detected keypoints to form person instances. Current grouping approaches rely on learned embedding from only visual features that completely ignore the spatial configuration of human poses. In this work, we formulate the grouping task as a graph partitioning problem, where we learn the affinity matrix with a Graph Neural Network (GNN). More specifically, we design a Geometry-aware Association GNN that utilizes spatial information of the keypoints and learns local affinity from the global context. The learned geometry-based affinity is further fused with appearance-based affinity to achieve robust keypoint association. Spectral clustering is used to partition the graph for the formation of the pose instances. Experimental results on two benchmark datasets show that our proposed method outperforms existing appearance-only grouping frameworks, which shows the effectiveness of utilizing spatial context for robust grouping. Source code is available at: https://github.com/jiahaoLjh/PoseGrouping.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561960","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561960","","Visualization;Codes;Automation;Conferences;Pose estimation;Benchmark testing;Graph neural networks","","3","","32","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"VOLDOR+SLAM: For the times when feature-based or direct methods are not good enough","Z. Min; E. Dunn",Stevens Institute of Technology; Stevens Institute of Technology,2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","13813","13819","We present a dense-indirect SLAM system using external dense optical flows as input. We extend the recent probabilistic visual odometry model VOLDOR [1], by incorporating the use of geometric priors to 1) robustly bootstrap estimation from monocular capture, while 2) seamlessly supporting stereo and/or RGB-D input imagery. Our customized back-end tightly couples our intermediate geometric estimates with an adaptive priority scheme managing the connectivity of an incremental pose graph. We leverage recent advances in dense optical flow methods to achieve accurate and robust camera pose estimates, while constructing fine-grain globally-consistent dense environmental maps. Our open source implementation [https://github.com/htkseason/VOLDOR] operates online at around 15 FPS on a single GTX1080Ti GPU.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561230","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561230","","Couplings;Three-dimensional displays;Simultaneous localization and mapping;Estimation;Graphics processing units;Tools;Probabilistic logic","","14","","36","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Not your grandmother’s toolbox – the Robotics Toolbox reinvented for Python","P. Corke; J. Haviland","Queensland University of Technology Centre for Robotics (QCR), Brisbane, Australia; Queensland University of Technology Centre for Robotics (QCR), Brisbane, Australia",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","11357","11363","For 25 years the Robotics Toolbox for MATLAB® has been used for teaching and research worldwide. This paper describes its successor – the Robotics Toolbox for Python. More than just a port, it takes advantage of popular open-source packages and resources to provide platform portability, fast browser-based 3D graphics, quality documentation, fast numerical and symbolic operations, powerful IDEs, shareable and web-browseable notebooks all powered by GitHub and the open-source community. The new Toolbox provides well-known functionality for spatial mathematics (homogeneous transformations, quaternions, triple angles and twists), trajectories, kinematics (zeroth to second order), dynamics and a rich assortment of robot models. In addition, we’ve taken the opportunity to add new capabilities such as branched mechanisms, collision checking, URDF import, and interfaces to ROS. With familiar, simple yet powerful functions; the clarity of Python syntax; but without the complexity of ROS; users from beginner to advanced will find this a powerful open-source toolset for ongoing robotics education and research.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561366","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561366","","Codes;Three-dimensional displays;Education;Dynamics;Tutorials;Tools;Trajectory","","51","","25","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Detecting and Mapping Trees in Unstructured Environments with a Stereo Camera and Pseudo-Lidar","B. H. Wang; C. Diaz-Ruiz; J. Banfi; M. Campbell","Sibley School of Mechanical & Aerospace Engineering, Cornell University, Ithaca, NY, USA; Sibley School of Mechanical & Aerospace Engineering, Cornell University, Ithaca, NY, USA; Sibley School of Mechanical & Aerospace Engineering, Cornell University, Ithaca, NY, USA; Sibley School of Mechanical & Aerospace Engineering, Cornell University, Ithaca, NY, USA",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","14120","14126","We present a method for detecting and mapping trees in noisy stereo camera point clouds, using a learned 3D object detector. Inspired by recent advancements in 3-D object detection using a pseudo-lidar representation for stereo data, we train a PointRCNN detector to recognize trees in forest-like environments. We generate detector training data with a novel automatic labeling process that clusters a fused global point cloud. This process annotates large stereo point cloud training data sets with minimal user supervision, and unlike previous pseudo-lidar detection pipelines, requires no 3D ground truth from other sensors such as lidar. Our mapping system additionally uses a Kalman filter to associate detections and consistently estimate the positions and sizes of trees. We collect a data set for tree detection consisting of 8680 stereo point clouds, and validate our method on an outdoors test sequence. Our results demonstrate robust tree recognition in noisy stereo data at ranges of up to 7 meters, on 720p resolution images from a Stereolabs ZED 2 camera. Code and data are available at https://github.com/brian-h-wang/pseudolidar-tree-detection.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9562056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9562056","","Meters;Three-dimensional displays;Pipelines;Training data;Detectors;Cameras;Robot sensing systems","","5","","21","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"PyTouch: A Machine Learning Library for Touch Processing","M. Lambeta; H. Xu; J. Xu; P. -W. Chou; S. Wang; T. Darrell; R. Calandra","Facebook AI Research, Menlo Park, USA; University of California, Berkeley, USA; Shanghai Jiao Tong University, China; Facebook AI Research, Menlo Park, USA; Massachusetts Institute of Technology, USA; University of California, Berkeley, USA; Facebook AI Research, Menlo Park, USA",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","13208","13214","With the increased availability of rich tactile sensors, there is an an equally proportional need for open-source and integrated software capable of efficiently and effectively processing raw touch measurements into high-level signals that can be used for control and decision-making. In this paper, we present PyTouch – the first machine learning library dedicated to the processing of touch sensing signals. PyTouch, is designed to be modular, easy-to-use and provides state-of-the-art touch processing capabilities as a service with the goal of unifying the tactile sensing community by providing a library for building scalable, proven, and performance-validated modules over which applications and research can be built upon. We evaluate PyTouch on real-world data from several tactile sensors on touch processing tasks such as touch detection, slip and object pose estimations. PyTouch is open-sourced at https://github.com/facebookresearch/pytouch.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561084","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561084","","Software libraries;Software architecture;Pose estimation;Tactile sensors;Machine learning;Libraries;Sensors","","11","","19","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Exploiting Local Geometry for Feature and Graph Construction for Better 3D Point Cloud Processing with Graph Neural Networks","S. Srivastava; G. Sharma","Centre for Development of Advanced Computing, Noida, India; TensorTour and IIT Kanpur",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","12903","12909","We propose simple yet effective improvements in point representations and local neighborhood graph construction within the general framework of graph neural networks (GNNs) for 3D point cloud processing. As a first contribution, we propose to augment the vertex representations with important local geometric information of the points, followed by nonlinear projection using a MLP. As a second contribution, we propose to improve the graph construction for GNNs for 3D point clouds. The existing methods work with a k-NN based approach for constructing the local neighborhood graph. We argue that it might lead to reduction in coverage in case of dense sampling by sensors in some regions of the scene. The proposed methods aims to counter such problems and improve coverage in such cases. As the traditional GNNs were designed to work with general graphs, where vertices may have no geometric interpretations, we see both our proposals as augmenting the general graphs to incorporate the geometric nature of 3D point clouds. While being simple, we demonstrate with multiple challenging benchmarks, with relatively clean CAD models, as well as with real world noisy scans, that the proposed method achieves state of the art results on benchmarks for 3D classification (ModelNet40) , part segmentation (ShapeNet) and semantic segmentation (Stanford 3D Indoor Scenes Dataset). We also show that the proposed network achieves faster training convergence, i.e. ∼ 40% less epochs for classification. The project details are available at https://siddharthsrivastava.github.io/publication/geomgcnn/","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561327","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561327","","Geometry;Training;Solid modeling;Three-dimensional displays;Semantics;Benchmark testing;Graph neural networks","","10","","47","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"PENet: Towards Precise and Efficient Image Guided Depth Completion","M. Hu; S. Wang; B. Li; S. Ning; L. Fan; X. Gong","College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; Department of Turing Solution, Hisil-icon, Huawei, Shanghai, China; Department of Turing Solution, Hisil-icon, Huawei, Shanghai, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","13656","13662","Image guided depth completion is the task of generating a dense depth map from a sparse depth map and a high quality image. In this task, how to fuse the color and depth modalities plays an important role in achieving good performance. This paper proposes a two-branch backbone that consists of a color-dominant branch and a depth-dominant branch to exploit and fuse two modalities thoroughly. More specifically, one branch inputs a color image and a sparse depth map to predict a dense depth map. The other branch takes as inputs the sparse depth map and the previously predicted depth map, and outputs a dense depth map as well. The depth maps predicted from two branches are complimentary to each other and therefore they are adaptively fused. In addition, we also propose a simple geometric convolutional layer to encode 3D geometric cues. The geometric encoded backbone conducts the fusion of different modalities at multiple stages, leading to good depth completion results. We further implement a dilated and accelerated CSPN++ to refine the fused depth map efficiently. The proposed full model ranks 1st in the KITTI depth completion online leaderboard at the time of submission. It also infers much faster than most of the top ranked methods. The code of this work is available at https://github.com/JUGGHM/PENet_ICRA2021.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561035","","Convolutional codes;Three-dimensional displays;Automation;Fuses;Image color analysis;Conferences;Color","","196","","32","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"A Large-Scale Dataset for Benchmarking Elevator Button Segmentation and Character Recognition","J. Liu; Y. Fang; D. Zhu; N. Ma; J. Pan; M. Q. . -H. Meng","Department of Electronic Engineering, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China; Department of Electronic Engineering, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China; Department of Electronic Engineering, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China; Department of Electronic Engineering, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China; Department of Electronic Engineering, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China; Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen, China",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","14018","14024","Human activities are hugely restricted by COVID-19, recently. Robots that can conduct inter-floor navigation attract much public attention since they can substitute human workers to conduct the service work. However, current robots either depend on human assistance or elevator retrofitting, and fully autonomous inter-floor navigation is still not available. As the very first step of inter-floor navigation, elevator button segmentation and recognition hold an important position. Therefore, we release the first large-scale publicly available elevator panel dataset in this work, containing 3,718 panel images with 35,100 button labels, to facilitate more powerful algorithms on autonomous elevator operation. Together with the dataset, a number of deep learning based implementations for button segmentation and recognition are also released to benchmark future methods in the community. The dataset is available at https://github.com/zhudelong/elevator_button_recognition","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9562109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9562109","","Performance evaluation;Deep learning;COVID-19;Image segmentation;Navigation;Conferences;Benchmark testing","","6","","32","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"RELLIS-3D Dataset: Data, Benchmarks and Analysis","P. Jiang; P. Osteen; M. Wigness; S. Saripalli","J. Mike Walker ’66 Department of Mechanical Engineering, Texas A&M University, College Station, TX, USA; DEVCOM Army Research Laboratory (ARL), Adelphi, MD, USA; DEVCOM Army Research Laboratory (ARL), Adelphi, MD, USA; J. Mike Walker ’66 Department of Mechanical Engineering, Texas A&M University, College Station, TX, USA",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","1110","1116","Semantic scene understanding is crucial for robust and safe autonomous navigation, particularly so in off-road environments. Recent deep learning advances for 3D semantic segmentation rely heavily on large sets of training data, however existing autonomy datasets either represent urban environments or lack multimodal off-road data. We fill this gap with RELLIS-3D, a multimodal dataset collected in an off-road environment, which contains annotations for 13,556 LiDAR scans and 6,235 images. The data was collected on the Rellis Campus of Texas A&M University, and presents challenges to existing algorithms related to class imbalance and environmental topography. Additionally, we evaluate the current state of the art deep learning semantic segmentation models on this dataset. Experimental results show that RELLIS-3D presents challenges for algorithms designed for segmentation in urban environments. This novel dataset provides the resources needed by researchers to continue to develop more advanced algorithms and investigate new research directions to enhance autonomous navigation in off-road environments. RELLIS-3D is available at https://github.com/unmannedlab/RELLIS-3D","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561251","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561251","","Deep learning;Image segmentation;Three-dimensional displays;Laser radar;Conferences;Semantics;Urban areas","","115","","36","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Auto-Tuned Sim-to-Real Transfer","Y. Du; O. Watkins; T. Darrell; P. Abbeel; D. Pathak",UC Berkeley; UC Berkeley; UC Berkeley; UC Berkeley; Carnegie Mellon University,2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","1290","1296","Policies trained in simulation often fail when transferred to the real world due to the ‘reality gap’ where the simulator is unable to accurately capture the dynamics and visual properties of the real world. Current approaches to tackle this problem, such as domain randomization, require prior knowledge and engineering to determine how much to randomize system parameters in order to learn a policy that is robust to sim-to-real transfer while also not being too conservative. We propose a method for automatically tuning simulator system parameters to match the real world using only raw RGB images of the real world without the need to define rewards or estimate state. Our key insight is to reframe the auto-tuning of parameters as a search problem where we iteratively shift the simulation system parameters to approach the real world system parameters. We propose a Search Param Model (SPM) that, given a sequence of observations and actions and a set of system parameters, predicts whether the given parameters are higher or lower than the true parameters used to generate the observations. We evaluate our method on multiple robotic control tasks in both sim-to-sim and sim-to-real transfer, demonstrating significant improvement over naive domain randomization. Project videos at https://yuqingd.github.io/autotuned-sim2real/.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9562091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9562091","","Knowledge engineering;Visualization;Automation;Conferences;Predictive models;Search problems;Task analysis","","36","","25","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"CLIPPER: A Graph-Theoretic Framework for Robust Data Association","P. C. Lusk; K. Fathian; J. P. How","Department of Aeronautics and Astronautics, Massachusetts Institute of Technology; Department of Aeronautics and Astronautics, Massachusetts Institute of Technology; Department of Aeronautics and Astronautics, Massachusetts Institute of Technology",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","13828","13834","We present CLIPPER (Consistent LInking, Pruning, and Pairwise Error Rectification), a framework for robust data association in the presence of noise and outliers. We formulate the problem in a graph-theoretic framework using the notion of geometric consistency. State-of-the-art techniques that use this framework utilize either combinatorial optimization techniques that do not scale well to large-sized problems, or use heuristic approximations that yield low accuracy in high-noise, high-outlier regimes. In contrast, CLIPPER uses a relaxation of the combinatorial problem and returns solutions that are guaranteed to correspond to the optima of the original problem. Low time complexity is achieved with an efficient projected gradient ascent approach. Experiments indicate that CLIPPER maintains a consistently low runtime of 15 ms where exact methods can require up to 24 s at their peak, even on small-sized problems with 200 associations. When evaluated on noisy point cloud registration problems, CLIPPER achieves 100% precision and 98% recall in 90% outlier regimes while competing algorithms begin degrading by 70% outliers. In an instance of associating noisy points of the Stanford Bunny with 990 outlier associations and only 10 inlier associations, CLIPPER successfully returns 8 inlier associations with 100% precision in 138 ms. Code is available at https://mit-acl.github.io/clipper.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561069","Ford Motor Company; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561069","","Runtime;Codes;Automation;Conferences;Noise measurement;Time complexity;Optimization","","38","","35","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Neighborhood Spatial Aggregation based Efficient Uncertainty Estimation for Point Cloud Semantic Segmentation","C. Qi; J. Yin; H. Liu; J. Liu","School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China; School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; City University of Hong Kong, Hong Kong, China",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","14025","14031","Uncertainty estimation for point cloud semantic segmentation is to quantify the confidence degree for the predicted label of points, which is essential for decision-making tasks. This paper proposes a neighborhood spatial aggregation based method, NSA-MC dropout, to achieve efficient uncertainty estimation for point cloud semantic segmentation. Unlike the traditional uncertainty estimation method MC dropout de-pending on repeated inferences, our NSA-MC dropout achieves uncertainty estimation through one-time inference. Specifically, a space-dependent method is designed to sample the model many times by performing stochastic forward pass through the model just once, and it approximates the repeated inferences based sampling process in MC dropout. Besides, a neighborhood spatial aggregation module, called NSA, aggregates neighborhood probabilistic outputs for each point and works with space-dependent sampling to establish output distribution. Finally, we propose an uncertainty-aware framework NSA-MC dropout to capture the uncertainty of prediction results efficiently. Experimental results show that our method obtains comparable performance with MC dropout. More significantly, our NSA-MC dropout has little influence on the efficiency of semantic inference. It is much faster than MC dropout, and the inference time does not establish a coupling relation with the sampling times. Our code is available at https://github.com/chaoqi7/Uncertainty_Estimation_PCSS","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9560972","Fundamental Research Funds for the Central Universities; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9560972","","Visualization;Uncertainty;Design methodology;Semantics;Decision making;Estimation;Stochastic processes","","3","","24","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"RetinaGAN: An Object-aware Approach to Sim-to-Real Transfer","D. Ho; K. Rao; Z. Xu; E. Jang; M. Khansari; Y. Bai","Everyday Robots, X The Moonshot Factory, Mountain View, CA, USA; Robotics at Google, Mountain View, CA, USA; University of California, Berkeley, Berkeley, CA, USA; Robotics at Google, Mountain View, CA, USA; Everyday Robots, X The Moonshot Factory, Mountain View, CA, USA; Everyday Robots, X The Moonshot Factory, Mountain View, CA, USA",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","10920","10926","The success of deep reinforcement learning (RL) and imitation learning (IL) in vision-based robotic manipulation typically hinges on the expense of large scale data collection. With simulation, data to train a policy can be collected efficiently at scale, but the visual gap between sim and real makes deployment in the real world difficult. We introduce RetinaGAN, a generative adversarial network (GAN) approach to adapt simulated images to realistic ones with object-detection consistency. RetinaGAN is trained in an unsupervised manner without task loss dependencies, and preserves general object structure and texture in adapted images. We evaluate our method on three real world tasks: grasping, pushing, and door opening. RetinaGAN improves upon the performance of prior sim-to-real methods for RL-based object instance grasping and continues to be effective even in the limited data regime. When applied to a pushing task in a similar visual domain, RetinaGAN demonstrates transfer with no additional real data requirements. We also show our method bridges the visual gap for a novel door opening task using imitation learning in a new visual domain. Visit the project website at retinagan.github.io","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561157","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561157","","Training;Visualization;Grasping;Reinforcement learning;Fasteners;Data collection;Generative adversarial networks","","40","","33","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Learning World Transition Model for Socially Aware Robot Navigation","Y. Cui; H. Zhang; Y. Wang; R. Xiong","State Key Laboratory of Industrial Control and Technology, Zhejiang University, Hangzhou, P.R. China; State Key Laboratory of Industrial Control and Technology, Zhejiang University, Hangzhou, P.R. China; State Key Laboratory of Industrial Control and Technology, Zhejiang University, Hangzhou, P.R. China; State Key Laboratory of Industrial Control and Technology, Zhejiang University, Hangzhou, P.R. China",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","9262","9268","Moving in dynamic pedestrian environments is one of the important requirements for autonomous mobile robots. We present a model-based reinforcement learning approach for robots to navigate through crowded environments. The navigation policy is trained with both real interaction data from multi-agent simulation and virtual data from a deep transition model that predicts the evolution of surrounding dynamics of mobile robots. A reward function considering social conventions is designed to guide the training of the policy. Specifically, the policy model takes laser scan sequence and robot’s own state as input and outputs steering command. The laser sequence is further transformed into stacked local obstacle maps disentangled from robot’s ego motion to separate the static and dynamic obstacles, simplifying the model training. We observe that the policy using our method can be trained with significantly less real interaction data in simulator but achieve similar level of success rate in social navigation tasks compared with other methods. Experiments are conducted in multiple social scenarios both in simulation and on real robots, the learned policy can guide the robots to the final targets successfully in a socially compliant manner. Code is available at https://github.com/YuxiangCui/model-based-social-navigation.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561973","Nature; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561973","","Training;Navigation;Dynamics;Reinforcement learning;Predictive models;Laser modes;Data models","","17","","24","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Emergent Hand Morphology and Control from Optimizing Robust Grasps of Diverse Objects","X. Pan; A. Garg; A. Anandkumar; Y. Zhu",NVIDIA; NVIDIA; NVIDIA; NVIDIA,2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","7540","7547","Evolution in nature illustrates that the creatures’ biological structure and their sensorimotor skills adapt to the environmental changes for survival. Likewise, the ability to morph and acquire new skills can facilitate an embodied agent to solve tasks of varying complexities. In this work, we introduce a data-driven approach where effective hand designs naturally emerge for the purpose of grasping diverse objects. Jointly optimizing morphology and control imposes computational challenges since it requires constant evaluation of a black-box function that measures the performance of a combination of embodiment and behavior. We develop a novel Bayesian Optimization algorithm that efficiently co-designs the morphology and grasping skills through learned latent-space representations. We design the grasping tasks based on a taxonomy of human grasp types: power grasp, pinch grasp, and lateral grasp. Through experimentation and comparative study, we demonstrate that our approach discovers robust and cost-efficient hand morphologies for grasping novel objects. Additional videos and results at https://xinleipan.github.io/emergent_morphology","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9562092","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9562092","","Costs;Taxonomy;Morphology;Optimization methods;Grasping;Robot sensing systems;Hardware","","4","","48","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Contextual Latent-Movements Off-Policy Optimization for Robotic Manipulation Skills","S. Tosatto; G. Chalvatzaki; J. Peters","Intelligent Autonomous Systems, Technische Universität Darmstadt, Darmstadt, Germany; Intelligent Autonomous Systems, Technische Universität Darmstadt, Darmstadt, Germany; Intelligent Autonomous Systems, Technische Universität Darmstadt, Darmstadt, Germany",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","10815","10821","Parameterized movement primitives have been extensively used for imitation learning of robotic tasks. However, the high-dimensionality of the parameter space hinders the improvement of such primitives in the reinforcement learning (RL) setting, especially for learning with physical robots. In this paper we propose a novel view on handling the demonstrated trajectories for acquiring low-dimensional, non-linear latent dynamics, using mixtures of probabilistic principal component analyzers (MPPCA) on the movements’ parameter space. Moreover, we introduce a new contextual off-policy RL algorithm, named LAtent-Movements Policy Optimization (LAMPO). LAMPO can provide gradient estimates from previous experience using self-normalized importance sampling, hence, making full use of samples collected in previous learning iterations. These advantages combined provide a complete framework for sample-efficient off-policy optimization of movement primitives for robot learning of high-dimensional manipulation skills. Our experimental results conducted both in simulation and on a real robot show that LAMPO provides sample-efficient policies against common approaches in literature. Code available at https://github.com/SamuelePolimi/lampo.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561870","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561870","","Monte Carlo methods;Codes;Heuristic algorithms;Conferences;Reinforcement learning;Probabilistic logic;Robot learning","","5","","46","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Context-Dependent Anomaly Detection for Low Altitude Traffic Surveillance","I. Bozcan; E. Kayacan","Department of Electrical and Computer Engineering, Artificial Intelligence in Robotics Laboratory (Air Lab), Aarhus University, Aarhus C, Denmark; Department of Electrical and Computer Engineering, Artificial Intelligence in Robotics Laboratory (Air Lab), Aarhus University, Aarhus C, Denmark",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","224","230","The detection of contextual anomalies is a challenging task for surveillance since an observation can be considered anomalous or normal in a specific environmental context. An unmanned aerial vehicle (UAV) can utilize its aerial monitoring capability and employ multiple sensors to gather contextual information about the environment and perform contextual anomaly detection. In this work, we introduce a deep neural network-based method (CADNet) to find point anomalies (i.e., single instance anomalous data) and contextual anomalies (i.e., context-specific abnormality) in an environment using a UAV. The method is based on a variational autoencoder (VAE) with a context sub-network. The context sub-network extracts contextual information regarding the environment using GPS and time data, then feeds it to the VAE to predict anomalies conditioned on the context. To the best of our knowledge, our method is the first contextual anomaly detection method for UAV-assisted aerial surveillance. We evaluate our method on the AU-AIR dataset in a traffic surveillance scenario. Quantitative comparisons against several baselines demonstrate the superiority of our approach in the anomaly detection tasks. The codes and data will be available at https://bozcani.github.io/cadnet.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9562043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9562043","","Surveillance;Traffic control;Unmanned aerial vehicles;Real-time systems;Sensors;Feeds;Task analysis","","18","","25","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Towards an Online RRT-based Path Planning Algorithm for Ackermann-steering Vehicles","J. Peng; Y. Chen; Y. Duan; Y. Zhang; J. Ji; Y. Zhang","School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","7407","7413","It is challenging to develop an online path planning algorithm for Ackermann-steering vehicles to find collision-free and kinematically-feasible paths, that is efficient for dense environments, adaptable to various environments, and suitable for environments with narrow passages. In this paper, we propose a kinematically constrained RRT-based path planning algorithm integrating with a trajectory parameter space (TP-space) with three novel improvements to meet the above requirements. In specific, we introduce a new way to choose candidate nodes to expand the tree for an RRT-based algorithm, which can significantly increase the success rate of the expansion and improve the efficiency of the algorithm. We also introduce a procedure to incrementally adjust the step size for the expansion, which enables the algorithm to automatically adapt to various environments. At last, we integrate rapidly-exploring random vines (RRV) with a TP-space to handle kinematic constraints and improve the performance of the algorithm to expand the tree through a narrow passage. We also prove that the algorithm is probabilistic complete and asymptotically near-optimal. An ablation study shows that all three improvements can notably improve the performance of the RRT-based path planning algorithm. We also evaluate the algorithm in various environments. The experimental results show that our algorithm achieves competitive performance compared with the state-of-the-art. The source code is available at https://github.com/PengJieb/fastbkrrt.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561207","Anhui Provincial Development and Reform Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561207","","Codes;Automation;Conferences;Kinematics;Bidirectional control;Probabilistic logic;Trajectory","","4","","15","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Reachable Polyhedral Marching (RPM): A Safety Verification Algorithm for Robotic Systems with Deep Neural Network Components","J. A. Vincent; M. Schwager","Department of Aeronautics and Astronautics, Stanford University, Stanford, CA, USA; Department of Aeronautics and Astronautics, Stanford University, Stanford, CA, USA",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","9029","9035","We present a method for computing exact reachable sets for deep neural networks with rectified linear unit (ReLU) activation. Our method is well-suited for use in rigorous safety analysis of robotic perception and control systems with deep neural network components. Our algorithm can compute both forward and backward reachable sets for a ReLU network iterated over multiple time steps, as would be found in a perception-action loop in a robotic system. Our algorithm is unique in that it builds the reachable sets by incrementally enumerating polyhedral cells in the input space, rather than iterating layer-by-layer through the network as in other methods. If an unsafe cell is found, our algorithm can return this result without completing the full reachability computation, thus giving an anytime property that accelerates safety verification. In addition, our method requires less memory during execution compared to existing methods where memory can be a limiting factor. We demonstrate our algorithm on safety verification of the ACAS Xu aircraft advisory system. We find unsafe actions many times faster than the fastest existing method and certify no unsafe actions exist in about twice the time of the existing method. We also compute forward and backward reachable sets for a learned model of pendulum dynamics over a 50 time step horizon in 87s on a laptop computer. Algorithm source code: https://github.com/StanfordMSL/Neural-Network-Reach.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561956","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561956","","Deep learning;Portable computers;Limiting;Costs;Heuristic algorithms;Conferences;Memory management","","24","","42","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Automated Extrinsic Calibration for 3D LiDARs with Range Offset Correction using an Arbitrary Planar Board","J. Kim; C. Kim; Y. Han; H. J. Kim","Department of Mechanical and Aerospace Engineering, Seoul National University and Automation and Systems Research Institute (ASRI), Seoul, South Korea; Department of Mechanical and Aerospace Engineering, Seoul National University and Automation and Systems Research Institute (ASRI), Seoul, South Korea; Department of Mechanical and Aerospace Engineering, Seoul National University and Automation and Systems Research Institute (ASRI), Seoul, South Korea; Department of Mechanical and Aerospace Engineering, Seoul National University and Automation and Systems Research Institute (ASRI), Seoul, South Korea",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","5082","5088","This paper proposes an automatic and accuracy- enhanced extrinsic calibration method for 3D LiDARs with a range offset correction, which needs only an arbitrarily-shaped single planar board. One of the most exhaustive parts of existing LiDAR calibration procedures is to manually find target objects from massive point clouds. To obviate user interventions, we propose an automated planar board detection from LiDAR range images. To extract a target completely, we suppress outliers and restore rejected inliers of the target board by introducing a target completion method. We empirically find that range measurements of various LiDARs are mainly skewed by constant offset values. To compensate for this, we suggest a range offset model for each laser channel in calibration procedures. The relative pose between LiDARs and range offsets are jointly estimated by minimizing bi-directional point- to-board distances within the iterative re-weighted least squares (IRLS) framework. To verify the suggested range offset model, we obtain and analyze extensive real-world measurements. By conducting experiments using the various sensor configurations and shapes of boards, we quantitatively and qualitatively confirm accuracy and versatility of the proposed method by comparing with the state-of-the-art LiDAR calibration methods. All the source code and data used in the paper are available at : https://github.com/JunhaAgu/AutoL2LCalib.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561175","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561175","","Solid modeling;Laser radar;Three-dimensional displays;Shape;Conferences;Measurement by laser beam;Robot sensing systems","","7","","30","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"ZePHyR: Zero-shot Pose Hypothesis Rating","B. Okorn; Q. Gu; M. Hebert; D. Held","Robotics Institute, Carnegie Mellon University, Pittsburgh, PA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","14141","14148","Pose estimation is a basic module in many robot manipulation pipelines. Estimating the pose of objects in the environment can be useful for grasping, motion planning, or manipulation. However, current state-of-the-art methods for pose estimation either rely on large annotated training sets or simulated data. Further, the long training times for these methods prohibit quick interaction with novel objects. To address these issues, we introduce a novel method for zero-shot object pose estimation in clutter. Our approach uses a hypothesis generation and scoring framework, with a focus on learning a scoring function that generalizes to objects not used for training. We achieve zero-shot generalization by rating hypotheses as a function of unordered point differences. We evaluate our method on challenging datasets with both textured and untextured objects in cluttered scenes and demonstrate that our method significantly outperforms previous methods on this task. We also demonstrate how our system can be used by quickly scanning and building a model of a novel object, which can immediately be used by our method for pose estimation. Our work allows users to estimate the pose of novel objects without requiring any retraining. Additional information can be found on our website https://bokorn.github.io/zephyr/","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9560874","United States Air Force; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9560874","","Training;Conferences;Pose estimation;Pipelines;Focusing;Grasping;Planning","","12","","53","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection","Y. Liu; L. Wang; M. Liu","Department of Electronic and Computer Engineering, Robotics and Multi-Perception Laborotary, The Hong Kong University of Science and Technology; Cloud Computing Lab of Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China; Department of Electronic and Computer Engineering, Robotics and Multi-Perception Laborotary, The Hong Kong University of Science and Technology",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","13018","13024","Object detection in 3D with stereo cameras is an important problem in computer vision, and is particularly crucial in low-cost autonomous mobile robots without LiDARs. Nowadays, most of the best-performing frameworks for stereo 3D object detection are based on dense depth reconstruction from disparity estimation, making them extremely computationally expensive. To enable real-world deployments of vision detection with binocular images, we take a step back to gain insights from 2D image-based detection frameworks and enhance them with stereo features. We incorporate knowledge and the inference structure from real-time one-stage 2D/3D object detector and introduce a light-weight stereo matching module. Our proposed framework, YOLOStereo3D, is trained on one single GPU and runs at more than ten fps. It demonstrates performance comparable to state-of-the-art stereo 3D detection frameworks without usage of LiDAR data. The code will be published in https://github.com/Owen-Liuyuxuan/visualDet3D.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561423","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561423","","Training;Three-dimensional displays;Laser radar;Computational modeling;Robot vision systems;Graphics processing units;Object detection","","42","","34","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Sparsity-Inducing Optimal Control via Differential Dynamic Programming","T. Dinev; W. Merkt; V. Ivan; I. Havoutis; S. Vijayakumar","Oxford Robotics Institute, University of Oxford, UK; Oxford Robotics Institute, University of Oxford, UK; Oxford Robotics Institute, University of Oxford, UK; Oxford Robotics Institute, University of Oxford, UK; Oxford Robotics Institute, University of Oxford, UK",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","8216","8222","Optimal control is a popular approach to synthesize highly dynamic motion. Commonly, L2 regularization is used on the control inputs in order to minimize energy used and to ensure smoothness of the control inputs. However, for some systems, such as satellites, the control needs to be applied in sparse bursts due to how the propulsion system operates. In this paper, we study approaches to induce sparsity in optimal control solutions—namely via smooth L1 and Huber regularization penalties. We apply these loss terms to state-of-the-art Differential Dynamic Programming (DDP)-based solvers to create a family of sparsity-inducing optimal control methods. We analyze and compare the effect of the different losses on inducing sparsity, their numerical conditioning, their impact on convergence, and discuss hyperparameter settings. We demonstrate our method in simulation and hardware experiments on canonical dynamics systems, control of satellites, and the NASA Valkyrie humanoid robot. We provide an implementation of our method and all examples for reproducibility on GitHub.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9560961","European Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9560961","","Satellites;NASA;Optimal control;Humanoid robots;Propulsion;Reproducibility of results;Hardware","","2","","19","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Learning to Rearrange Deformable Cables, Fabrics, and Bags with Goal-Conditioned Transporter Networks","D. Seita; P. Florence; J. Tompson; E. Coumans; V. Sindhwani; K. Goldberg; A. Zeng","Autolab at the University of California, Berkeley, USA; Google Research, USA; Google Research, USA; Google Research, USA; Google Research, USA; Autolab at the University of California, Berkeley, USA; Google Research, USA",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","4568","4575","Rearranging and manipulating deformable objects such as cables, fabrics, and bags is a long-standing challenge in robotic manipulation. The complex dynamics and high-dimensional configuration spaces of deformables, compared to rigid objects, make manipulation difficult not only for multi-step planning, but even for goal specification. Goals cannot be as easily specified as rigid object poses, and may involve complex relative spatial relations such as ""place the item inside the bag"". In this work, we develop a suite of simulated benchmarks with 1D, 2D, and 3D deformable structures, including tasks that involve image-based goal-conditioning and multi-step deformable manipulation. We propose embedding goal-conditioning into Transporter Networks, a recently proposed model architecture for learning robotic manipulation that rearranges deep features to infer displacements that can represent pick and place actions. We demonstrate that goal-conditioned Transporter Networks enable agents to manipulate deformable structures into flexibly specified configurations without test-time visual anchors for target locations. We also significantly extend prior results using Transporter Networks for manipulating deformable objects by testing on tasks with 2D and 3D deformables. Supplementary material is available at https://berkeleyautomation.github.io/bags/.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561391","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561391","","Visualization;Three-dimensional displays;Automation;Conferences;Benchmark testing;Fabrics;Planning","","79","","71","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Congestion-aware Multi-agent Trajectory Prediction for Collision Avoidance","X. Xie; C. Zhang; Y. Zhu; Y. N. Wu; S. -C. Zhu","Statistics Department, UCLA Center for Vision, Cognition, Learning, and Autonomy (VCLA); Statistics Department, UCLA Center for Vision, Cognition, Learning, and Autonomy (VCLA); Statistics Department, UCLA Center for Vision, Cognition, Learning, and Autonomy (VCLA); Statistics Department, UCLA Center for Vision, Cognition, Learning, and Autonomy (VCLA); Statistics Department, UCLA Center for Vision, Cognition, Learning, and Autonomy (VCLA)",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","13693","13700","Predicting agents’ future trajectories plays a crucial role in modern AI systems, yet it is challenging due to intricate interactions exhibited in multi-agent systems, especially when it comes to collision avoidance. To address this challenge, we propose to learn congestion patterns as contextual cues explicitly and devise a novel ""Sense–Learn–Reason–Predict"" framework by exploiting advantages of three different doctrines of thought, which yields the following desirable benefits: (i) Representing congestion as contextual cues via latent factors subsumes the concept of social force commonly used in physics- based approaches and implicitly encodes the distance as a cost, similar to the way a planning-based method models the environment. (ii) By decomposing the learning phases into two stages, a ""student"" can learn contextual cues from a ""teacher"" while generating collision-free trajectories. To make the framework computationally tractable, we formulate it as an optimization problem and derive an upper bound by leveraging the variational parametrization. In experiments, we demonstrate that the proposed model is able to generate collision- free trajectory predictions in a synthetic dataset designed for collision avoidance evaluation and remains competitive on the commonly used NGSIM US-101 highway dataset. Source code and dataset tools can be accessed via Github.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9560994","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9560994","","Road transportation;Upper bound;Computational modeling;Predictive models;Tools;Trajectory;Collision avoidance","","13","","90","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Development of a Perception System for an Autonomous Surface Vehicle using Monocular Camera, LIDAR, and Marine RADAR","T. Clunie; M. DeFilippo; M. Sacarny; P. Robinette","University of Massachusetts at Lowell, Lowell, MA, USA; Autonomous Underwater Vehicles Laboratory, MIT Sea Grant, Cambridge, MA, USA; Autonomous Underwater Vehicles Laboratory, MIT Sea Grant, Cambridge, MA, USA; University of Massachusetts at Lowell, Lowell, MA, USA",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","14112","14119","This paper describes a set of software modules and algorithms for maritime object detection and tracking. The approach described here is designed to work in conjunction with various sensors from a maritime surface vessel (e.g. marine RADAR, LIDAR, camera). The described system identifies obstacles from the input sensors, estimates their state, and fuses the obstacle data into a consolidated report. The system is verified using experiments conducted on a live system and successfully demonstrates the ability to detect and track obstacles up to 450m away while operating at 7 fps. The software is open source and available at https://github.com/uml-marine-robotics/asv_perception.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561275","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561275","Autonomous systems;marine robotics;intelligent robots;mobile agents unmanned autonomous vehicles;autonomous surface vehicles;marine RADAR;LIDAR;segmentation;classification;object detection;calibration;sensor fusion","Laser radar;Software algorithms;Object detection;Sensor fusion;Cameras;Robot sensing systems;Radar tracking","","19","","53","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"CloudAAE: Learning 6D Object Pose Regression with On-line Data Synthesis on Point Clouds","G. Gao; M. Lauri; X. Hu; J. Zhang; S. Frintrop","Department of Informatics, University of Hamburg, Germany; Department of Informatics, University of Hamburg, Germany; Department of Computer Science and Technology, State Key Laboratory of Intelligent Technology and Systems, Tsinghua University, China; Department of Informatics, University of Hamburg, Germany; Department of Informatics, University of Hamburg, Germany",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","11081","11087","It is often desired to train 6D pose estimation systems on synthetic data because manual annotation is expensive. However, due to the large domain gap between the synthetic and real images, synthesizing color images is expensive. In contrast, this domain gap is considerably smaller and easier to fill for depth information. In this work, we present a system that regresses 6D object pose from depth information represented by point clouds, and a lightweight data synthesis pipeline that creates synthetic point cloud segments for training. We use an augmented autoencoder (AAE) for learning a latent code that encodes 6D object pose information for pose regression. The data synthesis pipeline only requires texture-less 3D object models and desired viewpoints, and it is cheap in terms of both time and hardware storage. Our data synthesis process is up to three orders of magnitude faster than commonly applied approaches that render RGB image data. We show the effectiveness of our system on the LineMOD, LineMOD Occlusion, and YCB Video datasets. The implementation of our system is available at: https://github.com/GeeeG/CloudAAE.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561475","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561475","","Training;Solid modeling;Image segmentation;Three-dimensional displays;Pipelines;Pose estimation;Manuals","","33","","36","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Uncertainty-Aware Fast Curb Detection Using Convolutional Networks in Point Clouds","Y. Jung; M. Jeon; C. Kim; S. -W. Seo; S. -W. Kim","Seoul National University, South Korea; Seoul National University, South Korea; Seoul National University, South Korea; Seoul National University, South Korea; Seoul National University, South Korea",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","12882","12888","Curb detection is an essential function of autonomous vehicles in urban areas. However, curbs are difficult to detect in complex urban environments in which many dynamic objects exist. Additionally, curbs appear in a variety of shapes and sizes. Previous studies have been based on the traditional pipeline, which consists of the extraction and aggregation of hand-crafted features that are then fed to classifiers. However, this sequential process is inefficient and designing the hand-crafted features is a complex process. Recently, this kind of process has been replaced by Deep Neural Networks (DNN), in which classifiers and features are learned from large-scale data. Very few works have exploited DNN for the curb detection problem. Most works use multi-modal sensor-based methods that combine images and accumulated 3D point clouds from LIDAR. However, these approaches require synchronization and calibration between sensors. In addition, they do not quantify the uncertainty of their predictions for autonomous system safety. In this paper, we present a two-stage DNN-based curb detection method that includes uncertainty quantification. An autoencoder-based network predicts the curbs, and then conditional neural processes rectify the predictions with uncertainty estimations. The experimental results show that our approach achieves high accuracy and recall in complex areas. We also constructed a large-scale dataset to create benchmarks consisting of approximately 5,224 scans with bird’s-eye view labels collected from urban areas. To the best of our knowledge, there are no public datasets for DNN-based curb detectors. The benchmarks and datasets are publicly available at https://github.com/YounghwaJung/curb_detection_DNN.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561358","National Research Foundation of Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561358","","Deep learning;Uncertainty;Three-dimensional displays;Urban areas;Benchmark testing;Feature extraction;Safety","","10","","38","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"TSDF++: A Multi-Object Formulation for Dynamic Object Tracking and Reconstruction","M. Grinvald; F. Tombari; R. Siegwart; J. Nieto","Autonomous Systems Lab, ETH Zurich, Zurich, Switzerland; Technical University of Munich, Germany; Autonomous Systems Lab, ETH Zurich, Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Zurich, Switzerland",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","14192","14198","The ability to simultaneously track and reconstruct multiple objects moving in the scene is of the utmost importance for robotic tasks such as autonomous navigation and interaction. Virtually all of the previous attempts to map multiple dynamic objects have evolved to store individual objects in separate reconstruction volumes and track the relative pose between them. While simple and intuitive, such formulation does not scale well with respect to the number of objects in the scene and introduces the need for an explicit occlusion handling strategy. In contrast, we propose a map representation that allows maintaining a single volume for the entire scene and all the objects therein. To this end, we introduce a novel multi-object TSDF formulation that can encode multiple object surfaces at any given location in the map. In a multiple dynamic object tracking and reconstruction scenario, our representation allows maintaining accurate reconstruction of surfaces even while they become temporarily occluded by other objects moving in their proximity. We evaluate the proposed TSDF++ formulation on a public synthetic dataset and demonstrate its ability to preserve reconstructions of occluded surfaces when compared to the standard TSDF map representation. Code is available at https://github.com/ethz-asl/tsdf-plusplus.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9560923","Google; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9560923","","Surface reconstruction;Solid modeling;Three-dimensional displays;Shape;Pipelines;Cameras;Robustness","","16","","25","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"AU-Expression Knowledge Constrained Representation Learning for Facial Expression Recognition","T. Pu; T. Chen; Y. Xie; H. Wu; L. Lin","School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; DarkMatter AI Research, Guangzhou, China; DarkMatter AI Research, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","11154","11161","Recognizing human emotion/expressions automatically is quite an expected ability for intelligent robotics, as it can promote better communication and cooperation with humans. Current deep-learning-based algorithms may achieve impressive performance in some lab-controlled environments, but they always fail to recognize the expressions accurately for the uncontrolled in-the-wild situation. Fortunately, facial action units (AU) describe subtle facial behaviors, and they can help distinguish uncertain and ambiguous expressions. In this work, we explore the correlations among the action units and facial expressions, and devise an AU-Expression Knowledge Constrained Representation Learning (AUE-CRL) framework to learn the AU representations without AU annotations and adaptively use representations to facilitate facial expression recognition. Specifically, it leverages AU-expression correlations to guide the learning of the AU classifiers, and thus it can obtain AU representations without incurring any AU annotations. Then, it introduces a knowledge-guided attention mechanism that mines useful AU representations under the constraint of AU-expression correlations. In this way, the framework can capture local discriminative and complementary features to enhance facial representation for facial expression recognition. We conduct experiments on the challenging uncontrolled datasets to demonstrate the superiority of the proposed framework over current state-of-the-art methods. Codes and trained models are available at https://github.com/HCPLab-SYSU/AUE-CRL.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561252","National Natural Science Foundation of China; Natural Science Foundation of Guangdong Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561252","","Gold;Emotion recognition;Correlation;Codes;Automation;Annotations;Face recognition","","15","","49","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Fast Few-Shot Classification by Few-Iteration Meta-Learning","A. S. Tripathi; M. Danelljan; L. Van Gool; R. Timofte","Department of Electrical Engineering, Computer Vision Lab, ETH Zurich, Switzerland; Department of Electrical Engineering, Computer Vision Lab, ETH Zurich, Switzerland; Department of Electrical Engineering, Computer Vision Lab, ETH Zurich, Switzerland; Department of Electrical Engineering, Computer Vision Lab, ETH Zurich, Switzerland",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","9522","9528","Autonomous agents interacting with the real world need to learn new concepts efficiently and reliably. This requires learning in a low-data regime, which is a highly challenging problem. We address this task by introducing a fast optimization-based meta-learning method for few-shot classification. It consists of an embedding network, providing a general representation of the image, and a base learner module. The latter learns a linear classifier during the inference through an unrolled optimization procedure. We design an inner learning objective composed of (i) a robust classification loss on the support set and (ii) an entropy loss, allowing transductive learning from unlabeled query samples. By employing an efficient initialization module and a Steepest Descent based optimization algorithm, our base learner predicts a powerful classifier within only a few iterations. Further, our strategy enables important aspects of the base learner objective to be learned during meta-training. To the best of our knowledge, this work is the first to integrate both induction and transduction into the base learner in an optimization-based meta-learning framework. We perform a comprehensive experimental analysis, demonstrating the speed and effectiveness of our approach on four few-shot classification datasets. The Code is available at https://github.com/4rdhendu/FIML.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561269","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561269","","Codes;Conferences;Prediction algorithms;Inference algorithms;Entropy;Reliability;Iterative methods","","5","","28","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Learning Seed Placements and Automation Policies for Polyculture Farming with Companion Plants","Y. Avigal; A. Deza; W. Wong; S. Oehme; M. Presten; M. Theis; J. Chui; P. Shao; H. Huang; A. Kotani; S. Sharma; R. Parikh; M. Luo; S. Mukherjee; S. Carpin; J. H. Viers; S. Vougioukas; K. Goldberg","The AUTOLab, UC Berkeley; The AUTOLab, UC Berkeley; The AUTOLab, UC Berkeley; Department of Electrical and Computer Engineering, TU Munich; The AUTOLab, UC Berkeley; The AUTOLab, UC Berkeley; The AUTOLab, UC Berkeley; The AUTOLab, UC Berkeley; The AUTOLab, UC Berkeley; The AUTOLab, UC Berkeley; The AUTOLab, UC Berkeley; The AUTOLab, UC Berkeley; The AUTOLab, UC Berkeley; The AUTOLab, UC Berkeley; School of Engineering, UC Merced; Environmental Systems, School of Engineering, UC Merced; Biological and Agricultural Engineering, UC Davis; The AUTOLab, UC Berkeley",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","902","908","Polyculture farming is a sustainable farming technique based on synergistic interactions between differing plant types that make them more resistant to diseases and pests and better able to retain water. Reduced uniformity can reduce use of pesticides, fertilizer, and water, but is more labor intensive and more challenging to automate. We describe a scaled physical testbed (1.5m×3.0m) that uses a high resolution camera and soil sensors to monitor polyculture plants to facilitate tuning of plant growth, companion effects, and irrigation parameters for a first-order garden simulator. We use this simulator to develop a novel seed placement algorithm that increases coverage and diversity, and a learned pruning policy. In simulation experiments, the seed placement algorithm yields 60% more coverage and 10% more diversity than random seed placement and the learned pruning policy runs 1000X faster than a procedural lookahead policy to achieve high leaf coverage and plant diversity on adversarial gardens that include plant species with diverse growth rates. These models and policies provide the groundwork for a fully-automated system under development. Code, datasets and supplementary material can be found at https://github.com/BerkeleyAutomation/AlphaGarden/.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561431","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561431","","Resistance;Irrigation;Automation;Stochastic processes;Tools;Soil;Sensors","","8","","43","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Pylot: A Modular Platform for Exploring Latency-Accuracy Tradeoffs in Autonomous Vehicles","I. Gog; S. Kalra; P. Schafhalter; M. A. Wright; J. E. Gonzalez; I. Stoica",UC Berkeley; UC Berkeley; UC Berkeley; UC Berkeley; UC Berkeley; UC Berkeley,2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","8806","8813","We present Pylot, a platform for autonomous vehicle (AV) research and development, built with the goal to allow researchers to study the effects of the latency and accuracy of their models and algorithms on the end-to-end driving behavior of an AV. This is achieved through a modular structure enabled by our high-performance dataflow system that represents AV software pipeline components (object detectors, motion planners, etc.) as a dataflow graph of operators which communicate on data streams using timestamped messages. Pylot readily interfaces with popular AV simulators like CARLA, and is easily deployable to real-world vehicles with minimal code changes.To reduce the burden of developing an entire pipeline for evaluating a single component, Pylot provides several state-of-the-art reference implementations for the various components of an AV pipeline. Using these reference implementations, a Pylot-based AV pipeline is able to drive a real vehicle, and attains a high score on the CARLA Autonomous Driving Challenge. We also present several case studies enabled by Pylot, including evidence of a need for context-dependent components, and per-component time allocation. Pylot is open source, with the code available at https://github.com/erdos-project/pylot.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561747","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561747","","Codes;Runtime;Pipelines;Software algorithms;Reinforcement learning;Detectors;Resource management","","38","","66","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Compositional and Scalable Object SLAM","A. Sharma; W. Dong; M. Kaess","Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","11626","11632","We present a fast, scalable, and accurate Simultaneous Localization and Mapping (SLAM) system that represents indoor scenes as a graph of objects. Leveraging the observation that artificial environments are structured and occupied by recognizable objects, we show that a compositional and scalable object mapping formulation is amenable to a robust SLAM solution for drift-free large-scale indoor reconstruction. To achieve this, we propose a novel semantically assisted data association strategy that results in unambiguous persistent object landmarks and a 2.5D compositional rendering method that enables reliable frame-to-model RGB-D tracking. Consequently, we deliver an optimized online implementation that can run at near frame rate with a single graphics card, and provide a comprehensive evaluation against state-of-the-art baselines. An open-source implementation will be provided at https://github.com/rpl-cmu/object-slam.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561697","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561697","","Graphics;Simultaneous localization and mapping;Automation;Conferences;Rendering (computer graphics);Reliability;Open source software","","10","","25","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"EagerMOT: 3D Multi-Object Tracking via Sensor Fusion","A. Kim; A. Ošep; L. Leal-Taixé",Technical University of Munich; Technical University of Munich; Technical University of Munich,2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","11315","11321","Multi-object tracking (MOT) enables mobile robots to perform well-informed motion planning and navigation by localizing surrounding objects in 3D space and time. Existing methods rely on depth sensors (e.g., LiDAR) to detect and track targets in 3D space, but only up to a limited sensing range due to the sparsity of the signal. On the other hand, cameras provide a dense and rich visual signal that helps to localize even distant objects, but only in the image domain. In this paper, we propose EagerMOT, a simple tracking formulation that eagerly integrates all available object observations from both sensor modalities to obtain a well-informed interpretation of the scene dynamics. Using images, we can identify distant incoming objects, while depth estimates allow for precise trajectory localization as soon as objects are within the depth-sensing range. With EagerMOT, we achieve state-of-the-art results across several MOT tasks on the KITTI and NuScenes datasets. Our code is available at https://github.com/aleksandrkim61/EagerMOT","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9562072","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9562072","","Visualization;Three-dimensional displays;Target tracking;Sensor fusion;Robot sensing systems;Sensors;Trajectory","","134","","43","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"CNN-based Ego-Motion Estimation for Fast MAV Maneuvers","Y. Xu; G. C. H. E. de Croon","Micro Air Vehicle Laboratory, Faculty of Aerospace Engineering, Delft University of Technology, The Netherlands; Micro Air Vehicle Laboratory, Faculty of Aerospace Engineering, Delft University of Technology, The Netherlands",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","7606","7612","In the field of visual ego-motion estimation for Micro Air Vehicles (MAVs), fast maneuvers stay challenging mainly because of the big visual disparity and motion blur. In the pursuit of higher robustness, we study convolutional neural networks (CNNs) that predict the relative pose between subsequent images from a fast-moving monocular camera facing a planar scene. Aided by the Inertial Measurement Unit (IMU), we mainly focus on translational motion. The networks we study have similar small model sizes (around 1.35MB) and high inference speeds (around 10 milliseconds on a mobile GPU). Images for training and testing have realistic motion blur. Departing from a network framework that iteratively warps the first image to match the second with cascaded network blocks, we study different network architectures and training strategies. Simulated datasets and a self-collected MAV flight dataset are used for evaluation. The proposed setup shows better accuracy over existing networks and traditional feature-point-based methods during fast maneuvers. Moreover, self-supervised learning outperforms supervised learning. Videos and open-sourced code are available at https://github. com/tudelft/PoseNet_Planar","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561714","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561714","","Training;Visualization;Supervised learning;Estimation;Network architecture;Cameras;Unmanned aerial vehicles","","2","","39","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Dark Reciprocal-Rank: Teacher-to-student Knowledge Transfer from Self-localization Model to Graph-convolutional Neural Network","T. Koji; T. Kanji","Graduate School of Engineering, University of Fukui, Japan; Faculty of Engineering, University of Fukui, Japan",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","1846","1853","In visual robot self-localization, graph-based scene representation and matching have recently attracted research interest as robust and discriminative methods for self-localization. Although effective, their computational and storage costs do not scale well to large-size environments. To alleviate this problem, we formulate self-localization as a graph classification problem and attempt to use the graph convolutional neural network (GCN) as a graph classification engine. A straightforward approach is to use visual feature descriptors that are employed by state-of-the-art self-localization systems, directly as graph node features. However, their superior performance in the original self-localization system may not necessarily be replicated in GCN-based self-localization. To address this issue, we introduce a novel teacher-to-student knowledge-transfer scheme based on rank matching, in which the reciprocal-rank vector output by an off-the-shelf state-of-the-art teacher self-localization model is used as the dark knowledge to transfer. Experiments indicate that the proposed graph-convolutional self-localization network (GCLN) can significantly outperform state-of-the-art self-localization systems, as well as the teacher classifier. The code and dataset are available at https://github.com/KojiTakeda00/Reciprocal_rank_KT_GCN.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561158","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561158","","Visualization;Costs;Codes;Automation;Conferences;Neural networks;Convolutional neural networks","","2","","52","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Fast Object Segmentation Learning with Kernel-based Methods for Robotics","F. Ceola; E. Maiettini; G. Pasquale; L. Rosasco; L. Natale","Humanoid Sensing and Perception, Istituto Italiano di Tecnologia, Genoa, Italy; Humanoid Sensing and Perception, Istituto Italiano di Tecnologia, Genoa, Italy; Humanoid Sensing and Perception, Istituto Italiano di Tecnologia, Genoa, Italy; Laboratory for Computational and Statistical Learning and with Dipartimento di Informatica, Bioingegneria, Robotica e Ingegneria dei Sistemi, University of Genoa, Genoa, Italy; Humanoid Sensing and Perception, Istituto Italiano di Tecnologia, Genoa, Italy",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","13581","13588","Object segmentation is a key component in the visual system of a robot that performs tasks like grasping and object manipulation, especially in presence of occlusions. Like many other computer vision tasks, the adoption of deep architectures has made available algorithms that perform this task with remarkable performance. However, adoption of such algorithms in robotics is hampered by the fact that training requires large amount of computing time and it cannot be performed on-line.In this work, we propose a novel architecture for object segmentation, that overcomes this problem and provides comparable performance in a fraction of the time required by the state-of-the-art methods. Our approach is based on a pre-trained Mask R-CNN, in which various layers have been replaced with a set of classifiers and regressors that are retrained for a new task. We employ an efficient Kernel-based method that allows for fast training on large scale problems. Our approach is validated on the YCB-Video dataset which is widely adopted in the computer vision and robotics community, demonstrating that we can achieve and even surpass performance of the state-of-the-art, with a significant reduction (~6×) of the training time.The code to reproduce the experiments is publicly available on GitHub1.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561758","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561758","","Training;Computer vision;Pipelines;Pose estimation;Object segmentation;Object detection;Grasping","","3","","45","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"An Open-Source Mechanical Design of ALARIS Hand: A 6-DOF Anthropomorphic Robotic Hand","A. Nurpeissova; T. Tursynbekov; A. Shintemirov","School of Engineering and Digital Sciences, Nazarbayev University, Nur-Sultan (Astana), Kazakhstan; School of Engineering and Digital Sciences, Nazarbayev University, Nur-Sultan (Astana), Kazakhstan; School of Engineering and Digital Sciences, Nazarbayev University, Nur-Sultan (Astana), Kazakhstan",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","1177","1183","This paper presents a new open-source mechanical design of a 6-DOF anthropomorphic ALARIS robotic hand that can serve as a low-cost design platform for further customization and utilization for research and educational purposes. The presented hand design employs linkage-based three-phalange finger and two-phalange adaptive thumb designs with non-backdrivable worm-and-rack transmission mechanisms. Combination of design improvements and solutions, discussed in the paper, are implemented in a functional robotic hand prototype with powerful grasping capabilities, which utilizes off-the-shelf inexpensive components and 3D printing technology ensuring the hand low manufacturing cost and replicability. The open-source mechanical design of the presented ALARIS robotic hand is freely available for downloading from the authors’ research lab web-site https://www.alaris.kz and https://github.com/alarisnu/alaris_hand.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561977","Ministry of Education and Science; Nazarbayev University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561977","","Costs;Thumb;Prototypes;Grasping;Three-dimensional printing;Manufacturing;Task analysis","","4","","31","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Adversarially-trained Hierarchical Feature Extractor for Vehicle Re-identification","P. Shyam; K. -J. Yoon; K. -S. Kim","Department of Mechanical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea; Department of Mechanical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea; Department of Mechanical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","13400","13407","Vehicle Re-identification (Re-ID) aims to retrieve all instances of query vehicle images present in an image pool. However viewpoint, illumination, and occlusion variations along with subtle differences between two unique images pose a significant challenge towards achieving an effective system. In this paper, we emphasize upon enhancing the performance of visual feature based ReID system by improving feature embedding quality and propose (1) an attention-guided hierarchical feature extractor (HFE) that leverages the structure of a backbone CNN to extract coarse and fine-grained features and (2) to train the proposed network within a hard negative adversarial framework that generates samples exhibiting extreme variations, encouraging the network to extract important distinguishing features across varying scales. To demonstrate the effectiveness of the proposed framework we use VERI-Wild, VRIC and Veri-776 datasets that exhibit extreme intra-class and minute inter-class differences and achieve state-of-the-art (SoTA) performance. Codes related to this paper are publicly available at https://github.com/PS06/VReID.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561632","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561632","","Training;Visualization;Codes;Conferences;Lighting;Benchmark testing;Feature extraction","","5","","49","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"REGNet: REgion-based Grasp Network for End-to-end Grasp Detection in Point Clouds","B. Zhao; H. Zhang; X. Lan; H. Wang; Z. Tian; N. Zheng",NA; NA; NA; NA; NA; NA,2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","13474","13480","Reliable robotic grasping in unstructured environments is a crucial but challenging task. The main problem is to generate the optimal grasp of novel objects from partial noisy observations. This paper presents an end-to-end grasp detection network taking one single-view point cloud as input to tackle the problem. Our network includes three stages: Score Network (SN), Grasp Region Network (GRN), and Refine Network (RN). Specifically, SN regresses point grasp confidence and selects positive points with high confidence. Then GRN conducts grasp proposal prediction on the selected positive points. RN generates more accurate grasps by refining proposals predicted by GRN. To further improve the performance, we propose a grasp anchor mechanism, in which grasp anchors with assigned gripper orientations are introduced to generate grasp proposals. Experiments demonstrate that REGNet achieves a success rate of 79.34% and a completion rate of 96% in real-world clutter, which significantly outperforms several state-of-the-art point-cloud based methods, including GPD, PointNetGPD, and S4G. The code is available at https://github.com/zhaobinglei/REGNet for 3D Grasping.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561920","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561920","","Three-dimensional displays;Refining;Grasping;Proposals;Reliability;Noise measurement;Clutter","","45","","39","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"SMMR-Explore: SubMap-based Multi-Robot Exploration System with Multi-robot Multi-target Potential Field Exploration Method","J. Yu; J. Tong; Y. Xu; Z. Xu; H. Dong; T. Yang; Y. Wang","Department of Electronic Engineering, Tsinghua University, Beijing, China; Georgia Institute of Technology, GA, USA; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China",2021 IEEE International Conference on Robotics and Automation (ICRA),"18 Oct 2021","2021","","","8779","8785","Collaborative exploration in an unknown environment without external positioning under limited communication is an essential task for multi-robot applications. For inter-robot positioning, various Distributed Simultaneous Localization and Mapping (DSLAM) systems share the Place Recognition (PR) descriptors and sensor data to estimate the relative pose between robots and merge robots’ maps. As maps are constantly shared among robots in exploration, we design a map-based DSLAM framework, which only shares the submaps, eliminating the transfer of PR descriptors and sensor data. Our framework saves 30% of total communication traffic. For exploration, each robot is assigned to get much unknown information about environments with paying little travel cost. As the number of sampled points increases, the goal would change back and forth among sampled frontiers, leading to the downgrade in exploration efficiency and the overlap of trajectories. We propose an exploration strategy based on Multi-robot Multi-target Potential Field (MMPF), which can eliminate goal’s back-and-forth changes, boosting the exploration efficiency by 1.03 ×∼1.62 × with 3 % ∼ 40 % travel cost saved. Our SubMap-based Multi-robot Exploration method (SMMR-Explore) is evaluated on both Gazebo simulator and real robots. The simulator and the exploration framework are published as an open-source ROS project at https://github.com/efc-robot/SMMR-Explore.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561328","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561328","","Costs;Simultaneous localization and mapping;Conferences;Distributed databases;Collaboration;Robot sensing systems;Boosting","","41","","34","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"A ROS Based Open Source Simulation Environment for Robotics Beginners","L. Su; G. Qiu; W. Tang; M. Chen","College of Electronic and Information Engineering, Guangdong Key Laboratory of Intelligent Information Processing and Shenzhen Institute of Artificial Intelligence and Robotics for Society, Shenzhen University, China; College of Electronic and Information Engineering, Guangdong Key Laboratory of Intelligent Information Processing and Shenzhen Institute of Artificial Intelligence and Robotics for Society, Shenzhen University, China; College of Electronic and Information Engineering, Guangdong Key Laboratory of Intelligent Information Processing and Shenzhen Institute of Artificial Intelligence and Robotics for Society, Shenzhen University, China; Super-accurate Vision Science and Technology Ltd, Shenzhen, China",2021 6th International Conference on Robotics and Automation Engineering (ICRAE),"4 Jan 2022","2021","","","286","291","This paper presents an open source robot simulation environment based on the robot operating system (ROS). To help novice to learn robotics, we have designed several important experiments that most robotics beginners will need to practice. Our simulation environment provides an efficient, safe, and transferable testing and operating environment for the rapid verification of robot algorithms. Through practicing the experiments in this paper, robotics beginners can learn robotics knowledge faster, better and at a lower cost, which should great facilitate them to operate robots in real-world applications. Source code, simulation video and detailed wiki for the simulation environment are available online at https://github.com/Suyixiu/robot_sim.","","978-1-6654-0697-0","10.1109/ICRAE53653.2021.9657761","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9657761","Simulation environment;Learn robotics;ROS","Electronic publishing;Costs;Codes;Automation;Operating systems;Information services;Internet","","6","","26","IEEE","4 Jan 2022","","","IEEE","IEEE Conferences"
"Lightweight Monocular Depth Estimation through Guided Decoding","M. Rudolph; Y. Dawoud; R. Güldenring; L. Nalpantidis; V. Belagiannis","University of Duisburg-Essen, Essen, Germany; Ulm University, Ulm, Germany; DTU – Technical University of Denmark, Kgs. Lyngby, Denmark; DTU – Technical University of Denmark, Kgs. Lyngby, Denmark; Ulm University, Ulm, Germany",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","2344","2350","We present a lightweight encoder-decoder architecture for monocular depth estimation, specifically designed for embedded platforms. Our main contribution is the Guided Upsampling Block (GUB) for building the decoder of our model. Motivated by the concept of guided image filtering, GUB relies on the image to guide the decoder on upsampling the feature representation and the depth map reconstruction, achieving high resolution results with fine-grained details. Based on multiple GUBs, our model outperforms the related methods on the NYU Depth V2 dataset in terms of accuracy while delivering up to 35.1 fps on the NVIDIA Jetson Nano and up to 144.5 fps on the NVIDIA Xavier NX. Similarly, on the KITTI dataset, inference is possible with up to 23.7 fps on the Jetson Nano and 102.9 fps on the Xavier NX. Our code and models are made publicly available14https://github.com/mic-rud/GuidedDecoding.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812220","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812220","","Training;Image resolution;Stacking;Estimation;Image filtering;Hardware;Decoding","","21","","45","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Multi-Agent Variational Occlusion Inference Using People as Sensors","M. Itkina; Y. -J. Mun; K. Driggs-Campbell; M. J. Kochenderfer","Aeronautics and Astronautics Department, Stanford University, USA; Electrical and Computer Engineering Department, University of Illinois at Urbana-Champaign, USA; Electrical and Computer Engineering Department, University of Illinois at Urbana-Champaign, USA; Aeronautics and Astronautics Department, Stanford University, USA",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","4585","4591","Autonomous vehicles must reason about spatial occlusions in urban environments to ensure safety without being overly cautious. Prior work explored occlusion inference from observed social behaviors of road agents, hence treating people as sensors. Inferring occupancy from agent behaviors is an inherently multimodal problem; a driver may behave similarly for different occupancy patterns ahead of them (e.g., a driver may move at constant speed in traffic or on an open road). Past work, however, does not account for this multimodality, thus neglecting to model this source of aleatoric uncertainty in the relationship between driver behaviors and their environment. We propose an occlusion inference method that characterizes observed behaviors of human agents as sensor measurements, and fuses them with those from a standard sensor suite. To capture the aleatoric uncertainty, we train a conditional variational autoencoder with a discrete latent space to learn a multimodal mapping from observed driver trajectories to an occupancy grid representation of the view ahead of the driver. Our method handles multi-agent scenarios, combining measurements from multiple observed drivers using evidential theory to solve the sensor fusion problem. Our approach is validated on a cluttered, real-world intersection, outperforming baselines and demonstrating real-time capable performance. Our code is available at https://github.com/sisl/MultiAgentVariationalOcclusionInferenc","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811774","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811774","","Uncertainty;Roads;Urban areas;Sensor fusion;Sensor phenomena and characterization;Real-time systems;Cognition","","17","","41","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"CRAT-Pred: Vehicle Trajectory Prediction with Crystal Graph Convolutional Neural Networks and Multi-Head Self-Attention","J. Schmidt; J. Jordan; F. Gritschneder; K. Dietmayer","R&D, Mercedes-Benz AG, Stuttgart, Germany; R&D, Mercedes-Benz AG, Stuttgart, Germany; R&D, Mercedes-Benz AG, Stuttgart, Germany; Ulm University, Institute of Measurement, Control and Microtechnology, Ulm, Germany",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","7799","7805","Predicting the motion of surrounding vehicles is essential for autonomous vehicles, as it governs their own motion plan. Current state-of-the-art vehicle prediction models heavily rely on map information. In reality, however, this information is not always available. We therefore propose CRAT-Pred, a multi-modal and non-rasterization-based trajectory prediction model, specifically designed to effectively model social interactions between vehicles, without relying on map information. CRAT-Pred applies a graph convolution method originating from the field of material science to vehicle prediction, allowing to efficiently leverage edge features, and combines it with multi-head self-attention. Compared to other map-free approaches, the model achieves state-of-the-art performance with a significantly lower number of model parameters. In addition to that, we quantitatively show that the self-attention mechanism is able to learn social interactions between vehicles, with the weights representing a measurable interaction score. The source code is publicly available33Source code: https://github.com/schmidt-ju/crat-pred.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811637","Federal Ministry for Economic Affairs and Energy (BMWi)(grant numbers:19A19013A); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811637","","Weight measurement;Materials science and technology;Codes;Convolution;Crystals;Predictive models;Encoding","","37","","30","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"HoloSeg: An Efficient Holographic Segmentation Network for Real-time Scene Parsing","S. Li; Q. Yan; C. Liu; M. Liu; Q. Chen","Robotics and Artificial Intelligence Lab (RAIL), Tongji University, Shanghai, China; Robotics and Artificial Intelligence Lab (RAIL), Tongji University, Shanghai, China; Robotics and Artificial Intelligence Lab (RAIL), Tongji University, Shanghai, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong; Robotics and Artificial Intelligence Lab (RAIL), Tongji University, Shanghai, China",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","2395","2402","Real-time semantic segmentation is a crucial but challenging dense prediction task for scene parsing. However, the existing CNN-based methods commonly bias the model in favor of speed-boosting compromising spatial resolution due to business requirements and hardware constrains, which impedes the high-accuracy segmentation result. To address the dilemma, we provide a novel Holographic Segmentation Network (HoloSeg), which presents a strong ability of comprehensive information preservation and extraction, and achieves a better trade-off between speed and accuracy. We first design a Lossless Sample Pair (LSP) without any stride for early spatial preservation and later resolution recovery while modeling long-range context dependence. Then, we propose Distributed Pyramid Learning (DPL) to efficiently extract multiscale features and saves a lot of computation. Finally, we propose Resolution Fusion and Restoration (RFR) to fuse multi-level semantic representations across stages and generate output without decoder. Without bells and whistles, HoloSeg achieves state-of-the-art performance on the Cityscapes benchmark which reports 76.24% mIoU at 231 FPS. Code is available online: https://github.com/LiShuTJ/HoloSeg.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811930","National Natural Science Foundation of China(grant numbers:62073245,61733013,62173248,12002241); China Postdoctoral Science Foundation(grant numbers:2020M681393); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811930","","Fuses;Semantics;Neural networks;Benchmark testing;Feature extraction;Real-time systems;Hardware","","8","","61","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"LoGG3D-Net: Locally Guided Global Descriptor Learning for 3D Place Recognition","K. Vidanapathirana; M. Ramezani; P. Moghadam; S. Sridharan; C. Fookes","Robotics and Autonomous Systems Group, DATA61, CSIRO, Brisbane, QLD, Australia; Robotics and Autonomous Systems Group, DATA61, CSIRO, Brisbane, QLD, Australia; Robotics and Autonomous Systems Group, DATA61, CSIRO, Brisbane, QLD, Australia; SAIVT research programme in the School of Electrical Engineering and Robotics, Queensland University of Technology (QUT), Brisbane, Australia; SAIVT research programme in the School of Electrical Engineering and Robotics, Queensland University of Technology (QUT), Brisbane, Australia",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","2215","2221","Retrieval-based place recognition is an efficient and effective solution for re-localization within a pre-built map, or global data association for Simultaneous Localization and Mapping (SLAM). The accuracy of such an approach is heavily dependant on the quality of the extracted scene-level representation. While end-to-end solutions - which learn a global descriptor from input point clouds - have demonstrated promising results, such approaches are limited in their ability to enforce desirable properties at the local feature level. In this paper, we introduce a local consistency loss to guide the network towards learning local features which are consistent across revisits, hence leading to more repeatable global descriptors resulting in an overall improvement in 3D place recognition performance. We formulate our approach in an end-to-end trainable architecture called LoGG3D-Net. Experiments on two large-scale public benchmarks (KITTI and MulRan) show that our method achieves mean F1max scores of 0.939 and 0.968 on KITTI and MulRan respectively, achieving state-of-the-art performance while operating in near real-time. The open-source implementation is available at: https://github.com/csiro-robotics/LoGG3D-Net.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811753","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811753","","Point cloud compression;Training;Solid modeling;Three-dimensional displays;Simultaneous localization and mapping;Convolution;Benchmark testing","","44","","29","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"OSCAR: Data-Driven Operational Space Control for Adaptive and Robust Robot Manipulation","J. Wong; V. Makoviychuk; A. Anandkumar; Y. Zhu",Stanford University; NVIDIA; NVIDIA; NVIDIA,2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","10519","10526","Learning performant robot manipulation policies can be challenging due to high-dimensional continuous actions and complex physics-based dynamics. This can be alleviated through intelligent choice of action space. Operational Space Control (OSC) has been used as an effective task-space controller for manipulation. Nonetheless, its strength depends on the underlying modeling fidelity, and is prone to failure when there are modeling errors. In this work, we propose OSC for Adaptation and Robustness (OSCAR), a data-driven variant of OSC that compensates for modeling errors by inferring relevant dynamics parameters from online trajectories. OSCAR decomposes dynamics learning into task-agnostic and task-specific phases, decoupling the dynamics dependencies of the robot and the extrinsics due to its environment. This structure enables robust zero-shot performance under out-of-distribution and rapid adaptation to significant domain shifts through additional finetuning. We evaluate our method on a variety of simulated manipulation problems, and find substantial improvements over an array of controller baselines. For more results and information, please visit https://cremebrule.github.io/oscar-web/.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811967","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811967","","Training;Degradation;Adaptation models;Automation;Aerospace electronics;Robustness;Trajectory","","3","","53","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"A Single Correspondence Is Enough: Robust Global Registration to Avoid Degeneracy in Urban Environments","H. Lim; S. Yeon; S. Ryu; Y. Lee; Y. Kim; J. Yun; E. Jung; D. Lee; H. Myung","School of Electrical Engineering, KI-AI at KAIST (Korea Advanced Institute of Science and Technology), Daejeon, South Korea; NAVER LABS, Seongnam-si, Gyeonggi-do, South Korea; NAVER LABS, Seongnam-si, Gyeonggi-do, South Korea; NAVER LABS, Seongnam-si, Gyeonggi-do, South Korea; NAVER LABS, Seongnam-si, Gyeonggi-do, South Korea; NAVER LABS, Seongnam-si, Gyeonggi-do, South Korea; NAVER LABS, Seongnam-si, Gyeonggi-do, South Korea; NAVER LABS, Seongnam-si, Gyeonggi-do, South Korea; School of Electrical Engineering, KI-AI at KAIST (Korea Advanced Institute of Science and Technology), Daejeon, South Korea",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","8010","8017","Global registration using 3D point clouds is a crucial technology for mobile platforms to achieve localization or manage loop-closing situations. In recent years, numerous researchers have proposed global registration methods to address a large number of outlier correspondences. Unfortunately, the degeneracy problem, which represents the phenomenon in which the number of estimated inliers becomes lower than three, is still potentially inevitable. To tackle the problem, a degeneracy-robust decoupling-based global registration method is proposed, called Quatro. In particular, our method employs quasi-SO(3) estimation by leveraging the Atlanta world assumption in urban environments to avoid degeneracy in rotation estimation. Thus, the minimum degree of freedom (DoF) of our method is reduced from three to one. As verified in indoor and outdoor 3D LiDAR datasets, our proposed method yields robust global registration performance compared with other global registration methods, even for distant point cloud pairs. Furthermore, the experimental results confirm the applicability of our method as a coarse alignment. Our code is available: https://github.com/url-kaist/quatro","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812018","","Point cloud compression;Location awareness;Three-dimensional displays;Laser radar;Codes;Automation;Urban areas","","25","","74","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Deep-CNN based Robotic Multi-Class Under-Canopy Weed Control in Precision Farming","Y. Du; G. Zhang; D. Tsang; M. K. Jawed","Department of Mechanical & Aerospace Engineering, University of California, Los Angeles, Los Angeles, CA; Department of Mechanical & Aerospace Engineering, University of California, Los Angeles, Los Angeles, CA; Department of Mechanical & Aerospace Engineering, University of California, Los Angeles, Los Angeles, CA; Department of Mechanical & Aerospace Engineering, University of California, Los Angeles, Los Angeles, CA",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","2273","2279","Smart weeding systems to perform plant-specific operations can contribute to the sustainability of agriculture and the environment. Despite monumental advances in autonomous robotic technologies for precision weed management in recent years, work on under-canopy weeding in fields is yet to be realized. A prerequisite of such systems is reliable detection and classification of weeds to avoid mistakenly spraying and, thus, damaging the surrounding plants. Real-time multi-class weed identification enables species-specific treatment of weeds and significantly reduces the amount of herbicide use. Here, our first contribution is the first adequately large realistic image dataset AIWeeds (one/multiple kinds of weeds in one image), a library of about 10,000 annotated images of flax and the 14 most common weeds in fields and gardens taken from 20 different locations in North Dakota, California, and Central China. Second, we provide a full pipeline from model training with maximum efficiency to deploying the TensorRT-optimized model onto a single board computer. Based on AIWeeds and the pipeline, we present a baseline for classification performance using five benchmark CNN models. Among them, MobileNetV2, with both the shortest inference time and lowest memory consumption, is the qualified candidate for real-time applications. Finally, we deploy MobileNetV2 onto our own compact autonomous robot SAMBot for real-time weed detection. The 90% test accuracy realized in previously unseen scenes in flax fields (with a row spacing of 0.2-0.3 m), with crops and weeds, distortion, blur, and shadows, is a milestone towards precision weed control in the real world. We have publicly released the dataset and code to generate the results at https://github.com/StructuresComp/Multi-class-Weed-Classification.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812240","NSF(grant numbers:IIS - 1925360); National Institute of Food and Agriculture; USDA(grant numbers:2021-67022-34200); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812240","","Training;Computational modeling;Pipelines;Venus;Spraying;Benchmark testing;Real-time systems","","12","","26","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"GelSlim 3.0: High-Resolution Measurement of Shape, Force and Slip in a Compact Tactile-Sensing Finger","I. H. Taylor; S. Dong; A. Rodriguez",Massachusetts Institute of Technology; Massachusetts Institute of Technology; Massachusetts Institute of Technology,2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","10781","10787","This work presents a new version of tactile-sensing finger, GelSlim 3.0, which integrates the ability to sense high-resolution shape, force, and slip in a more compact form factor than previous implementations, designed for cluttered bin-picking scenarios. The novel design integrates real-time model-based algorithms to measure shape, estimate the 3-D contact force distribution, and detect incipient slip. The constraints imposed by the photometric stereo algorithm used for depth reconstruction and the implementation of a planar sensing surface make the miniaturization of previous designs nontrivial. To achieve a compact integration, we optimize the optical path from illumination source to camera. Using an optical simulation environment, we develop an illumination shaping lens and position the source LEDs and camera. The optimized optical configuration is integrated into a finger design composed of a robust and easily replaceable snap-to-fit fingertip module that facilitates manufacture, assembly, use, and repair. To stimulate future research in tactile-sensing and provide the robotics community access to a reliable and easily reproducible tactile finger with a diversity of sensing modalities, we open-source the design, fabrication methods, and software at https://github.com/mcubelab/gelslim.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811832","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811832","","Integrated optics;Force measurement;Shape;Fingers;Force;Optical device fabrication;Lighting","","99","","26","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Neural Descriptor Fields: SE(3)-Equivariant Object Representations for Manipulation","A. Simeonov; Y. Du; A. Tagliasacchi; J. B. Tenenbaum; A. Rodriguez; P. Agrawal; V. Sitzmann",Massachusetts Institute of Technology; Massachusetts Institute of Technology; Google Research; Massachusetts Institute of Technology; Massachusetts Institute of Technology; Massachusetts Institute of Technology; Massachusetts Institute of Technology,2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","6394","6400","We present Neural Descriptor Fields (NDFs), an object representation that encodes both points and relative poses between an object and a target (such as a robot gripper or a rack used for hanging) via category-level descriptors. We employ this representation for object manipulation, where given a task demonstration, we want to repeat the same task on a new object instance from the same category. We propose to achieve this objective by searching (via optimization) for the pose whose descriptor matches that observed in the demonstration. NDFs are conveniently trained in a self-supervised fashion via a 3D auto-encoding task that does not rely on expert-labeled keypoints. Further, NDFs are SE(3)-equivariant, guaranteeing performance that generalizes across all possible 3D object translations and rotations. We demonstrate learning of manipulation tasks from few (∼5-10) demonstrations both in simulation and on a real robot. Our performance generalizes across both object instances and 6-DoF object poses, and significantly outperforms a recent baseline that relies on 2D descriptors. Project website: https://yilundu.github.io/ndf/","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812146","DARPA(grant numbers:CW3031624); DSTA(grant numbers:DST00OECI20300823); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812146","","Three-dimensional displays;Automation;Search problems;Task analysis;Grippers;Robots;Optimization","","71","","52","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Camera-Tracklet-Aware Contrastive Learning for Unsupervised Vehicle Re-Identification","J. Yu; J. Kim; M. Kim; H. Oh","Institute for IT Convergence, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea; KAIST, School of Electrical Engineering, Daejeon, Republic of Korea; KAIST, School of Electrical Engineering, Daejeon, Republic of Korea; Institute for IT Convergence, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","905","911","Recently, vehicle re-identification methods based on deep learning constitute remarkable achievement. However, this achievement requires large-scale and well-annotated datasets. In constructing the dataset, assigning globally available identities (Ids) to vehicles captured from a great number of cameras is labour-intensive, because it needs to consider their subtle appearance differences or viewpoint variations. In this paper, we propose camera-tracklet-aware contrastive learning (CTACL) using the multi-camera tracklet information without vehicle identity labels. The proposed CTACL divides an unlabelled domain, i.e., entire vehicle images, into multiple camera-level subdomains and conducts contrastive learning within and beyond the subdomains. The positive and negative samples for contrastive learning are defined using tracklet Ids of each camera. Additionally, the domain adaptation across camera networks is introduced to improve the generalisation performance of learnt representations and alleviate the performance degradation resulted from the domain gap between the subdomains. We demonstrate the effectiveness of our approach on video-based and image-based vehicle Re-ID datasets. Experimental results show that the proposed method outperforms the recent state-of-the-art unsupervised vehicle Re-ID methods. The source code for this paper is publicly available on https://github.com/andreYoo/CTAM-CTACL-VVReID.git.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812007","Institute of Information & communications Technology Planning & Evaluation(grant numbers:2020-0-00048); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812007","","Degradation;Deep learning;Codes;Automation;Cameras;Boosting","","5","","39","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"RepAr-Net: Re-Parameterized Encoders and Attentive Feature Arsenals for Fast Video Denoising","S. P. Sharan; A. K. Krishna; A. Siddharth Rao; V. P. Gopi","Department of Electronics and Communication Engineering, National Institute of Technology Tiruchirappalli, India; Department of Electronics and Communication Engineering, National Institute of Technology Tiruchirappalli, India; Department of Electrical and Electronics Engineering, National Institute of Technology Tiruchirappalli, India; Department of Electronics and Communication Engineering, National Institute of Technology Tiruchirappalli, India",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","633","639","Real-time video denoising finds applications in several fields like mobile robotics, satellite television, and surveillance systems. Traditional denoising approaches are more common in such systems than their deep learning-based counterparts despite their inferior performance. The large size and heavy computational requirements of neural network-based denoising models pose a serious impediment to their deployment in real-time applications. In this paper, we propose RepAr-Net, a simple yet efficient architecture for fast video de noising. We propose to use temporally separable encoders to generate feature maps called arsenals that can be cached for reuse. We also incorporate re-parameterizable blocks that improve the representative power of the network without affecting the run-time. We benchmark our model on the Set-8 and 2017 DAVIS-Test datasets. Our model achieves state-of-the-art results with up to 29.62% improvement in PSNR and a 50% decrease in run times over existing methods. Our codes are open-sourced at: github.com/spider-tronix/RepAr-Net.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812394","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812394","","TV;Runtime;Computational modeling;Surveillance;Noise reduction;Satellite broadcasting;Computer architecture","","1","","23","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Cityscapes TL++: Semantic Traffic Light Annotations for the Cityscapes Dataset","J. Janosovits","Institute of Measurement and Control Systems, Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","2569","2575","There is a gap in holistic urban scene understanding between multi-modal datasets for segmentation and object detection on the one hand and traffic light datasets on the other hand. The role of traffic lights in the former is not labelled, making it difficult to use them for higher-level tasks and leave critical information of an intersection scene blank. Including traffic lights from traffic light specific datasets into the comprehensive semantic data introduces a penalty from the domain shift. We close this gap by providing semantically annotated traffic lights for the Cityscapes dataset. We demonstrate the domain shift penalty by using a traffic light dataset from a similar domain and show superior performance on data labelled in the original domain. We demonstrate an application by training a real-time capable network for semantic segmentation and object detection which can now additionally make sense of traffic lights, delivering an F1- Score of 66.4% on the important class of traffic lights relevant to the ego vehicle. The network is made publicly available at https://github.com/joeda/NNAD and the data at https://github.com/KIT-MRT/cityscapes-t1.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812144","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812144","Datasets for Robotic Vision;Deep Learning for Visual Perception;Deep Learning Methods","Training;Learning systems;Visualization;Automation;Annotations;Semantics;Object detection","","4","","28","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Concurrent Policy Blending and System Identification for Generalized Assistive Control","L. Bhan; M. Quinones-Grueiro; G. Biswas","Vanderbilt University, Nashville, TN; Vanderbilt University, Nashville, TN; Vanderbilt University, Nashville, TN",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","7499","7504","In this work, we address the problem of solving complex collaborative robotic tasks subject to multiple varying parameters. Our approach combines simultaneous policy blending with system identification to create generalized policies that are robust to changes in system parameters. We employ a blending network whose state space relies solely on parameter estimates from a system identification technique. As a result, this blending network learns how to handle parameter changes instead of trying to learn how to solve the task for a generalized parameter set simultaneously. We demonstrate our scheme's ability on a collaborative robot and human itching task in which the human has motor impairments. We then showcase our approach's efficiency with a variety of system identification techniques when compared to standard domain randomization. The code is available on Luke Bhan's Github.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811672","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811672","","Codes;Automation;Collaboration;System identification;Task analysis;Robots;Standards","","1","","19","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Scalable Simulation and Demonstration of Jumping Piezoelectric 2-D Soft Robots","Z. Zheng; P. Kumar; Y. Chen; H. Cheng; S. Wagner; M. Chen; N. Verma; J. C. Sturm","Department of Electrical and Computer Engineering, Princeton University, Princeton, New Jersey, U.S.A; Department of Electrical and Computer Engineering, Princeton University, Princeton, New Jersey, U.S.A; Department of Electrical and Computer Engineering, Princeton University, Princeton, New Jersey, U.S.A; Department of Electrical and Computer Engineering, Princeton University, Princeton, New Jersey, U.S.A; Department of Electrical and Computer Engineering, Princeton University, Princeton, New Jersey, U.S.A; Department of Electrical and Computer Engineering, Princeton University, Princeton, New Jersey, U.S.A; Department of Electrical and Computer Engineering, Princeton University, Princeton, New Jersey, U.S.A; Department of Electrical and Computer Engineering, Princeton University, Princeton, New Jersey, U.S.A",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","5199","5204","Soft robots have drawn great interest due to their ability to take on a rich range of shapes and motions, compared to traditional rigid robots. However, the motions, and underlying statics and dynamics, pose significant challenges to forming well-generalized and robust models necessary for robot design and control. In this work, we demonstrate a five-actuator soft robot capable of complex motions and develop a scalable simulation framework that reliably predicts robot motions. The simulation framework is validated by comparing its predictions to experimental results, based on a robot constructed from piezoelectric layers bonded to a steel-foil substrate. The simulation framework exploits the physics engine PyBullet, and employs discrete rigid-link elements connected by motors to model the actuators. We perform static and AC analyses to validate a single-unit actuator cantilever setup and observe close agreement between simulation and experiments for both the cases. The analyses are extended to the five-actuator robot, where simulations accurately predict the static and AC robot motions, including shapes for applied DC voltage inputs, nearly-static “inchworm” motion, and jumping (in vertical as well as vertical and horizontal directions). These motions exhibit complex non-linear behavior, with forward robot motion reaching ̴1 cm/s. Our open-source code can be found at: https://github.com/zhiwuz/sfers.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811927","Semiconductor Research Corporation (SRC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811927","","Robot motion;Actuators;Analytical models;Shape;Dynamics;Soft robotics;Predictive models","","6","","19","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"R3LIVE: A Robust, Real-time, RGB-colored, LiDAR-Inertial-Visual tightly-coupled state Estimation and mapping package","J. Lin; F. Zhang",NA; NA,2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","10672","10678","In this paper, we propose a novel LiDAR-Inertial-Visual sensor fusion framework termed R3LIVE, which takes advantage of measurement of LiDAR, inertial, and visual sensors to achieve robust and accurate state estimation. R3LIVE consists of two subsystems, a LiDAR-Inertial odometry (LIO) and a Visual-Inertial odometry (VIO). The LIO subsystem (FAST-LIO) utilizes the measurements from LiDAR and inertial sensors and builds the geometric structure (i.e., the positions of 3D points) of the map. The VIO subsystem uses the data of Visual-Inertial sensors and renders the map's texture (i.e., the color of 3D points). More specifically, the VIO subsystem fuses the visual data directly and effectively by minimizing the frame-to-map photometric error. The proposed system R3LIVE is developed based on our previous work R2LIVE, with a completely different VIO architecture design. The overall system is able to reconstruct the precise, dense, 3D, RGB-colored maps of the surrounding environment in real-time (see our attached video 11https://youtu.be/j5fT8NE5fdg). Our experiments show that the resultant system achieves higher robustness and accuracy in state estimation than its current counterparts. To share our findings and make contributions to the community, we open source R3LIVE on our Github 22https://github.com/hku-mars/r31ive","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811935","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811935","","Visualization;Three-dimensional displays;Laser radar;Streaming media;Sensor fusion;Robot sensing systems;Real-time systems","","152","","30","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Deploying Traffic Smoothing Cruise Controllers Learned from Trajectory Data","N. Lichtlé; E. Vinitsky; M. Nice; B. Seibold; D. Work; A. M. Bayen","Department of Computer Science, ENS Paris-Saclay, Paris-Saclay University; Department of Mechanical Engineering, UC Berkeley; Department of Civil and Environmental Engineering, Vanderbilt University; Department of Mathematics, Temple University; Department of Civil and Environmental Engineering, Vanderbilt University; Department of Electrical Engineering and Computer Science at UC Berkeley",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","2884","2890","Autonomous vehicle-based traffic smoothing con-trollers are often not transferred to real-world use due to challenges in calibrating many-agent traffic simulators. We show a pipeline to sidestep such calibration issues by collecting trajectory data and learning controllers directly from trajectory data that are then deployed zero-shot onto the highway. We construct a dataset of 772.3 kilometers of recorded drives on the I–24. We then construct a simple simulator using the recorded drives as the lead vehicle in front of a simulated platoon consisting of one autonomous vehicle and five human followers. Using policy-gradient methods with an asymmetric critic to learn the controller, we show that we are able to improve average MPG by 11% in simulation on congested trajectories. We deploy this controller to a mixed platoon of 4 autonomous Toyota RAV-4's and 7 human drivers in a validation experiment and demonstrate that the expected time-gap of the controller is maintained in the real world test. Finally, we release the driving dataset [1], the simulator, and the trained controller at https://github.com/nathanlct/trajectory-training-icra.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811912","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811912","","Road transportation;Smoothing methods;Automation;Pipelines;Lead;Trajectory;Calibration","","20","","28","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"DRG: A Dynamic Relation Graph for Unified Prior-Online Environment Modeling in Urban Autonomous Driving","R. Dempster; M. Al-Sharman; Y. Jain; J. Li; D. Rayside; W. Melek","Department of Electrical and Computer Engineering, WATonomous lab, University of Waterloo, ON, Canada; Department of Electrical and Computer Engineering, WATonomous lab, University of Waterloo, ON, Canada; Department of Electrical and Computer Engineering, WATonomous lab, University of Waterloo, ON, Canada; Computer Science Department, WATonomous lab, University of Waterloo, ON, Canada; Department of Electrical and Computer Engineering, WATonomous lab, University of Waterloo, ON, Canada; Department of Mechanical and Mechatronics Engineering, WATonomous lab, University of Waterloo, ON, Canada",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","8054","8060","Environment modeling is the backbone of how autonomous agents understand the world, and therefore has significant implications for decision-making and verification. Motivated by the success of relational mapping tools such as Lanelet2, we present the Dynamic Relation Graph (DRG). The DRG is a novel method for extending prior relational maps to include online observations, creating a unified en-vironment model which incorporates both prior and online data sources. Our prototype implementation models a finite set of heterogeneous features including road signage and pedestrian movement. However, the methodology behind the DRG can be expanded to a wider range of features in a fashion that does not increase the complexity of behavioral planning. Simulated stress tests indicate the DRG's effectiveness in decreasing decision-making complexity, and deployment on the University of Waterloo's WATonomous research vehicle demonstrates its practical utility. The prototype code will be released at github.com/WATonomous/DRG.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812290","NSERC(grant numbers:537104-18); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812290","","Soft sensors;Decision making;Prototypes;Autonomous agents;Complexity theory;Planning;Behavioral sciences","","4","","14","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"R-PCC: A Baseline for Range Image-based Point Cloud Compression","S. Wang; J. Jiao; P. Cai; L. Wang","Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong SAR, China; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong SAR, China; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong SAR, China; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong SAR, China",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","10055","10061","In autonomous vehicles or robots, point clouds from LiDAR can provide accurate depth information of objects compared with 2D images, but they also suffer a large volume of data, which is inconvenient for data storage or transmission. In this paper, we propose a Range image-based Point Cloud Compression method, R-PCC, which can reconstruct the point cloud with uniform or non-uniform accuracy loss. We segment the original large-scale point cloud into small and compact regions for spatial redundancy and salient region classification. Our range image-based method can keep and align all points from the original point cloud in the reconstructed point cloud, and the setting of the quantization module restricts the maximum reconstruction error. In the experiments, we prove that our easier FPS-based segmentation method can achieve better performance than instance-based segmentation methods such as DBSCAN, and our non-uniform compression framework shows a great improvement on the downstream tasks compared with the state-of-the-art large-scale point cloud compression methods. Our real-time method can achieve 40 × compression ratio without affecting downstream tasks, to act as a baseline for range image-based point cloud compression. The code is available on https://github.com/StevenWang30/R-PCC.git.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811880","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811880","","Point cloud compression;Image segmentation;Image coding;Three-dimensional displays;Simultaneous localization and mapping;Redundancy;Real-time systems","","10","","35","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Depth Completion Using Geometry-Aware Embedding","W. du; H. Chen; H. Yang; Y. Zhang","College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","8680","8686","Exploiting internal spatial geometric constraints of sparse LiDARs is beneficial to depth completion, however, has been not explored well. This paper proposes an efficient method to learn geometry-aware embedding, which encodes the local and global geometric structure information from 3D points, e.g., scene layout, object's sizes and shapes, to guide dense depth estimation. Specifically, we utilize the dynamic graph representation to model generalized geometric relationship from irregular point clouds in a flexible and efficient manner. Further, we joint this embedding and corresponded RGB appearance information to infer missing depths of the scene with well structure-preserved details. The key to our method is to integrate implicit 3D geometric representation into a 2D learning architecture, which leads to a better trade-off between the performance and efficiency. Extensive experiments demonstrate that the proposed method outperforms previous works and could reconstruct fine depths with crisp boundaries in regions that are over-smoothed by them. The ablation study gives more insights into our method that could achieve significant gains with a simple design, while having better generalization capability and stability. The code is available at https://github.com/Wenchao-Du/GAENet.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811556","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811556","","Point cloud compression;Three-dimensional displays;Laser radar;Codes;Automation;Shape;Layout","","14","","44","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Striking the Right Balance: Recall Loss for Semantic Segmentation","J. Tian; N. C. Mithun; Z. Seymour; H. -P. Chiu; Z. Kira",NA; NA; NA; NA; NA,2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","5063","5069","Class imbalance is a fundamental problem in computer vision applications such as semantic segmentation. Specifically, uneven class distributions in a training dataset often result in unsatisfactory performance on under-represented classes. Many works have proposed to weight the standard cross entropy loss function with pre-computed weights based on class statistics, such as the number of samples and class margins. There are two major drawbacks to these methods: 1) constantly up-weighting minority classes can introduce excessive false positives in semantic segmentation; 2) a minority class is not necessarily a hard class. The consequence is low precision due to excessive false positives. In this regard, we propose a hard-class mining loss by reshaping the vanilla cross entropy loss such that it weights the loss for each class dynamically based on instantaneous recall performance. We show that the novel recall loss changes gradually between the standard cross entropy loss and the inverse frequency weighted loss. Recall loss also leads to improved mean accuracy while offering competitive mean Intersection over Union (IoU) performance. On Synthia dataset11Synthia-Sequence Summer split, recall loss achieves 9% relative improvement on mean accuracy with competitive mean IoU using DeepLab-ResNet18 compared to the cross entropy loss. Code available at https://github.com/PotatoTian/recall-semseg.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811702","ONR(grant numbers:N00014-18-1-2829); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811702","","Training;Measurement;Computer vision;Codes;Automation;Semantics;Entropy","","16","","23","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Unified Data Collection for Visual-Inertial Calibration via Deep Reinforcement Learning","Y. Ao; L. Chen; F. Tschopp; M. Breyer; R. Siegwart; A. Cramariuc","Autonomous Systems Lab, ETH Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Switzerland; Arrival Ltd, United Kingdom; Autonomous Systems Lab, ETH Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Switzerland",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","1646","1652","Visual-inertial sensors have a wide range of applications in robotics. However, good performance often requires different sophisticated motion routines to accurately calibrate camera intrinsics and inter-sensor extrinsics. This work presents a novel formulation to learn a motion policy to be executed on a robot arm for automatic data collection for calibrating intrinsics and extrinsics jointly. Our approach models the calibration process compactly using model-free deep reinforcement learning to derive a policy that guides the motions of a robotic arm holding the sensor to efficiently collect measurements that can be used for both camera intrinsic calibration and camera-IMU extrinsic calibration. Given the current pose and collected measurements, the learned policy generates the subsequent transformation that optimizes sensor calibration accuracy. The evaluations in simulation and on a real robotic system show that our learned policy generates favorable motion trajectories and collects enough measurements efficiently that yield the desired intrinsics and extrinsics with short path lengths. In simulation, we are able to perform calibrations 10× faster than hand-crafted policies, which transfers to a real-world speed up of 3× over a human expert. The code of this work is publicly available at: https://github.com/ethz-asl/Learn-to-Calibrate.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811629","ETH Mobility Initiative; European Union's Horizon 2020 research and innovation programme(grant numbers:101017008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811629","","Current measurement;Robot vision systems;Reinforcement learning;Data collection;Manipulators;Cameras;Throughput","","2","","22","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"SAGE: SLAM with Appearance and Geometry Prior for Endoscopy","X. Liu; Z. Li; M. Ishii; G. D. Hager; R. H. Taylor; M. Unberath","Computer Science Department, Johns Hopkins University (JHU), Baltimore, MD, USA; Computer Science Department, Johns Hopkins University (JHU), Baltimore, MD, USA; Johns Hopkins Medical Institutions, Baltimore, MD, USA; Computer Science Department, Johns Hopkins University (JHU), Baltimore, MD, USA; Computer Science Department, Johns Hopkins University (JHU), Baltimore, MD, USA; Computer Science Department, Johns Hopkins University (JHU), Baltimore, MD, USA",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","5587","5593","In endoscopy, many applications (e.g., surgical navigation) would benefit from a real-time method that can simultaneously track the endoscope and reconstruct the dense 3D geometry of the observed anatomy from a monocular endoscopic video. To this end, we develop a Simultaneous Localization and Mapping system by combining the learning-based appearance and optimizable geometry priors and factor graph optimization. The appearance and geometry priors are explicitly learned in an end-to-end differentiable training pipeline to master the task of pair-wise image alignment, one of the core components of the SLAM system. In our experiments, the proposed SLAM system is shown to robustly handle the challenges of texture scarceness and illumination variation that are commonly seen in endoscopy. The system generalizes well to unseen endoscopes and subjects and performs favorably compared with a state-of-the-art feature-based SLAM system. The code repository is available at https://github.com/lppllpp1920/SAGE-SLAM.git.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812257","Intuitive Surgical; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812257","","Geometry;Training;Simultaneous localization and mapping;Three-dimensional displays;Endoscopes;Pipelines;Surgery","","19","","46","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Fast High-Quality Tabletop Rearrangement in Bounded Workspace","K. Gao; D. Lau; B. Huang; K. E. Bekris; J. Yu","Department of Computer Science, Rutgers University, NJ, USA; Department of Computer Science, Cornell University, NY, USA; Department of Computer Science, Rutgers University, NJ, USA; Department of Computer Science, Rutgers University, NJ, USA; Department of Computer Science, Rutgers University, NJ, USA",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","1961","1967","In this paper, we examine the problem of rearranging many objects on a tabletop in a cluttered setting using overhand grasps. Efficient solutions for the problem, which capture a common task that we solve on a daily basis, are essential in enabling truly intelligent robotic manipulation. In a given instance, objects may need to be placed at temporary positions (“buffers”) to complete the rearrangement, but allocating these buffer locations can be highly challenging in a cluttered environment. To tackle the challenge, a two-step baseline planner is first developed, which generates a primitive plan based on inherent combinatorial constraints induced by start and goal poses of the objects and then selects buffer locations assisted by the primitive plan. We then employ the “lazy” planner in a tree search framework which is further sped up by adapting a novel preprocessing routine. Simulation experiments show our methods can quickly generate high-quality solutions and are more robust in solving large-scale instances than existing state-of-the-art approaches. source: github.com/arc-l/TRLB","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812367","NSF(grant numbers:IIS-1845888,CCF-1934924,IIS-2132972); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812367","","Adaptation models;Automation;Task analysis;Robots","","16","","30","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Hybrid Physical Metric For 6-DoF Grasp Pose Detection","Y. Lu; B. Deng; Z. Wang; P. Zhi; Y. Li; S. Wang",Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University,2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","8238","8244","6-DoF grasp pose detection of multi-grasp and multi-object is a challenge task in the field of intelligent robot. To imitate human reasoning ability for grasping objects, data driven methods are widely studied. With the introduction of large-scale datasets, we discover that a single physical metric usually generates several discrete levels of grasp confidence scores, which cannot finely distinguish millions of grasp poses and leads to inaccurate prediction results. In this paper, we propose a hybrid physical metric to solve this evaluation insufficiency. First, we define a novel metric is based on the force-closure metric, supplemented by the measurement of the object flatness, gravity and collision. Second, we leverage this hybrid physical metric to generate elaborate confidence scores. Third, to learn the new confidence scores effectively, we design a multi-resolution network called Flatness Gravity Collision GraspNet (FGC-GraspNet). FGC-GraspNet proposes a multi-resolution features learning architecture for multiple tasks and introduces a new joint loss function that enhances the average precision of the grasp detection. The network evaluation and adequate real robot experiments demonstrate the effectiveness of our hybrid physical metric and FGC-GraspNet. Our method achieves 90.5% success rate in real-world cluttered scenes. Our code is available at https://github.com/luyh20IFGC-GraspNet.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811961","Tsinghua University(grant numbers:2019GQG0001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811961","","Measurement;Representation learning;Force measurement;Grasping;Hybrid power systems;Cognition;Task analysis","","9","","37","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Trajectory Prediction for Autonomous Driving with Topometric Map","J. Xu; L. Xiao; D. Zhao; Y. Nie; B. Dai","Unmanned Systems Research Center, National Innovation Institute of Defense Technology, Beijing, China; Unmanned Systems Research Center, National Innovation Institute of Defense Technology, Beijing, China; Unmanned Systems Research Center, National Innovation Institute of Defense Technology, Beijing, China; Unmanned Systems Research Center, National Innovation Institute of Defense Technology, Beijing, China; Unmanned Systems Research Center, National Innovation Institute of Defense Technology, Beijing, China",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","8403","8408","State-of-the-art autonomous driving systems rely on high definition (HD) maps for localization and navigation. However, building and maintaining HD maps is time-consuming and expensive. Furthermore, the HD maps assume structured environment such as the existence of major road and lanes, which are not present in rural areas. In this work, we propose an end-to-end transformer networks based approach for map-less autonomous driving. The proposed model takes raw LiDAR data and noisy topometric map as input and produces precise local trajectory for navigation. We demonstrate the effectiveness of our method in real-world driving data, including both urban and rural areas. The experimental results show that the proposed method outperforms state-of-the-art multimodal methods and is robust to the perturbations of the topometric map. The code of the proposed method is publicly available at https://github.com/Jiaolong/trajectory-prediction.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811712","National Natural Science Foundation of China(grant numbers:61803380,61790565); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811712","","Laser radar;Navigation;Roads;Perturbation methods;Semantics;Predictive models;Transformers","","4","","20","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Abstract Flow for Temporal Semantic Segmentation on the Permutohedral Lattice","P. Schutt; R. A. Rosu; S. Behnke","Autonomous Intelligent Systems Group, Computer Science VI, University of Bonn, Germany; Autonomous Intelligent Systems Group, Computer Science VI, University of Bonn, Germany; Autonomous Intelligent Systems Group, Computer Science VI, University of Bonn, Germany",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","5139","5145","Semantic segmentation is a core ability required by autonomous agents, as being able to distinguish which parts of the scene belong to which object class is crucial for navigation and interaction with the environment. Approaches which use only one time-step of data cannot distinguish between moving objects nor can they benefit from temporal integration. In this work, we extend a backbone LatticeNet to process temporal point cloud data. Additionally, we take inspiration from optical flow methods and propose a new module called Abstract Flow which allows the network to match parts of the scene with similar abstract features and gather the information temporally. We obtain state-of-the-art results on the SemanticKITTI dataset that contains LiDAR scans from real urban environments. We share the PyTorch implementation of TemporalLatticeNet at https://github.com/AIS-Bonn/temporal_latticenet.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811818","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811818","","Point cloud compression;Image motion analysis;Computer vision;Laser radar;Navigation;Semantics;Urban areas","","9","","23","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Strawberry picking point localization ripeness and weight estimation","A. Tafuro; A. Adewumi; S. Parsa; G. E. Amir; B. Debnath","LN25TS, Lincoln Institute for Agri-food Technology, University of Lincoln, Lincoln, United Kingdom; LN25TS, Lincoln Institute for Agri-food Technology, University of Lincoln, Lincoln, United Kingdom; LN25TS, Lincoln Institute for Agri-food Technology, University of Lincoln, Lincoln, United Kingdom; LN25TS, Lincoln Institute for Agri-food Technology, University of Lincoln, Lincoln, United Kingdom; LN25TS, Lincoln Institute for Agri-food Technology, University of Lincoln, Lincoln, United Kingdom",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","2295","2302","Labour shortage, difficulties in labour management, the digitalization of fruit production pipeline to reduce the fruit production costs have made robotic systems for selective harvesting of strawberries an important industry and academic research. One of the important components of such technologies yet to be developed is fruit picking perception. For picking strawberries, a robot needs to infer the location of picking points from the images of strawberries. Moreover, the size and weight of strawberries to be picked can help the robot to place the picked strawberries in proper punnets directly to be delivered to customers in supermarkets. This can save significant time and packing costs in packhouses. Geometry-based approaches are the most common approach to determine the picking point but they suffer from inaccuracies due to noise, occlusion, and varying shape and orientation of the berries. In contrast, we present two novel datasets of strawberries annotated with picking points, key-points (such as the shoulder points, the contact point between the calyx and flesh, and the point on the flesh farthest from the calyx), and the weight and size of the berries. We performed experiments with Detectron-2, which is an extended version of Mask-RCNN with key-points detection capability. The results show that the key-points detection approach works well for picking and grasping point localization. The second dataset also presents the dimensions and weight of strawberries. Our novel baseline model for weight estimation outperforms many state-of-the-art deep networks. The datasets and annotations are available at https://github.com/imanlab/strawberry-pp-w-r-dataset.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812303","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812303","","Location awareness;Industries;Image segmentation;Costs;Service robots;Shape;Pipelines","","15","","48","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Targeted Attack on Deep RL-based Autonomous Driving with Learned Visual Patterns","P. Buddareddygari; T. Zhang; Y. Yang; Y. Ren","Active Perception Group at the School of Computing and Augmented Intelligence, Arizona State University, Tempe, AZ, USA; College of Engineering, Cornell University, Ithaca, NY, USA; Active Perception Group at the School of Computing and Augmented Intelligence, Arizona State University, Tempe, AZ, USA; School for Engineering of Matter, Transport, and Energy, Arizona State University, Tempe",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","10571","10577","Recent studies demonstrated the vulnerability of control policies learned through deep reinforcement learning against adversarial attacks, raising concerns about the application of such models to risk-sensitive tasks such as autonomous driving. Threat models for these demonstrations are limited to (1) targeted attacks through real-time manipulation of the agent's observation, and (2) untargeted attacks through manipulation of the physical environment. The former assumes full access to the agent's states/observations at all times, while the latter has no control over attack outcomes. This paper investigates the feasibility of targeted attacks through visually learned patterns placed on physical objects in the environment, a threat model that combines the practicality and effectiveness of the existing ones. Through analysis, we demonstrate that a pre-trained policy can be hijacked within a time window, e.g., performing an unintended self-parking, when an adversarial object is present. To enable the attack, we adopt an assumption that the dynamics of both the environment and the agent can be learned by the attacker. Lastly, we empirically show the effectiveness of the proposed attack on different driving scenarios, perform a location robustness test, and study the tradeoff between the attack strength and its effectiveness Code is available at https://github.com/ASU-APG/ Targeted-Physical-Adversarial-Attacks-on-AD","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811574","National Science Foundation(grant numbers:1925403,2038666,2101052); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811574","","Visualization;Three-dimensional displays;Perturbation methods;Heuristic algorithms;Reinforcement learning;Robustness;Real-time systems","","2","","38","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Fast and Optimal Trajectory Planning for Multiple Vehicles in a Nonconvex and Cluttered Environment: Benchmarks, Methodology, and Experiments","Y. Ouyang; B. Li; Y. Zhang; T. Acarman; Y. Guo; T. Zhang","College of Mechanical and Vehicle Engineering, Hunan University, Changsha, China; College of Mechanical and Vehicle Engineering, Hunan University, Changsha, China; Department of Mechanical, Industrial and Aerospace Engineering, Concordia Institute of Aerospace Design and Innovation, Concordia University, Montreal, Canada; Computer Engineering Department, Galatasaray University, Istanbul, Turkey; Department of Automation, Tsinghua University, Beijing, China; College of Mechanical and Vehicle Engineering, Hunan University, Changsha, China",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","10746","10752","This paper is focused on the cooperative trajectory planning problem for multiple car-like robots in a cluttered and unstructured environment narrowed by static obstacles. The concerned multi-vehicle trajectory planning (MVTP) problem is challenging because i) the scenario is nonconvex and tiny; ii) the vehicle kinematics is nonconvex; and iii) a feasible homotopy class is unavailable a priori. We propose a two-stage MVTP method: Stage 1 identifies a feasible homotopy class, and Stage 2 quickly finds a local optimum based on the identified homotopy class. Numerical optimal control, adaptive scaling, grouping, and trust region construction strategies are integrated into the proposed planner. Our planner is extensively compared in 100 benchmark cases with the state-of-the-art MVTP methods such as incremental sequential convex programming, numerical optimal control, conflict-based search, priority-based trajectory optimizer, and optimal reciprocal collision avoidance. The simulation results demonstrate our method's superiority in runtime and optimality. Experiments with three car-like robots demonstrate the efficiency of our proposed planner. Source codes are in https://github.com/libai1943/MVTP_benchmark.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812126","Fundamental Research Funds for the Central Universities(grant numbers:531118010509); National Natural Science Foundation of China(grant numbers:62103139); Natural Sciences and Engineering Research Council of Canada; Galatasaray University(grant numbers:19.401.003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812126","","Runtime;Trajectory planning;Simulation;Optimal control;Kinematics;Benchmark testing;Programming","","11","","33","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Fast Road Segmentation via Uncertainty-aware Symmetric Network","Y. Chang; F. Xue; F. Sheng; W. Liang; A. Ming","Beijing University of Posts and Telecommunications, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","11124","11130","The high performance of RGB-D based road segmentation methods contrasts with their rare application in commercial autonomous driving, which is owing to two reasons: 1) the prior methods cannot achieve high inference speed and high accuracy in both ways; 2) the different properties of RGB and depth data are not well-exploited, limiting the reliability of predicted road. In this paper, based on the evidence theory, an uncertainty-aware symmetric network (USNet) is proposed to achieve a trade-off between speed and accuracy by fully fusing RGB and depth data. Firstly, cross-modal feature fusion operations, which are indispensable in the prior RGB-D based methods, are abandoned. We instead separately adopt two light-weight subnetworks to learn road representations from RGB and depth inputs. The light-weight structure guarantees the real-time inference of our method. Moreover, a multi-scale evidence collection (MEC) module is designed to collect evidence in multiple scales for each modality, which provides sufficient evidence for pixel class determination. Finally, in uncertainty-aware fusion (UAF) module, the uncertainty of each modality is perceived to guide the fusion of the two sub-networks. Experimental results demonstrate that our method achieves a state-of-the-art accuracy with real-time inference speed of $43+$ FPS. The source code is available at https://github.com/morancyc/USNet.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812452","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812452","","Limiting;Codes;Roads;Evidence theory;Computational modeling;Reliability theory;Real-time systems","","18","","42","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Explore-Bench: Data Sets, Metrics and Evaluations for Frontier-based and Deep-reinforcement-learning-based Autonomous Exploration","Y. Xu; J. Yu; J. Tang; J. Qiu; J. Wang; Y. Shen; Y. Wang; H. Yang","Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","6225","6231","Autonomous exploration and mapping of unknown terrains employing single or multiple robots is an essential task in mobile robotics and has therefore been widely investigated. Nevertheless, given the lack of unified data sets, metrics, and platforms to evaluate the exploration approaches, we develop an autonomous robot exploration benchmark en-titled Explore-Bench. The benchmark involves various explo-ration scenarios and presents two types of quantitative metrics to evaluate exploration efficiency and multi-robot cooperation. Explore-Bench is extremely useful as, recently, deep rein-forcement learning (DRL) has been widely used for robot exploration tasks and achieved promising results. However, training DRL-based approaches requires large data sets, and additionally, current benchmarks rely on realistic simulators with a slow simulation speed, which is not appropriate for training exploration strategies. Hence, to support efficient DRL training and comprehensive evaluation, the suggested Explore-Bench designs a 3-level platform with a unified data flow and 12 × speed-up that includes a grid-based simulator for fast evaluation and efficient training, a realistic Gazebo simulator, and a remotely accessible robot testbed for high-accuracy tests in physical environments. The practicality of the proposed benchmark is highlighted with the application of one DRL-based and three frontier-based exploration approaches. Fur-thermore, we analyze the performance differences and provide some insights about the selection and design of exploration methods. Our benchmark is available at https://github.com/efc-robot/Explore-Bench.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812344","National Natural Science Foundation of China(grant numbers:U19B2019,M-0248); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812344","","Training;Measurement;Automation;Design methodology;Benchmark testing;Data models;Task analysis","","22","","31","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Star-Convolution for Image-Based 3D Object Detection","Y. Liu; Z. Xu; M. Liu","Department of Electronic and Computer Engineering, Robotics and Multi-Perception Laborotary, The Hong Kong University of Science and Technology; Department of Electronic and Computer Engineering, Robotics and Multi-Perception Laborotary, The Hong Kong University of Science and Technology; Department of Electronic and Computer Engineering, Robotics and Multi-Perception Laborotary, The Hong Kong University of Science and Technology",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","5018","5024","3D object detection with only image inputs is an interesting and important problem in computer vision and autonomous driving. Nowadays, most existing monocular 3D object detection algorithms rely solely on the approximation power of convolutional neural networks to learn a mapping from pixels to 3D predictions without knowing the projection matrix of the camera. To endow the networks with camera projection knowledge, we propose the Star-Convolution module for application to image-based 3D detection. The introduced module increases the receptive field of the detector and embeds the camera's projection geometry inside the network while keeping the network end-to-end trainable. We test the module with different baselines in both monocular and stereo 3D object detection, and we achieve significant improvements on both tasks. The code will be published at https://github.com/Owen-Liuyuxuan/visualDet3D.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811612","National Natural Science Foundation of China(grant numbers:U1713211); Research Grant Council of Hong Kong SAR(grant numbers:11210017,21202816); Shenzhen Science, Technology and Innovation Comission (SZSTI)(grant numbers:JCYJ20160428154842603); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811612","","Training;Geometry;Solid modeling;Three-dimensional displays;Computational modeling;Object detection;Detectors","","","","34","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Efficient and High-quality Prehensile Rearrangement in Cluttered and Confined Spaces","R. Wang; Y. Miao; K. E. Bekris","Department of Computer Science, Rutgers University, NJ, USA; Department of Computer Science, Rutgers University, NJ, USA; Department of Computer Science, Rutgers University, NJ, USA",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","1968","1975","Prehensile object rearrangement in cluttered and confined spaces has broad applications but is also challenging. For instance, rearranging products in a grocery shelf means that the robot cannot directly access all objects and has limited free space. This is harder than tabletop rearrangement where objects are easily accessible with top-down grasps, which simplifies robot-object interactions. This work focuses on problems where such interactions are critical for completing tasks. It proposes a new efficient and complete solver under general constraints for monotone instances, which can be solved by moving each object at most once. The monotone solver reasons about robot-object constraints and uses them to effectively prune the search space. The new monotone solver is integrated with a global planner to solve non-monotone instances with high-quality solutions fast. Furthermore, this work contributes an effective pre-processing tool to significantly speed up online motion planning queries for rearrangement in confined spaces. Experiments further demonstrate that the proposed monotone solver, equipped with the pre-processing tool, results in 57.3% faster computation and 3 times higher success rate than state-of-the-art methods. Similarly, the resulting global planner is computationally more efficient and has a higher success rate, while producing high-quality solutions for non-monotone instances (i.e., only 1.3 additional actions are needed on average). Videos of demonstrating solutions on a real robotic system and codes can be found at https://github.com/Rui1223/uniform_object_rearrangement.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811934","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811934","","Codes;Automation;Planning;Computational efficiency;Task analysis;Robots;Videos","","19","","51","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Interleaving Monte Carlo Tree Search and Self-Supervised Learning for Object Retrieval in Clutter","B. Huang; T. Guo; A. Boularias; J. Yu","Department of Computer Science, Rutgers, the State University of New Jersey, Piscataway, NJ, USA; Department of Computer Science, Rutgers, the State University of New Jersey, Piscataway, NJ, USA; Department of Computer Science, Rutgers, the State University of New Jersey, Piscataway, NJ, USA; Department of Computer Science, Rutgers, the State University of New Jersey, Piscataway, NJ, USA",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","625","632","In this study, working with the task of object retrieval in clutter, we have developed a robot learning framework in which Monte Carlo Tree Search (MCTS) is first applied to enable a Deep Neural Network (DNN) to learn the intricate interactions between a robot arm and a complex scene containing many objects, allowing the DNN to partially clone the behavior of MCTS. In turn, the trained DNN is integrated into MCTS to help guide its search effort. We call this approach learning-guided Monte Carlo tree search for Object REtrieval (MORE), which delivers significant computational efficiency gains and added solution optimality. MORE is a self-supervised robotics framework/pipeline capable of working in the real world that successfully embodies the System 2 → System 1 learning philosophy proposed by Kahneman, where learned knowledge, used properly, can help greatly speed up a time-consuming decision process over time. Videos and supplementary material can be found at https://github.com/arc-l/more.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812132","NSF(grant numbers:IIS-1734492,IIS-1846043,IIS-1845888,CCF-1934924,IIS-2132972); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812132","","Monte Carlo methods;Philosophical considerations;Neural networks;Self-supervised learning;Search problems;Manipulators;Robot learning","","12","","48","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"HMD-former: a Transformer-based Human Mesh Deformer with Inter-layer Semantic Consistency","S. Zou; S. Liu; C. Li; L. Yao; S. Chen","Zhejiang University of Technology, HangZhou, China; Zhejiang University of Technology, HangZhou, China; Zhejiang University of Technology, HangZhou, China; Zhejiang University of Technology, HangZhou, China; Tianjin University of Technology, Tianjin, China",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","6409","6415","We present a transformer-based network, Human Mesh Deformer (HMD-former), to tackle the problem of 3D human mesh reconstruction from a single RGB image. HMD-former applies a pre-trained CNN to extract image grid features and a transformer decoder to gradually warp the template 3D mesh to the deformed mesh. On each decoder layer, the fine-grained local information of grid features is well utilized using cross-attention by softly and content-dependently transforming the grid features to vertex embeddings. Auxiliary losses and proposed bi-directional mapping layers inherently ensure semantic consistency throughout the whole decoder, which free the network from learning unnecessary embedding transformation between layers. This further induces each layer of the decoder to focus on refining vertex embeddings and makes the whole network work in a progressively refining manner. Experiments on different public datasets Human3.6M and 3DPW show better reconstruction accuracy and faster inference speed than previous state-of-the-art methods, demonstrating the effectiveness and generalizability of HMD-former. Code is publicly available at https://github.com/siyuzou/HMD-former.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812024","National Key R & D Program of China(grant numbers:2018YFB1305200); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812024","","Three-dimensional displays;Automation;Semantics;Refining;Fitting;Bidirectional control;Transformers","","1","","25","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"SPIN Road Mapper: Extracting Roads from Aerial Images via Spatial and Interaction Space Graph Reasoning for Autonomous Driving","W. G. C. Bandara; J. M. J. Valanarasu; V. M. Patel","Department of Electrical and Computer Engineering, The Johns Hopkins University, Baltimore, MD, USA; Department of Electrical and Computer Engineering, The Johns Hopkins University, Baltimore, MD, USA; Department of Electrical and Computer Engineering, The Johns Hopkins University, Baltimore, MD, USA",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","343","350","Road extraction is an essential step in building autonomous navigation systems. Detecting road segments is challenging as they are of varying widths, bifurcated throughout the image, and are often occluded by terrain, cloud, or other weather conditions. Using just convolution neural networks (ConvNets) for this problem is not effective as it is inefficient at capturing distant dependencies between road segments in the image which is essential to extract road connectivity. To this end, we propose a Spatial and Interaction Space Graph Reasoning (SPIN) module which when plugged into a ConvNet performs reasoning over graphs constructed on spatial and interaction spaces projected from the feature maps. Reasoning over spatial space extracts dependencies between different spatial regions and other contextual information. Reasoning over a projected interaction space helps in appropriate delineation of roads from other topographies present in the image. Thus, SPIN extracts long-range dependencies between road segments and effectively delineates roads from other semantics. We also introduce a SPIN pyramid which performs SPIN graph reasoning across multiple scales to extract multi-scale features. We propose a network based on stacked hourglass modules and SPIN pyramid for road segmentation which achieves better performance compared to existing methods. Moreover, our method is computationally efficient and significantly boosts the convergence speed during training, making it feasible for applying on large-scale high-resolution aerial images. Code available at: https://github.com/wgcban/SPIN_RoadMapper.git.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812134","ARO(grant numbers:W911NF-21-1-0135); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812134","","Training;Image segmentation;Roads;Buildings;Semantics;Surfaces;Feature extraction","","19","","46","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Traffic Context Aware Data Augmentation for Rare Object Detection in Autonomous Driving","N. Li; F. Song; Y. Zhang; P. Liang; E. Cheng","NullMax, Shanghai, China; NullMax, Shanghai, China; NullMax, Shanghai, China; School of Information Engineering, Zhengzhou University, China; NullMax, Shanghai, China",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","4548","4554","Detection of rare objects (e.g., traffic cones, traffic barrels and traffic warning triangles) is an important perception task to improve the safety of autonomous driving. Training of such models typically requires a large number of annotated data which is expensive and time consuming to obtain. To address the above problem, an emerging approach is to apply data augmentation to automatically generate cost-free training samples. In this work, we propose a systematic study on simple Copy-Paste data augmentation for rare object detection in autonomous driving. Specifically, local adaptive instance-level image transformation is introduced to generate realistic rare object masks from source domain to the target domain. Moreover, traffic scene context is utilized to guide the placement of masks of rare objects. To this end, our data augmentation generates training data with high quality and realistic characteristics by leveraging both local and global consistency. In addition, we build a new dataset named NM10k consisting 10k training images, 4k validation images and the corresponding labels with a diverse range of scenarios in autonomous driving. Experiments on NM10k show that our method achieves promising results on rare object detection. We also present a thorough study to illustrate the effectiveness of our local-adaptive and global constraints based Copy-Paste data augmentation for rare object detection. The data, development kit and more information of NM10k dataset are available online at: https://nullmax-vision.github.io.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811724","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811724","","Training;Context-aware services;Systematics;Training data;Object detection;Transforms;Data models","","5","","27","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"KinoJGM: A framework for efficient and accurate quadrotor trajectory generation and tracking in dynamic environments","Y. Wang; J. O'Keeffe; Q. Qian; D. Boyle","Dyson School of Design Engineering, Imperial College, London, United Kingdom; Dyson School of Design Engineering, Imperial College, London, United Kingdom; Dyson School of Design Engineering, Imperial College, London, United Kingdom; Dyson School of Design Engineering, Imperial College, London, United Kingdom",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","11036","11043","Unmapped areas and aerodynamic disturbances render autonomous navigation with quadrotors extremely challenging. To fly safely and efficiently, trajectory planners and trackers must be able to navigate unknown environments with unpredictable aerodynamic effects in real-time. When encountering aerodynamic effects such as strong winds, most current approaches to quadrotor trajectory planning and tracking will not attempt to deviate from a determined plan, even if it is risky, in the hope that any aerodynamic disturbances can be resisted by a robust controller. This paper presents a novel systematic trajectory planning and tracking framework for autonomous quadrotors. We propose a Kinodynamic Jump Space Search (Kino-JSS) to generate a safe and efficient route in unknown environments with aerodynamic disturbances. A real-time Gaussian Process is employed to model the errors caused by aerodynamic disturbances, which we then integrate with a Model Predictive Controller to achieve efficient and accurate trajectory optimization and tracking. We demonstrate our system to improve the efficiency of trajectory generation in unknown environments by up to 75% in the cases tested, compared with recent state-of-the-art. We also show that our system improves the accuracy of tracking in selected environments with unpredictable aerodynamic effects. Our implementation is available in an open source package11https://github.com/Alex-yanranwang/Imperial-KinoJGM.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812352","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812352","","Systematics;Trajectory planning;Navigation;Heuristic algorithms;Reinforcement learning;Predictive models;Aerodynamics","","4","","46","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"GOMP-FIT: Grasp-Optimized Motion Planning for Fast Inertial Transport","J. Ichnowski; Y. Avigal; Y. Liu; K. Goldberg","AUTOLab, University of California, Berkeley, CA; AUTOLab, University of California, Berkeley, CA; AUTOLab, University of California, Berkeley, CA; AUTOLab, University of California, Berkeley, CA",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","5255","5261","High-speed motions in pick-and-place operations are critical to making robots cost-effective in many automation scenarios, from warehouses and manufacturing to hospitals and homes. However, motions can be too fast-such as when the object being transported has an open-top, is fragile, or both. One way to avoid spills or damage, is to move the arm slowly. We propose an alternative: Grasp-Optimized Motion Planning for Fast Inertial Transport (GOMP-FIT), a time-optimizing motion planner based on our prior work, that includes con-straints based on accelerations at the robot end-effector. With GOMP-FIT, a robot can perform high-speed motions that avoid obstacles and use inertial forces to its advantage. In experiments transporting open-top containers with varying tilt tolerances, whereas GOMP computes sub-second motions that spill up to 90 % of the contents during transport, GOMP-FIT generates motions that spill 0 % of contents while being slowed by as little as 0 % when there are few obstacles, 30 % when there are high obstacles and 45-degree tolerances, and 50 % when there 15-degree tolerances and few obstacles. Videos and more at: https://berkeleyautomation.github.io/gomp-fit/.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812387","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812387","","Automation;Torque;Containers;End effectors;Trajectory;Planning;Reliability","","15","","38","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Augmenting Reinforcement Learning with Behavior Primitives for Diverse Manipulation Tasks","S. Nasiriany; H. Liu; Y. Zhu",The University of Texas at Austin; The University of Texas at Austin; The University of Texas at Austin,2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","7477","7484","Realistic manipulation tasks require a robot to interact with an environment with a prolonged sequence of motor actions. While deep reinforcement learning methods have recently emerged as a promising paradigm for automating manipulation behaviors, they usually fall short in long-horizon tasks due to the exploration burden. This work introduces Manipulation Primitive-augmented reinforcement Learning (MAPLE), a learning framework that augments standard reinforcement learning algorithms with a pre-defined library of behavior primitives. These behavior primitives are robust functional modules specialized in achieving manipulation goals, such as grasping and pushing. To use these heterogeneous primitives, we develop a hierarchical policy that involves the primitives and instantiates their executions with input parameters. We demonstrate that MAPLE out-performs baseline approaches by a significant margin on a suite of simulated manipulation tasks. We also quantify the compositional structure of the learned behaviors and highlight our method's ability to transfer policies to new task variants and to physical hardware. Videos and code are available at https://ut-austin-rpl.github.io/maple","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812140","NSF(grant numbers:CNS-1955523); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812140","","Scalability;Reinforcement learning;Grasping;Libraries;Hardware;Behavioral sciences;Task analysis","","39","","71","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Asynchronous Reinforcement Learning for Real-Time Control of Physical Robots","Y. Yuan; A. R. Mahmood","Department of Computing Science, University of Alberta, Edmonton, Canada; Department of Computing Science, University of Alberta, Edmonton, Canada",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","5546","5552","An oft-ignored challenge of real-world reinforcement learning is that the real world does not pause when agents make learning updates. As standard simulated environments do not address this real-time aspect of learning, most available implementations of RL algorithms process environment interactions and learning updates sequentially. As a consequence, when such implementations are deployed in the real world, they may make decisions based on significantly delayed observations and not act responsively. Asynchronous learning has been proposed to solve this issue, but no systematic comparison between sequential and asynchronous reinforcement learning was conducted using real-world environments. In this work, we set up two vision-based tasks with a robotic arm, implement an asynchronous learning system that extends a previous architecture, and compare sequential and asynchronous reinforcement learning across different action cycle times, sensory data dimensions, and mini-batch sizes. Our experiments show that when the time cost of learning updates increases, the action cycle time in sequential implementation could grow excessively long, while the asynchronous implementation can always maintain an appropriate action cycle time. Consequently, when learning updates are expensive, the performance of sequential learning diminishes and is outperformed by asynchronous learning by a substantial margin. Our system learns in real-time to reach and track visual targets from pixels within two hours of experience and does so directly using real robots, learning completely from scratch. Our code is available at: https://github.com/YufengYuan/ur5_async_r1.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811771","Alberta Machine Intelligence Institute (Amii); Natural Sciences and Engineering Research Council (NSERC) of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811771","","Visualization;Systematics;Costs;Reinforcement learning;Computer architecture;Robot sensing systems;Real-time systems","","4","","29","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Learning Observation-Based Certifiable Safe Policy for Decentralized Multi-Robot Navigation","Y. Cui; L. Lin; X. Huang; D. Zhang; Y. Wang; W. Jing; J. Chen; R. Xiong; Y. Wang","State Key Laboratory of Industrial Control Technology, Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; State Key Laboratory of Industrial Control Technology, Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; State Key Laboratory of Industrial Control Technology, Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; State Key Laboratory of Industrial Control Technology, Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; State Key Laboratory of Industrial Control Technology, Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Department of Autonomous, Driving Lab, Alibaba DAMO Academy, Hangzhou, China; Department of Autonomous, Driving Lab, Alibaba DAMO Academy, Hangzhou, China; State Key Laboratory of Industrial Control Technology, Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; State Key Laboratory of Industrial Control Technology, Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","5518","5524","Safety is of great importance in multi-robot navigation problems. In this paper, we propose a control barrier function (CBF) based optimizer that ensures robot safety with both high probability and flexibility, using only sensor measurement. The optimizer takes action commands from the policy network as initial values and provides refinement to drive the potentially dangerous ones back into safe regions. With the help of a deep world model that predicts the evolution of surrounding dynamics and the consequences of different actions, the CBF module can guide the optimization within a reasonable time horizon. We also present a novel joint training framework that improves the cooperation between the Reinforcement Learning (RL) based policy and the CBF-based optimizer by utilizing reward feedback from the CBF module. We observe that our policy can achieve a higher success rate while maintaining the safety of multiple robots in significantly fewer episodes. Experiments are conducted in multiple scenarios both in simulation and the real world, the results demonstrate the effectiveness of our method in maintaining the safety of multiple robots. Code is available at https://github.com/YuxiangCui/MARL-OCBF.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811950","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811950","","Training;Codes;Automation;Navigation;Reinforcement learning;Predictive models;Robot sensing systems","","5","","24","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Learning to Fill the Seam by Vision: Sub-millimeter Peg-in-hole on Unseen Shapes in Real World","L. Xie; H. Yu; Y. Zhao; H. Zhang; Z. Zhou; M. Wang; Y. Wang; R. Xiong","State Key Laboratory of Industrial Control Technology and Institute of Cyber-Systems and Control, Zhejiang University, Zhejiang, China; State Key Laboratory of Industrial Control Technology and Institute of Cyber-Systems and Control, Zhejiang University, Zhejiang, China; State Key Laboratory of Industrial Control Technology and Institute of Cyber-Systems and Control, Zhejiang University, Zhejiang, China; State Key Laboratory of Industrial Control Technology and Institute of Cyber-Systems and Control, Zhejiang University, Zhejiang, China; State Key Laboratory of Industrial Control Technology and Institute of Cyber-Systems and Control, Zhejiang University, Zhejiang, China; Application Innovate Lab, Huawei Incorporated Company, P.R. China; State Key Laboratory of Industrial Control Technology and Institute of Cyber-Systems and Control, Zhejiang University, Zhejiang, China; State Key Laboratory of Industrial Control Technology and Institute of Cyber-Systems and Control, Zhejiang University, Zhejiang, China",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","2982","2988","In the peg insertion task, human pays attention to the seam between the peg and the hole and tries to fill it continuously with visual feedback. By imitating the human's behavior, we design architectures with position and orientation estimators based on the seam representation for pose alignment, which proves to be general to the unseen peg geometries. By putting the estimators into the closed-loop control with reinforcement learning, we further achieve higher or comparable success rate, efficiency, and robustness compared with the baseline methods. The policy is trained totally in simulation without any manual intervention. To achieve sim-to-real, a learnable segmentation module with automatic data collecting and labeling can be easily trained to decouple the perception and the policy, which helps the model trained in simulation quickly adapting to the real world with negligible effort. Results are presented in simulation and on a physical robot. Code, videos, and supplemental material are available at https://github.com/xieliang555/SFN.git","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812429","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812429","","Adaptation models;Visualization;Shape;Reinforcement learning;Manuals;Robustness;Data models","","10","","20","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"A Linear Comb Filter for Event Flicker Removal","Z. Wang; D. Yuan; Y. Ng; R. Mahony","Systems Theory and Robotics Group, College of Engineering and Computer Science, The Australian National University; Systems Theory and Robotics Group, College of Engineering and Computer Science, The Australian National University; Systems Theory and Robotics Group, College of Engineering and Computer Science, The Australian National University; Systems Theory and Robotics Group, College of Engineering and Computer Science, The Australian National University",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","398","404","Event cameras are bio-inspired sensors that capture per-pixel asynchronous intensity change rather than the synchronous absolute intensity frames captured by a classical camera sensor. Such cameras are ideal for robotics applications since they have high temporal resolution, high dynamic range and low latency. However, due to their high temporal resolution, event cameras are particularly sensitive to flicker such as from fluorescent or LED lights. During every cycle from bright to dark, pixels that image a flickering light source generate many events that provide little or no useful information for a robot, swamping the useful data in the scene. In this paper, we propose a novel linear filter to preprocess event data to remove unwanted flicker events from an event stream. The proposed algorithm achieves over 4.6 times relative improvement in the signal-to-noise ratio when compared to the raw event stream due to the effective removal of flicker from fluorescent lighting. Thus, it is ideally suited to robotics applications that operate in indoor settings or scenes illuminated by flickering light sources. Code, Datasets and Video: https://github.com/ziweiWWANG/EFR","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812003","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812003","","Maximum likelihood detection;Robot vision systems;Nonlinear filters;Filtering algorithms;Fluorescence;Cameras;Light emitting diodes","","12","","37","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Probabilistic Inference of Simulation Parameters via Parallel Differentiable Simulation","E. Heiden; C. E. Denniston; D. Millard; F. Ramos; G. S. Sukhatme","Department of Computer Science, University of Southern California, Los Angeles, USA; Department of Computer Science, University of Southern California, Los Angeles, USA; Department of Computer Science, University of Southern California, Los Angeles, USA; NVIDIA, Seattle, USA; Department of Computer Science, University of Southern California, Los Angeles, USA",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","3638","3645","Reproducing real world dynamics in simulation is critical for the development of new control and perception methods. This task typically involves the estimation of simu-lation parameter distributions from observed rollouts through an inverse inference problem characterized by multi-modality and skewed distributions. We address this challenging problem through a novel Bayesian inference approach that approximates a posterior distribution over simulation parameters given real sensor measurements. By extending the commonly used Gaus-sian likelihood model for trajectories via the multiple-shooting formulation, our gradient-based particle inference algorithm, Stein Variational Gradient Descent, is able to identify highly nonlinear, underactuated systems. We leverage GPU code gen-eration and differentiable simulation to evaluate the likelihood and its gradient for many particles in parallel. Our algorithm infers nonparametric distributions over simulation parame-ters more accurately than comparable baselines and handles constraints over parameters efficiently through gradient-based optimization. We evaluate estimation performance on several physical experiments. On an underactuated mechanism where a 7-DOF robot arm excites an object with an unknown mass configuration, we demonstrate how the inference technique can identify symmetries between the parameters and provide highly accurate predictions. Website: https://uscresl.github.io/prob-diff-sim","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812293","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812293","","Uncertainty;Estimation;Predictive models;Probabilistic logic;Robot sensing systems;Prediction algorithms;Inference algorithms","","7","","68","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"PA-AWCNN: Two-stream Parallel Attention Adaptive Weight Network for RGB-D Action Recognition","L. Yao; S. Liu; C. Li; S. Zou; S. Chen; D. Guan","College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Engineering, Tianjin University of Technology, Tianjin, China; College of Humanities, Zhejiang University of Technology, Hangzhou, China",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","8741","8747","Due to overly relying on appearance information or adopting direct static feature fusion, most of the existing action recognition methods based on multi-modality have poor robustness and insufficient consideration of modality differences. To address these problems, we propose a two-stream adaptive weight integration network with a three-dimensional parallel attention module, PA-AWCNN. Firstly, a three-dimensional Parallel Attention (PA) module is proposed to effectively extract features of spatial, temporal and channel dimensions and reduce the cross-dimensional interference, to achieve better robustness. Secondly, a Common Feature-driven (CFD) feature integration module is proposed to dynamically integrate appearance and depth features with adaptive weights, utilizing modality differences to redeem the lack of each feature, thereby balance the influence of both. The proposed PA-AW CNN uses the representative integrated feature generated by attention enhancement and feature integration for action recognition; it can not only get higher recognition accuracy but also improve the performance of distinguishing similar actions. Experiments illustrate that the proposed method achieves com-parable performances to state-of-the-art methods and obtains the accuracy of 92.76% and 95.65% on NTU RGB+D Dataset and SBU Kinect Interaction Dataset, respectively. The code is publicly available at: https://github.com/Luu-Yao/PA-AWCNN.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811995","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811995","","Adaptive systems;Codes;Automation;Convolution;Interference;Feature extraction;Robustness","","4","","37","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Distributed Swarm Trajectory Optimization for Formation Flight in Dense Environments","L. Quan; L. Yin; C. Xu; F. Gao","State Key Laboratory of Industrial Control Technology, Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; State Key Laboratory of Industrial Control Technology, Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; State Key Laboratory of Industrial Control Technology, Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; State Key Laboratory of Industrial Control Technology, Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","4979","4985","For aerial swarms, navigation in a prescribed formation is widely practiced in various scenarios. However, the associated planning strategies typically lack the capability of avoiding obstacles in cluttered environments. To address this deficiency, we present an optimization-based method that ensures collision-free trajectory generation for formation flight. In this paper, a novel differentiable metric is proposed to quantify the overall similarity distance between formations. We then formulate this metric into an optimization framework, which achieves spatial-temporal planning using polynomial trajectories. Minimization over collision penalty is also incorporated into the framework, so that formation preservation and obstacle avoidance can be handled simultaneously. To validate the efficiency of our method, we conduct benchmark comparisons with other cutting-edge works. Integrated with an autonomous distributed aerial swarm system, the proposed method demonstrates its efficiency and robustness in real-world experiments with obstacle-rich surroundings11https://www.youtube.com/watch?v=lFumtOrJci4. We will release the source code for the reference of the community22https://github.com/ZJU-FAST-Lab/Swarm-Formation.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812050","National Natural Science Foundation of China(grant numbers:62003299); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812050","","Measurement;Codes;Automation;Navigation;Benchmark testing;Minimization;Robustness","","16","","30","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Semantically Grounded Object Matching for Robust Robotic Scene Rearrangement","W. Goodwin; S. Vaze; I. Havoutis; I. Posner","Oxford Robotics Institute, University of Oxford, Oxford; Visual Geometry Group, University of Oxford, Oxford; Oxford Robotics Institute, University of Oxford, Oxford; Oxford Robotics Institute, University of Oxford, Oxford",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","11138","11144","Object rearrangement has recently emerged as a key competency in robot manipulation, with practical solutions generally involving object detection, recognition, grasping and high-level planning. Goal-images describing a desired scene configuration are a promising and increasingly used mode of instruction. A key outstanding challenge is the accurate inference of matches between objects in front of a robot, and those seen in a provided goal image, where recent works have struggled in the absence of object-specific training data. In this work, we explore the deterioration of existing methods' ability to infer matches between objects as the visual shift between observed and goal scenes increases. We find that a fundamental limitation of the current setting is that source and target images must contain the same instance of every object, which restricts practical deployment. We present a novel approach to object matching that uses a large pre-trained vision-language model to match objects in a cross-instance setting by leveraging semantics together with visual features as a more robust, and much more general, measure of similarity. We demonstrate that this provides considerably improved matching performance in cross-instance settings, and can be used to guide multi-object rearrangement with a robot manipulator from an image that shares no object instances with the robot's scene. Our code is available at https://github.com/applied-ai-lab/object_matching.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811817","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811817","","Visualization;Grounding;Semantics;Training data;Object detection;Grasping;Manipulators","","17","","33","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Context is Everything: Implicit Identification for Dynamics Adaptation","B. Evans; A. Thankaraj; L. Pinto",New York University; New York University; New York University,2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","2642","2648","Understanding environment dynamics is necessary for robots to act safely and optimally in the world. In realistic scenarios, dynamics are non-stationary and the causal variables such as environment parameters cannot necessarily be precisely measured or inferred, even during training. We propose Implicit Identification for Dynamics Adaptation (IIDA), a simple method to allow predictive models to adapt to changing environment dynamics. IIDA assumes no access to the true variations in the world and instead implicitly infers properties of the environment from a small amount of contextual data. We demonstrate IIDA's ability to perform well in unseen environments through a suite of simulated experiments on MuJoCo environments and a real robot dynamic sliding task. In general, IIDA significantly reduces model error and results in higher task performance over commonly used methods. Our code, video of the method, and latest paper is available here https://bennevans.github.io/icra-iida/","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812119","Honda; Amazon; ONR(grant numbers:N00014-21-1-2404,N00014-21-1-2758); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812119","","Training;Adaptation models;Codes;Automation;Predictive models;Task analysis;Robots","","4","","40","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"HandoverSim: A Simulation Framework and Benchmark for Human-to-Robot Object Handovers","Y. -W. Chao; C. Paxton; Y. Xiang; W. Yang; B. Sundaralingam; T. Chen; A. Murali; M. Cakmak; D. Fox",NVIDIA; NVIDIA; UT Dallas.; NVIDIA; NVIDIA; MIT CSAIL.; NVIDIA; NVIDIA; NVIDIA,2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","6941","6947","We introduce a new simulation benchmark “Han-doverSim” for human-to-robot object handovers. To simulate the giver's motion, we leverage a recent motion capture dataset of hand grasping of objects. We create training and evaluation environments for the receiver with standardized protocols and metrics. We analyze the performance of a set of baselines and show a correlation with a real-world evaluation.11Code is open sourced at https://handover-sim.github.io.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812302","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812302","","Training;Measurement;Correlation;Protocols;Automation;Receivers;Grasping","","12","","48","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"LLOL: Low-Latency Odometry for Spinning Lidars","C. Qu; S. S. Shivakumar; W. Liu; C. J. Taylor","GRASP Laboratory, School of Engineering and Applied Sciences, University of Pennsylvania; GRASP Laboratory, School of Engineering and Applied Sciences, University of Pennsylvania; GRASP Laboratory, School of Engineering and Applied Sciences, University of Pennsylvania; GRASP Laboratory, School of Engineering and Applied Sciences, University of Pennsylvania",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","4149","4155","In this paper, we present a low-latency odometry system designed for spinning lidars. Many existing lidar odometry methods wait for an entire sweep from the lidar before processing the data. This introduces a large delay between the first laser firing and its pose estimate. To reduce this latency, we treat the spinning lidar as a streaming sensor and process packets as they arrive. This effectively distributes expensive operations across time, resulting in a very fast and lightweight system with a much higher throughput and lower latency. Our open source implementation is available at https://github.com/versatran01/llol.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811605","DARPA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811605","","Laser radar;Automation;Firing;Lasers;Throughput;Robot sensing systems;Delays","","10","","36","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"See Yourself in Others: Attending Multiple Tasks for Own Failure Detection","B. Sun; J. Xing; H. Blum; R. Siegwart; C. Cadena","Autonomous Systems Lab, ETH Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Switzerland",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","8409","8416","Autonomous robots deal with unexpected scenarios in real environments. Given input images, various visual perception tasks can be performed, e.g., semantic segmentation, depth estimation and normal estimation. These different tasks provide rich information for the whole robotic perception system. All tasks have their own characteristics while sharing some latent correlations. However, some of the task predictions may suffer from the unreliability dealing with complex scenes and anomalies. We propose an attention-based failure detection approach by exploiting the correlations among multiple tasks. The proposed framework infers task failures by evaluating the individual prediction, across multiple visual perception tasks for different regions in an image. The formulation of the evaluations is based on an attention network supervised by multi-task uncertainty estimation and their corresponding prediction errors. Our proposed framework11Code link https://github.com/ethz-asl/uncertainty_with_multiple_tasks. generates more accurate estimations of the prediction error for the different task's predictions.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812310","Swiss National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812310","","Training;Visualization;Uncertainty;Correlation;Semantics;Estimation;Multitasking","","7","","45","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Predicting Like A Pilot: Dataset and Method to Predict Socially-Aware Aircraft Trajectories in Non-Towered Terminal Airspace","J. Patrikar; B. Moon; J. Oh; S. Scherer","Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","2525","2531","Pilots operating aircraft in non-towered terminal airspace rely on their situational awareness and prior knowledge to predict the future trajectories of other agents. These predictions are conditioned on the past trajectories of other agents, agent-agent social interactions and environmental context such as airport location and weather. This paper provides a dataset, TrajAir, that captures this behaviour in non-towered terminal airspace around a regional airport. We also present a baseline socially-aware trajectory prediction algorithm, TrajAirNet, that uses the dataset to predict the trajectories of all agents. The dataset is collected for 111 days over 8 months and contains ADS-B transponder data along with the corresponding METAR weather data. The data is processed to be used as a benchmark with other publicly available social navigation datasets. To the best of the authors' knowledge, this is the first 3D social aerial navigation dataset, thus introducing social navigation for autonomous aviation. TrajAirNet combines state-of-the-art modules in social navigation to provide predictions in a static environment with a dynamic context. Both the TrajAir dataset and TrajAirNet prediction algorithm are open-source. [Dataset] 11Dataset: https://theairlab.org/trajair/ [Code]22Codebase: https://github.com/castacks/trajairnet [Video]33Video: https://youtu.be/e1AQXrxB2gw","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811972","U.S. Department of Energy(grant numbers:DE-EE0008463); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811972","","Automation;Three-dimensional displays;Heuristic algorithms;Prediction algorithms;Airports;Aircraft navigation;Trajectory","","8","","37","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"HGC-Net: Deep Anthropomorphic Hand Grasping in Clutter","Y. Li; W. Wei; D. Li; P. Wang; W. Li; J. Zhong","Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","714","720","Grasping in cluttered environments is one of the most fundamental skills in robotic manipulation. Most of the current works focus on estimating grasp poses for parallel-jaw or suction-cup end effectors. However, the study for dexterous anthropomorphic hand grasping in clutter remains a great challenge. In this paper, we propose HGC-Net, a single-shot network that learns to predict dense hand grasp configurations in clutter from single-view point cloud input. Our end-to-end neural network can predict hand grasp proposals efficiently and effectively. To enhance generalization, we built a large-scale synthetic grasping dataset with 179 household objects, 5K cluttered scenes and over 10M hand annotations. Experiments in simulation show that our model can predict dense and robust hand grasps and clear over 78% of unseen objects in clutter without any post-processing and outperform baseline methods by a large margin. Experiments on the real robot platform also demonstrate that the model trained on synthetic data performs well in natural environments. Code is available at https://github.com/yimingli1998/hgc_net.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811756","National Natural Science Foundation of China(grant numbers:91748131,62006229,61771471); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811756","","Point cloud compression;Annotations;Neural networks;Grasping;Predictive models;End effectors;Data models","","9","","36","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"A Divide-and-Merge Point Cloud Clustering Algorithm for LiDAR Panoptic Segmentation","Y. Zhao; X. Zhang; X. Huang","Department of Electrical and Computer Engineering, Worcester Polytechnic Institute, Massachusetts, USA; Department of Electrical and Computer Engineering, Worcester Polytechnic Institute, Massachusetts, USA; Department of Electrical and Computer Engineering, Worcester Polytechnic Institute, Massachusetts, USA",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","7029","7035","Clustering objects from the LiDAR point cloud is an important research problem with many applications such as autonomous driving. To meet the real-time requirement, existing research proposed to apply the connected-component-labeling (CCL) technique on LiDAR spherical range image with a heuristic condition to check if two neighbor points are connected. However, LiDAR range image is different from a binary image which has a deterministic condition to tell if two pixels belong to the same component. The heuristic condition used on the LiDAR range image only works empirically, which suggests the LiDAR clustering algorithm should be robust to potential failures of the empirical heuristic condition. To overcome this challenge, this paper proposes a divide-and-merge LiDAR clustering algorithm. This algorithm firstly conducts clustering in each evenly divided local region, then merges the local clustered small components by voting on edge point pairs. Assuming there are $N$ LiDAR points of objects in total with $m$ divided local regions, the time complexity of the proposed algorithm is $O(N)+O(m^{2})$. A smaller $m$ means the voting will involve more neighbor points, but the time complexity will become larger. So the $m$ controls the trade-off between the time complexity and the clustering accuracy. A proper $m$ helps the proposed algorithm work in real-time as well as maintain good performance. We evaluate the divide-and-merge clustering algorithm on the SemanticKITTI panoptic segmentation benchmark by cascading it with a state-of-the-art semantic segmentation model. The final performance evaluated through the leaderboard achieves the best among all published methods. The proposed algorithm is implemented with C++ and wrapped as a python function. It can be easily used with the modern deep learning framework in python. We released the code under the following link 11https://github.com/placeforyiming/Divide-and-Merge-LiDAR-Panoptic-Cluster.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812058","","Point cloud compression;Image segmentation;Laser radar;Codes;Semantics;Clustering algorithms;Solids","","10","","32","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"TartanDrive: A Large-Scale Dataset for Learning Off-Road Dynamics Models","S. Triest; M. Sivaprakasam; S. J. Wang; W. Wang; A. M. Johnson; S. Scherer","Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Electrical & Computer Engineering Dept., University of Pittsburgh, Pittsburgh, PA, USA; Mechanical Engineering Dept., Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Mechanical Engineering Dept., Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","2546","2552","We present TartanDrive, a large scale dataset for learning dynamics models for off-road driving. We collected a dataset of roughly 200,000 off-road driving interactions on a modified Yamaha Viking ATV with seven unique sensing modalities in diverse terrains. To the authors' knowledge, this is the largest real-world multi-modal off-road driving dataset, both in terms of number of interactions and sensing modalities. We also benchmark several state-of-the-art methods for model-based reinforcement learning from high-dimensional observations on this dataset. We find that extending these models to multi-modality leads to significant performance on off-road dynamics prediction, especially in more challenging terrains. We also identify some shortcomings with current neural network architectures for the off-road driving task. Our dataset is available at https://github.com/castacks/tartan_drive.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811648","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811648","","Automation;Neural networks;Reinforcement learning;Predictive models;Benchmark testing;Robot sensing systems;Sensors","","21","","40","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"VIRDO: Visio-tactile Implicit Representations of Deformable Objects","Y. Wi; P. Florence; A. Zeng; N. Fazeli","Robotics Department, University of Michigan, MI, USA; Robotics, Google, Mountain View, CA, USA; Robotics, Google, Mountain View, CA, USA; Robotics Department, University of Michigan, MI, USA",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","3583","3590","Deformable object manipulation requires computationally efficient representations that are compatible with robotic sensing modalities. In this paper, we present VIRDO: an implicit, multi-modal, and continuous representation for deformable-elastic objects. VIRDO operates directly on visual (point cloud) and tactile (reaction forces) modalities and learns rich latent embeddings of contact locations and forces to predict object deformations subject to external contacts. Here, we demonstrate VIRDOs ability to: i) produce high-fidelity cross-modal reconstructions with dense unsupervised correspondences, ii) generalize to unseen contact formations, and iii) state-estimation with partial visio-tactile feedback. https://github.com/MMintLab/VIRDO","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812097","Google; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812097","","Point cloud compression;Visualization;Automation;Robot sensing systems;Sensors;Computational efficiency;Strain","","16","","41","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Monocular Depth Distribution Alignment with Low Computation","F. Sheng; F. Xue; Y. Chang; W. Liang; A. Ming","Beijing University of Posts and Telecommunications, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","6548","6555","The performance of monocular depth estimation generally depends on the amount of parameters and computational cost. It leads to a large accuracy contrast between light-weight networks and heavy-weight networks, which limits their application in the real world. In this paper, we model the majority of accuracy contrast between them as the difference of depth distribution, which we call 'Distribution drift'. To this end, a distribution alignment network (DANet) is proposed. We firstly design a pyramid scene transformer (PST) module to capture inter-region interaction in multiple scales. By perceiving the difference of depth features between every two regions, DANet tends to predict a reasonable scene structure, which fits the shape of distribution to ground truth. Then, we propose a local-global optimization (LGO) scheme to realize the supervision of global range of scene depth. Thanks to the alignment of depth distribution shape and scene depth range, DANet sharply alleviates the distribution drift, and achieves a comparable performance with prior heavy-weight methods, but uses only 1% floating-point operations per second (FLOPs) of them. The experiments on two datasets, namely the widely used NYUDv2 dataset and the more challenging iBims-1 dataset, demonstrate the effectiveness of our method. The source code is available at https://github.com/YiLiM1/DANet.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811937","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811937","","Shape;Computational modeling;Estimation;Transformers;Real-time systems;Computational efficiency;Reliability","","5","","47","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"On the Feasibility of Learning Finger-gaiting In-hand Manipulation with Intrinsic Sensing","G. Khandate; M. Haas-Heger; M. Ciocarlie","Department of Computer Science, Columbia University, New York, NY, USA; Department of Mechanical Engineering, Columbia University, New York, NY, USA; Department of Mechanical Engineering, Columbia University, New York, NY, USA",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","2752","2758","Finger-gaiting manipulation is an important skill to achieve large-angle in-hand re-orientation of objects. However, achieving these gaits with arbitrary orientations of the hand is challenging due to the unstable nature of the task. In this work, we use model-free reinforcement learning (RL) to learn finger-gaiting only via precision grasps and demonstrate finger-gaiting for rotation about an axis using only on-board proprioceptive and tactile feedback. To tackle the inherent instability of precision grasping, we propose the use of initial state distributions that enable effective exploration of the state space. Our method can learn finger gaiting with better sample complexity than the state-of-the-art. The policies we obtain are robust to noise and perturbations, and transfer to novel objects. Videos can be found at https://roamlab.github.io/learnfg/","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812212","NSF(grant numbers:CMMI-1734557); ONR(grant numbers:N00014-19-1-2062); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812212","","Training;Perturbation methods;Force;Tactile sensors;Grasping;Reinforcement learning;Sensors","","14","","27","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Put the Bear on the Chair! Intelligent Robot Interaction with Previously Unseen Chairs via Robot Imagination","H. Wu; X. Meng; S. Ruan; G. S. Chirikjian","Department of Mechanical Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","6276","6282","In this paper, we study the problem of autonomously seating a teddy bear on a previously unseen chair. To achieve this goal, we present a novel method for robots to imagine the sitting pose of the bear by physically simulating a virtual humanoid agent sitting on the chair. We also develop a robotic system which leverages motion planning to plan SE(2) motions for a humanoid robot to walk to the chair and whole-body motions to put the bear on it. Furthermore, to cope with cases where the chair is not in an accessible pose for placing the bear, a human assistance module is introduced for a human to follow language instructions given by the robot to rotate the chair and help make the chair accessible. We implement our method with a robot arm and a humanoid robot. We calibrate the proposed system with 3 chairs and test on 12 previously unseen chairs in both accessible and inaccessible poses extensively. Results show that our method enables the robot to autonomously seat the teddy bear on the 12 previously unseen chairs with a very high success rate. The human assistance module is also shown to be very effective in changing the accessibility of the chair. Video demos and more details are available at https://chirikjianlab.github.io/putbearonchair/.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811619","NUS(grant numbers:R-265-000-665-133,R-265-000-665-731,C-265-000-071-001); National Research Foundation, Singapore(grant numbers:R-261-521-002-592); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811619","","Legged locomotion;Automation;Humanoid robots;Manipulators;Planning;Robots;Intelligent robots","","1","","39","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"When Being Soft Makes You Tough: A Collision-Resilient Quadcopter Inspired by Arthropods' Exoskeletons","R. de Azambuja; H. Fouad; Y. Bouteiller; C. Sol; G. Beltrame","MISTLab, Ecole Polytechnique de Montréal, Montréal, Canada; MISTLab, Ecole Polytechnique de Montréal, Montréal, Canada; MISTLab, Ecole Polytechnique de Montréal, Montréal, Canada; MISTLab, Ecole Polytechnique de Montréal, Montréal, Canada; MISTLab, Ecole Polytechnique de Montréal, Montréal, Canada",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","7854","7860","Flying robots are usually rather delicate and require protective enclosures when facing the risk of collision, while high complexity and reduced payload are recurrent problems with collision-resilient flying robots. Inspired by arthropods' exoskeletons, we design a simple, open source, easily manufactured, semi-rigid structure with soft joints that can withstand high-velocity impacts. With an exoskeleton, the protective shell becomes part of the main robot structure, thereby minimizing its loss in payload capacity. Our design is simple to build and customize using cheap components (e.g. bamboo skewers) and consumer-grade 3D printers. The result is CogniFly, a sub-250 g autonomous quadcopter that survives multiple collisions at speeds up to 7 m s−1. In addition to its collision-resilience, CogniFly carries sensors that allow it to fly for approx. 17 min without the need of GPS or an external motion capture system, and it has enough computing power to run deep neural network models on-board. This structure becomes an ideal platform for high-risk activities, such as flying in a cluttered environment or reinforcement learning training, by dramatically reducing the risks of damaging its own hardware or the environment. Source code, 3D files, instructions and videos are available (open source license) through the project's website: https://thecognifly.github.io.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811841","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811841","","Training;Three-dimensional displays;Exoskeletons;Reinforcement learning;Sensor systems;Sensors;Collision avoidance","","8","","27","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"ORFD: A Dataset and Benchmark for Off-Road Freespace Detection","C. Min; W. Jiang; D. Zhao; J. Xu; L. Xiao; Y. Nie; B. Dai","School of Computer Science, Peking University, Beijing, China; Unmanned Systems Research Center, National Innovation Institute of Defense Technology, Beijing, China; Unmanned Systems Research Center, National Innovation Institute of Defense Technology, Beijing, China; Unmanned Systems Research Center, National Innovation Institute of Defense Technology, Beijing, China; Unmanned Systems Research Center, National Innovation Institute of Defense Technology, Beijing, China; Unmanned Systems Research Center, National Innovation Institute of Defense Technology, Beijing, China; Unmanned Systems Research Center, National Innovation Institute of Defense Technology, Beijing, China",2022 International Conference on Robotics and Automation (ICRA),"12 Jul 2022","2022","","","2532","2538","Freespace detection is an essential component of autonomous driving technology and plays an important role in trajectory planning. In the last decade, deep learning based freespace detection methods have been proved feasible. However, these efforts were focused on urban road environments and few deep learning based methods were specifically designed for off-road freespace detection due to the lack of off-road dataset and benchmark. In this paper, we present the ORFD dataset, which, to our knowledge, is the first off-road freespace detection dataset. The dataset was collected in different scenes (woodland, farmland, grassland and countryside), different weather conditions (sunny, rainy, foggy and snowy) and different light conditions (bright light, daylight, twilight, darkness), which totally contains 12,198 LiDAR point cloud and RGB image pairs with the traversable area, non-traversable area and unreachable area annotated in detail. We propose a novel network named OFF-Net, which unifies Transformer architecture to aggregate local and global information, to meet the requirement of large receptive fields for freespace detection task. We also propose the cross-attention to dynamically fuse LiDAR and RGB image information for accurate off-road freespace detection. Dataset and code are publicly available at https://github.com/chaytonmin/OFF-Net.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812139","National Natural Science Foundation of China(grant numbers:61803380,61790565); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812139","","Deep learning;Point cloud compression;Laser radar;Trajectory planning;Aggregates;Roads;Robot vision systems","","35","","24","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Crop Type Classification using Multi-temporal Sentinel-2 Satellite Imagery: A Deep Semantic Segmentation Approach","A. H. Khan; Z. Zafar; M. Shahzad; K. Berns; M. M. Fraz","National University of Sciences and Technology (NUST), Islamabad, Pakistan; National University of Sciences and Technology (NUST), Islamabad, Pakistan; National University of Sciences and Technology (NUST), Islamabad, Pakistan; Robotics Research Lab, Technical University of Kaiserslautern, Germany; National University of Sciences and Technology (NUST), Islamabad, Pakistan",2023 International Conference on Robotics and Automation in Industry (ICRAI),"6 Apr 2023","2023","","","1","6","Crop Type classification using Semantic Segmentation and remote sensing data is an important tool for decision-making related to precision agriculture. Such classification remains an unsolved challenge due to the choice of landscape, processing methodology and selected satellite imagery and its optical features, and most importantly the availability and usage of such datasets in a developing country like Pakistan. State-of-the-art semantic segmentation models lack in processing the temporal dimension of time series imagery and evident solution to process multi-spectral bands available in the satellite imagery. We propose a methodology to overcome these shortcomings by selecting appropriate band combinations for crop type classification and treating time series visual data as a single image. The proposed methodology is evaluated on the data set of six different crops collected from National Agriculture Research Center (NARC) Islamabad. The experimental results yield 85% accuracy for classifying various crop types based on the evaluation of five different semantic segmentation models. The code and the trained models are available at https://github.com/asimniazi63/crop-type-narc for other researchers working in the same domain.","2831-3313","978-1-6654-6472-7","10.1109/ICRAI57502.2023.10089586","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10089586","Remote sensing;Crop type classification;Sentinel-2;Satellite imagery;Time series data;Semantic segmentation","Visualization;Satellites;Semantic segmentation;Time series analysis;Crops;Optical imaging;Agriculture","","4","","33","IEEE","6 Apr 2023","","","IEEE","IEEE Conferences"
"Learning to Influence Vehicles' Routing in Mixed-Autonomy Networks by Dynamically Controlling the Headway of Autonomous Cars","X. Ma; N. Mehr","Department of Electrical and Computer Engineering, UIUC, Urbana, IL, USA; Department of Aerospace Engineering, UIUC, Urbana, IL, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","3510","3516","It is known that autonomous cars can increase road capacities by maintaining a smaller headway through vehicle platooning. Recent works have shown that these capacity increases can influence vehicles' route choices in unexpected ways similar to the well-known Braess's paradox, such that the network congestion might increase. In this paper, we propose that in mixed-autonomy networks, i.e., networks where roads are shared between human-driven and autonomous cars, the headway of autonomous cars can be directly controlled to influence vehicles' routing and reduce congestion. We argue that the headway of autonomous cars - and consequently the capacity of link segments - is not just a fixed design choice; but rather, it can be leveraged as an infrastructure control strategy to dynamically regulate capacities. Imagine that similar to variable speed limits which regulate the maximum speed of vehicles on a road segment, a control policy regulates the headway of autonomous cars along each road segment. We seek to influence vehicles' route choices by directly controlling the headway of autonomous cars to prevent Braess-like unexpected outcomes and increase network efficiency. We model the dynamics of mixed-autonomy traffic networks while accounting for the vehicles' route choice dynamics. We train an RL policy that learns to regulate the headway of autonomous cars such that the total travel time in the network is minimized. We will show empirically that our trained policy can not only prevent Braess-like inefficiencies but also decrease total travel time11The code is available at: https://github.com/labicon/RL-Traffic-Dynamics.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160717","National Science Foundation(grant numbers:ECCS-2145134); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160717","","Image segmentation;Codes;Automation;Roads;Routing;Automobiles;Vehicle dynamics","","","","25","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"SCARP: 3D Shape Completion in ARbitrary Poses for Improved Grasping","B. Sen; A. Agarwal; G. Singh; B. B.; S. Sridhar; M. Krishna","Robotics Research Center, IIIT, Hyderabad; Robotics Research Center, IIIT, Hyderabad; Robotics Research Center, IIIT, Hyderabad; TCS Research, India; Brown University; Robotics Research Center, IIIT, Hyderabad",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","3838","3845","Recovering full 3D shapes from partial observations is a challenging task that has been extensively addressed in the computer vision community. Many deep learning methods tackle this problem by training 3D shape generation networks to learn a prior over the full 3D shapes. In this training regime, the methods expect the inputs to be in a fixed canonical form, without which they fail to learn a valid prior over the 3D shapes. We propose SCARP, a model that performs Shape C ompletion in ARbitrary Poses. Given a partial pointcloud of an object, SCARP learns a disentangled feature representation of pose and shape by relying on rotationally equivariant pose features and geometric shape features trained using a multi-tasking objective. Unlike existing methods that depend on an external canonicalization method, SCARP performs canonicalization, pose estimation, and shape completion in a single network, improving the performance by 45% over the existing baselines. In this work, we use SCARP for improving grasp proposals on tabletop objects. By completing partial tabletop objects directly in their observed poses, SCARP enables a SOTA grasp proposal network improve their proposals by 71.2% on partial shapes. Project page: https://bipashasen.github.io/scarp","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160365","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160365","","Training;Three-dimensional displays;Shape;Trajectory planning;Pose estimation;Grasping;Multitasking","","3","","51","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"ORORA: Outlier-Robust Radar Odometry","H. Lim; K. Han; G. Shin; G. Kim; S. Hong; H. Myung","School of Electrical Engineering, KAIST (Korea Advanced Institute of Science and Technology), Daejeon, Republic of Korea; School of Electrical Engineering, KAIST (Korea Advanced Institute of Science and Technology), Daejeon, Republic of Korea; A research intern in Urban Robotics Lab., KAIST, Daejeon, Republic of Korea; NAVER LABS, Seongnam, Gyeonggi-do, Republic of Korea; School of Electrical Engineering, KAIST (Korea Advanced Institute of Science and Technology), Daejeon, Republic of Korea; School of Electrical Engineering, KAIST (Korea Advanced Institute of Science and Technology), Daejeon, Republic of Korea",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","2046","2053","Radar sensors are emerging as solutions for perceiving surroundings and estimating ego-motion in extreme weather conditions. Unfortunately, radar measurements are noisy and suffer from mutual interference, which degrades the performance of feature extraction and matching, triggering imprecise matching pairs, which are referred to as outliers. To tackle the effect of outliers on radar odometry, $a$ novel outlier-robust method called ORORA is proposed, which is an abbreviation of Outlier-RObust RAdar odometry. To this end, a novel decoupling-based method is proposed, which consists of graduated non-convexity (GNC)-based rotation estimation and anisotropic component-wise translation estimation (A-COTE). Furthermore, our method leverages the anisotropic characteristics of radar measurements, each of whose uncertainty along the azimuthal direction is somewhat larger than that along the radial direction. As verified in the public dataset, it was demonstrated that our proposed method yields robust ego-motion estimation performance compared with other state-of-the-art methods. Our code is available at https://github.com/url-kaist/outlier-robust-radar-odometry.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160997","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160997","","Meteorological radar;Uncertainty;Simultaneous localization and mapping;Radar measurements;Estimation;Interference;Sensors","","13","","50","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"ADAPT: Action-aware Driving Caption Transformer","B. Jin; X. Liu; Y. Zheng; P. Li; H. Zhao; T. Zhang; Y. Zheng; G. Zhou; J. Liu","Chinese Academy of Sciences, Institute of Automation, China; Institute for AI Industry Research (AIR), Tsinghua University, China; Chinese Academy of Sciences, Institute of Automation, China; Department of Computer Science and Technology, Tsinghua University, China; Institute for AI Industry Research (AIR), Tsinghua University, China; Southern University of Science and Technology, China; Institute for AI Industry Research (AIR), Tsinghua University, China; Institute for AI Industry Research (AIR), Tsinghua University, China; Institute for AI Industry Research (AIR), Tsinghua University, China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","7554","7561","End-to-end autonomous driving has great potential in the transportation industry. However, the lack of transparency and interpretability of the automatic decision-making process hinders its industrial adoption in practice. There have been some early attempts to use attention maps or cost volume for better model explainability which is difficult for ordinary passengers to understand. To bridge the gap, we propose an end-to-end transformer-based architecture, ADAPT (Action-aware Driving cAPtion Transformer), which provides user-friendly natural language narrations and reasoning for each decision making step of autonomous vehicular control and action. ADAPT jointly trains both the driving caption task and the vehicular control prediction task, through a shared video representation. Experiments on BDD-X (Berkeley DeepDrive eXplanation) dataset demonstrate state-of-the-art performance of the ADAPT framework on both automatic metrics and human evaluation. To illustrate the feasibility of the proposed framework in real-world applications, we build a novel deployable system that takes raw car videos as input and outputs the action narrations and reasoning in real time. The code, models and data are available at https://github.com/jxbbb/ADAPT.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160326","Apollo-AIR Joint Research Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160326","","Measurement;Training;Adaptation models;Transportation industry;Decision making;Streaming media;Transformers","","24","","82","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"TransVisDrone: Spatio-Temporal Transformer for Vision-based Drone-to-Drone Detection in Aerial Videos","T. Sangam; I. R. Dave; W. Sultani; M. Shah","Center for Research in Computer Vision lab (CRCV), University of Central Florida, USA; Center for Research in Computer Vision lab (CRCV), University of Central Florida, USA; Information Technology University of the Punjab, Lahore, Pakistan; Center for Research in Computer Vision lab (CRCV), University of Central Florida, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","6006","6013","Drone-to-drone detection using visual feed has crucial applications, such as detecting drone collisions, detecting drone attacks, or coordinating flight with other drones. However, existing methods are computationally costly, follow non-end-to-end optimization, and have complex multi-stage pipelines, making them less suitable for real-time deployment on edge devices. In this work, we propose a simple yet effective framework, TransVisDrone, that provides an end-to-end solution with higher computational efficiency. We utilize CSPDarkNet-53 network to learn object-related spatial features and VideoSwin model to improve drone detection in challenging scenarios by learning spatio-temporal dependencies of drone motion. Our method achieves state-of-the-art performance on three challenging real-world datasets (Average Precision@0.5IOU): NPS 0.95, FLDrones 0.75, and AOT 0.80, and a higher throughput than previous methods. We also demonstrate its deployment capability on edge devices and its usefulness in detecting drone-collision (encounter). Project: https://tusharsangam.github.io/TransVisDrone-project-page/","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161433","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161433","","Performance evaluation;Visualization;Image edge detection;Robot vision systems;Transformers;Throughput;Real-time systems","","13","","50","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"FRIDA: A Collaborative Robot Painter with a Differentiable, Real2Sim2Real Planning Environment","P. Schaldenbrand; J. McCann; J. Oh","The Robotics Institute, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","11712","11718","Painting is an artistic process of rendering visual content that achieves the high-level communication goals of an artist that may change dynamically throughout the creative process. In this paper, we present a Framework and Robotics Initiative for Developing Arts (FRIDA) that enables humans to produce paintings on canvases by collaborating with a painter robot using simple inputs such as language descriptions or images. FRIDA introduces several technical innovations for computationally modeling a creative painting process. First, we develop a fully differentiable simulation environment for painting, adopting the idea of real to simulation to real (real2sim2real). We show that our proposed simulated painting environment is higher fidelity to reality than existing simulation environments used for robot painting. Second, to model the evolving dynamics of a creative process, we develop a planning approach that can continuously optimize the painting plan based on the evolving canvas with respect to the high-level goals. In contrast to existing approaches where the content generation process and action planning are performed independently and sequentially, FRIDA adapts to the stochastic nature of using paint and a brush by continually re-planning and re-assessing its semantic goals based on its visual perception of the painting progress. We describe the details on the technical approach as well as the system integration. FRIDA software is freely available at: https://github.com/cmubig/Frida.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160702","NSF(grant numbers:IIS-2112633); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160702","","Visualization;Brushes;Art;Computational modeling;Stochastic processes;Planning;Robots","","11","","35","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Learning Low-Frequency Motion Control for Robust and Dynamic Robot Locomotion","S. Gangapurwala; L. Campanaro; I. Havoutis","Dynamic Robots Systems (DRS) group, Oxford Robotics Institute, University of Oxford, UK; Dynamic Robots Systems (DRS) group, Oxford Robotics Institute, University of Oxford, UK; Dynamic Robots Systems (DRS) group, Oxford Robotics Institute, University of Oxford, UK",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","5085","5091","Robotic locomotion is often approached with the goal of maximizing robustness and reactivity by increasing motion control frequency. We challenge this intuitive notion by demonstrating robust and dynamic locomotion with a learned motion controller executing at as low as 8 Hz on a real ANYmal C quadruped. The robot is able to robustly and repeatably achieve a high heading velocity of 1.5 ms-1, traverse uneven terrain, and resist unexpected external perturbations. We further present a comparative analysis of deep reinforcement learning (RL) based motion control policies trained and executed at frequencies ranging from 5 Hz to 200 Hz. We show that low-frequency policies are less sensitive to actuation latencies and variations in system dynamics. This is to the extent that a successful sim- to-real transfer can be performed even without any dynamics randomization or actuation modeling. We support this claim through a set of rigorous empirical evaluations. Moreover, to assist reproducibility, we provide the training and deployment code along with an extended analysis at https://ori-drs.github.io/lfmc/.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160357","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160357","","Motion planning;Training;System dynamics;Dynamics;Resists;Reinforcement learning;Robustness","","4","","42","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"GraspNeRF: Multiview-based 6-DoF Grasp Detection for Transparent and Specular Objects Using Generalizable NeRF","Q. Dai; Y. Zhu; Y. Geng; C. Ruan; J. Zhang; H. Wang","Center on Frontiers of Computing Studies, Peking University; Center on Frontiers of Computing Studies, Peking University; Center on Frontiers of Computing Studies, Peking University; College of Intelligence Science and Technology, National University of Defense Technology; Center on Frontiers of Computing Studies, Peking University; Center on Frontiers of Computing Studies, Peking University",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","1757","1763","In this work, we tackle 6-DoF grasp detection for transparent and specular objects, which is an important yet challenging problem in vision-based robotic systems, due to the failure of depth cameras in sensing their geometry. We, for the first time, propose a multiview RGB-based 6-DoF grasp detection network, GraspNeRF, that leverages the generalizable neural radiance field (NeRF) to achieve material-agnostic object grasping in clutter. Compared to the existing NeRF-based 3-DoF grasp detection methods that rely on densely captured input images and time-consuming per-scene optimization, our system can perform zero-shot NeRF construction with sparse RGB inputs and reliably detect 6-DoF grasps, both in real-time. The proposed framework jointly learns generalizable NeRF and grasp detection in an end-to-end manner, optimizing the scene representation construction for the grasping. For training data, we generate a large-scale photorealistic domain-randomized synthetic dataset of grasping in cluttered tabletop scenes that enables direct transfer to the real world. Our extensive experiments in synthetic and real-world environments demonstrate that our method significantly outperforms all the baselines in all the experiments while remaining in real-time. Project page can be found at https://pku-epic.github.io/GraspNeRF.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160842","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160842","","Geometry;Robot vision systems;Training data;Grasping;6-DOF;Real-time systems;Sensors","","33","","25","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Calibration and Uncertainty Characterization for Ultra-Wideband Two-Way-Ranging Measurements","M. A. Shalaby; C. C. Cossette; J. R. Forbes; J. Le Ny","department of Mechanical Engineering, McGill University, Montreal, QC, Canada; department of Mechanical Engineering, McGill University, Montreal, QC, Canada; department of Mechanical Engineering, McGill University, Montreal, QC, Canada; department of Electrical Engineering, Polytechnique Montreal, Montreal, QC, Canada",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","4128","4134","Ultra-Wideband (UWB) systems are becoming increasingly popular for indoor localization, where range measurements are obtained by measuring the time-of-flight of radio signals. However, the range measurements typically suffer from a systematic error or bias that must be corrected for high-accuracy localization. In this paper, a ranging protocol is proposed alongside a robust and scalable antenna-delay calibration procedure to accurately and efficiently calibrate antenna delays for many UWB tags. Additionally, the bias and uncertainty of the measurements are modelled as a function of the received-signal power. The full calibration procedure is presented using experimental training data of 3 aerial robots fitted with 2 UWB tags each, and then evaluated on 2 test experiments. A localization problem is then formulated on the experimental test data, and the calibrated measurements and their modelled uncertainty are fed into an extended Kalman filter (EKF). The proposed calibration is shown to yield an average of 46% improvement in localization accuracy. Lastly, the paper is accompanied by an open-source UWB-calibration Python library, which can be found at https://github.com/decargroup/uwb_calibration.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160769","NSERC; CFI JELF program; FRQNT; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160769","","Location awareness;Antenna measurements;Uncertainty;Protocols;Power measurement;Measurement uncertainty;Ultra wideband antennas","","7","","21","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Toward Zero-Shot Sim-to-Real Transfer Learning for Pneumatic Soft Robot 3D Proprioceptive Sensing","U. Yoo; H. Zhao; A. Altamirano; W. Yuan; C. Feng","Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; New York University, Brooklyn, NY, USA; New York University, Brooklyn, NY, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; New York University, Brooklyn, NY, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","544","551","Pneumatic soft robots present many advantages in manipulation tasks. Notably, their inherent compliance makes them safe and reliable in unstructured and fragile environments. However, full-body shape sensing for pneumatic soft robots is challenging because of their high degrees of freedom and complex deformation behaviors. Vision-based proprioception sensing methods relying on embedded cameras and deep learning provide a good solution to proprioception sensing by extracting the full-body shape information from the high-dimensional sensing data. But the current training data collection process makes it difficult for many applications. To address this challenge, we propose and demonstrate a robust sim-to-real pipeline that allows the collection of the soft robot's shape information in high-fidelity point cloud representation. The model trained on simulated data was evaluated with real internal camera images. The results show that the model performed with averaged Chamfer distance of 8.85 mm and tip position error of 10.12 mm even with external perturbation for a pneumatic soft robot with a length of 100.0 mm. We also demonstrated the sim-to-real pipeline's potential for exploring different configurations of visual patterns to improve vision-based reconstruction results. The code and dataset are available at https://github.com/DeepSoRo/DeepSoRoSim2Real.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160384","NSF(grant numbers:2024882); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160384","","Visualization;Three-dimensional displays;Shape;Pipelines;Robot vision systems;Soft robotics;Robot sensing systems","","7","","30","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Road Anomaly Segmentation Based on Pixel-wise Logit Variance with Iterative Background Highlighting","D. Lee; H. -G. Kim; H. -J. Choi","School of Computing, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea; NAVER Cloud, Gyeonggi-do, Republic of Korea; School of Computing, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","9274","9280","Anomaly segmentation on the urban landscape scene is an important task in autonomous driving. This process exploits a pre-trained semantic segmentation network to estimate anomalous regions. Anomaly segmentation approaches implemented with extra requirements such as out-of-domain data, extra network, or network retraining might increase the computational cost or degradation of segmentation performance. In this study, to exploit information from the segmentation network for more robust anomaly segmentation, we propose the use of pixel-wise logit variance, which tends to be small for anomalies as network outputs even logits without confidence. Additionally iterative background highlighting is proposed to robustly detect anomalous objects on the background, which is implemented by feeding the logits back into the linear classifier of the network. We achieved state-of-the-art performance among anomaly segmentation approaches without extra requirements, reaching relative average precision improvements of 21.7% on the Fishyscapes Lost&Found and 17.4% on the Fishyscapes Static compared to the state-of-the-art method. The code of this work is available at our Github repository (https://github.com/hagg30/LogitVar).","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161159","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161159","","Training;Degradation;Uncertainty;Codes;Roads;Semantic segmentation;Computational efficiency","","","","41","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Perturbation-Based Best Arm Identification for Efficient Task Planning with Monte-Carlo Tree Search","D. Jin; J. Park; K. Lee","Department of Artificial Intelligence, Chung-Ang University, Seoul, Republic of Korea; Department of Artificial Intelligence, Chung-Ang University, Seoul, Republic of Korea; Department of Artificial Intelligence, Chung-Ang University, Seoul, Republic of Korea",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","5758","5764","Combining task and motion planning (TAMP) is crucial for intelligent robots to perform complex and long-horizon tasks. In TAMP, many approaches generally employ Monte-Carlo tree search (MCTS) with upper confidence bound (UCB) for task planning to handle exploration-exploitation trade-off and find globally optimal solutions. However, since UCB basically considers the estimation error caused by noise, the error caused by insufficient optimization of the sub-tree is not represented. Hence, UCB-based approaches have the disadvantage of not exploring underestimated sub-trees. To alleviate this issue, we propose a novel tree search method using perturbation-based best-arm identification (PBAI). We theoretically prove the bound of the simple regret of our method and empirically verify that PBAI finds the optimal task plans faster and more efficiently than the existing algorithms. The source code of our proposed algorithm is available at https://github.com/jdj2261/pytamp.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161169","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161169","","Estimation error;Monte Carlo methods;Automation;Source coding;Search methods;Perturbation methods;Planning","","","","32","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"DDS3D: Dense Pseudo-Labels with Dynamic Threshold for Semi-Supervised 3D Object Detection","J. Li; Z. Liu; J. Hou; D. Liang","School of Electronic Information and Communication, Huazhong University of Science and Technology; School of Electronic Information and Communication, Huazhong University of Science and Technology; School of Electronic Information and Communication, Huazhong University of Science and Technology; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","9245","9252","In this paper, we present a simple yet effective semi-supervised 3D object detector named DDS3D. Our main contributions have two-fold. On the one hand, different from previous works using Non-Maximal Suppression (NMS) or its variants for obtaining the sparse pseudo labels, we propose a dense pseudo-label generation strategy to get dense pseudo-labels, which can retain more potential supervision information for the student network. On the other hand, instead of traditional fixed thresholds, we propose a dynamic threshold manner to generate pseudo-labels, which can guarantee the quality and quantity of pseudo-labels during the whole training process. Benefiting from these two components, our DDS3D outperforms the state-of-the-art semi-supervised 3d object detection with mAP of 3.1% on the pedestrian and 2.1% on the cyclist under the same configuration of 1% samples. Extensive ablation studies on the KITTI dataset demonstrate the effectiveness of our DDS3D. The code and models will be made publicly available at https://github.com/hust-jy/DDS3D","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160489","Young Scientists Fund; National Natural Science Foundation of China(grant numbers:62206103); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160489","","Training;Solid modeling;Three-dimensional displays;Pedestrians;Codes;Automation;Filtering","","8","","52","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"L2E: Lasers to Events for 6-DoF Extrinsic Calibration of Lidars and Event Cameras","K. Ta; D. Bruggemann; T. Brödermann; C. Sakaridis; L. Van Gool","Computer Vision Lab, ETH Zurich, Zurich, Switzerland; Computer Vision Lab, ETH Zurich, Zurich, Switzerland; Computer Vision Lab, ETH Zurich, Zurich, Switzerland; Computer Vision Lab, ETH Zurich, Zurich, Switzerland; Computer Vision Lab, ETH Zurich, Zurich, Switzerland",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","11425","11431","As neuromorphic technology is maturing, its application to robotics and autonomous vehicle systems has become an area of active research. In particular, event cameras have emerged as a compelling alternative to frame-based cameras in low-power and latency-demanding applications. To enable event cameras to operate alongside staple sensors like lidar in perception tasks, we propose a direct, temporally-decoupled extrinsic calibration method between event cameras and lidars. The high dynamic range, high temporal resolution, and low-latency operation of event cameras are exploited to directly register lidar laser returns, allowing information-based correlation methods to optimize for the 6- DoF extrinsic calibration between the two sensors. This paper presents the first direct calibration method between event cameras and lidars, removing dependencies on frame-based camera intermediaries and/or highly-accurate hand measurements. Code: https://github.com/kev-in-ta/12e","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161220","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161220","","Laser radar;Correlation;Robot vision systems;Measurement by laser beam;Cameras;Calibration;Sensors","","8","","37","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Segregator: Global Point Cloud Registration with Semantic and Geometric Cues","P. Yin; S. Yuan; H. Cao; X. Ji; S. Zhang; L. Xie","Centre for Advanced Robotics Technology Innovation (CARTIN), School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Centre for Advanced Robotics Technology Innovation (CARTIN), School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Centre for Advanced Robotics Technology Innovation (CARTIN), School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Centre for Advanced Robotics Technology Innovation (CARTIN), School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Kowloon, Hong Kong SAR, China; Centre for Advanced Robotics Technology Innovation (CARTIN), School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","2848","2854","This paper presents Segregator, a global point cloud registration framework that exploits both semantic information and geometric distribution to efficiently build up outlier-robust correspondences and search for inliers. Current state-of-the-art algorithms rely on point features to set up putative correspondences and refine them by employing pair-wise distance consistency checks. However, such a scheme suffers from degenerate cases, where the descriptive capability of local point features downgrades, and unconstrained cases, where length-preserving (1-TRIMs)-based checks cannot sufficiently constrain whether the current observation is consistent with others, resulting in a complexified NP-complete problem to solve. To tackle these problems, on the one hand, we propose a novel degeneracy-robust and efficient corresponding procedure consisting of both instance-level semantic clusters and geometric-level point features. On the other hand, Gaussian distribution-based translation and rotation invariant measurements (G-TRIMs) are proposed to conduct the consistency check and further constrain the problem size. We validated our proposed algorithm on extensive real-world data-based experiments. The code is available: https://github.com/Pamphlett/Segregator.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160798","National Research Foundation, Singapore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160798","","Point cloud compression;Codes;Automation;Semantics;Pose estimation;Clustering algorithms;Size measurement","","8","","32","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Transferring Implicit Knowledge of Non-Visual Object Properties Across Heterogeneous Robot Morphologies","G. Tatiya; J. Francis; J. Sinapov","Department of Computer Science, Tufts University, United States; Bosch Center for AI, Germany; Department of Computer Science, Tufts University, United States",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","11315","11321","Humans leverage multiple sensor modalities when interacting with objects and discovering their intrinsic properties. Using the visual modality alone is insufficient for deriving intuition behind object properties (e.g., which of two boxes is heavier), making it essential to consider non-visual modalities as well, such as the tactile and auditory. Whereas robots may leverage various modalities to obtain object property understanding via learned exploratory interactions with objects (e.g., grasping, lifting, and shaking behaviors), challenges remain: the implicit knowledge acquired by one robot via object exploration cannot be directly leveraged by another robot with different morphology, because the sensor models, observed data distributions, and interaction capabilities are different across these different robot configurations. To avoid the costly process of learning interactive object perception tasks from scratch, we propose a multi-stage projection framework for each new robot for transferring implicit knowledge of object properties across heterogeneous robot morphologies. We evaluate our approach on the object-property recognition and object-identity recognition tasks, using a dataset containing two heterogeneous robots that perform 7,600 object interactions. Results indicate that knowledge can be transferred across robots, such that a newly-deployed robot can bootstrap its recognition models without exhaustively exploring all objects. We also propose a data augmentation technique and show that this technique improves the generalization of models. We release code, datasets, and additional results, here: https://github.com/gtatiya/Implicit-Knowledge-Transfer.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160811","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160811","","Visualization;Codes;Automation;Morphology;Grasping;Robot sensing systems;Data augmentation","","5","","40","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Holo-Dex: Teaching Dexterity with Immersive Mixed Reality","S. P. Arunachalam; I. Güzey; S. Chintala; L. Pinto",New York University; New York University; Meta AI Research; New York University,2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","5962","5969","A fundamental challenge in teaching robots is to provide an effective interface for human teachers to demonstrate useful skills to a robot. This challenge is exacerbated in dexterous manipulation, where teaching high-dimensional, contact-rich behaviors often require esoteric teleoperation tools. In this work, we present Holo − Dex, a framework for dexter-ous manipulation that places a teacher in an immersive mixed reality through commodity VR headsets. The high-fidelity hand pose estimator onboard the headset is used to teleoperate the robot and collect demonstrations for a variety of general-purpose dexterous tasks. Given these demonstrations, we use powerful feature learning combined with non-parametric imi-tation to train dexterous skills. Our experiments on six common dexterous tasks, including in-hand rotation, spinning, and bottle opening, indicate that HOLO-DEX can both collect high-quality demonstration data and train skills in a matter of hours. Finally, we find that our trained skills can exhibit generalization on objects not seen in training. Videos of HOLO − DEX are available on {https://holo-dex.github.io/.}","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160547","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160547","","Headphones;Training;Representation learning;Automation;Mixed reality;Behavioral sciences;Spinning","","10","","70","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"D-Align: Dual Query Co-attention Network for 3D Object Detection Based on Multi-frame Point Cloud Sequence","J. Lee; J. Koh; Y. Lee; J. W. Choi","Department of Future Mobility, Hanyang University, Seoul, Korea; Department of Electrical Engineering, Hanyang University, Seoul, Korea; Department of Electrical Engineering, Hanyang University, Seoul, Korea; Department of Electrical Engineering, Hanyang University, Seoul, Korea",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","9238","9244","LiDAR sensors are widely used for 3D object detection in various mobile robotics applications. LiDAR sensors continuously generate point cloud data in real-time. Conventional 3D object detectors detect objects using a set of points acquired over a fixed duration. However, recent studies have shown that the performance of object detection can be further enhanced by utilizing spatio-temporal information obtained from point cloud sequences. In this paper, we propose a new 3D object detector, named D-Align, which can effectively produce strong bird's-eye-view (BEV) features by aligning and aggregating the features obtained from a sequence of point sets. The proposed method includes a novel dual-query co-attention network that uses two types of queries, including target query set (T-QS) and support query set (S-QS), to update the features of target and support frames, respectively. D-Align aligns S-QS to T-QS based on the temporal context features extracted from the adjacent feature maps and then aggregates S-QS with T-QS using a gated fusion mechanism. The dual queries are updated through multiple attention layers to progressively enhance the target frame features used to produce the detection results. Our experiments on the nuScenes dataset show that the proposed D-Align method greatly improved the performance of a single frame-based baseline method and significantly outperformed the latest 3D object detectors. Code is available at https://github.com/junhyung-SPALab/D-Align.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160484","National Research Foundation of Korea (NRF)(grant numbers:2020R1A2C2012146); Hanyang University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160484","","Point cloud compression;Three-dimensional displays;Laser radar;Aggregates;Object detection;Detectors;Transforms","","6","","29","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"AeriaLPiPS: A Local Planner for Aerial Vehicles with Geometric Collision Checking","J. S. Smith; P. Vela","School of Electrical and Computer Engineering and the Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, Atlanta, GA, USA; School of Electrical and Computer Engineering and the Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, Atlanta, GA, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","4092","4098","Real-time navigation in non-trivial environments by micro aerial vehicles (MAVs) predominantly relies on modelling the MAV with idealized geometry, such as a sphere. Simplified, conservative representations increase the likelihood of a planner failing to identify valid paths. That likelihood increases the more a robot's geometry differs from the idealized version. Few current approaches consider these situations; we are unaware of any that do so using perception space representations. This work introduces the egocan, a perception space obstacle representation using line-of-sight free space estimates, and 3D Gap, a perception space approach to gap finding for identifying goal-directed, collision-free directions of travel through 3D space. Both are integrated, with real-time considerations in mind, to define a local planner module of a hierarchical navigation system. The result is Aerial Local Planning in Perception Space (AeriaLPiPS). AeriaLPiPS is shown to be capable of safely navigating a MAV with non-idealized geometry through various environments, including those impassable by traditional real-time approaches. The open source implementation of this work is available at github.com/ivaROS/AeriaLPiPS.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160852","NSF(grant numbers:1849333); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160852","","Geometry;Three-dimensional displays;Navigation;Aerospace electronics;Real-time systems;Space exploration;Planning","","","","18","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Incremental Few-Shot Object Detection via Simple Fine-Tuning Approach","T. -M. Choi; J. -H. Kim","School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","9289","9295","In this paper, we explore incremental few-shot object detection (iFSD), which incrementally learns novel classes using only a few examples without revisiting base classes. Previous iFSD works achieved the desired results by applying metalearning. However, meta-learning approaches show insufficient performance that is difficult to apply to practical problems. In this light, we propose a simple fine-tuning-based approach, the Incremental Two-stage Fine-tuning Approach (iTFA) for iFSD, which contains three steps: 1) base training using abundant base classes with the class-agnostic box regressor, 2) separation of the RoI feature extractor and classifier into the base and novel class branches for preserving base knowledge, and 3) fine-tuning the novel branch using only a few novel class examples. We evaluate our iTFA on the real-world datasets PASCAL VOC, COCO, and LVIS. iTFA achieves competitive performance in COCO and shows a 30% higher AP accuracy than meta-learning methods in the LVIS dataset. Experimental results show the effectiveness and applicability of our proposed method11Code is available at https://github.com/TMIU/iTFA.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160283","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160283","","Metalearning;Training;Adaptation models;Automation;Object detection;Detectors;Feature extraction","","4","","32","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Ditto in the House: Building Articulation Models of Indoor Scenes through Interactive Perception","C. -C. Hsu; Z. Jiang; Y. Zhu","Department of Computer Science, University of Texas at Austin; Department of Computer Science, University of Texas at Austin; Department of Computer Science, University of Texas at Austin",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","3933","3939","Virtualizing the physical world into virtual models has been a critical technique for robot navigation and planning in the real world. To foster manipulation with articulated objects in everyday life, this work explores building articulation models of indoor scenes through a robot's purposeful inter-actions in these scenes. Prior work on articulation reasoning primarily focuses on siloed objects of limited categories. To extend to room-scale environments, the robot has to efficiently and effectively explore a large-scale 3D space, locate articulated objects, and infer their articulations. We introduce an interactive perception approach to this task. Our approach, named Ditto in the House, discovers possible articulated objects through affordance prediction, interacts with these objects to produce articulated motions, and infers the articulation properties from the visual observations before and after each interaction. It tightly couples affordance prediction and articulation inference to improve both tasks. We demonstrate the effectiveness of our approach in both simulation and real-world scenes. Code and additional results are available at https://ut-austin-rpl.github.io/HouseDitto/","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161431","NSF(grant numbers:CNS-1955523,FRR-2145283); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161431","","Visualization;Three-dimensional displays;Navigation;Affordances;Buildings;Estimation;Planning","","12","","41","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Curriculum-Based Imitation of Versatile Skills","M. X. Li; O. Celik; P. Becker; D. Blessing; R. Lioutikov; G. Neumann","Intuitive Robots Lab, Karlsruhe Institute of Technology, Karlsruhe, Germany; Autonomous Learning Robots, Karlsruhe Institute of Technology, Karlsruhe, Germany; Autonomous Learning Robots, Karlsruhe Institute of Technology, Karlsruhe, Germany; Autonomous Learning Robots, Karlsruhe Institute of Technology, Karlsruhe, Germany; Intuitive Robots Lab, Karlsruhe Institute of Technology, Karlsruhe, Germany; Autonomous Learning Robots, Karlsruhe Institute of Technology, Karlsruhe, Germany",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","2951","2957","Learning skills by imitation is a promising concept for the intuitive teaching of robots. A common way to learn such skills is to learn a parametric model by maximizing the likelihood given the demonstrations. Yet, human demonstrations are often multi-modal, i.e., the same task is solved in multiple ways which is a major challenge for most imitation learning methods that are based on such a maximum likelihood (ML) objective. The ML objective forces the model to cover all data, it prevents specialization in the context space and can cause mode-averaging in the behavior space, leading to suboptimal or potentially catastrophic behavior. Here, we alleviate those issues by introducing a curriculum using a weight for each data point, allowing the model to specialize on data it can represent while incentivizing it to cover as much data as possible by an entropy bonus. We extend our algorithm to a Mixture of (linear) Experts (MoE) such that the single components can specialize on local context regions, while the MoE covers all data points. We evaluate our approach in complex simulated and real robot control tasks and show it learns from versatile human demonstrations and significantly outperforms current SOTA methods. 11A reference implementation can be found at https://github.com/intuitive-robots/ML-Cur","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160543","Deutsche Forschungsgemeinschaft (German Research Foundation)(grant numbers:448648559); Carl Zeiss Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160543","Imitation;Versatility;Curriculum Learning","Learning systems;Robot control;Entropy;Data models;Behavioral sciences;Parametric statistics;Task analysis","","","","27","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"TrafficGen: Learning to Generate Diverse and Realistic Traffic Scenarios","L. Feng; Q. Li; Z. Peng; S. Tan; B. Zhou","ETH Zurich; The University of Edinburgh; University of California, Los Angeles; The University of Texas at Austin; University of California, Los Angeles",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","3567","3575","Diverse and realistic traffic scenarios are crucial for evaluating the AI safety of autonomous driving systems in simulation. This work introduces a data-driven method called TrafficGen for traffic scenario generation. It learns from the fragmented human driving data collected in the real world and then generates realistic traffic scenarios. TrafficGen is an autoregressive neural generative model with an encoder-decoder architecture. In each autoregressive iteration, it first encodes the current traffic context with the attention mechanism and then decodes a vehicle's initial state followed by generating its long trajectory. We evaluate the trained model in terms of vehicle placement and trajectories, and the experimental result shows our method has substantial improvements over baselines for generating traffic scenarios. After training, TrafficGen can also augment existing traffic scenarios, by adding new vehicles and extending the fragmented trajectories. We further demonstrate that importing the generated scenarios into a simulator as an interactive training environment improves the performance and safety of a driving agent learned from reinforcement learning. Model and data are available at https://metadriverse.github.io/trafficgen.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160296","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160296","","Training;Automation;Reinforcement learning;Data models;Trajectory;Safety;Noise measurement","","31","","32","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Planning Assembly Sequence with Graph Transformer","L. Ma; J. Gong; H. Xu; H. Chen; H. Zhao; W. Huang; G. Zhou","Institute for AI Industry Research (AIR), Tsinghua University, Beijing, P.R. China; Institute for AI Industry Research (AIR), Tsinghua University, Beijing, P.R. China; Qianzhi Technology, China; Qianzhi Technology, China; Institute for AI Industry Research (AIR), Tsinghua University, Beijing, P.R. China; Institute for AI Industry Research (AIR), Tsinghua University, Beijing, P.R. China; Institute for AI Industry Research (AIR), Tsinghua University, Beijing, P.R. China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","12395","12401","Assembly Sequence Planning (ASP) is the essential process for modern manufacturing, proven to be NP-complete thus its effective and efficient solution has been a challenge for researchers in the field. In this paper, we present a graph-transformer based framework for the ASP problem which is trained and demonstrated on a self-collected ASP database. The ASP database contains a self-collected set of LEGO models. The LEGO model is abstracted to a heterogeneous graph structure after a thorough analysis of the original structure and feature extraction. The ground truth assembly sequence is first generated by brute-force search and then adjusted manually to be in line with human rational habits. Based on this self-collected ASP dataset, we propose a heterogeneous graph-transformer framework to learn the latent rules for assembly planning. We evaluated the proposed framework in a series of experiments. The results show that the similarity of the predicted and ground truth sequences can reach 0.44, a medium correlation measured by Kendall's τ. Meanwhile, we compared the different effects of node features and edge features and generated a feasible and reasonable assembly sequence as a benchmark for further research. Our dataset and code are available on: htps://github.com/AIR-DISCOVER/ICRA_ASP.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160424","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160424","","Analytical models;Correlation;Codes;Automation;Databases;Benchmark testing;Transformers","","8","","32","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Parallel Reinforcement Learning Simulation for Visual Quadrotor Navigation","J. Saunders; S. Saeedi; W. Lil","Department of Computer Science, University of Bath, UK; Department of Mechanical and Industrial Engineering, Toronto Metropolitan University, Toronto, Canada; Department of Computer Science, University of Bath, UK",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","1357","1363","Reinforcement learning (RL) is an agent-based approach for teaching robots to navigate within the physical world. Gathering data for RL is known to be a laborious task, and real-world experiments can be risky. Simulators facilitate the collection of training data in a quicker and more cost-effective manner. However, RL frequently requires a significant number of simulation steps for an agent to become skilful at simple tasks. This is a prevalent issue within the field of RL-based visual quadrotor navigation where state dimensions are typically very large and dynamic models are complex. Furthermore, rendering images and obtaining physical properties of the agent can be computationally expensive. To solve this, we present a simulation framework, built on AirSim, which provides efficient parallel training. Building on this framework, Ape-X is modified to incorporate parallel training of AirSim environments to make use of numerous networked computers. Through experiments we were able to achieve a reduction in training time from 3.9 hours to 11 minutes, for a toy problem, using the aforementioned framework and a total of 74 agents and two networked computers. Further details including a github repo and videos about our project, PRL4AirSim, can be found at https://sites.google.com/view/prl4airsim/home","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160675","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160675","","Training;Computers;Visualization;Navigation;Computational modeling;Atmospheric modeling;Reinforcement learning","","1","","37","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Source-free Unsupervised Domain Adaptation for 3D Object Detection in Adverse Weather","D. Hegde; V. Kilic; V. Sindagi; A. B. Cooper; M. Foster; V. M. Patel","Johns Hopkins University, United States; Johns Hopkins University, United States; Johns Hopkins University, United States; Johns Hopkins University, United States; Johns Hopkins University, United States; Johns Hopkins University, United States",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","6973","6980","A domain shift exists between the distributions of large scale, outdoor lidar datasets due to being captured using different types of lidar sensors, in different locations, and under varying weather conditions. Inclement weather in particular affects the quality of lidar data, adding artifacts such as scattered and missed points, leading to a drop in performance of 3D object detection networks trained on standard lidar datasets. Domain adaptation methods seek to adapt source-trained neural networks to a target domain. Pseudo-label based self training approaches are popular methods for source-free unsupervised domain adaptation. However, their efficacy depends on the quality of the labels generated by the source trained model. These labels may be incorrect with high confidence, rendering thresholding methods ineffective. In order to avoid reinforcing errors caused by label noise, we propose an uncertainty-aware mean teacher framework which implicitly filters incorrect pseudo-labels during training. Leveraging model uncertainty allows the mean teacher network to perform implicit filtering by down-weighing losses corresponding to uncertain pseudo-labels. Effectively, we perform automatic soft-sampling of pseudo-labeled data while aligning predictions from the student and teacher networks. We demonstrate our domain adaptation method on an adverse weather dataset created by augmenting lidar scenes from KITTI with rain, snow, and fog and show that it out-performs current domain adaptation frameworks. We make our code publicly available 11https://github.com/deeptibhegde/UncertaintyAwareMeanTeacher.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161341","ARO(grant numbers:W911NF2110135); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161341","","Training;Adaptation models;Solid modeling;Laser radar;Three-dimensional displays;Uncertainty;Snow","","8","","41","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Synthetic-to-Real Domain Adaptation for Action Recognition: A Dataset and Baseline Performances","A. V. Reddy; K. Shah; W. Paul; R. Mocharla; J. Hoffman; K. D. Katyal; D. Manocha; C. M. de Melo; R. Chellappa","Dept. of Electrical & Computer Engineering, Johns Hopkins University, Baltimore, MD, USA; Dept. of Electrical & Computer Engineering, Johns Hopkins University, Baltimore, MD, USA; Johns Hopkins University Applied Physics Lab, Laurel, MD, USA; Johns Hopkins University Applied Physics Lab, Laurel, MD, USA; Georgia Institute of Technology, Atlanta, GA, USA; Johns Hopkins University Applied Physics Lab, Laurel, MD, USA; University of Maryland, College Park, MD, USA; Army Research Lab, Adelphi, MD, USA; Dept. of Electrical & Computer Engineering, Johns Hopkins University, Baltimore, MD, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","11374","11381","Human action recognition is a challenging problem, particularly when there is high variability in factors such as subject appearance, backgrounds and viewpoint. While deep neural networks (DNNs) have been shown to perform well on action recognition tasks, they typically require large amounts of high-quality labeled data to achieve robust performance across a variety of conditions. Synthetic data has shown promise as a way to avoid the substantial costs and potential ethical concerns associated with collecting and labeling enormous amounts of data in the real-world. However, synthetic data may differ from real data in important ways. This phenomenon, known as domain shift, can limit the utility of synthetic data in robotics applications. To mitigate the effects of domain shift, substantial effort is being dedicated to the development of domain adaptation (DA) techniques. Yet, much remains to be understood about how best to develop these techniques. In this paper, we introduce a new dataset called Robot Control Gestures (RoCoG-v2). The dataset is composed of both real and synthetic videos from seven gesture classes, and is intended to support the study of synthetic-to-real domain shift for video-based action recognition. Our work expands upon existing datasets by focusing the action classes on gestures for human-robot teaming, as well as by enabling investigation of domain shift in both ground and aerial views. We present baseline results using state-of-the-art action recognition and domain adaptation algorithms and offer initial insight on tackling the synthetic-to-real and ground-to-air domain shifts. Instructions on accessing the dataset can be found at https://github.com/reddyav1/RoCoG-v2.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160416","Army Research Laboratory (ARL)(grant numbers:W911NF-21-2-0211,W911NF-21-2-0076); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160416","","Training;Deep learning;Robot control;X3D;Skeleton;Real-time systems;Task analysis","","8","","59","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Learning from Physical Human Feedback: An Object-Centric One-Shot Adaptation Method","A. Shek; B. Y. Su; R. Chen; C. Liu","Carnegie Mellon University, Pittsburgh, PA, United States; Carnegie Mellon University, Pittsburgh, PA, United States; Carnegie Mellon University, Pittsburgh, PA, United States; Carnegie Mellon University, Pittsburgh, PA, United States",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","9910","9916","For robots to be effectively deployed in novel environments and tasks, they must be able to understand the feedback expressed by humans during intervention. This can either correct undesirable behavior or indicate additional preferences. Existing methods either require repeated episodes of interactions or assume prior known reward features, which is data-inefficient and can hardly transfer to new tasks. We relax these assumptions by describing human tasks in terms of object-centric sub-tasks and interpreting physical interventions in relation to specific objects. Our method, Object Preference Adaptation (OPA), is composed of two key stages: 1) pre-training a base policy to produce a wide variety of behaviors, and 2) online-updating according to human feedback. The key to our fast, yet simple adaptation is that general interaction dynamics between agents and objects are fixed, and only object-specific preferences are updated. Our adaptation occurs online, requires only one human intervention (one-shot), and produces new behaviors never seen during training. Trained on cheap synthetic data instead of expensive human demonstrations, our policy correctly adapts to human perturbations on realistic tasks on a physical 7DOF robot. Videos, code, and supplementary material: https://alvinosaur.github.io/AboutMe/projects/opa.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161416","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161416","","Training;Perturbation methods;Inspection;Production facilities;Behavioral sciences;Task analysis;Robots","","1","","20","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"ALAN: Autonomously Exploring Robotic Agents in the Real World","R. Mendonca; S. Bahl; D. Pathak",Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University,2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","3044","3050","Robotic agents that operate autonomously in the real world need to continuously explore their environment and learn from the data collected, with minimal human supervision. While it is possible to build agents that can learn in such a manner without supervision, current methods struggle to scale to the real world. Thus, we propose ALAN, an autonomously exploring robotic agent, that can perform tasks in the real world with little training and interaction time. This is enabled by measuring environment change, which reflects object movement and ignores changes in the robot position. We use this metric directly as an environment-centric signal, and also maximize the uncertainty of predicted environment change, which provides agent-centric exploration signal. We evaluate our approach on two different real-world play kitchen settings, enabling a robot to efficiently explore and discover manipulation skills, and perform tasks specified via goal images. Videos can be found at https://robo-explorer.github.io/","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161016","","Training;Measurement;Uncertainty;Automation;Current measurement;Position measurement;Task analysis","","4","","48","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"FDLNet: Boosting Real-time Semantic Segmentation by Image-size Convolution via Frequency Domain Learning","Q. Yan; S. Li; C. Liu; M. Liu; Q. Chen","Tongji University, Shanghai, China; Tongji University, Shanghai, China; Tongji University, Shanghai, China; Hong Kong University of Science and Technology, Hong Kong; Tongji University, Shanghai, China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","8155","8162","This paper proposes a novel real-time semantic segmentation network via frequency domain learning, called FDLNet, which revisits the segmentation task from two critical perspectives: spatial structure description and multilevel feature fusion. We first devise an image-size convolution (IS-Conv) as a global frequency-domain learning operator to capture long-range dependency in a single shot. To model spatial structure information, we construct the global structure representation path (GSRP) based on IS-Conv, which learns a unified edge-region representation with affordable complexity. For efficient and lightweight multi-level feature fusion, we propose the factorized stereoscopic attention (FSA) module, which alleviates semantic confusion and reduces feature redundancy by introducing level-wise attention before channel and spatial attention. Combining the above modules, we propose a concise semantic segmentation framework named FDLNet. We experimentally demonstrate the effectiveness and superiority of the proposed method. FDLNet achieves state-of-the-art performance on the Cityscapes, which reports 76.32% mIoU at 150+ FPS and 79.0% mIoU at 41+ FPS. The code is available at https://github.com/qyan0131/FDLNet.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161421","National Natural Science Foundation of China(grant numbers:62173248,62073245); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161421","","Convolution;Frequency-domain analysis;Semantic segmentation;Stereo image processing;Image edge detection;Semantics;Redundancy","","1","","50","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"CrossDTR: Cross-view and Depth-guided Transformers for 3D Object Detection","C. -Y. Tseng; Y. -R. Chen; H. -Y. Lee; T. -H. Wu; W. -C. Chen; W. H. Hsu",National Taiwan University; National Taiwan University; National Taiwan University; National Taiwan University; National Taiwan University; National Taiwan University,2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","4850","4857","To achieve accurate 3D object detection at a low cost for autonomous driving, many multi-camera methods have been proposed and solved the occlusion problem of monocular approaches. However, due to the lack of accurate estimated depth, existing multi-camera methods often generate multiple bounding boxes along a ray of depth direction for difficult small objects such as pedestrians, resulting in an extremely low recall. Furthermore, directly applying depth prediction modules to existing multi-camera methods, generally composed of large network architectures, cannot meet the real-time requirements of self-driving applications. To address these issues, we propose Cross-view and Depth-guided Transformers for 3D Object Detection, CrossDTR. First, our lightweight depth predictor is designed to produce precise object-wise sparse depth maps and low-dimensional depth embeddings without extra depth datasets during supervision. Second, a cross-view depth-guided transformer is developed to fuse the depth embeddings as well as image features from cameras of different views and generate 3D bounding boxes. Extensive experiments demonstrated that our method hugely surpassed existing multi-camera methods by 10 percent in pedestrian detection and about 3 percent in overall mAP and NDS metrics. Also, computational analyses showed that our method is 5 times faster than prior approaches. Our codes will be made publicly available at https://github.com/sty61010/CrossDTR.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161451","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161451","","Measurement;Three-dimensional displays;Pedestrians;Costs;Fuses;Object detection;Detectors","","3","","49","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Joint Camera Intrinsic and LiDAR-Camera Extrinsic Calibration","G. Yan; F. He; C. Shi; P. Wei; X. Cai; Y. Li","Shanghai AI Laboratory, Autonomous Driving Group, China; Shanghai AI Laboratory, Autonomous Driving Group, China; Shanghai AI Laboratory, Autonomous Driving Group, China; Shanghai AI Laboratory, Autonomous Driving Group, China; Shanghai AI Laboratory, Autonomous Driving Group, China; Shanghai AI Laboratory, Autonomous Driving Group, China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","11446","11452","Sensor-based environmental perception is a crucial step for autonomous driving systems, for which an accurate calibration between multiple sensors plays a critical role. For the calibration of LiDAR and camera, the existing method is generally to calibrate the intrinsic of the camera first and then calibrate the extrinsic of the LiDAR and camera. If the camera's intrinsic is not calibrated correctly in the first stage, it is not easy to calibrate the LiDAR-camera extrinsic accurately. Due to the complex internal structure of the camera and the lack of an effective quantitative evaluation method for the camera's intrinsic calibration, in the actual calibration, the accuracy of extrinsic parameter calibration is often reduced due to the tiny error of the camera's intrinsic parameters. To this end, we propose a novel target-based joint calibration method of the camera intrinsic and LiDAR-camera extrinsic parameters. Firstly, we design a novel calibration board pattern, adding four circular holes around the checkerboard for locating the LiDAR pose. Subsequently, a cost function defined under the reprojection constraints of the checkerboard and circular holes features is designed to solve the camera's intrinsic parameters, distortion factor, and LiDAR-camera extrinsic parameter. In the end, quantitative and qualitative experiments are conducted in actual and simulated environments, and the result shows the proposed method can achieve accuracy and robust performance. The open-source code is available at https://github.com/OpenCalib/JointCalib.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160542","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160542","","Point cloud compression;Laser radar;Codes;Cameras;Distortion;Sensor systems;Production facilities","","26","","37","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Semantic-SuPer: A Semantic-aware Surgical Perception Framework for Endoscopic Tissue Identification, Reconstruction, and Tracking","S. Lin; A. J. Miao; J. Lu; S. Yu; Z. -Y. Chiu; F. Richter; M. C. Yip","Department of Electrical and Computer Engineering, University of California San Diego, La Jolla, USA; Department of Electrical and Computer Engineering, University of California San Diego, La Jolla, USA; Department of Electrical and Computer Engineering, University of California San Diego, La Jolla, USA; Department of Electrical and Computer Engineering, University of California San Diego, La Jolla, USA; Department of Electrical and Computer Engineering, University of California San Diego, La Jolla, USA; Department of Electrical and Computer Engineering, University of California San Diego, La Jolla, USA; Department of Electrical and Computer Engineering, University of California San Diego, La Jolla, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","4739","4746","Accurate and robust tracking and reconstruction of the surgical scene is a critical enabling technology toward autonomous robotic surgery. Existing algorithms for 3D perception in surgery mainly rely on geometric information, while we propose to also leverage semantic information inferred from the endoscopic video using image segmentation algorithms. In this paper, we present a novel, comprehensive surgical per-ception framework, Semantic-SuPer, that integrates geometric and semantic information to facilitate data association, 3D reconstruction, and tracking of endoscopic scenes, benefiting downstream tasks like surgical navigation. The proposed frame-work is demonstrated on challenging endoscopic data with deforming tissue, showing its advantages over our baseline and several other state-of-the-art approaches. Our code and dataset are available at https://github.com/ucsdarclab/Python-SuPer.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160746","Telemedicine and Advanced Technology Research Center (TATRC)(grant numbers:MTEC-21-06-MPAI-004); NSF(grant numbers:2045803); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160746","","Three-dimensional displays;Automation;Uncertainty;Semantic segmentation;Semantics;Surgery;Estimation","","11","","68","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"DS-K3DOM: 3-D Dynamic Occupancy Mapping with Kernel Inference and Dempster-Shafer Evidential Theory","J. Han; Y. Min; H. -J. Chae; B. -M. Jeong; H. -L. Choi","Department of Aerospace Engineering, KAIST Institutes for Robotics, Korea Advanced Institide of Science and Technology (KAIST), Daejeon, South Korea; Laboratory for Information and Decision Systems (LIDS), Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Aerospace Engineering, KAIST Institutes for Robotics, Korea Advanced Institide of Science and Technology (KAIST), Daejeon, South Korea; Department of Aerospace Engineering, KAIST Institutes for Robotics, Korea Advanced Institide of Science and Technology (KAIST), Daejeon, South Korea; Department of Aerospace Engineering, KAIST Institutes for Robotics, Korea Advanced Institide of Science and Technology (KAIST), Daejeon, South Korea",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","6217","6223","Occupancy mapping has been widely utilized to represent the surroundings for autonomous robots to perform tasks such as navigation and manipulation. While occupancy mapping in 2-D environments has been well-studied, there have been few approaches suitable for 3-D dynamic occupancy mapping which is essential for aerial robots. This paper presents a novel 3-D dynamic occupancy mapping algorithm called DS-K3DOM. We first establish a Bayesian method to sequentially update occupancy maps for a stream of measurements based on the random finite set theory. Then, we approximate it with particles in the Dempster-Shafer domain to enable real-time computation. Moreover, the algorithm applies kernel-based inference with Dirichlet basic belief assignment to enable dense mapping from sparse measurements. The efficacy of the proposed algorithm is demonstrated through simulations and real experimentsiiThe code is available at: https://github.com/JuyeopHan/dsk3dom_public.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160364","National Research Foundation of Korea(NRF)(grant numbers:2020M3C1C1A01082375); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160364","","Atmospheric measurements;Heuristic algorithms;Sensor fusion;Approximation algorithms;Particle measurements;Set theory;Robot sensing systems","","","","18","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Informable Multi-Objective and Multi-Directional RRT* System for Robot Path Planning","J. -K. Huang; Y. Tan; D. Lee; V. R. Desaraju; J. W. Grizzle",NA; NA; NA; NA; NA,2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","5666","5673","Multi-objective or multi-destination path planning is crucial for mobile robotics applications such as mobility as a service, robotics inspection, and electric vehicle charging for long trips. This work proposes an anytime iterative system to concurrently solve the multi-objective path planning problem and determine the visiting order of destinations. The system is comprised of an anytime informable multi-objective and multi-directional RRT* algorithm to form a simple connected graph, and a solver that consists of an enhanced cheapest insertion algorithm and a genetic algorithm to solve approximately the relaxed traveling salesman problem in polynomial time. Moreover, a list of waypoints is often provided for robotics inspection and vehicle routing so that the robot can preferentially visit certain equipment or areas of interest. We show that the proposed system can inherently incorporate such knowledge to navigate challenging topology. The proposed anytime system is evaluated on large and complex graphs built for real-world driving applications. C++ implementations are available at: https://github.com/UMich-BipedLab/IMOMD-RRTStar.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160838","NSF(grant numbers:2118818); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160838","","Mobility as a service;Navigation;Vehicle routing;Inspection;Traveling salesman problems;Approximation algorithms;Path planning","","4","","68","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"WS-3D-Lane: Weakly Supervised 3D Lane Detection With 2D Lane Labels","J. Ai; W. Ding; J. Zhao; J. Zhong","SAIC AI Lab, Shanghai, China; SAIC AI Lab, Shanghai, China; SAIC AI Lab, Shanghai, China; SAIC AI Lab, Shanghai, China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","5595","5601","Compared to 2D lanes, real 3D lane data is difficult to collect accurately. In this paper, we propose a novel method for training 3D lanes with only 2D lane labels, called weakly supervised 3D lane detection WS-3D-Lane. By assumptions of constant lane width and equal height on adjacent lanes, we indirectly supervise 3D lane heights in the training. To overcome the problem of the dynamic change of the camera pitch during data collection, a camera pitch self-calibration method is proposed. In anchor representation, we propose a double-layer anchor with non-maximum suppression (NMS) method, which enables the anchor-based method to predict two lane lines that are close. Experiments are conducted on the base of 3D-LaneNet under two supervision methods. Under weakly supervised setting, our WS-3D-Lane outperforms previous 3D-LaneN et: F-score rises to 92.3% on Apollo 3D synthetic dataset, and F1 rises to 74.5% on ONCE-3DLanes. Meanwhile, WS-3D-Lane in purely supervised setting makes more increments and outperforms state-of-the-art. To the best of our knowledge, WS-3D- Lane is the first try of 3D lane detection under weakly supervised setting. Our code is available on https://github.com/SAIC-Vision/WS-3D-Lane.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161184","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161184","","Training;Solid modeling;Three-dimensional displays;Codes;Automation;Lane detection;Production","","1","","30","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Learning-Based Dimensionality Reduction for Computing Compact and Effective Local Feature Descriptors","H. Dong; X. Chen; M. Dusmanu; V. Larsson; M. Pollefeys; C. Stachniss","University of Bonn, Germany; University of Bonn, Germany; ETH Zurich, Switzerland; Lund University, Sweden; ETH Zurich, Switzerland; Department of Engineering Science, University of Oxford, UK",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","6189","6195","A distinctive representation of image patches in form of features is a key component of many computer vision and robotics tasks, such as image matching, image retrieval, and visual localization. State-of-the-art descriptors, from hand-crafted descriptors such as SIFT to learned ones such as HardNet, are usually high-dimensional; 128 dimensions or even more. The higher the dimensionality, the larger the memory consumption and computational time for approaches using such descriptors. In this paper, we investigate multi-layer perceptrons (MLPs) to extract low-dimensional but high-quality descriptors. We thoroughly analyze our method in unsuper-vised, self-supervised, and supervised settings, and evaluate the dimensionality reduction results on four representative descriptors. We consider different applications, including visual localization, patch verification, image matching and retrieval. The experiments show that our lightweight MLPs trained using supervised method achieve better dimensionality reduction than PCA. The lower-dimensional descriptors generated by our approach outperform the original higher-dimensional descriptors in downstream tasks, especially for the hand-crafted ones. The code is available at https://github.com/PRBonn/descriptor-dr.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161381","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161381","","Dimensionality reduction;Location awareness;Visualization;Runtime;Image matching;Memory management;Supervised learning","","1","","46","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Wild-Places: A Large-Scale Dataset for Lidar Place Recognition in Unstructured Natural Environments","J. Knights; K. Vidanapathirana; M. Ramezani; S. Sridharan; C. Fookes; P. Moghadam","Robotics and Autonomous Systems Group, DATA61, CSIRO, Australia; Robotics and Autonomous Systems Group, DATA61, CSIRO, Australia; Robotics and Autonomous Systems Group, DATA61, CSIRO, Australia; School of Electrical Engineering and Robotics, Queensland University of Technology (QUT), Australia; School of Electrical Engineering and Robotics, Queensland University of Technology (QUT), Australia; Robotics and Autonomous Systems Group, DATA61, CSIRO, Australia",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","11322","11328","Many existing datasets for lidar place recognition are solely representative of structured urban environments, and have recently been saturated in performance by deep learning based approaches. Natural and unstructured environments present many additional challenges for the tasks of long-term localisation but these environments are not represented in currently available datasets. To address this we introduce Wild-Places, a challenging large-scale dataset for lidar place recognition in unstructured, natural environments. Wild-Places contains eight lidar sequences collected with a handheld sensor payload over the course of fourteen months, containing a total of 63K undistorted lidar submaps along with accurate 6DoF ground truth. This dataset contains multi-ple revisits both within and between sequences, allowing for both intra-sequence (i.e., loop closure detection) and inter-sequence (i.e., re-localisation) tasks. We also benchmark several state-of-the-art approaches to demonstrate the challenges that this dataset introduces, particularly the case of long-term place recognition due to natural environments changing over time. Our dataset and code is available at https://csiro-robotics.github.io/Wild-Places","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160432","CSIRO; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160432","","Deep learning;Laser radar;Codes;Automation;Urban areas;Benchmark testing;Robot sensing systems","","26","","38","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Self-Supervised Learning of Action Affordances as Interaction Modes","L. Wang; N. Dvornik; R. Dubeau; M. Mittal; A. Garg","University of Toronto & Vector Institute, Canada; Nvidia, United States; University of Toronto & Vector Institute, Canada; ETH Zurich, Switzerland; University of Toronto & Vector Institute, Canada",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","7279","7286","When humans perform a task with an articulated object, they interact with the object only in a handful of ways, while the space of all possible interactions is nearly endless. This is because humans have prior knowledge about what interactions are likely to be successful, i.e., to open a new door we first try the handle. While learning such priors without supervision is easy for humans, it is notoriously hard for machines. In this work, we tackle unsupervised learning of priors of useful interactions with articulated objects, which we call interaction modes. In contrast to the prior art, we use no supervision or privileged information; we only assume access to the depth sensor in the simulator to learn the interaction modes. More precisely, we define a successful interaction as the one changing the visual environment substantially and learn a generative model of such interactions, that can be conditioned on the desired goal state of the object. In our experiments, we show that our model covers most of the human interaction modes, outperforms existing state-of-the-art methods for affordance learning, and can generalize to objects never seen during training. Additionally, we show promising results in the goal-conditional setup, where our model can be quickly fine-tuned to perform a given task. We show in the experiments that such affordance learning predicts interaction which covers most modes of interaction for the querying articulated object and can be fine-tuned to a goal-conditional model. For supplementary: https://actaim.github.io/.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161371","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161371","","Training;Adaptation models;Visualization;Affordances;Self-supervised learning;Predictive models;Diversity methods","","1","","47","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Robust Imaging Sonar-based Place Recognition and Localization in Underwater Environments","H. Kim; G. Kang; S. Jeong; S. Ma; Y. Cho","Dept. Electrical and Computer Engineering, Inha University, Incheon, South Korea; Dept. Electrical and Computer Engineering, Inha University, Incheon, South Korea; Dept. Electrical and Computer Engineering, Inha University, Incheon, South Korea; Dept. Electrical and Computer Engineering, Inha University, Incheon, South Korea; Dept. Electrical and Computer Engineering, Inha University, Incheon, South Korea",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","1083","1089","Place recognition using SOund Navigation and Ranging (SONAR) images is an important task for simultaneous localization and mapping (SLAM) in underwater environments. This paper proposes a robust and efficient imaging SONAR-based place recognition, SONAR context, and loop closure method. Unlike previous methods, our approach encodes geometric information based on the characteristics of raw SONAR measurements without prior knowledge or training. We also design a hierarchical searching procedure for fast retrieval of candidate SONAR frames and apply adaptive shifting and padding to achieve robust matching on rotation and translation changes. In addition, we can derive the initial pose through adaptive shifting and apply it to the iterative closest point (ICP)-based loop closure factor. We evaluate the SONAR context's performance in the various underwater sequences such as simulated open water, real water tank, and real underwater environments. The proposed approach shows the robustness and improvements of place recognition on various datasets and evaluation metrics. Supplementary materials are available at https://github.com/sparolab/sonar_context.git.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161518","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161518","","Training;Image recognition;Simultaneous localization and mapping;Sonar measurements;Sonar;Imaging;Sensor phenomena and characterization","","2","","20","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Sim2Real2: Actively Building Explicit Physics Model for Precise Articulated Object Manipulation","L. Ma; J. Meng; S. Liu; W. Chen; J. Xu; R. Chen","Department of Mechanical Engineering, State Key Laboratory of Tribology, the Beijing Key Laboratory of Precision/Ultra-Precision Manufacturing Equipment Control, Tsinghua University, Beijing, China; School of Modern Post, Beijing University of Posts and Telecommunications, Beijing, China; AVIC Chengdu Aircraft Industrial (Group) Co., Ltd, Chengdu, Sichuan, China; Department of Mechanical Engineering, State Key Laboratory of Tribology, the Beijing Key Laboratory of Precision/Ultra-Precision Manufacturing Equipment Control, Tsinghua University, Beijing, China; Department of Mechanical Engineering, State Key Laboratory of Tribology, the Beijing Key Laboratory of Precision/Ultra-Precision Manufacturing Equipment Control, Tsinghua University, Beijing, China; Department of Mechanical Engineering, State Key Laboratory of Tribology, the Beijing Key Laboratory of Precision/Ultra-Precision Manufacturing Equipment Control, Tsinghua University, Beijing, China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","11698","11704","Accurately manipulating articulated objects is a challenging yet important task for real robot applications. In this paper, we present a novel framework called Sim2Real2 to enable the robot to manipulate an unseen articulated object to the desired state precisely in the real world with no human demonstrations. We leverage recent advances in physics simulation and learning-based perception to build the interactive explicit physics model of the object and use it to plan a long-horizon manipulation trajectory to accomplish the task. However, the interactive model cannot be correctly estimated from a static observation. Therefore, we learn to predict the object affordance from a single-frame point cloud, control the robot to actively interact with the object with a one-step action, and capture another point cloud. Further, the physics model is constructed from the two point clouds. Experimental results show that our framework achieves about 70% manipulations with < 30% relative error for common articulated objects, and 30% manipulations for difficult objects. Our proposed framework also enables advanced manipulation strategies, such as manipulating with different tools. Code and videos are available on our project webpage: https://ttimelord.github.io/Sim2Real2-site/","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160370","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160370","","Point cloud compression;Propioception;Search problems;Robot learning;Real-time systems;Trajectory;Sensors","","5","","29","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Monocular Visual-Inertial Odometry with Planar Regularities","C. Chen; P. Geneva; Y. Peng; W. Lee; G. Huang","Robot Perception and Navigation Group (RPNG), University of Delaware, Newark, DE, USA; Robot Perception and Navigation Group (RPNG), University of Delaware, Newark, DE, USA; Robot Perception and Navigation Group (RPNG), University of Delaware, Newark, DE, USA; Robot Perception and Navigation Group (RPNG), University of Delaware, Newark, DE, USA; Robot Perception and Navigation Group (RPNG), University of Delaware, Newark, DE, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","6224","6231","State-of-the-art monocular visual-inertial odometry (VIO) approaches rely on sparse point features in part due to their efficiency, robustness, and prevalence, while ignoring high-level structural regularities such as planes that are common to man-made environments and can be exploited to further constrain motion. Generally, planes can be observed by a camera for significant periods of time due to their large spatial presence and thus, are amenable for long-term navigation. Therefore, in this paper, we design a novel real-time monocular VIO system that is fully regularized by planar features within a lightweight multi-state constraint Kalman filter (MSCKF). At the core of our method is an efficient robust monocular-based plane detection algorithm, which does not require additional sensing modalities such as a stereo or depth camera as commonly seen in the literature, while enabling real-time regularization of point features to environmental planes. Specifically, in the proposed MSCKF, long-lived planes are maintained in the state vector, while shorter ones are marginalized after use for efficiency. Planar regularities are applied to both in-state SLAM features and out-of-state MSCKF features, thus fully exploiting the environmental plane information to improve VIO performance. The proposed approach is evaluated with extensive Monte-Carlo simulations and different real-world experiments including an author-collected AR scenario, and shown to outperform the point-based VIO in structured environments. Video Demonstration https://youtu.be/bec7LbYaOS8AR Table Dataset https://github.com/rpng/ar_table_dataset","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160620","University of Delaware (UD); NSF(grant numbers:IIS-1924897,SCH-2014264,CNS-2018905); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160620","","Simultaneous localization and mapping;Monte Carlo methods;Navigation;Cameras;Feature extraction;Real-time systems;Robustness","","7","","76","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Self-Improving Safety Performance of Reinforcement Learning Based Driving with Black-Box Verification Algorithms","R. Dagdanov; H. Durmus; N. K. Ure","Department of Aeronautical Engineering, ITU Artificial Intelligence and Data Science Research Center, Istanbul Technical University, Turkey; Department of Electronics and Communication Engineering, Eatron Technologies, Istanbul Technical University, Turkey; Department of Computer Engineering, ITU Artificial Intelligence and Data Science Application and Research Center, Istanbul Technical University, Turkey",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","5631","5637","In this work, we propose a self-improving artificial intelligence system to enhance the safety performance of reinforcement learning (RL)-based autonomous driving (AD) agents using black-box verification methods. RL algorithms have become popular in AD applications in recent years. However, the performance of existing RL algorithms heavily depends on the diversity of training scenarios. A lack of safety-critical scenarios during the training phase could result in poor generalization performance in real-world driving applications. We propose a novel framework in which the weaknesses of the training set are explored through black-box verification methods. After discovering AD failure scenarios, the RL agent's training is re-initiated via transfer learning to improve the performance of previously unsafe scenarios. Simulation results demonstrate that our approach efficiently discovers safety failures of action decisions in RL-based adaptive cruise control (ACC) applications and significantly reduces the number of vehicle collisions through iterative applications of our method. The source code is publicly available at https://github.com/data-and-decision-lab/self-improving-RL.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160883","Istanbul Technical University BAP(grant numbers:MOA-2019-42321); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160883","Deep Reinforcement Learning;Autonomous Driving;Black-Box Verification","Training;Source coding;Simulation;Transfer learning;Closed box;Reinforcement learning;Safety","","2","","23","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"EXOT: Exit-aware Object Tracker for Safe Robotic Manipulation of Moving Object","H. Kim; H. J. Yoon; M. Kim; D. -S. Han; B. -T. Zhang","Interdisciplinary Program in Neuroscience, Seoul National University, Seoul, Korea; Interdisciplinary Program in Artificial Intelligence, Seoul National University; Dept. of Computer Science and Engineering, Seoul National University; Dept. of Computer Science and Engineering, Seoul National University; AIIS, Seoul National University",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","813","819","Current robotic hand manipulation narrowly operates with objects in predictable positions in limited environments. Thus, when the location of the target object deviates severely from the expected location, a robot sometimes responds in an unexpected way, especially when it operates with a human. For safe robot operation, we propose the EXit-aware Object Tracker (EXOT) on a robot hand camera that recognizes an object's absence during manipulation. The robot decides whether to proceed by examining the tracker's bounding box output containing the target object. We adopt an out-of-distribution classifier for more accurate object recognition since trackers can mistrack a background as a target object. To the best of our knowledge, our method is the first approach of applying an out-of-distribution classification technique to a tracker output. We evaluate our method on the first-person video benchmark dataset, TREK-150, and on the custom dataset, RMOT-223, that we collect from the UR5e robot. Then we test our tracker on the UR5e robot in real-time with a conveyor-belt sushi task, to examine the tracker's ability to track target dishes and to determine the exit status. Our tracker shows 38% higher exit-aware performance than a baseline method. The dataset and the code will be released at https://github.com/hskAlena/EXOT.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160481","MSIT(grant numbers:2021-0-01343); Seoul National University(grant numbers:25%,2021-0-02068-AIHub/25%,2022-0-00951-LBA/25%,2022-0-00953-PICA/25%); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160481","","Training;Target tracking;Codes;Robot vision systems;Streaming media;Cameras;Real-time systems","","","","34","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Multi-to-Single Knowledge Distillation for Point Cloud Semantic Segmentation","S. Qiu; F. Jiang; H. Zhang; X. Xue; J. Pu",Fudan University; Fudan University; Mogo Auto; Fudan University; Fudan University,2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","9303","9309","3D point cloud semantic segmentation is one of the fundamental tasks for environmental understanding. Although significant progress has been made in recent years, the performance of classes with few examples or few points is still far from satisfactory. In this paper, we propose a novel multi-to-single knowledge distillation framework for the 3D point cloud semantic segmentation task to boost the performance of those hard classes. Instead of fusing all the points of multi-scans directly, only the instances that belong to the previously defined hard classes are fused. To effectively and sufficiently distill valuable knowledge from multi-scans, we leverage a multilevel distillation framework, i.e., feature representation distillation, logit distillation, and affinity distillation. We further develop a novel instance-aware affinity distillation algorithm for capturing high-level structural knowledge to enhance the distillation efficacy for hard classes. Finally, we conduct experiments on the SemanticKITTI dataset, and the results on both the validation and test sets demonstrate that our method yields substantial improvements compared with the baseline method. The code is available at https://github.com/skyshoumeng/M2SKD.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160496","NSFC(grant numbers:62176061); STCSM(grant numbers:22511105000); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160496","","Point cloud compression;Training;Three-dimensional displays;Codes;Automation;Fuses;Semantic segmentation","","2","","37","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Low-level controller in response to changes in quadrotor dynamics","J. -K. Cho; C. Kim; M. K. M. Jaffar; M. W. Otte; S. -W. Kim","Seoul National University, Seoul, South Korea; Seoul National University, Seoul, South Korea; University of Maryland, Maryland, United States; University of Maryland, Maryland, United States; Seoul National University, Seoul, South Korea",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","5317","5323","The dynamics of all real quadrotors inevitably differ even if they are the same product. In particular, the dynamics can change significantly during the flight due to additional device attachments or overheating motors. In this study, we focus on training a low-level controller, which operates in response to dynamics-changes without prior knowledge or fine-tuning of the parameters, using reinforcement learning. We randomize the dynamics of quadrotors in the simulator and train the policy based on dynamics information extracted from the state-action history through recurrent neural networks (RNNs). In addition, our experiment demonstrates the difficulties in applying existing actor-critic structures that extract dynamics information using end-to-end RNNs for unstable quadrotors; hence, we propose a novel structure with better performance. Finally, the excellent performance of the proposed controller is verified by testing experiments that stabilize quadrotors with different dynamics. The experiment videos and the code can be found at https://github.com/jackyoung96/RNN-Quadrotor-controller.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160987","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160987","","Training;Recurrent neural networks;Propellers;Heuristic algorithms;Reinforcement learning;Data mining;History","","","","20","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Pose Relation Transformer Refine Occlusions for Human Pose Estimation","H. -g. Chi; S. Chi; S. Chan; K. Ramani","School of Electrical and Computer Engineering, Purdue University; School of Electrical and Computer Engineering, Purdue University; School of Electrical and Computer Engineering, Purdue University; School of Electrical and Computer Engineering, Purdue University",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","6138","6145","Accurately estimating the human pose is an essential task for many applications in robotics. However, existing pose estimation methods suffer from poor performance when occlusion occurs. Recent advances in NLP have been very successful in predicting the missing words conditioned on visible words. We draw upon the sentence completion analogy in NLP to guide our model to address occlusions in the pose estimation problem. We propose a novel approach that can mitigate the effect of occlusions motivated by the sentence completion task of NLP. In an analogous manner, we designed our model to reconstruct occluded joints given the visible joints utilizing joint correlations by capturing the implicit joint connectivity through the attention mechanism. In this work, we propose a POse Relation Transformer (PORT) that captures the global context of the pose using self-attention and a local context by aggregating adjacent joint features. To supervise PORT in learning joint correlations, we guide PORT to reconstruct randomly masked joints, which we call Masked Joint Modeling (MJM). PORT trained with MJM adds to existing keypoint detection methods and successfully refines occlusions. Notably, PORT is a model-agnostic plug-and-play module for pose refinement under occlusion that can be plugged into any keypoint detector with substantially low computational costs. We conducted extensive experiments to demonstrate the advantage of PORT mitigating the occlusion on the hand and body pose PORT improves the pose estimation accuracy of existing human pose estimation methods by up to 16% with only 5% of additional parameters. The code is publicly available at https://github.com/stnoah1/PORT.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161259","US National Science Foundation(grant numbers:FW-HTF 1839971); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161259","","Correlation;Codes;Computational modeling;Pose estimation;Detectors;Transformers;Computational efficiency","","1","","64","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Learned Risk Metric Maps for Kinodynamic Systems","R. E. Allen; W. Xiao; D. Rus","Massachusetts Institute of Technology, Lincoln Laboratory, Lexington, MA, USA; Massachusetts Institute of Technology, Computer Science and Artificial Intelligence Laboratory, Cambridge, MA, USA; Massachusetts Institute of Technology, Computer Science and Artificial Intelligence Laboratory, Cambridge, MA, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","961","967","We present Learned Risk Metric Maps (LRMM) for real-time estimation of coherent risk metrics of high-dimensional dynamical systems operating in unstructured, partially observed environments. LRMM models are simple to design and train-requiring only procedural generation of obstacle sets, state and control sampling, and supervised training of a function approximator-which makes them broadly applicable to arbitrary system dynamics and obstacle sets. In a parallel autonomy setting, we demonstrate the model's ability to rapidly infer collision probabilities of a fast-moving car-like robot driving recklessly in an obstructed environment; allowing the LRMM agent to intervene, take control of the vehicle, and avoid collisions. In this time-critical scenario, we show that LRMMs can evaluate risk metrics 20-100x times faster than alternative safety algorithms based on control barrier functions (CBFs) and Hamilton-Jacobi reachability (HJ-reach), leading to 5–15 % fewer obstacle collisions by the LRMM agent than CBFs and HJ-reach. This performance improvement comes in spite of the fact that the LRMM model only has access to local/partial observation of obstacles, whereas the CBF and HJ-reach agents are granted privileged/global information. We also show that our model can be equally well trained on a 12-dimensional quadrotor system operating in an obstructed indoor environment. The LRMM codebase is provided at https://github.com/mit-drl/pyrmm.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160680","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160680","","Measurement;Training;System dynamics;Estimation;Real-time systems;Safety;Time factors","","","","34","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"nerf2nerf: Pairwise Registration of Neural Radiance Fields","L. Goli; D. Rebain; S. Sabour; A. Garg; A. Tagliasacchi",University of Toronto; UBC; University of Toronto; University of Toronto; University of Toronto,2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","9354","9361","We introduce a technique for pairwise registration of neural fields that extends classical optimization-based local registration (i.e. ICP) to operate on Neural Radiance Fields (NeRF)-neural 3D scene representations trained from collections of calibrated images. NeRF does not decompose illumination and color, so to make registration invariant to illumination, we introduce the concept of a “surface field” - a field distilled from a pre-trained NeRF model that measures the likelihood of a point being on the surface of an object. We then cast nerf2nerf registration as a robust optimization that iteratively seeks a rigid transformation that aligns the surface fields of the two scenes. We evaluate the effectiveness of our technique by introducing a dataset of pre-trained NeRF scenes - our synthetic scenes enable quantitative evaluations and comparisons to classical registration techniques, while our real scenes demonstrate the validity of our technique in real-world scenarios. Additional results available at: https://nerf2nerf.github.io","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160794","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160794","","Point cloud compression;Analytical models;Visualization;Image color analysis;Pipelines;Urban areas;Lighting","","14","","65","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Adaptive Sampling-based Particle Filter for Visual-inertial Gimbal in the Wild","X. Kang; A. Herrera; H. Lema; E. Valencia; P. Vandewalle","PSI Department of Electrical Engineering, (ESAT), KU Leuven, Belgium; Department of Mechanical Engineering, Escuela Politecnica Nacional (EPN), Ecuador; Department of Mechanical Engineering, Escuela Politecnica Nacional (EPN), Ecuador; Department of Mechanical Engineering, Escuela Politecnica Nacional (EPN), Ecuador; PSI Department of Electrical Engineering, (ESAT), KU Leuven, Belgium",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","2738","2744","In this paper, we present a Computer Vision (CV) based tracking and fusion algorithm, dedicated to a 3D printed gimbal system on drones flying in nature. The whole gimbal system can stabilize the camera orientation robustly in challenging environments by using skyline and ground plane as references. Our main contributions are the following: a) a light-weight Resnet-18 backbone network model was trained from scratch, and deployed onto the Jetson Nano platform to segment the image specifically into binary parts (ground and sky); b) our geometry assumption from the skyline and ground cues delivers the potential for robust visual tracking in the wild by using the skyline and ground plane as references; c) a manifold surface-based adaptive particle sampling can fuse orientation from multiple sensor sources flexibly. The whole algorithm pipeline is tested on our 3D-printed gimbal module with Jetson Nano. The experiments were performed on top of a building in a real landscape. The public code link: https://github.com/alexandor91/gimbal-fusion.git.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160395","VLIR-UOS(grant numbers:EC2020SIN278A101); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160395","","Manifolds;Geometry;Visualization;Image segmentation;Three-dimensional displays;Fuses;Pipelines","","1","","40","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Dynamic Control Barrier Function-based Model Predictive Control to Safety-Critical Obstacle-Avoidance of Mobile Robot","Z. Jian; Z. Yan; X. Lei; Z. Lu; B. Lan; X. Wang; B. Liang","Center for Artificial Intelligence and Robotics, Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Center for Artificial Intelligence and Robotics, Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Department of D-MAVT, ETH Zurich, Zurich, Switzerland; School of Mechanical Engineering and Automation at Harbin Institute of Technology, Shenzhen, China; Jianghuai Advance Technology Center, China; Center for Artificial Intelligence and Robotics, Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Center for Artificial Intelligence and Robotics, Shenzhen International Graduate School, Tsinghua University, Shenzhen, China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","3679","3685","This paper presents an efficient and safe method to avoid static and dynamic obstacles based on LiDAR. First, point cloud is used to generate a real-time local grid map for obstacle detection. Then, obstacles are clustered by DBSCAN algorithm and enclosed with minimum bounding ellipses (MBEs). In addition, data association is conducted to match each MBE with the obstacle in the current frame. Considering MBE as an observation, Kalman filter (KF) is used to estimate and predict the motion state of the obstacle. In this way, the trajectory of each obstacle in the forward time domain can be parameterized as a set of ellipses. Due to the uncertainty of the MBE, the semi-major and semi-minor axes of the parameterized ellipse are extended to ensure safety. We extend the traditional Control Barrier Function (CBF) and propose Dynamic Control Barrier Function (D-CBF). We combine D-CBF with Model Predictive Control (MPC) to implement safety-critical dynamic obstacle avoidance. Experiments in simulated and real scenarios are conducted to verify the effectiveness of our algorithm. The source code is released for the reference of the community11Code: https://github.com/jianzhuozhuTHU/MPC-D-CBF..","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160857","National Natural Science Foundation of China(grant numbers:U21B6002,U1813216); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160857","","Uncertainty;Laser radar;Heuristic algorithms;Source coding;Prediction algorithms;Trajectory;Kalman filters","","34","","37","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"DexGraspNet: A Large-Scale Robotic Dexterous Grasp Dataset for General Objects Based on Simulation","R. Wang; J. Zhang; J. Chen; Y. Xu; P. Li; T. Liu; H. Wang",Peking University; Peking University; Peking University; Peking University; Beijing Institute for General Artificial Intelligence; Beijing Institute for General Artificial Intelligence; Peking University,2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","11359","11366","Robotic dexterous grasping is the first step to enable human-like dexterous object manipulation and thus a crucial robotic technology. However, dexterous grasping is much more under-explored than object grasping with parallel grippers, partially due to the lack of a large-scale dataset. In this work, we present a large-scale robotic dexterous grasp dataset, DexGraspNet, generated by our proposed highly efficient synthesis method that can be generally applied to any dexterous hand. Our method leverages a deeply accelerated differentiable force closure estimator and thus can efficiently and robustly synthesize stable and diverse grasps on a large scale. We choose ShadowHand and generate 1.32 million grasps for 5355 objects, covering more than 133 object categories and containing more than 200 diverse grasps for each object instance, with all grasps having been validated by the Isaac Gym simulator. Compared to the previous dataset from Liu et al. generated by GraspIt!, our dataset has not only more objects and grasps, but also higher diversity and quality. Via performing cross-dataset experiments, we show that training several algorithms of dexterous grasp synthesis on our dataset significantly outperforms training on the previous one. To access our data and code, including code for human and Allegro grasp synthesis, please visit our project page: https://pku-epic.github.io/DexGraspNet/.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160982","National Key R&D Program of China(grant numbers:2022ZD0114900); Beijing Municipal Science & Technology Commission(grant numbers:Z221100003422004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160982","","Training;Measurement;Codes;Automation;Force;Grasping;Grippers","","27","","65","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"UDepth: Fast Monocular Depth Estimation for Visually-guided Underwater Robots","B. Yu; J. Wu; M. J. Islam","Electrical and Computer Engineering (ECE) department, Robot Perception and Intelligence (RoboPI) laboratory, University of Florida, USA; Electrical and Computer Engineering (ECE) department, Robot Perception and Intelligence (RoboPI) laboratory, University of Florida, USA; Electrical and Computer Engineering (ECE) department, Robot Perception and Intelligence (RoboPI) laboratory, University of Florida, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","3116","3123","In this paper, we present a fast monocular depth estimation method for enabling 3D perception capabilities of low-cost underwater robots. We formulate a novel end-to-end deep visual learning pipeline named UDepth, which incorporates domain knowledge of image formation characteristics of natural underwater scenes. First, we adapt a new input space from raw RGB image space by exploiting underwater light attenuation prior, and then devise a least-squared formulation for coarse pixel-wise depth prediction. Subsequently, we extend this into a domain projection loss that guides the end-to-end learning of UDepth on over 9K RGB-D training samples. UDepth is designed with a computationally light MobileNetV2 backbone and a Transformer-based optimizer for ensuring fast inference rates on embedded systems. By domain-aware design choices and through comprehensive experimental analyses, we demonstrate that it is possible to achieve state-of-the-art depth estimation performance while ensuring a small computational footprint. Specifically, with 70 % −80 % less network parameters than existing benchmarks, UDepth achieves comparable and often better depth estimation performance. While the full model offers over 66 FPS (13 FPS) inference rates on a single GPU (CPU core), our domain projection for coarse depth prediction runs at 51.5 FPS rates on single-board Jetson TX2s. The inference pipelines are available at https://github.com/uf-robopi/UDepth.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161471","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161471","","Autonomous underwater vehicles;Visualization;Three-dimensional displays;Computational modeling;Pipelines;Estimation;Attenuation","","21","","84","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Obstacle avoidance using Raycasting and Riemannian Motion Policies at kHz rates for MAVs","M. Pantic; I. Meijer; R. Bähnemann; N. Alatur; O. Andersson; C. Cadena; R. Siegwart; L. Ott","Autonomous Systems Lab, ETH, Zürich, Switzerland; Autonomous Systems Lab, ETH, Zürich, Switzerland; Autonomous Systems Lab, ETH, Zürich, Switzerland; Autonomous Systems Lab, ETH, Zürich, Switzerland; Autonomous Systems Lab, ETH, Zürich, Switzerland; Autonomous Systems Lab, ETH, Zürich, Switzerland; Autonomous Systems Lab, ETH, Zürich, Switzerland; Autonomous Systems Lab, ETH, Zürich, Switzerland",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","1666","1672","This paper presents a novel method for using Riemannian Motion Policies on volumetric maps, shown in the example of obstacle avoidance for Micro Aerial Vehicles (MAVs), Today, most robotic obstacle avoidance algorithms rely on sampling or optimization-based planners with volumetric maps. However, they are computationally expensive and often have inflexible monolithic architectures. Riemannian Motion Policies are a modular, parallelizable, and efficient navigation alternative but are challenging to use with the widely used voxel-based environment representations. We propose using GPU raycasting and tens of thousands of concurrent policies to provide direct obstacle avoidance using Riemannian Motion Policies in voxelized maps without needing map smoothing or pre-processing. Additionally, we present how the same method can directly plan on LiDAR scans without any intermediate map. We show how this reactive approach compares favorably to traditional planning methods and can evaluate up to 200 million rays per second. We demonstrate the planner successfully on a real MAV for static and dynamic obstacles. The presented planner is made available as an open-source package11https://github.com/ethz-asl/reactive_avoidance.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161365","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161365","","Laser radar;Smoothing methods;Navigation;Graphics processing units;Computer architecture;Parallel processing;Planning","","5","","18","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Feature-Realistic Neural Fusion for Real-Time, Open Set Scene Understanding","K. Mazur; E. Sucar; A. J. Davison","Dyson Robotics Lab, Imperial College, London, UK; Dyson Robotics Lab, Imperial College, London, UK; Dyson Robotics Lab, Imperial College, London, UK",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","8201","8207","General scene understanding for robotics requires flexible semantic representation, so that novel objects and structures which may not have been known at training time can be identified, segmented and grouped. We present an algorithm which fuses general learned features from a standard pre-trained network into a highly efficient 3D geometric neural field representation during real-time SLAM. The fused 3D feature maps inherit the coherence of the neural field's geometry representation. This means that tiny amounts of human labelling interacting at runtime enable objects or even parts of objects to be robustly and accurately segmented in an open set manner. Project page: https://makezur.github.io/FeatureRealisticFusion/","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160800","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160800","","Training;Three-dimensional displays;Simultaneous localization and mapping;Runtime;Semantic segmentation;Semantics;Real-time systems","","17","","24","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Knowledge Distillation for Feature Extraction in Underwater VSLAM","J. Yang; M. Gong; G. Nair; J. H. Lee; J. Monty; Y. Pu","the Department of Electrical and Electronic Engineering, The University of Melbourne; School of Mathematics and Statistics, The University of Melbourne; the Department of Electrical and Electronic Engineering, The University of Melbourne; the Department of Mechanical Engineering, The University of Melbourne; the Department of Mechanical Engineering, The University of Melbourne; the Department of Electrical and Electronic Engineering, The University of Melbourne",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","5163","5169","In recent years, learning-based feature detection and matching have outperformed manually-designed methods in in-air cases. However, it is challenging to learn the features in the underwater scenario due to the absence of annotated underwater datasets. This paper proposes a cross-modal knowl-edge distillation framework for training an underwater feature detection and matching network (UFEN). In particular, we use in-air RGBD data to generate synthetic underwater images based on a physical underwater imaging formation model and employ these as the medium to distil knowledge from a teacher model SuperPoint pretrained on in-air images. We embed UFEN into the ORB-SLAM3 framework to replace the ORB feature by introducing an additional binarization layer. To test the effectiveness of our method, we built a new underwater dataset with groundtruth measurements named EASI (https://github.com/Jinghe-mel/UFEN-SLAM), recorded in an indoor water tank for different turbidity levels. The experimental results on the existing dataset and our new dataset demonstrate the effectiveness of our method.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161047","","Training;Automation;Feature detection;Imaging;Manuals;Feature extraction;Data models","","4","","33","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Unsupervised Road Anomaly Detection with Language Anchors","B. Tian; M. Liu; H. -a. Gao; P. Li; H. Zhao; G. Zhou","Department of Computer Science and Technology, Tsinghua University, China; Department of Computer Science and Technology, Tsinghua University, China; Department of Computer Science and Technology, Tsinghua University, China; Department of Computer Science and Technology, Tsinghua University, China; Institute for AI Industry Research (AIR), Tsinghua University, China; Institute for AI Industry Research (AIR), Tsinghua University, China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","7778","7785","Road anomaly detection is critical to safe autonomous driving, because current road scene understanding models are usually trained in a closed-set manner and fail to identify unknown objects. What's worse, it is difficult, if not impossible, to collect a large-scale dataset with anomaly annotations. So this paper studies unsupervised anomaly detection which finds out anomaly regions using scene parsing logits solely. While former methods depend on the weights learned from the closed training set as anchors for logit generation, we resort to language anchors that are learned from enormous paired vision and language data. Thanks to rich open-set semantic information contained in these language anchors, our method performs better than former unsupervised counterparts while maintaining the advantage of training without accessing any out-of-distribution data. We delve into this new paradigm and identify the superiority of using pair-wise binary logits, which we credit to a better understanding of the negation language anchor. Last but not least, we find that the former top-1 selection of semantic labels for uncertainty measurement is problematic in many cases and a new blended standardization strategy brings clear improvements to our solution. We report state-of-the-art performance on FS LostAndFound, LostAndFound and RoadAnomaly datasets among comparable methods. The codes are publicly available at https://github.com/TB5z035/URAD-LA.git","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160470","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160470","","Training;Uncertainty;Codes;Roads;Semantics;Measurement uncertainty;Standardization","","9","","46","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"SGPT: The Secondary Path Guides the Primary Path in Transformers for HOI Detection","S. Chan; W. Wang; Z. Shao; C. Bai","College of Computer Science and Technology, Zhejiang University of Technology, Zhejiang, China; College of Computer Science and Technology, Zhejiang University of Technology, Zhejiang, China; College of Information Science and Engineering, Hunan Normal University, Changsha, China; College of Computer Science and Technology, Zhejiang University of Technology, Zhejiang, China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","7583","7590","HOI detection is essential for human-computer interaction, especially in behavior detection and robot manipulation. Existing mainstream transformer methods of HOI detection are focused on single-stream detection only, e.g., $image \rightarrow HOI(\mathcal{P}_{1})$, or $image \rightarrow HO\rightarrow I(\mathcal{P}_{2})$. Both paths have their own characteristics of concern, so we propose a novel method, using the Secondary path $(\mathcal{P}_{2})$ Guides the Primary path $(\mathcal{P}_{1})$ in Transformers (SGPT). SGPT contains two core modules: the Dual-Path Consistency (DPC) module and the Instance Interaction Attention (IIA) module. DPC keeps human, object and interaction consistent on the dual-path and lets $\mathcal{P}_{2}$ guide $\mathcal{P}_{1}$ to learn more meaningful features. IIA fuses human and object to enhance interaction in $\mathcal{P}_{2}$, which allows instance to constrain interaction. Our proposed dual-path are employed during training, and only the $\mathcal{P}_{1}$ path is used for inference. Hence, SGPT improves generalization without increasing model capacity in HICO-DET and V-COCO datasets compared to the state-of-the-arts. The code of this work is available at https://github.com/visualVk/sgpt.git.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160329","National Natural Science Foundation of China(grant numbers:61906168,U20A20196,62272267,61976191); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LY23F020023); Construction of Hubei Provincial Key Laboratory(grant numbers:2022SDSJ01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160329","","Training;Human computer interaction;Codes;Automation;Fuses;Transformer cores;Transformers","","6","","46","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Flipbot: Learning Continuous Paper Flipping via Coarse-to-Fine Exteroceptive-Proprioceptive Exploration","C. Zhao; C. Jiang; J. Cai; M. Y. Wang; H. Yu; Q. Chen","The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong; The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong; The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong; The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong; The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong; The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","10282","10288","This paper tackles the task of singulating and grasping paper-like deformable objects. We refer to such tasks as paper-flipping. In contrast to manipulating deformable objects that lack compression strength (such as shirts and ropes), minor variations in the physical properties of the paper-like deformable objects significantly impact the results, making manipulation highly challenging. Here, we present Flipbot, a novel solution for flipping paper-like deformable objects. Flipbot allows the robot to capture object physical properties by integrating exteroceptive and proprioceptive perceptions that are indispensable for manipulating deformable objects. Furthermore, by incorporating a proposed coarse-to-fine exploration process, the system is capable of learning the optimal control parameters for effective paper-flipping through proprioceptive and exteroceptive inputs. We deploy our method on a real-world robot with a soft gripper and learn in a self-supervised manner. The resulting policy demonstrates the effectiveness of Flipbot on paper-flipping tasks with various settings beyond the reach of prior studies, including but not limited to flipping pages throughout a book and emptying paper sheets in a box. The code is available here: https://robotll.github.io/Flipbot/.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160774","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160774","","Pneumatic actuators;Propioception;Process control;Optimal control;Grasping;Robustness;Encoding","","","","23","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Continuity-Aware Latent Interframe Information Mining for Reliable UAV Tracking","C. Fu; M. Cai; S. Li; K. Lu; H. Zuo; C. Liu","School of Mechanical Engineering, Tongji University, Shanghai, China; School of Mechanical Engineering, Tongji University, Shanghai, China; School of Mechanical Engineering, Tongji University, Shanghai, China; School of Mechanical Engineering, Tongji University, Shanghai, China; School of Mechanical Engineering, Tongji University, Shanghai, China; College of Mechanical and Electrical Engineering, Harbin Engineering University, Harbin, China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","1327","1333","Unmanned aerial vehicle (UAV) tracking is crucial for autonomous navigation and has broad applications in robotic automation fields. However, reliable UAV tracking remains a challenging task due to various difficulties like frequent occlusion and aspect ratio change. Additionally, most of the existing work mainly focuses on explicit information to improve tracking performance, ignoring potential interframe connections. To address the above issues, this work proposes a novel framework with continuity-aware latent interframe information mining for reliable UAV tracking, i.e., ClimRT. Specifically, a new efficient continuity-aware latent interframe information mining network (ClimNet) is proposed for UAV tracking, which can generate highly-effective latent frame between two adjacent frames. Besides, a novel location-continuity Transformer (LCT) is designed to fully explore continuity-aware spatial-temporal information, thereby markedly enhancing UAV tracking. Extensive qualitative and quantitative experiments on three authoritative aerial benchmarks strongly validate the robustness and reliability of ClimRT in UAV tracking performance. Furthermore, real-world tests on the aerial platform validate its practicability and effectiveness. The code and demo materials are released at https://github.com/vision4robotics/ClimRT.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160673","National Natural Science Foundation of China(grant numbers:62173249); Natural Science Foundation of Shanghai(grant numbers:20ZR1460100); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160673","","Automation;Codes;Benchmark testing;Autonomous aerial vehicles;Transformers;Robustness;Task analysis","","4","","40","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Option-Aware Adversarial Inverse Reinforcement Learning for Robotic Control","J. Chen; T. Lan; V. Aggarwal","School of Industrial Engineering, Purdue University, West Lafayette, IN, USA; Department of Electrical and Computer Engineering, George Washington University, Washington D.C., USA; CS Department, KAUST, Thuwal, Saudi Arabia",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","5902","5908","Hierarchical Imitation Learning (HIL) has been proposed to recover highly-complex behaviors in long-horizon tasks from expert demonstrations by modeling the task hierarchy with the option framework. Existing methods either overlook the causal relationship between the subtask and its corresponding policy or cannot learn the policy in an end-to-end fashion, which leads to suboptimality. In this work, we develop a novel HIL algorithm based on Adversarial Inverse Reinforcement Learning and adapt it with the Expectation-Maximization algorithm in order to directly recover a hierarchical policy from the unannotated demonstrations. Further, we introduce a directed information term to the objective function to enhance the causality and propose a Variational Autoencoder framework for learning with our objectives in an end-to-end fashion. Theoretical justifications and evaluations on challenging robotic control tasks are provided to show the superiority of our algorithm. The codes are available at https://github.com/LucasCJYSDL/HierAIRL.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160374","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160374","","Codes;Automation;Reinforcement learning;Solids;Multitasking;Linear programming;Behavioral sciences","","","","43","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"STPOTR: Simultaneous Human Trajectory and Pose Prediction Using a Non-Autoregressive Transformer for Robot Follow-Ahead","M. Mahdavian; P. Nikdel; M. TaherAhmadi; M. Chen","School of Computing Science, Simon Fraser University (SFU), Burnaby, Canada; School of Computing Science, Simon Fraser University (SFU), Burnaby, Canada; School of Computing Science, Simon Fraser University (SFU), Burnaby, Canada; School of Computing Science, Simon Fraser University (SFU), Burnaby, Canada",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","9959","9965","In this paper, we greatly expand the capability of robots to perform the follow-ahead task and variations of this task through development of a neural network model to predict future human motion from an observed human motion history. We propose a non-autoregressive transformer architecture to leverage its parallel nature for easier training and fast, accurate predictions at test time. The proposed architecture divides human motion prediction into two parts: 1) the human trajectory, which is the 3D positions of the hip joint over time, and 2) the human pose which is the 3D positions of all other joints over time with respect to a fixed hip joint. We propose to make the two predictions simultaneously, as the shared representation can improve the model performance. Therefore, the model consists of two sets of encoders and decoders. First, a multi-head attention module applied to encoder outputs improves human trajectory. Second, another multi-head self-attention module applied to encoder outputs concatenated with decoder outputs facilitates the learning of temporal dependencies. Our model is well-suited for robotic applications in terms of test accuracy and speed, and compares favorably with respect to state-of-the-art methods. We demonstrate the real-world applicability of our work via the Robot Follow-Ahead task, a challenging yet practical case study for our proposed model. The human motion predicted by our model enables the robot follow-ahead in scenarios that require taking detailed human motion into account such as sit-to-stand, stand-to-sit. It also enables simple control policies to trivially generalize to many different variations of human following, such as follow-beside. Our code and data are available at the following Github page: https://github.com/mmahdavian/STPOTR","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160538","CIFAR; SFU; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160538","","Training;Three-dimensional displays;Predictive models;Transformers;Trajectory;Decoding;Task analysis","","14","","34","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Uncertainty Quantification of Collaborative Detection for Self-Driving","S. Su; Y. Li; S. He; S. Han; C. Feng; C. Ding; F. Miao","Department of Computer Science and Engineering, University of Connecticut, Storrs Mansfield, CT, USA; Tandon School of Engineering, New York University, Brooklyn, NY, USA; Department of Computer Science and Engineering, University of Connecticut, Storrs Mansfield, CT, USA; Department of Computer Science and Engineering, University of Connecticut, Storrs Mansfield, CT, USA; Tandon School of Engineering, New York University, Brooklyn, NY, USA; Department of Computer Science and Engineering, University of Connecticut, Storrs Mansfield, CT, USA; Department of Computer Science and Engineering, University of Connecticut, Storrs Mansfield, CT, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","5588","5594","Sharing information between connected and autonomous vehicles (CAVs) fundamentally improves the performance of collaborative object detection for self-driving. However, CAVs still have uncertainties on object detection due to practical challenges, which will affect the later modules in self-driving such as planning and control. Hence, uncertainty quantification is crucial for safety-critical systems such as CAVs. Our work is the first to estimate the uncertainty of collaborative object detection. We propose a novel uncertainty quantification method, called Double- M Quantification, which tailors a moving block bootstrap (MBB) algorithm with direct modeling of the multivariant Gaussian distribution of each corner of the bounding box. Our method captures both the epistemic uncertainty and aleatoric uncertainty with one inference pass based on the offline Double- M training process. And it can be used with different collaborative object detectors. Through experiments on the comprehensive collaborative perception dataset, we show that our Double-M method achieves more than 4× improvement on uncertainty score and more than 3% accuracy improvement, compared with the state-of-the-art uncertainty quantification methods. Our code is public on https://coperception.github.io/double-m-quantification/.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160367","NSF(grant numbers:1932250,1952096,2047354,2121391); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160367","","Training;Uncertainty;Collaboration;Estimation;Object detection;Detectors;Gaussian distribution","","21","","43","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Decoupling Skill Learning from Robotic Control for Generalizable Object Manipulation","K. Lu; B. Yang; B. Wang; A. Markham","Department of Computer Science, University of Oxford, Oxford, UK; Department of Computing, vLAR Group, Hong Kong Polytechnic University, HKSAR; Department of Computer Science, University of Oxford, Oxford, UK; Department of Computer Science, University of Oxford, Oxford, UK",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","864","870","Recent works in robotic manipulation through reinforcement learning (RL) or imitation learning (IL) have shown potential for tackling a range of tasks e.g., opening a drawer or a cupboard. However, these techniques generalize poorly to unseen objects. We conjecture that this is due to the high-dimensional action space for joint control. In this paper, we take an alternative approach and separate the task of learning ‘what to do’ from ‘how to do it’ i.e., whole-body control. We pose the RL problem as one of determining the skill dynamics for a disembodied virtual manipulator interacting with articulated objects. The whole-body robotic kinematic control is optimized to execute the high-dimensional joint motion to reach the goals in the workspace. It does so by solving a quadratic programming (QP) model with robotic singularity and kinematic constraints. Our experiments on manipulating complex articulated objects show that the proposed approach is more generalizable to unseen objects with large intra-class variations, outperforming previous approaches. The evaluation results indicate that our approach generates more compliant robotic motion and outperforms the pure RL and IL baselines in task success rates. Additional information and videos are available at https://kl-research.github.io/decoupskill.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160332","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160332","","Training;Robot motion;Dynamics;Kinematics;Reinforcement learning;Trajectory;Quadratic programming","","3","","33","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"RGB-Event Fusion for Moving Object Detection in Autonomous Driving","Z. Zhou; Z. Wu; R. Boutteau; F. Yang; C. Demonceaux; D. Ginhac","ImViA, University of Burgundy (Université de Bourgogne), Dijon, France; ImViA, University of Burgundy (Université de Bourgogne), Dijon, France; LITIS UR 4108, Univ Rouen Normandie, Rouen, France; ImViA, University of Burgundy (Université de Bourgogne), Dijon, France; ImViA, University of Burgundy (Université de Bourgogne), Dijon, France; ImViA, University of Burgundy (Université de Bourgogne), Dijon, France",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","7808","7815","Moving Object Detection (MOD) is a critical vision task for successfully achieving safe autonomous driving. Despite plausible results of deep learning methods, most existing approaches are only frame-based and may fail to reach reasonable performance when dealing with dynamic traffic participants. Recent advances in sensor technologies, especially the Event camera, can naturally complement the conventional camera approach to better model moving objects. However, event-based works often adopt a pre-defined time window for event representation, and simply integrate it to estimate image intensities from events, neglecting much of the rich temporal information from the available asynchronous events. Therefore, from a new perspective, we propose RENet, a novel RGB-Event fusion Network, that jointly exploits the two complementary modalities to achieve more robust MOD under challenging scenarios for autonomous driving. Specifically, we first design a temporal multi-scale aggregation module to fully leverage event frames from both the RGB exposure time and larger intervals. Then we introduce a bi-directional fusion module to attentively calibrate and fuse multi-modal features. To evaluate the performance of our network, we carefully select and annotate a sub-MOD dataset from the commonly used DSEC dataset. Extensive experiments demonstrate that our proposed method performs significantly better than the state-of-the-art RGB-Event fusion alternatives. The source code and dataset are publicly available at: https://github.com/ZZY-Zhou/RENet.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161563","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161563","","Deep learning;Fuses;Source coding;Object detection;Bidirectional control;Cameras;Robot sensing systems","","21","","56","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Loc-NeRF: Monte Carlo Localization using Neural Radiance Fields","D. Maggio; M. Abate; J. Shi; C. Mario; L. Carlone","Laboratory for Information & Decision Systems, Massachusetts Institute of Technology, Cambridge, MA, USA; Laboratory for Information & Decision Systems, Massachusetts Institute of Technology, Cambridge, MA, USA; Laboratory for Information & Decision Systems, Massachusetts Institute of Technology, Cambridge, MA, USA; Draper Perception and Embedded ML Group, Draper, Cambridge, MA, USA; Laboratory for Information & Decision Systems, Massachusetts Institute of Technology, Cambridge, MA, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","4018","4025","We present Loc-NeRF, a real-time vision-based robot localization approach that combines Monte Carlo localization and Neural Radiance Fields (NeRF). Our system uses a pre-trained NeRF model as the map of an environment and can localize itself in real-time using an RGB camera as the only exteroceptive sensor onboard the robot. While neural radiance fields have seen significant applications for visual rendering in computer vision and graphics, they have found limited use in robotics. Existing approaches for NeRF-based localization require both a good initial pose guess and significant computation, making them impractical for real-time robotics applications. By using Monte Carlo localization as a workhorse to estimate poses using a NeRF map model, LocNeRF is able to perform localization faster than the state of the art and without relying on an initial pose estimate. In addition to testing on synthetic data, we also run our system using real data collected by a Clearpath Jackal UGV and demonstrate for the first time the ability to perform real-time and global localization (albeit over a small workspace) with neural radiance fields. We make our code publicly available at https://github.com/MIT-SPARK/Loc-NeRF.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160782","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160782","","Location awareness;Visualization;Monte Carlo methods;Computational modeling;Robot vision systems;Rendering (computer graphics);Real-time systems","","37","","52","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Visual Language Maps for Robot Navigation","C. Huang; O. Mees; A. Zeng; W. Burgard","University of Freiburg, Germany; University of Freiburg, Germany; Google Research, USA; University of Technology Nuremberg, Germany",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","10608","10615","Grounding language to the visual observations of a navigating agent can be performed using off-the-shelf visual-language models pretrained on Internet-scale data (e.g., image captions). While this is useful for matching images to natural language descriptions of object goals, it remains disjoint from the process of mapping the environment, so that it lacks the spatial precision of classic geometric maps. To address this problem, we propose VLMaps, a spatial map representation that directly fuses pretrained visual-language features with a 3D reconstruction of the physical world. VLMaps can be autonomously built from video feed on robots using standard exploration approaches and enables natural language indexing of the map without additional labeled data. Specifically, when combined with large language models (LLMs), VLMaps can be used to (i) translate natural language commands into a sequence of open-vocabulary navigation goals (which, beyond prior work, can be spatial by construction, e.g., “in between the sofa and the TV” or “three meters to the right of the chair”) directly localized in the map, and (ii) can be shared among multiple robots with different embodiments to generate new obstacle maps on-the-fly (by using a list of obstacle categories). Extensive experiments carried out in simulated and real-world environments show that VLMaps enable navigation according to more complex language instructions than existing methods. Videos are available at https://vlmaps.github.io.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160969","German Federal Ministry of Education and Research(grant numbers:01IS18040B-OML); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160969","","Meters;Visualization;Three-dimensional displays;TV;Navigation;Grounding;Natural languages","","97","","50","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Train What You Know – Precise Pick-and-Place with Transporter Networks","G. Sóti; X. Huang; C. Wurll; B. Hein","Robotics and Autonomous Systems, Institute of Applied Research, Karlsruhe University of Applied Sciences, Karlsruhe, Germany; Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Karlsruhe, Germany; Robotics and Autonomous Systems, Institute of Applied Research, Karlsruhe University of Applied Sciences, Karlsruhe, Germany; Robotics and Autonomous Systems, Institute of Applied Research, Karlsruhe University of Applied Sciences, Karlsruhe, Germany",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","5814","5820","Precise pick-and-place is essential in robotic applications. To this end, we define an exact training method and an iterative inference method that improve pick-and-place precision with Transporter Networks [1]. We conduct a large scale experiment on 8 simulated tasks. A systematic analysis shows, that the proposed modifications have a significant positive effect on model performance. Considering picking and placing independently, our methods achieve up to 60% lower rotation and translation errors than baselines. For the whole pick-and-place process we observe 50% lower rotation errors for most tasks with slight improvements in terms of translation errors. Furthermore, we propose architectural changes that retain model performance and reduce computational costs and time. We validate our methods with an interactive teaching procedure on real hardware. Supplementary material is available at: https://gergely-soti.github.io/p3","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161242","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161242","","Training;Analytical models;Systematics;Shape;Computational modeling;Pipelines;Computational efficiency","","4","","24","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Uncertainty-aware LiDAR Panoptic Segmentation","K. Sirohi; S. Marvi; D. Büscher; W. Burgard","Department of Computer Science, University of Freiburg, Germany; Department of Computer Science, University of Freiburg, Germany; Department of Computer Science, University of Freiburg, Germany; Department of Engineering, Technical University, Nürnberg, Germany",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","8277","8283","Modern autonomous systems often rely on LiDAR scanners, in particular for autonomous driving scenarios. In this context, reliable scene understanding is indispensable. Conventional learning-based methods generally try to achieve maximum performance for this task, while neglecting a proper estimation of the associated uncertainties. In this work, we introduce a novel approach for solving the task of uncertainty- aware panoptic segmentation using LiDAR point clouds. Our proposed EvLPSNet network is the first to solve this task efficiently in a sampling-free manner. It aims to predict per-point semantic and instance segmentations, together with per-point uncertainty estimates. Moreover, it incorporates methods that utilize the uncertainties to improve the segmentation performance. We provide several strong baselines combining state-of- the-art LiDAR panoptic segmentation networks with sampling- free uncertainty estimation techniques. Extensive evaluations show that we achieve the best performance on uncertainty- aware panoptic segmentation quality and calibration compared to these baselines. We make our code available at: https://github.com/kshitij3112/EvLPSNet","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160355","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160355","","Point cloud compression;Laser radar;Uncertainty;Runtime;Three-dimensional displays;Semantics;Estimation","","5","","31","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"4DRadarSLAM: A 4D Imaging Radar SLAM System for Large-scale Environments based on Pose Graph Optimization","J. Zhang; H. Zhuge; Z. Wu; G. Peng; M. Wen; Y. Liu; D. Wang","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","8333","8340","LiDAR-based SLAM may easily fail in adverse weathers (e.g., rain, snow, smoke, fog), while mmWave Radar remains unaffected. However, current researches are primarily focused on 2D $(x,y)$ or 3D ($x, y$, doppler) Radar and 3D LiDAR, while limited work can be found for 4D Radar ($x, y, z$, doppler). As a new entrant to the market with unique characteristics, 4D Radar outputs 3D point cloud with added elevation information, rather than 2D point cloud; compared with 3D LiDAR, 4D Radar has noisier and sparser point cloud, making it more challenging to extract geometric features (edge and plane). In this paper, we propose a full system for 4D Radar SLAM consisting of three modules: 1) Front-end module performs scan-to-scan matching to calculate the odometry based on GICP, considering the probability distribution of each point; 2) Loop detection utilizes multiple rule-based loop pre-filtering steps, followed by an intensity scan context step to identify loop candidates, and odometry check to reject false loop; 3) Back-end builds a pose graph using front-end odometry, loop closure, and optional GPS data. Optimal pose is achieved through $\mathrm{g}2\mathrm{o}$. We conducted real experiments on two platforms and five datasets (ranging from 240m to 4.8km) and will make the code open-source to promote further research at: https://github.com/zhuge2333/4DRadarSLAM","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160670","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160670","","Point cloud compression;Three-dimensional displays;Simultaneous localization and mapping;Laser radar;Rain;Snow;Radar imaging","","31","","39","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"A Probabilistic Framework for Visual Localization in Ambiguous Scenes","F. Zangeneh; L. Bruns; A. Dekel; A. Pieropan; P. Jensfelt","Division of Robotics, Perception and Learning, KTH Royal Institute of Technology, Stockholm, Sweden; Division of Robotics, Perception and Learning, KTH Royal Institute of Technology, Stockholm, Sweden; Univrses AB, Stockholm, Sweden; Univrses AB, Stockholm, Sweden; Division of Robotics, Perception and Learning, KTH Royal Institute of Technology, Stockholm, Sweden",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","3969","3975","Visual localization allows autonomous robots to relocalize when losing track of their pose by matching their current observation with past ones. However, ambiguous scenes pose a challenge for such systems, as repetitive structures can be viewed from many distinct, equally likely camera poses, which means it is not sufficient to produce a single best pose hypothesis. In this work, we propose a probabilistic framework that for a given image predicts the arbitrarily shaped posterior distribution of its camera pose. We do this via a novel formulation of camera pose regression using variational inference, which allows sampling from the predicted distribution. Our method outperforms existing methods on localization in ambiguous scenes. We open-source our approach and share our recorded data sequence at github.com/efreidun/vapor.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160466","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160466","","Location awareness;Visualization;Automation;Robot vision systems;Mixture models;Cameras;Probabilistic logic","","3","","38","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"SHAIL: Safety-Aware Hierarchical Adversarial Imitation Learning for Autonomous Driving in Urban Environments","A. Jamgochian; E. Buehrle; J. Fischer; M. J. Kochenderfer","Stanford University, Stanford, CA, USA; Karlsruhe Institute of Technology, Karlsruhe, Germany; Karlsruhe Institute of Technology, Karlsruhe, Germany; Stanford University, Stanford, CA, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","1530","1536","Designing a safe and human-like decision-making system for an autonomous vehicle is a challenging task. Generative imitation learning is one possible approach for automating policy-building by leveraging both real-world and simulated decisions. Previous work that applies generative imitation learning to autonomous driving policies focuses on learning a low-level controller for simple settings. However, to scale to complex settings, many autonomous driving systems combine fixed, safe, optimization-based low-level controllers with high-level decision-making logic that selects the appropriate task and associated controller. In this paper, we attempt to bridge this gap in complexity by employing Safety-Aware Hierarchical Adversarial Imitation Learning (SHAIL), a method for learning a high-level policy that selects from a set of low-level controller instances in a way that imitates low-level driving data on-policy. We introduce an urban roundabout simulator that controls non-ego vehicles using real data from the Interaction dataset. We then demonstrate empirically that even with simple controller options, our approach can produce better behavior than previous approaches in driver imitation that have difficulty scaling to complex environments. Our implementation is available at https://github.com/sisl/InteractionImitation.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161449","National Science Foundation(grant numbers:DGE-1656518); Federal Ministry for Transport, Innovation and Technology (bmvit); Federal Ministry for Digital, Business and Enterprise (bmdw); Austrian Research Promotion Agency (FFG); Styrian Business Promotion Agency (SFG); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161449","","Technological innovation;Decision making;Semantics;Safety;Behavioral sciences;Trajectory;Task analysis","","9","","32","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"AANet: Aggregation and Alignment Network with Semi-hard Positive Sample Mining for Hierarchical Place Recognition","F. Lu; L. Zhang; S. Dong; B. Chen; C. Yuan","Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences, Chongqing, China; Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; School of Automation, Central South University, Changsha, China; Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","11771","11778","Visual place recognition (VPR) is one of the research hotspots in robotics, which uses visual information to locate robots. Recently, the hierarchical two-stage VPR methods have become popular in this field due to the trade-off between accuracy and efficiency. These methods retrieve the top-k candidate images using the global features in the first stage, then re-rank the candidates by matching the local features in the second stage. However, they usually require additional al-gorithms (e.g. RANSAC) for geometric consistency verification in re-ranking, which is time-consuming. Here we propose a Dynamically Aligning Local Features (DALF) algorithm to align the local features under spatial constraints. It is significantly more efficient than the methods that need geometric consistency verification. We present a unified network capable of extracting global features for retrieving candidates via an aggregation module and aligning local features for re-ranking via the DALF alignment module. We call this network AANet. Meanwhile, many works use the simplest positive samples in triplet for weakly supervised training, which limits the ability of the network to recognize harder positive pairs. To address this issue, we propose a Semi-hard Positive Sample Mining (ShPSM) strategy to select appropriate hard positive images for training more robust VPR networks. Extensive experiments on four benchmark VPR datasets show that the proposed AANet can outperform several state-of-the-art methods with less time consumption. The code is released at https://github.com/Lu-Feng/AANet.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160734","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160734","","Training;Visualization;Codes;Automation;Heuristic algorithms;Computer architecture;Benchmark testing","","4","","43","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Boosting 3D Point Cloud Registration by Transferring Multi-modality Knowledge","M. Yuan; X. Huang; K. Fu; Z. Li; M. Wang","Digital Medical Research Center, School of Basic Medical Science, Fudan University, Shanghai, China; Shanghai artificial intelligence Lab., China; Digital Medical Research Center, School of Basic Medical Science, Fudan University, Shanghai, China; Digital Medical Research Center, School of Basic Medical Science, Fudan University, Shanghai, China; Digital Medical Research Center, School of Basic Medical Science, Fudan University, Shanghai, China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","11734","11741","The recent multi-modality models have achieved great performance in many vision tasks because the extracted features contain the multi-modality knowledge. However, most of the current registration descriptors have only concentrated on local geometric structures. This paper proposes a method to boost point cloud registration accuracy by transferring the multi-modality knowledge of pre-trained multi-modality model to a new descriptor neural network. Different to the previous multi-modality methods that requires both modalities, the proposed method only requires point clouds during inference. Specifically, we propose an ensemble descriptor neural network combining pre-trained sparse convolution branch and a new point-based convolution branch. By fine-tuning on a single modality data, the proposed method achieves new state-of-the-art results on 3DMatch and competitive accuracy on 3DLoMatch and KITTI. The code and the trained model will be released at https://github.com/phdymz/DBENet.git.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161411","National Natural Science Foundation of China(grant numbers:62076070); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161411","","Point cloud compression;Knowledge engineering;Three-dimensional displays;Codes;Convolution;Neural networks;Feature extraction","","10","","42","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Viewer-Centred Surface Completion for Unsupervised Domain Adaptation in 3D Object Detection","D. Tsai; J. S. Berrio; M. Shan; E. Nebot; S. Worrall","Australian Centre for Field Robotics (ACFR) at the University of Sydney, NSW, Australia; Australian Centre for Field Robotics (ACFR) at the University of Sydney, NSW, Australia; Australian Centre for Field Robotics (ACFR) at the University of Sydney, NSW, Australia; Australian Centre for Field Robotics (ACFR) at the University of Sydney, NSW, Australia; Australian Centre for Field Robotics (ACFR) at the University of Sydney, NSW, Australia",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","9346","9353","Every autonomous driving dataset has a different configuration of sensors, originating from distinct geographic regions and covering various scenarios. As a result, 3D detectors tend to overfit the datasets they are trained on. This causes a drastic decrease in accuracy when the detectors are trained on one dataset and tested on another. We observe that lidar scan pattern differences form a large component of this reduction in performance. We address this in our approach, SEE-VCN, by designing a novel viewer-centred surface completion network (VCN) to complete the surfaces of objects of interest within an unsupervised domain adaptation framework, SEE [1]. With SEE-VCN, we obtain a unified representation of objects across datasets, allowing the network to focus on learning geometry, rather than overfitting on scan patterns. By adopting a domain-invariant representation, SEE-VCN can be classed as a multi-target domain adaptation approach where no annotations or re-training is required to obtain 3D detections for new scan patterns. Through extensive experiments, we show that our approach outperforms previous domain adaptation methods in multiple domain adaptation settings. Our code and data are available at https://github.com/darrenjkt/SEE-VCN.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160707","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160707","","Point cloud compression;Knowledge engineering;Geometry;Three-dimensional displays;Laser radar;Codes;Detectors","","4","","57","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"An Architecture for Reactive Mobile Manipulation On-The-Move","B. Burgess-Limerick; C. Lehnert; J. Leitner; P. Corke","the Queensland University of Technology Centre for Robotics (QCR), Brisbane, Australia; the Queensland University of Technology Centre for Robotics (QCR), Brisbane, Australia; LYRO Robotics, Brisbane, Australia; the Queensland University of Technology Centre for Robotics (QCR), Brisbane, Australia",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","1623","1629","We present a generalised architecture for reactive mobile manipulation while a robot's base is in motion toward the next objective in a high-level task. By performing tasks on-the-move, overall cycle time is reduced compared to methods where the base pauses during manipulation. Reactive control of the manipulator enables grasping objects with unpredictable motion while improving robustness against perception errors, environmental disturbances, and inaccurate robot control compared to open-loop, trajectory-based planning approaches. We present an example implementation of the architecture and investigate the performance on a series of pick and place tasks with both static and dynamic objects and compare the performance to baseline methods. Our method demonstrated a real-world success rate of over 99%, failing in only a single trial from 120 attempts with a physical robot system. The architecture is further demonstrated on other mobile manipulator platforms in simulation. Our approach reduces task time by up to 48%, while also improving reliability, gracefulness, and predictability compared to existing architectures for mobile manipulation. See benburgesslimerick.github.io/ManipulationOnTheMove for supplementary materials.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161021","","Robot control;Dynamics;Grasping;Switches;Pressing;Kinematics;Robustness","","9","","29","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"SyreaNet: A Physically Guided Underwater Image Enhancement Framework Integrating Synthetic and Real Images","J. Wen; J. Cui; Z. Zhao; R. Yan; Z. Gao; L. Dou; B. M. Chen","Department of Mechanical and Automation Engineering, Chinese University of Hong Kong, Shatin, N.T., Hong Kong; Department of Mathematics and Theories, Peng Cheng Laboratory, Shen-zhen, China; Department of Mechanical and Automation Engineering, Chinese University of Hong Kong, Shatin, N.T., Hong Kong; Department of Mechanical and Automation Engineering, Chinese University of Hong Kong, Shatin, N.T., Hong Kong; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Automation, Beijing Institute of Technology, Beijing, China; Department of Mechanical and Automation Engineering, Chinese University of Hong Kong, Shatin, N.T., Hong Kong",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","5177","5183","Underwater image enhancement (UIE) is vital for high-level vision-related underwater tasks. Although learning-based UIE methods have made remarkable achievements in recent years, it's still challenging for them to consistently deal with various underwater conditions, which could be caused by: 1) the use of the simplified atmospheric image formation model in UIE may result in severe errors; 2) the network trained solely with synthetic images might have difficulty in generalizing well to real underwater images. In this work, we, for the first time, propose a framework SyreaNet for UIE that integrates both synthetic and real data under the guidance of the revised underwater image formation model and novel domain adaptation (DA) strategies. First, an underwater image synthesis module based on the revised model is proposed. Then, a physically guided disentangled network is designed to predict the clear images by combining both synthetic and real underwater images. The intra- and inter-domain gaps are abridged by fully exchanging the domain knowledge. Extensive experiments demonstrate the superiority of our framework over other state-of-the-art (SOTA) learning-based UIE methods qualitatively and quantitatively. The code and dataset are publicly available at https://github.com/RockWenJJ/SyreaNet.git.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161531","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161531","","Adaptation models;Codes;Automation;Image color analysis;Image synthesis;Atmospheric modeling;Data models","","12","","45","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Allowing Safe Contact in Robotic Goal-Reaching: Planning and Tracking in Operational and Null Spaces","X. Zhu; W. Lian; B. Yuan; C. D. Freeman; M. Tomizuka","Mechanical Systems Control Lab, Mechanical Engineering, UC Berkeley, CA, USA; Intrinsic Innovation LLC, CA, USA; X, The Moonshot Factory, CA, USA; Google Research, CA, USA; Mechanical Systems Control Lab, Mechanical Engineering, UC Berkeley, CA, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","8120","8126","In recent years, impressive results have been achieved in robotic manipulation. While many efforts focus on generating collision-free reference signals, few allow safe contact between the robot bodies and the environment. However, in human's daily manipulation, contact between arms and obstacles is prevalent and even necessary. This paper investigates the benefit of allowing safe contact during robotic manipulation and advocates generating and tracking compliance reference signals in both operational and null spaces. In addition, to optimize the collision-allowed trajectories, we present a hybrid solver that integrates sampling- and gradient-based approaches. We evaluate the proposed method on a goal-reaching task in five simulated and real-world environments with different collisional conditions. We show that allowing safe contact improves goal-reaching efficiency and provides feasible solutions in highly collisional scenarios where collision-free constraints cannot be enforced. Moreover, we demonstrate that planning in null space, in addition to operational space, improves trajectory safety. Further information is available at https://rolandzhu.github.io/ContactReach/.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160649","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160649","","Tracking;Null space;Aerospace electronics;Prediction algorithms;Trajectory;Safety;Planning","","1","","42","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"LATITUDE: Robotic Global Localization with Truncated Dynamic Low-pass Filter in City-scale NeRF","Z. Zhu; Y. Chen; Z. Wu; C. Hou; Y. Shi; C. Li; P. Li; H. Zhao; G. Zhou","Institute for AI Industry Research (AIR), Tsinghua University, China; Institute for AI Industry Research (AIR), Tsinghua University, China; Institute for AI Industry Research (AIR), Tsinghua University, China; Institute for AI Industry Research (AIR), Tsinghua University, China; Institute for AI Industry Research (AIR), Tsinghua University, China; Institute for AI Industry Research (AIR), Tsinghua University, China; Institute for AI Industry Research (AIR), Tsinghua University, China; Institute for AI Industry Research (AIR), Tsinghua University, China; Institute for AI Industry Research (AIR), Tsinghua University, China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","8326","8332","Neural Radiance Fields (NeRFs) have made great success in representing complex 3D scenes with high-resolution details and efficient memory. Nevertheless, current NeRF - based pose estimators have no initial pose prediction and are prone to local optima during optimization. In this paper, we present LATITUDE: Global Localization with Truncated Dynamic Low-pass Filter, which introduces a two-stage localization mechanism in city-scale NeRF. In place recognition stage, we train a regressor through images generated from trained NeRFs, which provides an initial value for global localization. In pose optimization stage, we minimize the residual between the observed image and rendered image by directly optimizing the pose on the tangent plane. To avoid falling into local optimum, we introduce a Truncated Dynamic Low-pass Filter (TDLF) for coarse-to-fine pose registration. We evaluate our method on both synthetic and real-world data and show its potential applications for high-precision navigation in large-scale city scenes. Codes and dataset will be publicly available at https://github.com/jike5/LATITUDE.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161570","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161570","","Location awareness;Three-dimensional displays;Image recognition;Codes;Navigation;Urban areas;Memory management","","9","","31","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"ProgPrompt: Generating Situated Robot Task Plans using Large Language Models","I. Singh; V. Blukis; A. Mousavian; A. Goyal; D. Xu; J. Tremblay; D. Fox; J. Thomason; A. Garg",University of Southern California; NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA; University of Southern California; NVIDIA,2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","11523","11530","Task planning can require defining myriad domain knowledge about the world in which a robot needs to act. To ameliorate that effort, large language models (LLMs) can be used to score potential next actions during task planning, and even generate action sequences directly, given an instruction in natural language with no additional domain information. However, such methods either require enumerating all possible next steps for scoring, or generate free-form text that may contain actions not possible on a given robot in its current context. We present a programmatic LLM prompt structure that enables plan generation functional across situated environments, robot capabilities, and tasks. Our key insight is to prompt the LLM with program-like specifications of the available actions and objects in an environment, as well as with example programs that can be executed. We make concrete recommendations about prompt structure and generation constraints through ablation experiments, demonstrate state of the art success rates in VirtualHome household tasks, and deploy our method on a physical robot arm for tabletop tasks. Website at progprompt.github.io","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161317","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161317","","Automation;Natural languages;Manipulators;Planning;Task analysis","","148","","39","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"LATTE: LAnguage Trajectory TransformEr","A. Bucker; L. Figueredo; S. Haddadin; A. Kapoor; S. Ma; S. Vemprala; R. Bonatti","Technische Universität, München; Technische Universität, München; Technische Universität, München; Microsoft; Microsoft; Microsoft; Microsoft",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","7287","7294","Natural language is one of the most intuitive ways to express human intent. However, translating instructions and commands towards robotic motion generation and deployment in the real world is far from being an easy task. The challenge of combining a robot's inherent low-level geometric and kinodynamic constraints with a human's high-level semantic instructions traditionally is solved using task-specific solutions with little generalizability between hardware platforms, often with the use of static sets of target actions and commands. This work instead proposes a flexible language-based framework that allows a user to modify generic robotic trajectories. Our method leverages pre-trained language models (BERT and CLIP) to encode the user's intent and target objects directly from a free-form text input and scene images, fuses geometrical features generated by a transformer encoder network, and finally outputs trajectories using a transformer decoder, without the need of priors related to the task or robot information. We significantly extend our own previous work presented in [1] by expanding the trajectory parametrization space to 3D and velocity as opposed to just XY movements. In addition, we now train the model to use actual images of the objects in the scene for context (as opposed to textual descriptions), and we evaluate the system in a diverse set of scenarios beyond manipulation, such as aerial and legged robots. Our simulated and real-life experiments demonstrate that our transformer model can successfully follow human intent, modifying the shape and speed of trajectories within multiple environments. Codebase avail-able at: https://github.com/arthurfenderbucker/LaTTe-Language-Trajectory-TransformEr.git.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161068","","Robot motion;Legged locomotion;Three-dimensional displays;Shape;Semantics;Natural languages;Transformers","","14","","44","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Neural Contact Fields: Tracking Extrinsic Contact with Tactile Sensing","C. Higuera; S. Dong; B. Boots; M. Mukadam",University of Washington; University of Washington; University of Washington; Meta AI,2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","12576","12582","We present Neural Contact Fields, a method that brings together neural fields and tactile sensing to address the problem of tracking extrinsic contact between object and environment. Knowing where the external contact occurs is a first step towards methods that can actively control it in facilitating downstream manipulation tasks. Prior work for localizing environmental contacts typically assume a contact type (e.g. point or line), does not capture contact/no-contact transitions, and only works with basic geometric-shaped objects. Neural Contact Fields are the first method that can track arbitrary multi-modal extrinsic contacts without making any assumptions about the contact type. Our key insight is to estimate the probability of contact for any 3D point in the latent space of object's shapes, given vision-based tactile inputs that sense the local motion resulting from the external contact. In experiments, we find that Neural Contact Fields are able to localize multiple contact patches without making any assumptions about the geometry of the contact, and capture contact/no-contact transitions for known categories of objects with unseen shapes in unseen environment configurations. In addition to Neural Contact Fields, we also release our YCB-Extrinsic-Contact dataset of simulated extrinsic contact interactions to enable further research in this area. Project page: https://github.com/carolinahiguera/NCF","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160526","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160526","","Geometry;Three-dimensional displays;Automation;Shape;Tracking;Robot sensing systems;Sensors","","5","","37","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Unsupervised RGB-to-Thermal Domain Adaptation via Multi-Domain Attention Network","L. Gan; C. Lee; S. -J. Chung","Division of Engineering and Applied Science, California Institute of Technology, Pasadena, CA, USA; Division of Engineering and Applied Science, California Institute of Technology, Pasadena, CA, USA; Division of Engineering and Applied Science, California Institute of Technology, Pasadena, CA, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","6014","6020","This work presents a new method for unsupervised thermal image classification and semantic segmentation by transferring knowledge from the RGB domain using a multi-domain attention network. Our method does not require any thermal annotations or co-registered RGB-thermal pairs, enabling robots to perform visual tasks at night and in adverse weather conditions without incurring additional costs of data labeling and registration. Current unsupervised domain adaptation methods look to align global images or features across domains. However, when the domain shift is significantly larger for cross-modal data, not all features can be transferred. We solve this problem by using a shared backbone network that promotes generalization, and domain-specific attention that reduces negative transfer by attending to domain-invariant and easily-transferable features. Our approach outperforms the state-of-the-art RGB-to-thermal adaptation method in classification benchmarks, and is successfully applied to thermal river scene segmentation using only synthetic RGB images. Our code is made publicly available at https://github.com/ganlumomo/thermal-uda-attention.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160872","Ford Motor Company; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160872","","Adaptation models;Visualization;Costs;Semantic segmentation;Performance gain;Rivers;Labeling","","7","","45","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"How Does It Feel? Self-Supervised Costmap Learning for Off-Road Vehicle Traversability","M. G. Castro; S. Triest; W. Wang; J. M. Gregory; F. Sanchez; J. G. Rogers; S. Scherer","Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; DEVCOM Army Research Laboratory, Adelphi, MD, USA; Booz Allen Hamilton, McLean, VA, USA; DEVCOM Army Research Laboratory, Adelphi, MD, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","931","938","Estimating terrain traversability in off-road environments requires reasoning about complex interaction dynamics between the robot and these terrains. However, it is challenging to create informative labels to learn a model in a supervised manner for these interactions. We propose a method that learns to predict traversability costmaps by combining exteroceptive environmental information with proprioceptive terrain interaction feedback in a self-supervised manner. Additionally, we propose a novel way of incorporating robot velocity into the costmap prediction pipeline. We validate our method in multiple short and large-scale navigation tasks on challenging off-road terrains using two different large, all-terrain robots. Our short-scale navigation results show that using our learned costmaps leads to overall smoother navigation, and provides the robot with a more fine-grained understanding of the robot-terrain interactions. Our large-scale navigation trials show that we can reduce the number of interventions by up to 57% compared to an occupancy-based navigation baseline in challenging off-road courses ranging from 400 m to 3150 m. Appendix and full experiment videos can be found in our website: https://mateoguaman.github.io/hdif.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160856","Army Research Laboratory(grant numbers:W911NF-21-2-0152); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160856","","Automation;Navigation;Pipelines;Propioception;Distance measurement;Cognition;Vehicle dynamics","","25","","44","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"FloorplanNet: Learning Topometric Floorplan Matching for Robot Localization","D. Feng; Z. He; J. Hou; S. Schwertfeger; L. Zhang","School of Information Science and Technology, ShanghaiTech University, Shanghai, China; Robotics and Auto-Driving Laboratory (RAL), Baidu Research; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; Robotics and Auto-Driving Laboratory (RAL), Baidu Research",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","6168","6174","Given a building floorplan, humans can localize themselves by matching the observation of the environment with the floorplan using geometric, semantic, and topological clues. Inspired by this insight, this paper proposes a learning- based topometric robot localization method FloorplanNet, which implements a match between a metric robot map and the potentially inaccurate building floorplan in nonuniform scales and different shapes by semantic information. The method uses a novel Graph Neural Network to learn descriptors of nodes from topometric graphs generated from the input maps. We demonstrate that our method can match the 3D point cloud sub-map generated by the robot during the SLAM process with the 2D map. Furthermore, we apply our map-matching algorithm for real-world robot localization. We evaluate our method on several publicly available real-world datasets. Even though our network is solely trained using simulation data, our method demonstrates high robustness and effectiveness in real- world indoor environments and outperforms the existing SOTA map-matching algorithms. We further develop a simulator that automatically creates and annotates the required training data to train our neural networks. The method and simulator are released at: https://github.com/fengdelin/FloorplanNet.git","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160977","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160977","","Three-dimensional displays;Semantics;Buildings;Training data;Robot localization;Robustness;Graph neural networks","","4","","32","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"FogROS2: An Adaptive Platform for Cloud and Fog Robotics Using ROS 2","J. Ichnowski; K. Chen; K. Dharmarajan; S. Adebola; M. Danielczuk; V. Mayoral-Vilches; N. Jha; H. Zhan; E. Llontop; D. Xu; C. Buscaron; J. Kubiatowicz; I. Stoica; J. Gonzalez; K. Goldberg","Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA, USA; Acceleration Robotics, Vitoria, Álava, Spain; Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","5493","5500","Mobility, power, and price points often dictate that robots do not have sufficient computing power on board to run contemporary robot algorithms at desired rates. Cloud computing providers such as AWS, GCP, and Azure offer immense computing power and increasingly low latency on demand, but tapping into that power from a robot is non-trivial. We present FogROS2, an open-source platform to facilitate cloud and fog robotics that is included in the Robot Operating System 2 (ROS 2) distribution. FogROS2 is distinct from its predecessor FogROS1 in 9 ways, including lower latency, overhead, and startup times; improved usability, and additional automation, such as region and computer type selection. Additionally, FogROS2 gains performance, timing, and additional improvements associated with ROS 2. In common robot applications, FogROS2 reduces SLAM latency by 50 %, reduces grasp planning time from 14 s to 1.2 s, and speeds up motion planning 45x. When compared to FogROS1, FogROS2 reduces network utilization by up to 3.8x, improves startup time by 63 %, and network round-trip latency by 97 % for images using video compression. The source code, examples, and documentation for FogROS2 are available at https://github.com/BerkeleyAutomation/FogROS2, and is available through the official ROS 2 repository at https://index.ros.org/p/FogROS2/.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161307","UC Berkeley; CITRIS; NSF; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161307","","Cloud computing;Codes;Automation;Simultaneous localization and mapping;Source coding;Video compression;Planning","","18","","48","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Cerberus: Low-Drift Visual-Inertial-Leg Odometry For Agile Locomotion","S. Yang; Z. Zhang; Z. Fu; Z. Manchester","Department of Mechanical Engineering, Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Mechanical Engineering, Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Mechanical Engineering, Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Mechanical Engineering, Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","4193","4199","We present an open-source Visual-Inertial-Leg Odometry (VILO) state estimation solution for legged robots, called Cerberus, which precisely estimates position on various terrains in real-time using a set of standard sensors, including stereo cameras, IMU, joint encoders, and contact sensors. In addition to estimating robot states, we perform online kinematic parameter calibration and outlier rejection to substantially reduce position drift. Hardware experiments in various indoor and outdoor environments validate that online calibration of kinematic parameters can reduce estimation drift to less than 1% during long-distance, high-speed locomotion. Our drift results are better than those of any other state estimation method using the same set of sensors reported in the literature. Moreover, our state estimator performs well even when the robot experiences large impacts and camera occlusion. The implementation of the state estimator, along with the datasets used to compute our results, is available at https://github.com/ShuoYangRobotics/Cerberus.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160486","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160486","","Legged locomotion;Robot vision systems;Kinematics;Cameras;Real-time systems;Sensors;Calibration","","12","","34","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"STD: Stable Triangle Descriptor for 3D place recognition","C. Yuan; J. Lin; Z. Zou; X. Hong; F. Zhang","Department of Mechanical Engineering, The University of Hong Kong, Hong Kong Special Administrative Region, People's Republic of China; Department of Mechanical Engineering, The University of Hong Kong, Hong Kong Special Administrative Region, People's Republic of China; Department of Mechanical Engineering, The University of Hong Kong, Hong Kong Special Administrative Region, People's Republic of China; School of System Design and Intelligent Manufacturing, Southern University of Science and Technology, Shenzhen, People's Republic of China; Department of Mechanical Engineering, The University of Hong Kong, Hong Kong Special Administrative Region, People's Republic of China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","1897","1903","In this work, we present a novel global descriptor termed stable triangle descriptor (STD) for 3D place recognition. For a triangle, its shape is uniquely determined by the length of the sides or included angles. Moreover, the shape of triangles is completely invariant to rigid transformations. Based on this property, we first design an algorithm to efficiently extract local key points from the 3D point cloud and encode these key points into triangular descriptors. Then, place recognition is achieved by matching the side lengths (and some other information) of the descriptors between point clouds. The point correspondence obtained from the descriptor matching pair can be further used in geometric verification, which greatly improves the accuracy of place recognition. In our experiments, we extensively compare our proposed system against other state-of-the-art systems (i.e., M2DP, Scan Context) on public datasets (i.e., KITTI, NCLT, and Complex-Urban) and our self-collected dataset (with a non-repetitive scanning solid-state LiDAR). All the quantitative results show that STD has stronger adaptability and a great improvement in precision over its counterparts. To share our findings and make contributions to the community, we open source our code on our GitHub: github.com/hku-mars/STD.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160413","Shenzhen Science and Technology Project(grant numbers:JSGG20211029095803004,JSGG20201103100401004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160413","","Point cloud compression;Three-dimensional displays;Laser radar;Codes;Automation;Shape;Databases","","34","","36","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Sequential Bayesian Optimization for Adaptive Informative Path Planning with Multimodal Sensing","J. Ott; E. Balaban; M. J. Kochenderfer","Department of Aeronautics & Astronautics, Stanford University; NASA Ames Research Center; Department of Aeronautics & Astronautics, Stanford University",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","7894","7901","Adaptive Informative Path Planning with Multi-modal Sensing (AIPPMS) considers the problem of an agent equipped with multiple sensors, each with different sensing accuracy and energy costs. The agent's goal is to explore the environment and gather information subject to its resource constraints in unknown, partially observable environments. Previous work has focused on the less general Adaptive Informative Path Planning (AIPP) problem, which considers only the effect of the agent's movement on received observations. The AIPPMS problem adds additional complexity by requiring that the agent reasons jointly about the effects of sensing and movement while balancing resource constraints with information objectives. We formulate the AIPPMS problem as a belief Markov decision process with Gaussian process beliefs and solve it using a sequential Bayesian optimization approach with online planning. Our approach consistently outperforms previous AIPPMS solutions by more than doubling the average reward received in almost every experiment while also reducing the root-mean-square error in the environment belief by 50%. We completely open-source our implementation to aid in further development and comparison.11https://github.com/sisl/SBO_AIPPMS","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160859","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160859","","Costs;Multimodal sensors;Gaussian processes;Markov processes;Robot sensing systems;Path planning;Bayes methods","","8","","34","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Open-vocabulary Queryable Scene Representations for Real World Planning","B. Chen; F. Xia; B. Ichter; K. Rao; K. Gopalakrishnan; M. S. Ryoo; A. Stone; D. Kappler",Everyday Robots; Robotics at Google; Robotics at Google; Robotics at Google; Robotics at Google; Robotics at Google; Robotics at Google; Everyday Robots,2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","11509","11522","Large language models (LLMs) have unlocked new capabilities of task planning from human instructions. However, prior attempts to apply LLMs to real-world robotic tasks are limited by the lack of grounding in the surrounding scene. In this paper, we develop NLMap, an open-vocabulary and queryable scene representation to address this problem. NLMap serves as a framework to gather and integrate contextual information into LLM planners, allowing them to see and query available objects in the scene before generating a context-conditioned plan. NLMap first establishes a natural language queryable scene representation with Visual Language models (VLMs). An LLM based object proposal module parses instructions and proposes involved objects to query the scene representation for object availability and location. An LLM planner then plans with such information about the scene. NLMap allows robots to operate without a fixed list of objects nor executable options, enabling real robot operation unachievable by previous methods. Project website: https://nlmap-saycan.github.io.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161534","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161534","","Visualization;Automation;Grounding;Natural languages;Planning;Proposals;Task analysis","","42","","38","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Deep Underwater Monocular Depth Estimation with Single-Beam Echosounder","H. Liu; M. Roznere; A. Q. Li","Department of Computer Science, Dartmouth College, USA; Department of Computer Science, Dartmouth College, USA; Department of Computer Science, Dartmouth College, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","1090","1097","Underwater depth estimation is essential for safe Autonomous Underwater Vehicles (AUV) navigation. While there has been recent advances in out-of-water monocular depth estimation, it is difficult to apply these methods to the underwater domain due to the lack of well-established datasets with labelled ground truths. In this paper, we propose a novel method for self-supervised underwater monocular depth estimation by leveraging a low-cost single-beam echosounder (SBES). We also present a synthetic dataset for underwater depth estimation to facilitate visual learning research in the underwater domain, available at https://github.com/hdacnw/sbes-depth. We evaluated our method on the proposed dataset with results outperforming previous methods and tested our method in a dataset we collected with an inexpensive AUV. We further investigated the use of SBES as an additional component in our self-supervised method for up-to-scale depth estimation providing insights on next research directions.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161439","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161439","","Visualization;Autonomous underwater vehicles;Automation;Navigation;Estimation;Synthetic data","","1","","51","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Distributed Potential iLQR: Scalable Game-Theoretic Trajectory Planning for Multi-Agent Interactions","Z. Williams; J. Chen; N. Mehr","Department of Electrical and Computer Engineering, University of Illinois Urbana-Champaign, Urbana, IL, USA; Raytheon BBN Technologies in the Network & Cyber Technologies Group, Cambridge, MA, USA; Raytheon BBN Technologies in the Network & Cyber Technologies Group, Cambridge, MA, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","01","07","In this work, we develop a scalable, local tra-jectory optimization algorithm that enables robots to interact with other robots. It has been shown that agents' interactions can be successfully captured in game-theoretic formulations, where the interaction outcome can be best modeled via the equilibria of the underlying dynamic game. However, it is typically challenging to compute equilibria of dynamic games as it involves simultaneously solving a set of coupled optimal control problems. Existing solvers operate in a centralized fashion and do not scale up tractably to multiple interacting agents. We enable scalable distributed game-theoretic planning by leveraging the structure inherent in multi-agent interactions, namely, interactions belonging to the class of dynamic potential games. Since equilibria of dynamic potential games can be found by minimizing a single potential function, we can apply distributed and decentralized control techniques to seek equi-libria of multi-agent interactions in a scalable and distributed manner. We compare the performance of our algorithm with a centralized interactive planner in a number of simulation studies and demonstrate that our algorithm results in better efficiency and scalability. We further evaluate our method in hardware experiments involving multiple quadcopters.11Code Repository - https://github.com/labicon/dp-ilqr","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161176","National Science Foundation(grant numbers:ECCS-2145134 CAREER); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161176","","Trajectory planning;Heuristic algorithms;Scalability;Decentralized control;Optimal control;Games;Hardware","","9","","40","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"AvoidBench: A high-fidelity vision-based obstacle avoidance benchmarking suite for multi-rotors","H. Yu; G. C. H. E. de Croon; C. De Wagter","Faculty of Aerospace Engineering, Delft University of Technology, Delft, HS, The Netherlands; Faculty of Aerospace Engineering, Delft University of Technology, Delft, HS, The Netherlands; Faculty of Aerospace Engineering, Delft University of Technology, Delft, HS, The Netherlands",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","9183","9189","Obstacle avoidance is an essential topic in the field of autonomous drone research. When choosing an avoidance algorithm, many different options are available, each with their advantages and disadvantages. As there is currently no consensus on testing methods, it is quite challenging to compare the performance between algorithms. In this paper, we propose AvoidBench, a benchmarking suite which can evaluate the performance of vision-based obstacle avoidance algorithms by subjecting them to a series of tasks. Thanks to the high fidelity of multi-rotors dynamics from RotorS and virtual scenes of Unity3D, AvoidBench can realize realistic simulated flight experiments. Compared to current drone simulators, we propose and implement both performance and environment metrics to reveal the suitability of obstacle avoidance algorithms for environments of different complexity. To illustrate AvoidBench's usage, we compare three algorithms: Ego-planner, MBPlanner, and Agile-autonomy. The trends observed are validated with real-world obstacle avoidance experiments. Code is available at: https://github.com/tudelft/AvoidBench","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161097","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161097","","Measurement;Heuristic algorithms;Rotors;Benchmark testing;Market research;Data models;Complexity theory","","6","","20","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"STEPS: Joint Self-supervised Nighttime Image Enhancement and Depth Estimation","Y. Zheng; C. Zhong; P. Li; H. -a. Gao; Y. Zheng; B. Jin; L. Wang; H. Zhao; G. Zhou; Q. Zhang; D. Zhao","School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; Department of Computer Science and Technology, Tsinghua University, China; Department of Computer Science and Technology, Tsinghua University, China; Department of Computer Science and Technology, Tsinghua University, China; Institute for AI Industry Research (AIR), Tsinghua University, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; Department of Computer Science and Technology, Tsinghua University, China; Institute for AI Industry Research (AIR), Tsinghua University, China; Institute for AI Industry Research (AIR), Tsinghua University, China; Institute of Automation, Chinese Academy of Sciences, China; Institute of Automation, Chinese Academy of Sciences, China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","4916","4923","Self-supervised depth estimation draws a lot of attention recently as it can promote the 3D sensing capa-bilities of self-driving vehicles. However, it intrinsically relies upon the photometric consistency assumption, which hardly holds during nighttime. Although various supervised night-time image enhancement methods have been proposed, their generalization performance in challenging driving scenarios is not satisfactory. To this end, we propose the first method that jointly learns a nighttime image enhancer and a depth estimator, without using ground truth for either task. Our method tightly entangles two self-supervised tasks using a newly proposed uncertain pixel masking strategy. This strategy originates from the observation that nighttime images not only suffer from underexposed regions but also from overexposed regions. By fitting a bridge-shaped curve to the illumination map distribution, both regions are suppressed and two tasks are bridged naturally. We benchmark the method on two established datasets: nuScenes and RobotCar and demonstrate state-of-the-art performance on both of them. Detailed ablations also reveal the mechanism of our proposal. Last but not least, to mitigate the problem of sparse ground truth of existing datasets, we provide a new photo-realistically enhanced nighttime dataset based upon CARLA. It brings meaningful new challenges to the community. Codes, data, and models are available at https://github.com/ucaszyp/STEPS.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160708","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160708","","Three-dimensional displays;Fitting;Estimation;Lighting;Benchmark testing;Robot sensing systems;Sensors","","19","","56","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Depth Is All You Need for Monocular 3D Detection","D. Park; J. Li; D. Chen; V. Guizilini; A. Gaidon",Toyota Research Institute; Toyota Research Institute; NA; NA; NA,2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","7024","7031","A key contributor to recent progress in 3D detection from single images is monocular depth estimation. Existing methods focus on how to leverage depth explicitly, by generating pseudo-pointclouds or providing attention cues for image features. More recent works leverage depth prediction as a pretraining task and fine-tune the depth representation while training it for 3D detection. However, the adaptation is limited in scale by manual labels. In this work, we propose further aligning the depth representation with the target domain in an unsupervised fashion. Our methods leverage commonly available LiDAR or RGB videos during training time to fine-tune the depth representation, which leads to improved 3D detectors. Especially when using RGB videos, we show that our two-stage training by first generating depth pseudo-labels is critical, because of the inconsistency in loss distribution between the two tasks. With either type of reference data, our multi-task learning approach improves over the state of the art on both KITTI and NuScenes, while matching the test-time complexity of its single-task sub-network. Source code and pretrained models are available on https://github.com/TRI-ML/DD3D.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160483","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160483","","Training;Representation learning;Three-dimensional displays;Laser radar;Source coding;Estimation;Manuals","","3","","57","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"SLAMesh: Real-time LiDAR Simultaneous Localization and Meshing","J. Ruan; B. Li; Y. Wang; Y. Sun","The Hong Kong Polytechnic University, Hong Kong; Zhejiang University of Science and Technology, Hangzhou, China; The Hong Kong Polytechnic University, Hong Kong; The Hong Kong Polytechnic University, Hong Kong",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","3546","3552","Most current LiDAR simultaneous localization and mapping (SLAM) systems build maps in point clouds, which are sparse when zoomed in, even though they seem dense to human eyes. Dense maps are essential for robotic applications, such as map-based navigation. Due to the low memory cost, mesh has become an attractive dense model for mapping in recent years. However, existing methods usually produce mesh maps by using an offline post-processing step to generate mesh maps. This two-step pipeline does not allow these methods to use the built mesh maps online and to enable localization and meshing to benefit each other. To solve this problem, we propose the first CPU-only real-time LiDAR SLAM system that can simultaneously build a mesh map and perform localization against the mesh map. A novel and direct meshing strategy with Gaussian process reconstruction realizes the fast building, registration, and updating of mesh maps. We perform experiments on several public datasets. The results show that our SLAM system can run at around 40Hz. The localization and meshing accuracy also outperforms the state-of-the-art methods, including the TSDF map and Poisson reconstruction. Our code and video demos are available at: https://github.com/lab-sun/SLAMesh.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161425","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161425","","Location awareness;Point cloud compression;Simultaneous localization and mapping;Laser radar;Buildings;Pipelines;Streaming media","","13","","32","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"CenterLineDet: CenterLine Graph Detection for Road Lanes with Vehicle-mounted Sensors by Transformer for HD Map Generation","Z. Xu; Y. Liu; Y. Sun; M. Liu; L. Wang","The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong; The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong; The Hong Kong Polytechnic University, Kowloon, Hong Kong; The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong; The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","3553","3559","With the fast development of autonomous driving technologies, there is an increasing demand for high-definition (HD) maps, which provide reliable and robust prior information about the static part of the traffic environments. As one of the important elements in HD maps, road lane centerline is critical for downstream tasks, such as prediction and planning. Manually annotating centerlines for road lanes in HD maps is labor-intensive, expensive and inefficient, severely restricting the wide applications of autonomous driving systems. Previous work seldom explores the lane centerline detection problem due to the complicated topology and severe overlapping issues of lane centerlines. In this paper, we propose a novel method named CenterLineDet to detect lane centerlines for automatic HD map generation. Our CenterLineDet is trained by imitation learning and can effectively detect the graph of centerlines with vehicle-mounted sensors (i.e., six cameras and one LiDAR) through iterations. Due to the use of the DETR-like transformer network, CenterLineDet can handle complicated graph topology, such as lane intersections. The proposed approach is evaluated on the large-scale public dataset NuScenes. The superiority of our CenterLineDet is demonstrated by the comparative results. Our code, supplementary materials, and video demonstrations are available at https://tonyxuqaq.github.io/projects/CenterLineDet/.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161508","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161508","","Laser radar;Network topology;Roads;Transformers;Topology;Sensors;Planning","","8","","29","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Fast-Grasp'D: Dexterous Multi-finger Grasp Generation Through Differentiable Simulation","D. Turpin; T. Zhong; S. Zhang; G. Zhu; E. Heiden; M. Macklin; S. Tsogkas; S. Dickinson; A. Garg",University of Toronto; University of Toronto; University of Toronto; University of Toronto; Nvidia; Nvidia; University of Toronto; University of Toronto; University of Toronto,2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","8082","8089","Multi-finger grasping relies on high quality training data, which is hard to obtain: human data is hard to transfer and synthetic data relies on simplifying assumptions that reduce grasp quality. By making grasp simulation differentiable, and contact dynamics amenable to gradient-based optimization, we accelerate the search for high-quality grasps with fewer limiting assumptions. We present Grasp'D-1M: a large-scale dataset for multi-finger robotic grasping, synthesized with Fast-Grasp'D, a novel differentiable grasping simulator. Grasp'D-1M contains one million training examples for three robotic hands (three, four and five-fingered), each with multimodal visual inputs (RGB+depth+segmentation, available in mono and stereo). Grasp synthesis with Fast-Grasp'D is 10x faster than GraspIt! [1] and 20x faster than the prior Grasp'D differentiable simulator [2]. Generated grasps are more stable and contact-rich than GraspIt! grasps, regardless of the distance threshold used for contact generation. We validate the usefulness of our dataset by retraining an existing vision-based grasping pipeline [3] on Grasp'D-1M, and showing a dramatic increase in model performance, predicting grasps with 30% more contact, a 33% higher epsilon metric, and 35% lower simulated displacement. Additional details at fast-graspd.github.io.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160314","NSERC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160314","","Training;Visualization;Pipelines;Training data;Grasping;Predictive models;Stability analysis","","10","","61","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"PriorLane: A Prior Knowledge Enhanced Lane Detection Approach Based on Transformer","Q. Qiu; H. Gao; W. Hua; G. Huang; X. He","Zhejiang Lab, Hangzhou, P. R. China; Zhejiang Lab, Hangzhou, P. R. China; Zhejiang Lab, Hangzhou, P. R. China; Zhejiang Lab, Hangzhou, P. R. China; State Key Lab of CAD&CG, College of Computer Science, Zhejiang University, Hangzhou, P. R. China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","5618","5624","Lane detection is one of the fundamental modules in self-driving. In this paper we employ a transformer-only method for lane detection, thus it could benefit from the blooming development of fully vision transformer and achieve the state-of-the-art (SOTA) performance on both CULane and TuSimple benchmarks, by fine-tuning the weight fully pre-trained on large datasets. More importantly, this paper proposes a novel and general framework called PriorLane, which is used to enhance the segmentation performance of the fully vision transformer by introducing the low-cost local prior knowledge. Specifically, PriorLane utilizes an encoder-only transformer to fuse the feature extracted by a pre-trained segmentation model with prior knowledge embeddings. Note that a Knowledge Embedding Alignment (KEA) module is adapted to enhance the fusion performance by aligning the knowledge embedding. Extensive experiments on our Zjlab dataset show that PriorLane outperforms SOTA lane detection methods by a 2.82% mIoU when prior knowledge is employed, and the code will be released at: https://github.com/vincentqqb/PriorLane.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161356","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161356","Lane detection;Vision transformer;Prior knowledge;Fusion;Knowledge Alignment","Image segmentation;Adaptation models;Codes;Automation;Lane detection;Fuses;Benchmark testing","","7","","28","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Self-supervised Multi-frame Monocular Depth Estimation with Pseudo-LiDAR Pose Enhancement","W. Wu; G. Wang; J. Zhong; H. Wang; Z. Liu","MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Key Laboratory of System Control and Information Processing of Ministry of Education, Key Laboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai Engineering Research Center of Intelligent Control and Management, Insititute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Key Laboratory of System Control and Information Processing of Ministry of Education, Key Laboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai Engineering Research Center of Intelligent Control and Management, Insititute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Key Laboratory of System Control and Information Processing of Ministry of Education, Key Laboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai Engineering Research Center of Intelligent Control and Management, Insititute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China; MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","10018","10025","Depth estimation is one of the most important tasks in scene understanding. In the existing joint self-supervised learning approaches of depth-pose estimation, depth estimation and pose estimation networks are independent of each other. They only use the adjacent image frames for pose estimation and lack the use of the estimated geometric information. To enhance the depth-pose association, we propose a monocular multi-frame unsupervised depth estimation framework, named PLPE-Depth. There are a depth estimation network and two pose estimation networks with image input and pseudo-LiDAR input. The main idea of our approach is to use the pseudo-LiDAR reconstructed from the depth map to estimate the pose of adjacent frames. We propose depth re-estimation with a better pose between the image pose and the pseudo-LiDAR pose to improve the accuracy of estimation. Besides, we improve the reconstruction loss and design a pseudo-LiDAR pose enhancement loss to facilitate the joint learning. Our approach enhances the use of the estimated depth information and strengthens the coupling between depth estimation and pose estimation. Experiments on the KITTI dataset show that our depth estimation achieves state-of-the-art performance at low resolution. Our source codes will be released on https://github.com/IRMVLabIPLPE-Depth.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160391","Natural Science Foundation of China(grant numbers:U1913204); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160391","","Couplings;Estimation error;Laser radar;Automation;Source coding;Pose estimation;Estimation","","3","","73","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Learning Neuro-symbolic Programs for Language Guided Robot Manipulation","N. K; H. Singh; V. Bindal; A. Tuli; V. Agrawal; R. Jain; P. Singla; R. Paul",IIT Delhi.; IIT Delhi.; IIT Delhi.; IIT Delhi.; IIT Delhi.; IIT Delhi.; IIT Delhi.; IIT Delhi.,2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","7973","7980","Given a natural language instruction and an input scene, our goal is to train a model to output a manipulation program that can be executed by the robot. Prior approaches for this task possess one of the following limitations: (i) rely on hand-coded symbols for concepts limiting generalization beyond those seen during training [1] (ii) infer action sequences from instructions but require dense sub-goal supervision [2] or (iii) lack semantics required for deeper object-centric reasoning inherent in interpreting complex instructions [3]. In contrast, our approach can handle linguistic as well as perceptual variations, end-to-end trainable and requires no intermediate supervision. The proposed model uses symbolic reasoning constructs that operate on a latent neural object-centric representation, allowing for deeper reasoning over the input scene. Central to our approach is a modular structure consisting of a hierarchical instruction parser and an action simulator to learn disentangled action representations. Our experiments on a simulated environment with a 7-DOF manipulator, consisting of instructions with varying number of steps and scenes with different number of objects, demonstrate that our model is robust to such variations and significantly outperforms baselines, particularly in the generalization settings. The code, dataset and experiment videos are available at https://nsrmp.github.io","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160545","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160545","","Training;Limiting;Semantics;Symbols;Linguistics;Manipulators;Cognition","","4","","36","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Tackling Clutter in Radar Data - Label Generation and Detection Using PointNet++","J. Kopp; D. Kellner; A. Piroli; K. Dietmayer","Institute of Measurement, Control and Microtechnology, Ulm University, Ulm, Germany; BMW AG, Munich, Germany; Institute of Measurement, Control and Microtechnology, Ulm University, Ulm, Germany; Institute of Measurement, Control and Microtechnology, Ulm University, Ulm, Germany",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","1493","1499","Radar sensors employed for environment perception, e.g. in autonomous vehicles, output a lot of unwanted clutter. These points, for which no corresponding real objects exist, are a major source of errors in following processing steps like object detection or tracking. We therefore present two novel neural network setups for identifying clutter. The input data, network architectures and training configuration are adjusted specifically for this task. Special attention is paid to the downsampling of point clouds composed of multiple sensor scans. In an extensive evaluation, the new setups display substantially better performance than existing approaches. Because there is no suitable public data set in which clutter is annotated, we design a method to automatically generate the respective labels. By applying it to existing data with object annotations and releasing its code, we effectively create the first freely available radar clutter data set representing realworld driving scenarios. Code and instructions are accessible at www.github.com/kopp-j/clutter-ds.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160222","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160222","","Training;Point cloud compression;Codes;Annotations;Radar clutter;Radar detection;Radar tracking","","8","","28","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Local Neural Descriptor Fields: Locally Conditioned Object Representations for Manipulation","E. Chun; Y. Du; A. Simeonov; T. Lozano-Perez; L. Kaelbling","Computer Science and Artificial Intelligence Laboratory, MIT, USA; Computer Science and Artificial Intelligence Laboratory, MIT, USA; Computer Science and Artificial Intelligence Laboratory, MIT, USA; Computer Science and Artificial Intelligence Laboratory, MIT, USA; Computer Science and Artificial Intelligence Laboratory, MIT, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","1830","1836","A robot operating in a household environment will see a wide range of unique and unfamiliar objects. While a system could train on many of these, it is infeasible to predict all the objects a robot will see. In this paper, we present a method to generalize object manipulation skills acquired from a limited number of demonstrations, to novel objects from unseen shape categories. Our approach, Local Neural Descriptor Fields (L-NDF), utilizes neural descriptors defined on the local geometry of the object to effectively transfer manipulation demonstrations to novel objects at test time. In doing so, we leverage the local geometry shared between objects to produce a more general manipulation framework. We illustrate the efficacy of our approach in manipulating novel objects in novel poses - both in simulation and in the real world. Project website, videos, and code: https://elchun.github.io/lndf/.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160423","NSF(grant numbers:2214177); AFOSR(grant numbers:FA9550-22-1-0249); ONR MURI(grant numbers:N00014-22-1-2740); ARO(grant numbers:W911NF-23-1-0034); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160423","","Geometry;Codes;Automation;Shape;Grasping;Task analysis;Robots","","7","","30","USGov","4 Jul 2023","","","IEEE","IEEE Conferences"
"Zero-shot Object Detection Based on Dynamic Semantic Vectors","H. Li; J. Mei; J. Zhou; Y. Hu","Research Center for Advanced Computing Systems, Zhejiang Laboratory, Hangzhou, China; Research Center for Advanced Computing Systems, Zhejiang Laboratory, Hangzhou, China; Research Center for Advanced Computing Systems, Zhejiang Laboratory, Hangzhou, China; Research Center for Advanced Computing Systems, Zhejiang Laboratory, Hangzhou, China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","9267","9273","Zero-shot object detection has shown its ability to overcome the problems of data scarcity and novel classes. Existing methods generally utilize static semantic vectors to classify objects and guide the network to map visual features to semantic vectors. However, the distribution of semantic vectors cannot adequately represent visual features, which makes migration from seen to unseen classes difficult. This work explores the dynamic semantic vector method to align the distributions of semantic vectors and visual features. The main challenge is to get a more reasonable distribution of semantic vectors. To address this issue, we proposed a two-way classification branch network and introduce N-pair loss into the dynamic semantic vector optimization process. Experiments on the MS-COCO dataset and SiTi (a real-world autonomous driving dataset collected by us) demonstrate the effectiveness and generalization of our method. Our code is available at https://github.com/HaoyuLizju/ZSD_tcb","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160870","Key Research Project of Zhejiang Lab(grant numbers:2022PC0AC01); National Natural Science Foundation of China(grant numbers:62203424,62176250); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160870","","Visualization;Codes;Automation;Semantics;Object detection;Pareto optimization;Autonomous vehicles","","3","","33","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Structure PLP-SLAM: Efficient Sparse Mapping and Localization using Point, Line and Plane for Monocular, RGB-D and Stereo Cameras","F. Shu; J. Wang; A. Pagani; D. Stricker",DFKI - German Research Center for Artificial Intelligence; DFKI - German Research Center for Artificial Intelligence; DFKI - German Research Center for Artificial Intelligence; DFKI - German Research Center for Artificial Intelligence,2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","2105","2112","This paper presents a visual SLAM system that uses both points and lines for robust camera localization, and simultaneously performs a piece-wise planar reconstruction (PPR) of the environment to provide a structural map in real-time. One of the biggest challenges in parallel tracking and mapping with a monocular camera is to keep the scale consistent when reconstructing the geometric primitives. This further introduces difficulties in graph optimization of the bundle adjustment (BA) step. We solve these problems by proposing several run-time optimizations on the reconstructed lines and planes. Our system is able to run with depth and stereo sensors in addition to the monocular setting. Our proposed SLAM tightly incorporates the semantic and geometric features to boost both frontend pose tracking and backend map optimization. We evaluate our system exhaustively on various datasets, and show that we outperform state-of-the-art methods in terms of trajectory precision. The code of PLP-SLAM has been made available in open-source for the research community (https://github.com/PeterFWS/Structure-PLP-SLAM).","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160452","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160452","","Location awareness;Visualization;Simultaneous localization and mapping;Semantics;Cameras;Sensor systems;Robustness","","30","","55","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Robust Collaborative 3D Object Detection in Presence of Pose Errors","Y. Lu; Q. Li; B. Liu; M. Dianati; C. Feng; S. Chen; Y. Wang","Cooperative Medianet Innovation Center (CMIC), Shanghai Jiao Tong University, China; Nanjing University, China; Meta Reality Labs, USA; University of Warwick, UK; New York University, USA; Cooperative Medianet Innovation Center (CMIC), Shanghai Jiao Tong University, China; Cooperative Medianet Innovation Center (CMIC), Shanghai Jiao Tong University, China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","4812","4818","Collaborative 3D object detection exploits information exchange among multiple agents to enhance accuracy of object detection in presence of sensor impairments such as occlusion. However, in practice, pose estimation errors due to imperfect localization would cause spatial message misalignment and significantly reduce the performance of collaboration. To alleviate adverse impacts of pose errors, we propose CoAlign, a novel hybrid collaboration framework that is robust to unknown pose errors. The proposed solution relies on a novel agent-object pose graph modeling to enhance pose consistency among collaborating agents. Furthermore, we adopt a multiscale data fusion strategy to aggregate intermediate features at multiple spatial resolutions. Comparing with previous works, which require ground-truth pose for training supervision, our proposed CoAlign is more practical since it doesn't require any ground-truth pose supervision in the training and makes no specific assumptions on pose errors. Extensive evaluation of the proposed method is carried out on multiple datasets, certifying that CoAlign significantly reduce relative localization error and achieving the state of art detection performance when pose errors exist. Code are made available for the use of the research community at https://github.com/yifanlu0227/CoAlign.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160546","National Natural Science Foundation of China(grant numbers:62171276); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160546","","Training;Location awareness;Three-dimensional displays;Pose estimation;Collaboration;Object detection;Robustness","","21","","35","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Real-Time Reinforcement Learning for Vision-Based Robotics Utilizing Local and Remote Computers","Y. Wang; G. Vasan; A. R. Mahmood","Department of Computing Science, University of Alberta, Edmonton, AB., Canada; Department of Computing Science, University of Alberta, Edmonton, AB., Canada; Department of Computing Science, University of Alberta, Edmonton, AB., Canada",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","9435","9441","Real-time learning is crucial for robotic agents adapting to ever-changing, non-stationary environments. A common setup for a robotic agent is to have two different computers simultaneously: a resource-limited local computer tethered to the robot and a powerful remote computer connected wirelessly. Given such a setup, it is unclear to what extent the performance of a learning system can be affected by resource limitations and how to efficiently use the wirelessly connected powerful computer to compensate for any performance loss. In this paper, we implement a real-time learning system called the Remote-Local Distributed (ReLoD) system to distribute computations of two deep reinforcement learning (RL) algorithms, Soft Actor-Critic (SAC) and Proximal Policy Optimization (PPO), between a local and a remote computer. The performance of the system is evaluated on two vision-based control tasks developed using a robotic arm and a mobile robot. Our results show that SAC's performance degrades heavily on a resource-limited local computer. Strikingly, when all computations of the learning system are deployed on a remote workstation, SAC fails to compensate for the performance loss, indicating that, without careful consideration, using a powerful remote computer may not result in performance improvement. However, a carefully chosen distribution of computations of SAC consistently and substantially improves its performance on both tasks. On the other hand, the performance of PPO remains largely unaffected by the distribution of computations. In addition, when all computations happen solely on a powerful tethered computer, the performance of our system remains on par with an existing system that is well-tuned for using a single machine. ReLoD is the only publicly available system for real-time RL that applies to multiple robots for vision-based tasks. The source code can be found at https://github.com/rlai-lab/relod","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160684","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160684","","Computers;Learning systems;Source coding;Reinforcement learning;Real-time systems;Remote working;Mobile robots","","1","","23","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Demonstration-Guided Reinforcement Learning with Efficient Exploration for Task Automation of Surgical Robot","T. Huang; K. Chen; B. Li; Y. -H. Liu; Q. Dou","Department of Computer Science and Engineering, The Chinese University of Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong; Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong; Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","4640","4647","Task automation of surgical robot has the potentials to improve surgical efficiency. Recent reinforcement learning (RL) based approaches provide scalable solutions to surgical automation, but typically require extensive data collection to solve a task if no prior knowledge is given. This issue is known as the exploration challenge, which can be alleviated by providing expert demonstrations to an RL agent. Yet, how to make effective use of demonstration data to improve exploration efficiency still remains an open challenge. In this work, we introduce Demonstration-guided EXploration (DEX), an efficient reinforcement learning algorithm that aims to overcome the exploration problem with expert demonstrations for surgical automation. To effectively exploit demonstrations, our method estimates expert-like behaviors with higher values to facilitate productive interactions, and adopts non-parametric regression to enable such guidance at states unobserved in demonstration data. Extensive experiments on 10 surgical manipulation tasks from SurRoL, a comprehensive surgical simulation platform, demonstrate significant improvements in the exploration efficiency and task success rates of our method. Moreover, we also deploy the learned policies to the da Vinci Research Kit (dVRK) platform to show the effectiveness on the real robot. Code is available at https://github.com/med-air/DEX.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160327","CUHK T Stone Robotics Institute, Hong Kong Innovation and Technology Commission(grant numbers:ITS/237/21FP); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160327","","Automation;Medical robotics;Codes;Reinforcement learning;Data collection;Space exploration;Behavioral sciences","","9","","70","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Occlusion-Aware Crowd Navigation Using People as Sensors","Y. -J. Mun; M. Itkina; S. Liu; K. Driggs-Campbell","Electrical and Computer Engineering Department, University of Illinois at Urbana-Champaign, USA; Aeronautics and Astronautics Department, Stanford University, USA; Electrical and Computer Engineering Department, University of Illinois at Urbana-Champaign, USA; Electrical and Computer Engineering Department, University of Illinois at Urbana-Champaign, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","12031","12037","Autonomous navigation in crowded spaces poses a challenge for mobile robots due to the highly dynamic, partially observable environment. Occlusions are highly prevalent in such settings due to a limited sensor field of view and obstructing human agents. Previous work has shown that observed interactive behaviors of human agents can be used to estimate potential obstacles despite occlusions. We propose integrating such social inference techniques into the planning pipeline. We use a variational autoencoder with a specially designed loss function to learn representations that are meaningful for occlusion inference. This work adopts a deep reinforcement learning approach to incorporate the learned representation into occlusion-aware planning. In simulation, our occlusion-aware policy achieves comparable collision avoidance performance to fully observable navigation by estimating agents in occluded spaces. We demonstrate successful policy transfer from simulation to the real-world Turtlebot 2i. To the best of our knowledge, this work is the first to use social occlusion inference for crowd navigation. Our implementation is available at https://github.com/yejimun/PaS_CrowdNav.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160715","National Science Foundation(grant numbers:2143435); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160715","","Deep learning;Pedestrians;Navigation;Pipelines;Reinforcement learning;Robot sensing systems;Feature extraction","","6","","43","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Multi-source Domain Adaptation for Unsupervised Road Defect Segmentation","J. Yu; H. Oh; S. Fichera; P. Paoletti; S. Luo","Department of Engineering, King's College London, Strand, London, United Kingdom; Institute for Information Technology Convergence, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea; Robotiz3D, Sci-Tech Daresbury, United Kingdom; Robotiz3D, Sci-Tech Daresbury, United Kingdom; Department of Engineering, King's College London, Strand, London, United Kingdom",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","5638","5644","The performance of road defect segmentation (a.k.a. pixel-level road defect detection) has been improved alongside with remarkable achievement of deep learning. Those improvements need a large-scale and well-constructed dataset. However, road surface materials or designs vary from country to country, and the patterns of defects are hard to pre-define. In this paper, we propose a novel multi-source domain adaptation method to boost the performance of road defect segmentation on an unlabelled dataset. The proposed method generates multi-source ensembled labels using transferred information from models trained with multiple labelled source domains, which are utilised as supervisory signals for the unlabelled target domain. Furthermore, to reduce the domain gap between each source domain and a target domain, these domains are re-aligned with outlier repositioning to improve the defect segmentation performance. We demonstrate the effectiveness of our proposed method on Cracktree200, CRACK500, CFD, and Crack360 datasets. Experimental results show that the proposed method outperforms the existing unsupervised road defect segmentation methods and achieves competitive performance compared with recent supervised methods. The source code is publicly available on https://github.com/andreYoo/MSDA_RDS.git.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161099","","Deep learning;Adaptation models;Automation;Roads;Source coding;Transfer learning;Maintenance engineering","","4","","37","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"SGDViT: Saliency-Guided Dynamic Vision Transformer for UAV Tracking","L. Yao; C. Fu; S. Li; G. Zheng; J. Ye","School of Mechanical Engineering, Tongji University, Shanghai, China; School of Mechanical Engineering, Tongji University, Shanghai, China; School of Mechanical Engineering, Tongji University, Shanghai, China; Department of Computer Science, the University of Hong Kong, Hong Kong, China; School of Mechanical Engineering, Tongji University, Shanghai, China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","3353","3359","Vision-based object tracking has boosted extensive autonomous applications for unmanned aerial vehicles (UAVs). However, the dynamic changes in flight maneuver and viewpoint encountered in UAV tracking pose significant difficulties, e.g., aspect ratio change, and scale variation. The conventional cross-correlation operation, while commonly used, has limitations in effectively capturing perceptual similarity and incorporates extraneous background information. To mitigate these limitations, this work presents a novel saliency-guided dynamic vision Transformer (SGDViT) for UAV tracking. The proposed method designs a new task-specific object saliency mining network to refine the cross-correlation operation and effectively discriminate foreground and background information. Additionally, a saliency adaptation embedding operation dynamically generates tokens based on initial saliency, thereby reducing the computational complexity of the Transformer architecture. Finally, a lightweight saliency filtering Transformer further refines saliency information and increases the focus on appearance information. The efficacy and robustness of the proposed approach have been thoroughly assessed through experiments on three widely-used UAV tracking benchmarks and real-world scenarios, with results demonstrating its superiority. The source code and demo videos are available at https://github.com/vision4robotics/SGDViT.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161487","Natural Science Foundation of Shanghai(grant numbers:20ZR1460100); National Natural Science Foundation of China(grant numbers:62173249); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161487","","Filtering;Source coding;Design methodology;Transformers;Autonomous aerial vehicles;Robustness;Object tracking","","24","","42","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"RGB-Only Reconstruction of Tabletop Scenes for Collision-Free Manipulator Control","Z. Tang; B. Sundaralingam; J. Tremblay; B. Wen; Y. Yuan; S. Tyree; C. Loop; A. Schwing; S. Birchfield",NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA; University of Illinois Urbana-Champaign; NVIDIA,2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","1778","1785","We present a system for collision-free control of a robot manipulator that uses only RGB views of the world. Perceptual input of a tabletop scene is provided by multiple images of an RGB camera (without depth) that is either handheld or mounted on the robot end effector. A NeRF-like process is used to reconstruct the 3D geometry of the scene, from which the Euclidean full signed distance function (ESDF) is computed. A model predictive control algorithm is then used to control the manipulator to reach a desired pose while avoiding obstacles in the ESDF. We show results on a real dataset collected and annotated in our lab. Our results are also available at https://ngp-mpc.github.io/.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160247","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160247","","Geometry;Three-dimensional displays;Robot vision systems;Control systems;Cameras;Prediction algorithms;End effectors","","4","","59","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"FreDSNet: Joint Monocular Depth and Semantic Segmentation with Fast Fourier Convolutions from Single Panoramas","B. Berenguel-Baeta; J. Bermudez-Cameo; J. J. Guerrero","Instituto de Investigacion en Ingenieria de Aragon, I3A, Universidad de Zaragoza, Spain; Instituto de Investigacion en Ingenieria de Aragon, I3A, Universidad de Zaragoza, Spain; Instituto de Investigacion en Ingenieria de Aragon, I3A, Universidad de Zaragoza, Spain",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","6080","6086","In this work we present FreDSNet, a deep learning solution which obtains semantic 3D understanding of indoor environments from single panoramas. Omnidirectional images reveal task-specific advantages when addressing scene understanding problems due to the 360-degree contextual information about the entire environment they provide. However, the inherent characteristics of the omnidirectional images add additional problems to obtain an accurate detection and segmentation of objects or a good depth estimation. To overcome these problems, we exploit convolutions in the frequential domain obtaining a wider receptive field in each convolutional layer. These convolutions allow to leverage the whole context information from omnidirectional images. FreDSNet is the first network that jointly provides monocular depth estimation and semantic segmentation from a single panoramic image exploiting fast Fourier convolutions. Our experiments show that FreDSNet has slight better performance than the sole state-of-the-art method that obtains both semantic segmentation and depth estimation from panoramas. FreDSNet code is publicly available in https://github.com/Sbrunoberenguel/FreDSNet","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161142","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161142","","Convolutional codes;Training;Visualization;Three-dimensional displays;Semantic segmentation;Semantics;Estimation","","4","","32","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Model- and Acceleration-based Pursuit Controller for High-Performance Autonomous Racing","J. Becker; N. Imholz; L. Schwarzenbach; E. Ghignone; N. Baumann; M. Magno","Center for Project Based Learning, D-ITET, ETH, Zurich; Center for Project Based Learning, D-ITET, ETH, Zurich; Center for Project Based Learning, D-ITET, ETH, Zurich; Center for Project Based Learning, D-ITET, ETH, Zurich; Center for Project Based Learning, D-ITET, ETH, Zurich; Center for Project Based Learning, D-ITET, ETH, Zurich",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","5276","5283","Autonomous racing is a research field gaining large popularity, as it pushes autonomous driving algorithms to their limits and serves as a catalyst for general autonomous driving. For scaled autonomous racing platforms, the computational constraint and complexity often limit the use of Model Predictive Control (MPC). As a consequence, geometric controllers are the most frequently deployed controllers. They prove to be performant while yielding implementation and operational simplicity. Yet, they inherently lack the incorporation of model dynamics, thus limiting the race car to a velocity domain where tire slip can be neglected. This paper presents Model- and Acceleration-based Pursuit (MAP) a high-performance model-based trajectory tracking controller that preserves the simplicity of geometric approaches while leveraging tire dynamics. The proposed algorithm allows accurate tracking of a trajectory at unprecedented velocities compared to State-of-the-Art (SotA) geometric controllers. The MAP controller is experimentally validated and outperforms the reference geometric controller four-fold in terms of lateral tracking error, yielding a tracking error of 0.055 m at tested speeds up to 11 m/s on a scaled racecar. Code: https://github.com/ETH-PBL/MAP-Controller.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161472","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161472","","Adaptation models;Codes;Trajectory tracking;Computational modeling;Tires;Trajectory;System identification","","10","","37","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"A generic diffusion-based approach for 3D human pose prediction in the wild","S. Saadatnejad; A. Rasekh; M. Mofayezi; Y. Medghalchi; S. Rajabzadeh; T. Mordan; A. Alahi","EPFL, Lausanne, Switzerland; EPFL; EPFL; EPFL; EPFL; EPFL, Lausanne, Switzerland; EPFL, Lausanne, Switzerland",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","8246","8253","Predicting 3D human poses in real-world scenarios, also known as human pose forecasting, is inevitably subject to noisy inputs arising from inaccurate 3D pose estimations and occlusions. To address these challenges, we propose a diffusion-based approach that can predict given noisy observations. We frame the prediction task as a denoising problem, where both observation and prediction are considered as a single sequence containing missing elements (whether in the observation or prediction horizon). All missing elements are treated as noise and denoised with our conditional diffusion model. To better handle long-term forecasting horizon, we present a temporal cascaded diffusion model. We demonstrate the benefits of our approach on four publicly available datasets (Human3.6M, HumanEva-I, AMASS, and 3DPW), outperforming the state-of-the-art. Additionally, we show that our framework is generic enough to improve any 3D pose prediction model as a pre-processing step to repair their inputs and a post-processing step to refine their outputs. The code is available online: https://github.com/vita-epfl/DePOSit.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160399","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160399","","Solid modeling;Three-dimensional displays;Computational modeling;Noise reduction;Pose estimation;Predictive models;Maintenance engineering","","16","","64","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"TODE-Trans: Transparent Object Depth Estimation with Transformer","K. Chen; S. Wang; B. Xia; D. Li; Z. Kan; B. Li","University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; Huazhong University of Science and Technology, Wuhan, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","4880","4886","Transparent objects are widely used in industrial automation and daily life. However, robust visual recognition and perception of transparent objects have always been a major challenge. Currently, most commercial-grade depth cameras are still not good at sensing the surfaces of transparent objects due to the refraction and reflection of light. In this work, we present a transformer-based transparent object depth estimation approach from a single RGB-D input. We observe that the global characteristics of the transformer make it easier to extract contextual information to perform depth estimation of transparent areas. In addition, to better enhance the fine-grained features, a feature fusion module (FFM) is designed to assist coherent prediction. Our empirical evidence demonstrates that our model delivers significant improvements in recent popular datasets, e.g., 25% gain on RMSE and 21% gain on REL compared to previous state-of-the-art convolutional-based counterparts in ClearGrasp dataset. Extensive results show that our transformer-based model enables better aggregation of the object's RGB and inaccurate depth information to obtain a better depth representation. Our code and the pre-trained model are available at https://github.com/yuchendoudou/TODE.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160537","National Natural Science Foundation of China(grant numbers:62173314,U2013601,U19B2044,61836011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160537","","Visualization;Automation;Semantics;Estimation;Transformers;Feature extraction;Robot sensing systems","","8","","29","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Visual Pitch and Roll Estimation For Inland Water Vessels","D. Griesser; G. Umlauf; M. O. Franz","Institute for Optical Systems, University of Applied Sciences, Konstanz, Germany; Institute for Optical Systems, University of Applied Sciences, Konstanz, Germany; Institute for Optical Systems, University of Applied Sciences, Konstanz, Germany",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","1961","1967","Motion estimation is an essential element for autonomous vessels. It is used e.g. for lidar motion compensation as well as mapping and detection tasks in a maritime environment. Because the use of gyroscopes is not reliable and a high performance inertial measurement unit is quite expensive, we present an approach for visual pitch and roll estimation that utilizes a convolutional neural network for water segmentation, a stereo system for reconstruction and simple geometry to estimate pitch and roll. The algorithm is validated on a novel, publicly available dataset22https://git.ios.htwg-konstanz.de/dgriesse/constance_orientation_dataset/archive/main/constance_orientation_dataset-main.zip recorded at Lake Constance. Our experiments show that the pitch and roll estimator provides accurate results in comparison to an Xsens IMU sensor. We can further improve the pitch and roll estimation by sensor fusion with a gyroscope. The algorithm is available in its implementation as a ROS node33https://github.com/dionysos4/water_surface_detector.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160460","BMBF(grant numbers:01IS19083A); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160460","","Cloud computing;Surface reconstruction;Estimation;Visual systems;Time measurement;Gyroscopes;Reliability","","2","","31","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"GANet: Goal Area Network for Motion Forecasting","M. Wang; X. Zhu; C. Yu; W. Li; Y. Ma; R. Jin; X. Ren; D. Ren; M. Wang; W. Yang",Peking University; The Chinese University of Hong Kong; Meituan; Inceptio; ShanghaiTech University; National University of Defense Technology; Academy of Military Sciences; Meituan; Fudan University; National University of Defense Technology,2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","1609","1615","Predicting the future motion of road participants is crucial for autonomous driving but is extremely challenging due to staggering motion uncertainty. Recently, most motion forecasting methods resort to the goal-based strategy, i.e., predicting endpoints of motion trajectories as conditions to regress the entire trajectories, so that the search space of solution can be reduced. However, accurate goal coordinates are hard to predict and evaluate. In addition, the point representation of the destination limits the utilization of a rich road context, leading to inaccurate prediction results in many cases. Goal area, i.e., the possible destination area, rather than goal coordinate, could provide a more soft constraint for searching potential trajectories by involving more tolerance and guidance. In view of this, we propose a new goal area-based framework, named Goal Area Network (GANet), for motion forecasting, which models goal areas as preconditions for trajectory prediction, performing more robustly and accurately. Specifically, we propose a GoICrop (Goal Area of Interest) operator to effectively aggregate semantic lane features in goal areas and model actors' future interactions as feedback, which benefits a lot for future trajectory estimations. GANet ranks the 1st on the leaderboard of Argoverse Challenge among all public literature (till the paper submission). Code will be available at https://github.com/kingwmk/GANet.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160468","National Natural Science Foundation of China(grant numbers:91948303-1); National Key R&D Program of China(grant numbers:2021ZD0140301); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160468","","Uncertainty;Codes;Roads;Aggregates;Semantics;Estimation;Predictive models","","34","","32","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"NeRF-Loc: Visual Localization with Conditional Neural Radiance Field","J. Liu; Q. Nie; Y. Liu; C. Wang","Tencent, Shennan Boulevard, Nanshan District, Shenzhen, China; Tencent, Shennan Boulevard, Nanshan District, Shenzhen, China; Tencent, Shennan Boulevard, Nanshan District, Shenzhen, China; Tencent, Shennan Boulevard, Nanshan District, Shenzhen, China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","9385","9392","We propose a novel visual re-localization method based on direct matching between the implicit 3D descriptors and the 2D image with transformer. A conditional neural radiance field(NeRF) is chosen as the 3D scene representation in our pipeline, which supports continuous 3D descriptors generation and neural rendering. By unifying the feature matching and the scene coordinate regression to the same framework, our model learns both generalizable knowledge and scene prior respectively during two training stages. Furthermore, to improve the localization robustness when domain gap exists between training and testing phases, we propose an appearance adaptation layer to explicitly align styles between the 3D model and the query image. Experiments show that our method achieves higher localization accuracy than other learning-based approaches on multiple benchmarks. Code is available at https://github.com/JenningsL/nerf-loc.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161420","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161420","","Location awareness;Training;Adaptation models;Visualization;Solid modeling;Three-dimensional displays;Pipelines","","11","","38","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"PredRecon: A Prediction-boosted Planning Framework for Fast and High-quality Autonomous Aerial Reconstruction","C. Feng; H. Li; F. Gao; B. Zhou; S. Shen","Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; State Key Laboratory of Industrail Control Technology, Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; School of Artificial Intelligence, Sun Yat-Sen University, Zhuhai, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","1207","1213","Autonomous UAV path planning for 3D reconstruction has been actively studied in various applications for high-quality 3D models. However, most existing works have adopted explore-then-exploit, prior-based or exploration-based strategies, demonstrating inefficiency with repeated flight and low autonomy. In this paper, we propose PredRecon, a prediction-boosted planning framework that can autonomously generate paths for high 3D reconstruction quality. We obtain inspiration from humans can roughly infer the complete construction structure from partial observation. Hence, we devise a surface prediction module (SPM) to predict the coarse complete surfaces of the target from the current partial reconstruction. Then, the uncovered surfaces are produced by online volumetric mapping waiting for observation by UAV. Lastly, a hierarchical planner plans motions for 3D reconstruction, which sequentially finds efficient global coverage paths, plans local paths for maximizing the performance of Multi-View Stereo (MVS), and generates smooth trajectories for image-pose pairs acquisition. We conduct benchmarks in the realistic simulator, which validates the performance of PredRecon compared with the classical and state-of-the-art methods. The open-source code is released at https://github.com/HKUST-Aerial-Robotics/PredRecon.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160933","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160933","","Surface reconstruction;Solid modeling;Three-dimensional displays;Benchmark testing;Autonomous aerial vehicles;Surface roughness;Rough surfaces","","8","","37","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation","Z. Liu; H. Tang; A. Amini; X. Yang; H. Mao; D. L. Rus; S. Han","Massachusetts Institute of Technology, Cambridge, MA, USA; Massachusetts Institute of Technology, Cambridge, MA, USA; Massachusetts Institute of Technology, Cambridge, MA, USA; Massachusetts Institute of Technology, Cambridge, MA, USA; OmniML, San Jose, CA, USA; Massachusetts Institute of Technology, Cambridge, MA, USA; Massachusetts Institute of Technology, Cambridge, MA, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","2774","2781","Multi-sensor fusion is essential for an accurate and reliable autonomous driving system. Recent approaches are based on point-level fusion: augmenting the LiDAR point cloud with camera features. However, the camera-to-LiDAR projection throws away the semantic density of camera features, hindering the effectiveness of such methods, especially for semantic-oriented tasks (such as 3D scene segmentation). In this paper, we propose BEVFusion, an efficient and generic multi-task multi-sensor fusion framework. It unifies multi-modal features in the shared bird's-eye view (BEV) representation space, which nicely preserves both geometric and semantic information. To achieve this, we diagnose and lift the key efficiency bottlenecks in the view transformation with optimized BEV pooling, reducing latency by more than $\mathbf{40}\times$. BEVFusion is fundamentally task-agnostic and seamlessly supports different 3D perception tasks with almost no architectural changes. It establishes the new state of the art on the nuScenes benchmark, achieving 1.3% higher mAP and NDS on 3D object detection and 13.6% higher mIoU on BEV map segmentation, with 1.9× lower computation cost. Code to reproduce our results is available at https://github.com/mit-han-lab/bevfusion.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160968","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160968","","Point cloud compression;Three-dimensional displays;Laser radar;Semantics;Object detection;Sensor fusion;Multitasking","","358","","60","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"ViPFormer: Efficient Vision-and-Pointcloud Transformer for Unsupervised Pointcloud Understanding","H. Sun; Y. Wang; X. Cai; X. Bai; D. Li","Department of Computer Science, School of Information, Renmin University of China, Beijing, China; Department of Computer Science, School of Information, Renmin University of China, Beijing, China; Department of Computer Science, School of Information, Renmin University of China, Beijing, China; Department of Computer Science, School of Information, Renmin University of China, Beijing, China; Department of Computer Science, School of Information, Renmin University of China, Beijing, China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","7234","7242","Recently, a growing number of work design unsupervised paradigms for point cloud processing to alleviate the limitation of expensive manual annotation and poor transferability of supervised methods. Among them, CrossPoint follows the contrastive learning framework and exploits image and point cloud data for unsupervised point cloud understanding. Although the promising performance is presented, the unbalanced architecture makes it unnecessarily complex and inefficient. For example, the image branch in CrossPoint is ~8.3x heavier than the point cloud branch leading to higher complexity and latency. To address this problem, in this paper, we propose a lightweight Vision-and-Pointcloud Transformer (ViPFormer) to unify image and point cloud processing in a single architecture. ViPFormer learns in an unsupervised manner by optimizing intra-modal and cross-modal contrastive objectives. Then the pretrained model is transferred to various downstream tasks, including 3D shape classification and semantic segmentation. Experiments on different datasets show ViPFormer surpasses previous state-of-the-art unsupervised methods with higher accuracy, lower model complexity and runtime latency. Finally, the effectiveness of each component in ViPFormer is validated by extensive ablation studies. The implementation of the proposed method is available at https://github.com/auniquesun/ViPFormer.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160658","National Natural Science Foundation of China(grant numbers:61972404,12071478); Public Computing Cloud, Renmin University of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160658","","Point cloud compression;Solid modeling;Runtime;Three-dimensional displays;Shape;Semantic segmentation;Manuals","","3","","62","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Test-time Domain Adaptation for Monocular Depth Estimation","Z. Li; S. Shi; B. Schiele; D. Dai",Max Planck Institute for Informatics; Max Planck Institute for Informatics; Max Planck Institute for Informatics; Max Planck Institute for Informatics,2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","4873","4879","Test-time domain adaptation, i.e. adapting source-pretrained models to the test data on-the-fly in a source-free, unsupervised manner, is a highly practical yet very challenging task. Due to the domain gap between source and target data, inference quality on the target domain can drop drastically especially in terms of absolute scale of depth. In addition, unsupervised adaptation can degrade the model performance due to inaccurate pseudo labels. Furthermore, the model can suffer from catastrophic forgetting when errors are accumulated over time. We propose a test-time domain adaptation framework for monocular depth estimation which achieves both stability and adaptation performance by benefiting from both self-training of the supervised branch and pseudo labels from self-supervised branch, and is able to tackle the above problems: our scale alignment scheme aligns the input features between source and target data, correcting the absolute scale inference on the target domain; with pseudo label consistency check, we select confident pixels thus improve pseudo label quality; regularisation and self-training schemes are applied to help avoid catastrophic forgetting. Without requirement of further supervisions on the target domain, our method adapts the source-trained models to the test data with significant improvements over the direct inference results, providing scale-aware depth map outputs that outperform the state-of-the-arts. Code is available at https://github.com/Malefikus/ada-depth.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161304","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161304","","Training;Deep learning;Adaptation models;Estimation;Performance gain;Data models;Stability analysis","","1","","38","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Guided Conditional Diffusion for Controllable Traffic Simulation","Z. Zhong; D. Rempe; D. Xu; Y. Chen; S. Veer; T. Che; B. Ray; M. Pavone",Columbia University; Stanford University; Georgia Tech; NVIDIA Research; NVIDIA Research; NVIDIA Research; Columbia University; Stanford University,2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","3560","3566","Controllable and realistic traffic simulation is critical for developing and verifying autonomous vehicles. Typical heuristic-based traffic models offer flexible control to make vehicles follow specific trajectories and traffic rules. On the other hand, data-driven approaches generate realistic and human-like behaviors, improving transfer from simulated to real-world traffic. However, to the best of our knowledge, no traffic model offers both controllability and realism. In this work, we develop a conditional diffusion model for controllable traffic generation (CTG) that allows users to control desired properties of trajectories at test time (e.g., reach a goal or follow a speed limit) while maintaining realism and physical feasibility through enforced dynamics. The key technical idea is to leverage recent advances from diffusion modeling and differentiable logic to guide generated trajectories to meet rules defined using signal temporal logic (STL). We further extend guidance to multi-agent settings and enable interaction-based rules like collision avoidance. CTG is extensively evaluated on the nuScenes dataset for diverse and composite rules, demonstrating improvement over strong baselines in terms of the controllability-realism tradeoff. Demo videos can be found at https://aiasd.github.io/ctg.github.io","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161463","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161463","","Pedestrians;Traffic control;Controllability;Trajectory;Behavioral sciences;Vehicle dynamics;Task analysis","","35","","34","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"3DSGrasp: 3D Shape-Completion for Robotic Grasp","S. S. Mohammadi; N. F. Duarte; D. Dimou; Y. Wang; M. Taiana; P. Morerio; A. Dehban; P. Moreno; A. Bernardino; A. Del Bue; J. Santos-Victor","Department of Marine, Electrical, Electronic and Telecommunications Engineering, University of Genoa, Italy; Vislab, ISR|Lisboa, Instituto Superior Técnico, Universidade de Lisboa, Portugal; Vislab, ISR|Lisboa, Instituto Superior Técnico, Universidade de Lisboa, Portugal; Pattern Analysis & Computer Vision (PAVIS), Istituto Italiano di Tecnologia (IIT), Genoa, Italy; Pattern Analysis & Computer Vision (PAVIS), Istituto Italiano di Tecnologia (IIT), Genoa, Italy; Pattern Analysis & Computer Vision (PAVIS), Istituto Italiano di Tecnologia (IIT), Genoa, Italy; Vislab, ISR|Lisboa, Instituto Superior Técnico, Universidade de Lisboa, Portugal; Vislab, ISR|Lisboa, Instituto Superior Técnico, Universidade de Lisboa, Portugal; Vislab, ISR|Lisboa, Instituto Superior Técnico, Universidade de Lisboa, Portugal; Pattern Analysis & Computer Vision (PAVIS), Istituto Italiano di Tecnologia (IIT), Genoa, Italy; Vislab, ISR|Lisboa, Instituto Superior Técnico, Universidade de Lisboa, Portugal",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","3815","3822","Real-world robotic grasping can be done robustly if a complete 3D Point Cloud Data (PCD) of an object is available. However, in practice, PCDs are often incomplete when objects are viewed from few and sparse viewpoints before the grasping action, leading to the generation of wrong or inaccurate grasp poses. We propose a novel grasping strategy, named 3DSGrasp, that predicts the missing geometry from the partial PCD to produce reliable grasp poses. Our proposed PCD completion network is a Transformer-based encoder-decoder network with an Offset-Attention layer. Our network is inherently invariant to the object pose and point's permutation, which generates PCDs that are geometrically consistent and completed properly. Experiments on a wide range of partial PCD show that 3DSGrasp outperforms the best state-of-the-art method on PCD completion tasks and largely improves the grasping success rate in real-world scenarios. The code and dataset are available at: https://github.com/NunoDuarte/3DSGrasp.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160350","FCT-IST(grant numbers:PD/BDI135116/2017); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160350","","Geometry;Point cloud compression;Three-dimensional displays;Shape;Robot vision systems;Grasping;Transformers","","10","","43","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"3D-DAT: 3D-Dataset Annotation Toolkit for Robotic Vision","M. Suchi; B. Neuberger; A. Salykov; J. -B. Weibel; T. Patten; M. Vincze","Vision for Robotics Laboratory, Automation and Control Institute, TU Wien, Vienna, Austria; Vision for Robotics Laboratory, Automation and Control Institute, TU Wien, Vienna, Austria; Vision for Robotics Laboratory, Automation and Control Institute, TU Wien, Vienna, Austria; Vision for Robotics Laboratory, Automation and Control Institute, TU Wien, Vienna, Austria; Vision for Robotics Laboratory, Automation and Control Institute, TU Wien, Vienna, Austria; Vision for Robotics Laboratory, Automation and Control Institute, TU Wien, Vienna, Austria",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","9162","9168","Robots operating in the real world are expected to detect, classify, segment, and estimate the pose of objects to accomplish their task. Modern approaches using deep learning not only require large volumes of data but also pixel-accurate annotations in order to evaluate the performance and therefore safety of these algorithms. At present, publicly available tools for annotating data are scarce and those that are available rely on depth sensors, which excludes their use for transparent, metallic, and general non-Lambertian objects. To address this issue, we present a novel method for creating valuable datasets that can be used in these more difficult cases. Our key contribution is a purely RGB-based scene-level annotation approach that uses a neural radiance field-based method to automatically align objects. A set of user studies demonstrates the accuracy and speed of our approach over a purely manual or depth sensor assisted pipeline. We provide an open-source implementation of each component and a ROS-based recorder for capturing data with a eye-in-hand robot system. Code will be made available at https://github.com/markus-suchi/3D-DAT.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160669","Austrian Science Foundation (FWF); CHIST-ERA(grant numbers:I3967-N30 BURG,I3968-N30 HEAP,I3969-N30 InDex); EC(grant numbers:101017089); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160669","","Training;Solid modeling;Three-dimensional displays;Annotations;Pipelines;Robot vision systems;Manuals","","6","","46","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Deep Masked Graph Matching for Correspondence Identification in Collaborative Perception","P. Gao; Q. Zhu; H. Lu; C. Gan; H. Zhang","Department of Computer Science, University of Maryland, College Park, MD, USA; Department of Computer Science, Colorado School of Mines, Golden, CO, USA; Toyota Motor North America, Mountain View, CA, USA; MIT-IBM Watson AI Lab, Cambridge, MA, USA; Manning College of Information and Computer Sciences (CICS), University of Massachusetts Amherst, Amherst, MA, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","6117","6123","Correspondence identification (CoID) is an essential component for collaborative perception in multi-robot systems, such as connected autonomous vehicles. The goal of CoID is to identify the correspondence of objects observed by multiple robots in their own field of view in order for robots to consistently refer to the same objects. CoID is challenging due to perceptual aliasing, object non-covisibility, and noisy sensing. In this paper, we introduce a novel deep masked graph matching approach to enable CoID and address the challenges. Our approach formulates CoID as a graph matching problem and we design a masked neural network to integrate the multimodal visual, spatial, and GPS information to perform CoID. In addition, we design a new technique to explicitly address object non-covisibility caused by occlusion and the vehicle's limited field of view. We evaluate our approach in a variety of street environments using a high-fidelity simulation that integrates the CARLA and SUMO simulators. The experimental results show that our approach outperforms the previous approaches and achieves state-of-the- art CoID performance in connected autonomous driving applications. Our work is available at: https://github.com/gaopeng5/DMGM.git.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161231","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161231","","Training;Visualization;Neural networks;Collaboration;Robot sensing systems;Sensors;Object recognition","","2","","46","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"GSMR-CNN: An End-to-End Trainable Architecture for Grasping Target Objects from Multi-Object Scenes","V. Holomjova; A. J. Starkey; P. Meißner",University of Aberdeen; University of Aberdeen; University of Aberdeen,2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","3808","3814","We present an end-to-end trainable multi-task model that locates and retrieves target objects from multi-object scenes. The model is an extension of the Siamese Mask R-CNN, which combines the components of Siamese Neural Networks (SNNs) and Mask R-CNN for performing one-shot instance segmentation. The proposed network, called Grasping Siamese Mask R-CNN (GSMR-CNN), extends Siamese Mask R-CNN by adding an additional branch for grasp detection in parallel to the previous object detection head branches. This allows our model to identify a target object with a suitable grasp simultaneously, as opposed to other approaches that require the training of separate models to achieve the same task. The inherent SNN properties enable the proposed model to generalize and recognize new object categories that were not present during training, which is beyond the capabilities of standard object detectors. Moreover, an end-to-end solution uses shared features entailing less model parameters. The model achieves grasp accuracy scores of 92.1 % and 90.4% on the OCID grasp dataset on image-wise and object-wise splits. Physical experiments show that the model achieves a grasp success rate of 76.4 % when correctly identifying the object. Code and models are available at https://github.com/valerijah/grasping_siamese_mask_rcnn","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161009","","Training;Head;Neural networks;Object detection;Grasping;Predictive models;Multitasking","","3","","31","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"DFR-FastMOT: Detection Failure Resistant Tracker for Fast Multi-Object Tracking Based on Sensor Fusion","M. Nagy; M. Khonji; J. Dias; S. Javed","Department of Electrical Engineering and Computer Science, Khalifa University, Abu Dhabi, United Arab Emirates; Department of Electrical Engineering and Computer Science, Khalifa University, Abu Dhabi, United Arab Emirates; Department of Electrical Engineering and Computer Science, Khalifa University, Abu Dhabi, United Arab Emirates; Department of Electrical Engineering and Computer Science, Khalifa University, Abu Dhabi, United Arab Emirates",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","827","833","Persistent multi-object tracking (MOT) allows autonomous vehicles to navigate safely in highly dynamic environments. One of the well-known challenges in MOT is object occlusion when an object becomes unobservant for subsequent frames. The current MOT methods store objects information, such as trajectories, in internal memory to recover the objects after occlusions. However, they retain short-term memory to save computational time and avoid slowing down the MOT method. As a result, they lose track of objects in some occlusion scenarios, particularly long ones. In this paper, we propose DFR-FastMOT, a light MOT method that uses data from a camera and LiDAR sensors and relies on an algebraic formulation for object association and fusion. The formulation boosts the computational time and permits long-term memory that tackles more occlusion scenarios. Our method shows outstanding tracking performance over recent learning and non-learning benchmarks with about 3% and 4% margin in MOTA, respectively. Also, we conduct extensive experiments that simulate occlusion phenomena by employing detectors with various distortion levels. The proposed solution enables superior performance under various distortion levels in detection over current state-of-art methods. Our framework processes about 7,763 frames in 1.48 seconds, which is seven times faster than recent benchmarks. The framework will be available at https://github.com/MohamedNagyMostafa/DFR-FastMOT.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160328","Khalifa University(grant numbers:CIRA-2020-286,KKJRC-2019-Trans1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160328","","Resistance;Laser radar;Navigation;Sensor fusion;Benchmark testing;Sensor phenomena and characterization;Distortion","","6","","37","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Follow The Rules: Online Signal Temporal Logic Tree Search for Guided Imitation Learning in Stochastic Domains","J. J. Aloor; J. Patrikar; P. Kapoor; J. Oh; S. Scherer","Department of Aerospace Engineering, Indian Institute of Technology, Carnegie Mellon University, Kharagpur, WB, India; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","1320","1326","Seamlessly integrating rules in Learning-from-Demonstrations (LfD) policies is a critical requirement to enable the real-world deployment of AI agents. Recently, Signal Temporal Logic (STL) has been shown to be an effective language for encoding rules as spatio-temporal constraints. This work uses Monte Carlo Tree Search (MCTS) as a means of integrating STL specification into a vanilla LfD policy to improve constraint satisfaction. We propose augmenting the MCTS heuristic with STL robustness values to bias the tree search towards branches with higher constraint satisfaction. While the domain-independent method can be applied to integrate STL rules online into any pre-trained LfD algorithm, we choose goal-conditioned Generative Adversarial Imitation Learning as the offline LfD policy. We apply the proposed method to the domain of planning trajectories for General Aviation aircraft around a non-towered airfield. Results using the simulator trained on real-world data showcase 60% improved performance over baseline LfD methods that do not use STL heuristics. [Code]11Codebase: https://github.com/castacks/mcts-stl-planning [Video]22Video: https://youtu.be/fiFCwc57MQs","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160953","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160953","","Monte Carlo methods;Automation;Airports;Robustness;Encoding;Trajectory;Planning","","6","","26","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Edge Grasp Network: A Graph-Based SE(3)-invariant Approach to Grasp Detection","H. Huang; D. Wang; X. Zhu; R. Walters; R. Platt","Khoury College of Computer Science, Northeastern University; Khoury College of Computer Science, Northeastern University; Khoury College of Computer Science, Northeastern University; Khoury College of Computer Science, Northeastern University; Khoury College of Computer Science, Northeastern University",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","3882","3888","Given point cloud input, the problem of 6-DoF grasp pose detection is to identify a set of hand poses in SE(3) from which an object can be successfully grasped. This important problem has many practical applications. Here we propose a novel method and neural network model that enables better grasp success rates relative to what is available in the literature. The method takes standard point cloud data as input and works well with single-view point clouds observed from arbitrary viewing directions. Videos and code are available at https://haojhuang.github.io/edge_grasp_page/.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160728","NSF(grant numbers:1724257,1724191,1763878,1750649,2107256,2134178,2107256); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160728","","Point cloud compression;Codes;Automation;Image edge detection;Neural networks;Grasping;Feature extraction","","10","","35","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Adaptive Risk-Tendency: Nano Drone Navigation in Cluttered Environments with Distributional Reinforcement Learning","C. Liu; E. -J. van Kampen; G. C. H. E. de Croon",Delft University of Technology; Delft University of Technology; Delft University of Technology,2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","7198","7204","Enabling the capability of assessing risk and making risk-aware decisions is essential to applying reinforcement learning to safety-critical robots like drones. In this paper, we investigate a specific case where a nano quadcopter robot learns to navigate an apriori-unknown cluttered environment under partial observability. We present a distributional reinforcement learning framework to generate adaptive risk-tendency policies. Specifically, we propose to use lower tail conditional variance of the learnt return distribution as intrinsic uncertainty estimation, and use exponentially weighted average forecasting (EWAF) to adapt the risk-tendency in accordance with the estimated uncertainty. In simulation and real-world empirical results, we show that (1) the most effective risk-tendency varies across states, (2) the agent with adaptive risk-tendency achieves superior performance compared to risk-neutral policy or risk-averse policy baselines. Code and video can be found in this repository: https://github.com/tudelft/risk-sensitive-rl.git","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160324","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160324","","Adaptation models;Uncertainty;Navigation;Estimation;Reinforcement learning;Tail;Observability","","13","","37","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Suture Thread Spline Reconstruction from Endoscopic Images for Robotic Surgery with Reliability-driven Keypoint Detection","N. Joglekar; F. Liu; R. Orosco; M. Yip","Department of Electrical and Computer Engineering, University of California, San Diego, CA, USA; Department of Electrical and Computer Engineering, University of California, San Diego, CA, USA; Department of Surgery, University of New Mexico, Albuquerque, NM, USA; Department of Electrical and Computer Engineering, University of California, San Diego, CA, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","4747","4753","Automating the process of manipulating and delivering sutures during robotic surgery is a prominent problem at the frontier of surgical robotics, as automating this task can significantly reduce surgeons' fatigue during tele-operated surgery and allow them to spend more time addressing higher-level clinical decision making. Accomplishing autonomous suturing and suture manipulation in the real world requires accurate suture thread localization and reconstruction, the process of creating a 3D shape representation of suture thread from 2D stereo camera surgical image pairs. This is a very challenging problem due to how limited pixel information is available for the threads, as well as their sensitivity to lighting and specular reflection. We present a suture thread reconstruction work that uses reliable keypoints and a Minimum Variation Spline (MVS) smoothing optimization to construct a 3D centerline from a segmented surgical image pair. This method is comparable to previous suture thread reconstruction works, with the possible benefit of increased accuracy of grasping point estimation. Our code and datasets will be available at: https://github.com/ucsdarclab/thread-reconstruction.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161539","NSF(grant numbers:2045803); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161539","","Three-dimensional displays;Smoothing methods;Sensitivity;Shape;Surgery;Reflection;Reliability","","1","","13","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Real-Time Unified Trajectory Planning and Optimal Control for Urban Autonomous Driving Under Static and Dynamic Obstacle Constraints","R. Dempster; M. Al-Sharman; D. Rayside; W. Melek","Department of Electrical and Computer Engineering, University of Waterloo.; Department of Electrical and Computer Engineering, University of Waterloo.; Department of Electrical and Computer Engineering, University of Waterloo.; Department of Mechanical and Mechatronics Engineering, University of Waterloo.",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","10139","10145","Trajectory planning and control have historically been separated into two modules in automated driving stacks. Trajectory planning focuses on higher-level tasks like avoiding obstacles and staying on the road surface, whereas the controller tries its best to follow an ever changing reference trajectory. We argue that this separation is (1) flawed due to the mismatch between planned trajectories and what the controller can feasibly execute, and (2) unnecessary due to the flexibility of the model predictive control (MPC) paradigm. Instead, in this paper, we present a unified MPC-based trajectory planning and control scheme that guarantees feasibility with respect to road boundaries, the static and dynamic environment, and enforces passenger comfort constraints. The scheme is evaluated rigorously in a variety of scenarios focused on proving the effectiveness of the optimal control problem (OCP) design and real-time solution methods. The prototype code will be released at github.com/WATonomous/control.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160577","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160577","","Trajectory planning;Roads;Optimal control;Prototypes;Turning;Real-time systems;Trajectory","","4","","21","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Learning Dexterous Manipulation from Exemplar Object Trajectories and Pre-Grasps","S. Dasari; A. Gupta; V. Kumar","Robotics Institute, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University; Meta AI Research",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","3889","3896","Learning diverse dexterous manipulation behaviors with assorted objects remains an open grand challenge. While policy learning methods offer a powerful avenue to attack this problem, these approaches require extensive per-task engineering and algorithmic tuning. This paper seeks to escape these constraints, by developing a Pre-Grasp informed Dexterous Manipulation (PGDM) framework that generates diverse dexter-ous manipulation behaviors, without any task-specific reasoning or hyper-parameter tuning. At the core of PGDM is a well known robotics construct, pre-grasps (i.e. the hand-pose preparing for object interaction). This simple primitive is enough to induce efficient exploration strategies for acquiring complex dexterous manipulation behaviors. To exhaustively verify these claims, we introduce TCDM, a benchmark of 50 diverse manipulation tasks defined over multiple objects and dexterous manipulators. Tasks for TCDM are defined automatically using exemplar object trajectories from diverse sources (animators, human behaviors, etc.), without any per-task engineering and/or supervision. Our experiments validate that PGDM's exploration strategy, induced by a surprisingly simple ingredient (single pre-grasp pose), matches the performance of prior methods, which require expensive per-task feature/reward engineering, expert supervision, and hyper-parameter tuning. For animated visualizations, trained policies, and project code, please refer to https://pregrasps.github.io/.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161147","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161147","","Learning systems;Visualization;Codes;Automation;Benchmark testing;Manipulators;Cognition","","9","","49","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Analyzing Infrastructure LiDAR Placement with Realistic LiDAR Simulation Library","X. Cai; W. Jiang; R. Xu; W. Zhao; J. Ma; S. Liu; Y. Li","Autonomous Driving Group, Shanghai AI Laboratory, China; Institute of Artificial Intelligence, Beihang University; University of California, Los Angeles, USA; Autonomous Driving Group, Shanghai AI Laboratory, China; University of California, Los Angeles, USA; Institute of Artificial Intelligence, Beihang University; Autonomous Driving Group, Shanghai AI Laboratory, China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","5581","5587","Recently, Vehicle-to-Everything (V2X) cooperative perception has attracted increasing attention. Infrastructure sensors play a critical role in this research field; however, how to find the optimal placement of infrastructure sensors is rarely studied. In this paper, we investigate the problem of infrastructure sensor placement and propose a pipeline that can efficiently and effectively find optimal installation positions for infrastructure sensors in a realistic simulated environment. To better simulate and evaluate LiDAR place-ment, we establish a Realistic LiDAR Simulation library that can simulate the unique characteristics of different popular LiDARs and produce high-fidelity LiDAR point clouds in the CARLA simulator. Through simulating point cloud data in different LiDAR placements, we can evaluate the perception accuracy of these placements using multiple detection models. Then, we analyze the correlation between the point cloud distribution and perception accuracy by calculating the density and uniformity of regions of interest. Experiments show that when using the same number and type of LiDAR, the placement scheme optimized by our proposed method improves the average precision by 15%, compared with the conventional placement scheme in the standard lane scene. We also analyze the correlation between perception performance in the region of interest and LiDAR point cloud distribution and validate that density and uniformity can be indicators of performance. Both the RLS Library and related code will be released at https://github.com/PJLab-ADG/LiDARSimLib-and-Placement-Evaluation.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161027","","Point cloud compression;Measurement;Laser radar;Correlation;Three-dimensional displays;Pipelines;Libraries","","12","","37","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Mono-STAR: Mono-Camera Scene-Level Tracking and Reconstruction","H. Chang; D. M. Ramesh; S. Geng; Y. Gan; A. Boularias","Department of Computer Science, Rutgers University, New Brunswick, USA; Department of Computer Science, Rutgers University, New Brunswick, USA; Department of Computer Science, Rutgers University, New Brunswick, USA; Department of Computer Science, Rutgers University, New Brunswick, USA; Department of Computer Science, Rutgers University, New Brunswick, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","820","826","We present Mono-STAR, the first real-time 3D reconstruction system that simultaneously supports semantic fusion, fast motion tracking, non-rigid object deformation, and topological change under a unified framework. The proposed system solves a new optimization problem incorporating optical-flow-based 2D constraints to deal with fast motion and a novel semantic-aware deformation graph (SAD-graph) for handling topology change. We test the proposed system under various challenging scenes and demonstrate that it significantly outperforms existing state-of-the-art methods. Supplementary material, including videos, can be found at https://github.com/changhaonan/Mono-STAR-demo.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160778","NSF(grant numbers:1846043,2132972); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160778","","Three-dimensional displays;Tracking;Deformation;Semantics;Stars;Real-time systems;Topology","","3","","32","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"DribbleBot: Dynamic Legged Manipulation in the Wild","Y. Ji; G. B. Margolis; P. Agrawal","Improbable AI Lab, Massachusetts Institute of Technology, USA; Improbable AI Lab, Massachusetts Institute of Technology, USA; Improbable AI Lab, Massachusetts Institute of Technology, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","5155","5162","DribbleBot (Dexterous Ball Manipulation with a Legged Robot) is a legged robotic system that can dribble a soccer ball under the same real-world conditions as humans. We identify key challenges of in-the-wild soccer ball manipulation, including variable ball motion dynamics and perception using body-mounted cameras. To overcome these challenges, we propose a domain and task specification for learning viable soccer dribbling behaviors in simulation that transfer to real fields. Our system provides promising evidence that current legged robots are physically capable and adequately sensorized for varied and dynamic real-world soccer play. Video is available at https://gmargoll.github.io/dribblebot.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160325","National Science Foundation(grant numbers:PHY-2019786); United States Air Force Artificial Intelligence Accelerator and was accomplished(grant numbers:FA8750-19-2-1000); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160325","","Legged locomotion;Dynamics;Robot vision systems;Cameras;Trajectory;Behavioral sciences;Quadrupedal robots","","18","","46","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Edge-guided Multi-domain RGB-to-TIR image Translation for Training Vision Tasks with Challenging Labels","D. Lee; M. Jeon; Y. Cho; A. Kim","Dept. of Mechanical Engineering, SNU, Seoul, S. Korea; Institute of Advanced Machines and Design, SNU, Seoul, S.Korea; Dept. Electrical Engineering, Inha University, Incheon, S. Korea; Dept. of Mechanical Engineering, SNU, Seoul, S. Korea",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","8291","8298","The insufficient number of annotated thermal infrared (TIR) image datasets not only hinders TIR image-based deep learning networks to have comparable performances to that of RGB but it also limits the supervised learning of TIR image-based tasks with challenging labels. As a remedy, we propose a modified multidomain RGB to TIR image translation model focused on edge preservation to employ annotated RGB images with challenging labels. Our proposed method not only preserves key details in the original image but also leverages the optimal TIR style code to portray accurate TIR characteristics in the translated image, when applied on both synthetic and real world RGB images. Using our translation model, we have enabled the supervised learning of deep TIR image-based optical flow estimation and object detection that ameliorated in deep TIR optical flow estimation by reduction in end point error by 56.5% on average and the best object detection mAP of 23.9% respectively. Our code and supplementary materials are available at https://github.com/rpmsnu/sRGB-TIR.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161210","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161210","","Training;Codes;Three-dimensional displays;Image edge detection;Semantic segmentation;Supervised learning;Estimation","","9","","42","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Light-Weight Pointcloud Representation with Sparse Gaussian Process","M. Ali; L. Liu","Luddy School of Informatics, Computing, and Engineering, Indiana University, Bloomington, IN, USA; Luddy School of Informatics, Computing, and Engineering, Indiana University, Bloomington, IN, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","4931","4937","This paper presents a framework to represent high-fidelity pointcloud sensor observations for efficient communication and storage. The proposed approach exploits Sparse Gaussian Process to encode pointcloud into a compact form. Our approach represents both the free space and the occupied space using only one model (one 2D Sparse Gaussian Process) instead of the existing two-model framework (two 3D Gaussian Mixture Models). We achieve this by proposing a variance-based sampling technique that effectively discriminates between the free and occupied space. The new representation requires less memory footprint and can be transmitted across limited-bandwidth communication channels. The framework is extensively evaluated in simulation and it is also demonstrated using a real mobile robot equipped with a 3D LiDAR. Our method results in a 70~100 times reduction in the communication rate compared to sending the raw pointcloud. We have provided a demonstration video11Video: https://youtu.be/BQZzXiCFGrM and open-sourced our code 22Code: https://github.com/mahmoud-a-ali/vsgp_pcl.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161111","National Science Foundation(grant numbers:2006886,2047169); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161111","","Solid modeling;Three-dimensional displays;Laser radar;Memory management;Collaboration;Communication channels;Robot sensing systems","","6","","30","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Self-Adaptive Driving in Nonstationary Environments through Conjectural Online Lookahead Adaptation","T. Li; H. Lei; Q. Zhu","Department of Electrical and Computer Engineering, New York University, Brooklyn, NY, USA; Department of Electrical and Computer Engineering, New York University, Brooklyn, NY, USA; Department of Electrical and Computer Engineering, New York University, Brooklyn, NY, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","7205","7211","Powered by deep representation learning, re-inforcement learning (RL) provides an end-to-end learning framework capable of solving self-driving (SD) tasks without manual designs. However, time-varying nonstationary environments cause proficient but specialized RL policies to fail at execution time. For example, an RL-based SD policy trained under sunny days does not generalize well to rainy weather. Even though meta learning enables the RL agent to adapt to new tasks/environments, its offline operation fails to equip the agent with online adaptation ability when facing nonstationary environments. This work proposes an online meta reinforcement learning algorithm based on the conjectural online lookahead adaptation (COLA). COLA determines the online adaptation at every step by maximizing the agent's conjecture of the future performance in a lookahead horizon. Experimental results demonstrate that under dynamically changing weather and lighting conditions, the COLA-based self-adaptive driving outperforms the baseline policies regarding online adaptability. A demo video, source code, and appendixes are available at https://github.com/Panshark/COLA","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161368","National Science Foundation(grant numbers:ECCS-1847056); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161368","","Representation learning;Metalearning;Heuristic algorithms;Source coding;Lighting;Reinforcement learning;Manuals","","1","","40","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"FewSOL: A Dataset for Few-Shot Object Learning in Robotic Environments","J. J. P; Y. -W. Chao; Y. Xiang","Department of Computer Science, University of Texas at Dallas, Richardson, TX, USA; NVIDIA, Seattle, WA, USA; Department of Computer Science, University of Texas at Dallas, Richardson, TX, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","9140","9146","We introduce the Few-Shot Object Learning (FEWSOL) dataset for object recognition with a few images per object. We captured 336 real-world objects with 9 RGB-D images per object from different views. Fewsol has object segmentation masks, poses, and attributes. In addition, synthetic images generated using 330 3D object models are used to augment the dataset. We investigated (i) few-shot object classification and (ii) joint object segmentation and few-shot classification with state-of-the-art methods for few-shot learning and meta-learning using our dataset. The evaluation results show the presence of a large margin to be improved for few-shot object classification in robotic environments, and our dataset can be used to study and enhance few-shot object recognition for robot perception 11Dataset and code available at https://irvlutd.github.io/FewSOL.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161143","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161143","","Metalearning;Solid modeling;Three-dimensional displays;Codes;Automation;Object segmentation;Object recognition","","4","","39","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Density Planner: Minimizing Collision Risk in Motion Planning with Dynamic Obstacles using Density-based Reachability","L. Lützow; Y. Meng; A. C. Armijos; C. Fan","Department of Informatics, Technical University of Munich, Germany; Department of Aeronautics and Astronautics, Massachusetts Institute of Technology, USA; Division of Systems Engineering, Boston University, USA; Department of Aeronautics and Astronautics, Massachusetts Institute of Technology, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","7886","7893","Uncertainty is prevalent in robotics. Due to measurement noise and complex dynamics, we cannot estimate the exact system and environment state. Since conservative motion planners are not guaranteed to find a safe control strategy in a crowded, uncertain environment, we propose a density-based method. Our approach uses a neural network and the Liouville equation to learn the density evolution for a system with an uncertain initial state. We can plan for feasible and probably safe trajectories by applying a gradient-based optimization procedure to minimize the collision risk. We conduct motion planning experiments on simulated environments and environments generated from real-world data and outperform baseline methods such as model predictive control and nonlinear programming. While our method requires offline planning, the online run time is 100 times smaller compared to model predictive control. The code and supplementary material can be found at https://mit-realm.github.io/density_planner/.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161378","German Academic Exchange Service; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161378","","Uncertainty;Dynamics;Programming;Predictive models;Planning;Trajectory;Noise measurement","","3","","36","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Neural-Kalman GNSS/INS Navigation for Precision Agriculture","Y. Du; S. S. Saha; S. S. Sandha; A. Lovekin; J. Wu; S. Siddharth; M. Chowdhary; M. K. Jawed; M. Srivastava","Department of Mechanical & Aerospace Engineering, University of California, Los Angeles, Los Angeles, CA, United States; Department of Electrical & Computer Engineering, University of California, Los Angeles, Los Angeles, CA, United States; Amazon, Seattle, WA, United States; Department of Mechanical & Aerospace Engineering, University of California, Los Angeles, Los Angeles, CA, United States; Department of Electrical & Computer Engineering, University of California, Los Angeles, Los Angeles, CA, United States; STMicroelectronics, Santa Clara, CA, United States; STMicroelectronics, Santa Clara, CA, United States; Department of Mechanical & Aerospace Engineering, University of California, Los Angeles, Los Angeles, CA, United States; Department of Electrical & Computer Engineering, University of California, Los Angeles, Los Angeles, CA, United States",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","9622","9629","Precision agricultural robots require high-resolution navigation solutions. In this paper, we introduce a robust neural-inertial sequence learning approach to track such robots with ultra-intermittent GNSS updates. First, we propose an ultra-lightweight neural-Kalman filter that can track agricultural robots within 1.4 m (1.4–5.8× better than competing techniques), while tracking within 2.75 m with 20 mins of GPS outage. Second, we introduce a user-friendly video-processing toolbox to generate high-resolution (±5 cm) position data for fine-tuning pre-trained neural-inertial models in the field. Third, we introduce the first and largest (6.5 hours, 4.5 km, 3 phases) public neural-inertial navigation dataset for precision agricultural robots. The dataset, toolbox, and code are available at: https://github.com/nesl/agrobot.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161351","Semiconductor Research Corporation (SRC); DARPA; Army Research Laboratory (ARL)(grant numbers:W911NF-17-2-0196); Air Force Office of Scientific Research (AFOSR)(grant numbers:FA9550-22-1-0193); National Science Foundation (NSF)(grant numbers:CNS-1705135,CNS-1822935,IIS-1925360,CNS-2213839,CMMI-2047663); King Abdullah University of Science and Technology (KAUST); National Institute of Food and Agriculture(grant numbers:2021-67022-342000,2021-67022-34200); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161351","","Agricultural robots;Global navigation satellite system;Codes;Automation;Navigation;Data models;Agriculture","","5","","75","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Dextrous Tactile In-Hand Manipulation Using a Modular Reinforcement Learning Architecture","J. Pitz; L. Röstel; L. Sievers; B. Bäuml","DLR Institute of Robotics and Mechatronics, Technical University of Munich; DLR Institute of Robotics and Mechatronics, Technical University of Munich; DLR Institute of Robotics and Mechatronics, Technical University of Munich; DLR Institute of Robotics and Mechatronics, Technical University of Munich",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","1852","1858","Dextrous in-hand manipulation with a multi-fingered robotic hand is a challenging task, esp. when performed with the hand oriented upside down, demanding permanent force-closure, and when no external sensors are used. For the task of reorienting an object to a given goal orientation (vs. infinitely spinning it around an axis), the lack of external sensors is an additional fundamental challenge as the state of the object has to be estimated all the time, e.g., to detect when the goal is reached. In this paper, we show that the task of reorienting a cube to any of the 24 possible goal orientations in a π/2-raster using the torque-controlled DLR-Hand II is possible. The task is learned in simulation using a modular deep reinforcement learning architecture: the actual policy has only a small observation time window of 0.5 s but gets the cube state as an explicit input which is estimated via a deep differentiable particle filter trained on data generated by running the policy. In simulation, we reach a success rate of 92% while applying significant domain randomization. Via zero-shot Sim2Real-transfer on the real robotic system, all 24 goal orientations can be reached with a high success rate. (Web: dlr-alr.github.io/dlr-tactile-manipulation)","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160756","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160756","","Training;Deep learning;Uncertainty;Reinforcement learning;Particle filters;Sensors;Service-oriented architecture","","14","","15","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Deep Interactive Full Transformer Framework for Point Cloud Registration","G. Chen; M. Wang; Q. Zhang; L. Yuan; T. Liu; Y. Yue","School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; Pecheng Lab, School of Electrical and Computer Engineering at Peking University, Shenzhen, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","2825","2832","Point cloud registration is a crucial technology in the fields of robotics and computer vision. Despite the significant advances in point cloud registration enabled by Transformer-based methods, limitations persist due to indistinct feature extraction, noise sensitivity, and outlier handling. These limitations stem from three factors: (1) the inefficiency of convolutional neural networks (CNNs) to capture global relationships due to their local receptive fields, resulting in extracted features susceptible to noise; (2) the shallow-wide architecture of Transformers, coupled with a lack of positional information, leading to inefficient information interaction and indistinct feature extraction; and (3) the omission of geometrical compatibility leads to ambiguous identification of incorrect correspondences. To overcome these limitations, we propose the Deep Interactive Full Transformer (DIFT) network for point cloud registration, which consists of three key components: (1) a Point Cloud Structure Extractor (PSE) for modeling global relationships and retrieving structural information; (2) a Point Feature Transformer (PFT) for establishing comprehensive associations and directly learning the relative positions between points; and (3) a Geometric Matching-based Correspondence Confidence Evaluation (GMCCE) method for measuring spatial consistency and estimating correspondence confidence. Experimental results on ModelNet40 and 3DMatch datasets demonstrate the superior performance of our proposed method compared to existing state-of-the-art methods. The code for our method is publicly available at https://github.com/CGuangyan-BIT/DIFT.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160863","National Natural Science Foundation of China(grant numbers:62003039,62233002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160863","","Point cloud compression;Sensitivity;Position measurement;Transformers;Feature extraction;Robot sensing systems;Robustness","","4","","49","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"kollagen: A Collaborative SLAM Pose Graph Generator","R. C. Sundin; D. Umsonst","Ericsson Research, Stockholm, Sweden; Ericsson Research, Stockholm, Sweden",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","9176","9182","In this paper, we address the lack of datasets for – and the issue of reproducibility in – collaborative SLAM pose graph optimizers by providing a novel pose graph generator. Our pose graph generator, kollagen, is based on a random walk in a planar grid world, similar to the popular M3500 dataset for single agent SLAM. It is simple to use and the user can set several parameters, e.g., the number of agents, the number of nodes, loop closure generation probabilities, and standard deviations of the measurement noise. Furthermore, a qualitative execution time analysis of our pose graph generator showcases the speed of the generator in the tunable parameters. In addition to the pose graph generator, our paper provides two example datasets that researchers can use out-of-the-box to evaluate their algorithms. One of the datasets has 8 agents, each with 3500 nodes, and 67645 constraints in the pose graphs, while the other has 5 agents, each with 10000 nodes, and 76134 constraints. In addition, we show that current state-of-the-art pose graph optimizers are able to process our generated datasets and perform pose graph optimization. The data generator can be found at https://github.com/EricssonResearch/kollagen.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160514","","Simultaneous localization and mapping;Current measurement;Collaboration;Generators;Reproducibility of results;Noise measurement;Odometry","","","","20","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Legs as Manipulator: Pushing Quadrupedal Agility Beyond Locomotion","X. Cheng; A. Kumar; D. Pathak",Carnegie Mellon University; UC Berkeley; Carnegie Mellon University,2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","5106","5112","Locomotion has seen dramatic progress for walking or running across challenging terrains. However, robotic quadrupeds are still far behind their biological counterparts, such as dogs, which display a variety of agile skills and can use the legs beyond locomotion to perform several basic manipulation tasks like interacting with objects and climbing. In this paper, we take a step towards bridging this gap by training quadruped robots not only to walk but also to use the front legs to climb walls, press buttons, and perform object interaction in the real world. To handle this challenging optimization, we decouple the skill learning broadly into locomotion, which involves anything that involves movement whether via walking or climbing a wall, and manipulation, which involves using one leg to interact while balancing on the other three legs. These skills are trained in simulation using curriculum and transferred to the real world using our proposed sim2real variant that builds upon recent locomotion success. Finally, we combine these skills into a robust long-term plan by learning a behavior tree that encodes a high-level task hierarchy from one clean expert demonstration. We evaluate our method in both simulation and real-world showing successful executions of both short as well as long-range tasks and how robustness helps confront external perturbations. Videos at https://robot-skills.github.io/.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161470","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161470","","Legged locomotion;Training;Presses;Perturbation methods;Pressing;Robustness;Quadrupedal robots","","23","","36","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Shunted Collision Avoidance for Multi-UAV Motion Planning with Posture Constraints","G. Xu; D. Zhu; J. Cao; Y. Liu; J. Yang","Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Research and Development Academy of Machinery Equipment, Beijing, China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","3671","3678","This paper investigates the problem of fixed-wing unmanned aerial vehicles (UAV s) motion planning with posture constraints and the problem of the more general symmetrical situations where UAVs have more than one optimal solution. In this paper, the posture constraints are formulated in the 3D Dubins method, and the symmetrical situations are overcome by a more collaborative strategy called the shunted strategy. The effectiveness of the proposed method has been validated by conducting extensive simulation experiments. Meanwhile, we compared the proposed method with the other state-of-the-art methods, and the comparison results show that the proposed method advances the previous works. Finally, the practicability of the proposed algorithm was analyzed by the statistic in computational cost. The source code of our method can be available at https://github.com/wuuya1/SCA.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160979","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160979","Collision avoidance;multi-UAV motion plan-ning;posture constraints","Three-dimensional displays;Automation;Computational modeling;Source coding;Collaboration;Autonomous aerial vehicles;Turning","","2","","29","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Model Predictive Optimized Path Integral Strategies","D. M. Asmar; R. Senanayake; S. Manuel; M. J. Kochenderfer","Stanford Intelligent Systems Laboratory (SISL), Stanford University; Stanford Intelligent Systems Laboratory (SISL), Stanford University; Stanford Intelligent Systems Laboratory (SISL), Stanford University; Stanford Intelligent Systems Laboratory (SISL), Stanford University",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","3182","3188","We generalize the derivation of model predictive path integral control (MPPI) to allow for a single joint distribution across controls in the control sequence. This reformation allows for the implementation of adaptive importance sampling (AIS) algorithms into the original importance sampling step while still maintaining the benefits of MPPI such as working with arbitrary system dynamics and cost functions. The benefit of optimizing the proposal distribution by integrating AIS at each control step is demonstrated in simulated environments including controlling multiple cars around a track. The new algorithm is more sample efficient than MPPI, achieving better performance with fewer samples. This performance disparity grows as the dimension of the action space increases. Results from simulations suggest the new algorithm can be used as an anytime algorithm, increasing the value of control at each iteration versus relying on a large set of samples. Repository—https://github.com/sisl/MPOPIS","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160929","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160929","","Adaptation models;Monte Carlo methods;System dynamics;Heuristic algorithms;Simulation;Predictive models;Aerospace electronics","","8","","34","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"CabiNet: Scaling Neural Collision Detection for Object Rearrangement with Procedural Scene Generation","A. Murali; A. Mousavian; C. Eppner; A. Fishman; D. Fox",NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA,2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","1866","1874","We address the important problem of generalizing robotic rearrangement to clutter without any explicit object models. We first generate over 650K cluttered scenes-orders of magnitude more than prior work-in diverse everyday environments, such as cabinets and shelves. We render synthetic partial point clouds from this data and use it to train our CabiNet model architecture. CabiNet is a collision model that accepts object and scene point clouds, captured from a single-view depth observation, and predicts collisions for SE(3) object poses in the scene. Our representation has a fast inference speed of 7μs/query with nearly 20% higher performance than baseline approaches in challenging environments. We use this collision model in conjunction with a Model Predictive Path Integral (MPPI) planner to generate collision-free trajectories for picking and placing in clutter. CabiNet also predicts waypoints, computed from the scene's signed distance field (SDF), that allows the robot to navigate tight spaces during rearrangement. This improves rearrangement performance by nearly 35% compared to baselines. We systematically evaluate our approach, procedurally generate simulated experiments, and demonstrate that our approach directly transfers to the real world, despite training exclusively in simulation. Supplementary material and videos of robot experiments in completely unknown scenes are available at: cabinet-object-rearrangement.github.io.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161528","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161528","","Point cloud compression;Training;Computational modeling;Computer architecture;Predictive models;Trajectory;Clutter","","4","","63","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Learning Depth Completion of Transparent Objects using Augmented Unpaired Data","F. Erich; B. Leme; N. Ando; R. Hanai; Y. Domae","Industrial CPS Research Center, National Institute of Advanced Industrial Science and Technology, Japan; Horticultural Sciences Department, University of Florida, USA; Industrial CPS Research Center, National Institute of Advanced Industrial Science and Technology, Japan; Industrial CPS Research Center, National Institute of Advanced Industrial Science and Technology, Japan; Industrial CPS Research Center, National Institute of Advanced Industrial Science and Technology, Japan",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","4887","4894","We propose a technique for depth completion of transparent objects using augmented data captured directly from real environments with complicated geometry. Using cyclic adversarial learning we train translators to convert between painted versions of the objects and their real transparent counterpart. The translators are trained on unpaired data, hence datasets can be created rapidly and without any manual labeling. Our technique does not make any assumptions about the geometry of the environment, unlike SOTA systems that assume easily observable occlusion and contact edges, such as ClearGrasp. We show how our technique outperforms ClearGrasp in a dishwasher environment, in which occlusion and contact edges are difficult to observe. We also show how the technique can be used to create an object manipulation application with a humanoid robot. Supplementary URI: https://ftorise.github.io/faking_depth_web/.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160619","JST [Moonshot R&D](grant numbers:JPMJMS2031); New Energy and Industrial Technology Development Organization (NEDO)(grant numbers:JPNP20016); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160619","","Geometry;Training;Automation;Machine vision;Training data;Humanoid robots;Switches","","5","","42","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Vitreoretinal Surgical Robotic System with Autonomous Orbital Manipulation using Vector-Field Inequalities","Y. Koyama; M. M. Marinho; K. Harada","Department of Mechanical Engineering, University of Tokyo, Tokyo, Japan; Department of Mechanical Engineering, University of Tokyo, Tokyo, Japan; Department of Mechanical Engineering, University of Tokyo, Tokyo, Japan",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","4654","4660","Vitreoretinal surgery pertains to the treatment of delicate tissues on the fundus of the eye using thin instruments. Surgeons frequently rotate the eye during surgery, which is called orbital manipulation, to observe regions around the fundus without moving the patient. In this paper, we propose the autonomous orbital manipulation of the eye in robot-assisted vitreoretinal surgery with our tele-operated surgical system. In a simulation study, we preliminarily investigated the increase in the manipulability of our system using orbital manipulation. Furthermore, we demonstrated the feasibility of our method in experiments with a physical robot and a realistic eye model, showing an increase in the view-able area of the fundus when compared to a conventional technique. Source code and minimal example available at https://github.com/mmmarinho/icra2023_orbitalmanipulation.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160795","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160795","","Jacobian matrices;Medical robotics;Automation;Eyes;Source coding;Instruments;Force","","5","","30","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Dexterous Imitation Made Easy: A Learning-Based Framework for Efficient Dexterous Manipulation","S. P. Arunachalam; S. Silwal; B. Evans; L. Pinto",New York University; New York University; New York University; New York University,2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","5954","5961","Optimizing behaviors for dexterous manipulation has been a longstanding challenge in robotics, with a variety of methods from model-based control to model-free reinforcement learning having been previously explored in literature. Such prior work often require extensive trial-and-error training along with task-specific tuning of reward functions, which makes applying dexterous manipulation for general purpose problems quite impractical. A sample-efficient and practical alternate to trial-and-error learning is imitation learning. However, collecting and learning from demonstrations in dexterous manipulation is quite challenging due to the high-dimensional action-space involved with multi-finger control. In this work, we propose ‘Dexterous Imitation Made Easy’ (DIME) a new imitation learning framework for dexterous manipulation. DIME only requires a single RGB camera that observes a human operator to teleoperate a robotic hand. Once demonstrations are collected, DIME employs state-of-the-art imitation learning methods to train dexterous manipulation policies. On real robot benchmarks we demonstrate that DIME can be used to solve complex, in-hand manipulation tasks such as ‘flipping’, ‘spinning’, and ‘rotating’ objects with just 30 demonstrations and no additional robot training. Our code, pre-collected demonstrations, and robot videos are publicly available at: https://nyu-robot-learning.github.io/dime.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160275","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160275","","Training;Learning systems;Codes;Robot vision systems;Reinforcement learning;Cameras;Task analysis","","23","","55","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"E-VFIA: Event-Based Video Frame Interpolation with Attention","O. S. Kılıç; A. Akman; A. A. Alatan","Department and Center for Image Analysis (OGAM), Electrical and Electronics Engineering, Middle East Technical University (METU), Ankara, Turkey; Department and Center for Image Analysis (OGAM), Electrical and Electronics Engineering, Middle East Technical University (METU), Ankara, Turkey; Department and Center for Image Analysis (OGAM), Electrical and Electronics Engineering, Middle East Technical University (METU), Ankara, Turkey",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","8284","8290","Video frame interpolation (VFI) is a fundamental vision task that aims to synthesize several frames between two consecutive original video images. Most algorithms aim to accomplish VFI by using only keyframes, which is an ill-posed problem since the keyframes usually do not yield any accurate precision about the trajectories of the objects in the scene. On the other hand, event-based cameras provide more precise information between the keyframes of a video. Some recent state-of-the-art event-based methods approach this problem by utilizing event data for better optical flow estimation to interpolate for video frame by warping. Nonetheless, those methods heavily suffer from the ghosting effect. On the other hand, some of kernel-based VFI methods that only use frames as input, have shown that deformable convolutions, when backed up with transformers, can be a reliable way of dealing with long-range dependencies. We propose event-based video frame interpolation with attention (E-VFIA), as a lightweight kernelbased method. E-VFIA fuses event information with standard video frames by deformable convolutions to generate high quality interpolated frames. The proposed method represents events with high temporal resolution and uses a multi-head selfattention mechanism to better encode event-based information, while being less vulnerable to blurring and ghosting artifacts; thus, generating crispier frames. The simulation results show that the proposed technique outperforms current state-of-the-art methods (both frame and event-based) with a significantly smaller model size. Multimedia material: The code is available at https://github.com/ahmetakman/E-VFIA","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160276","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160276","","Convolutional codes;Interpolation;Visualization;Fuses;Simulation;Cameras;Transformers","","","","27","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"V2XP-ASG: Generating Adversarial Scenes for Vehicle-to-Everything Perception","H. Xiang; R. Xu; X. Xia; Z. Zheng; B. Zhou; J. Ma","University of California, Los Angeles, CA, USA; University of California, Los Angeles, CA, USA; University of California, Los Angeles, CA, USA; University of California, Los Angeles, CA, USA; University of California, Los Angeles, CA, USA; University of California, Los Angeles, CA, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","3584","3591","Recent advancements in Vehicle-to-Everything communication technology have enabled autonomous vehicles to share sensory information to obtain better perception performance. With the rapid growth of autonomous vehicles and intelligent infrastructure, the V2X perception systems will soon be deployed at scale, which raises a safety-critical question: how can we evaluate and improve its performance under challenging traffic scenarios before the real-world deployment? Collecting diverse large-scale real-world test scenes seems to be the most straightforward solution, but it is expensive and time-consuming, and the collections can only cover limited scenarios. To this end, we propose the first open adversarial scene generator V2XP-ASG that can produce realistic, challenging scenes for modern LiDAR-based multi-agent perception systems. V2XP-ASG learns to construct an adversarial collaboration graph and simultaneously perturb multiple agents' poses in an adversarial and plausible manner. The experiments demonstrate that V2XP-ASG can effectively identify challenging scenes for a large range of V2X perception systems. Meanwhile, by training on the limited number of generated challenging scenes, the accuracy of V2X perception systems can be further improved by 12.3% on challenging and 4% on normal scenes. Our code will be released at https://github.com/XHwind/V2XP-ASG.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161384","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161384","","Training;Codes;Automation;Collaboration;Probabilistic logic;Generators;Communications technology","","19","","62","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"The Reflectance Field Map: Mapping Glass and Specular Surfaces in Dynamic Environments","P. Foster; C. Johnson; B. Kuipers","Cruise Automation, San Francisco, CA, USA; May Mobility, Ann Arbor, MI, USA; Computer Science and Engineering, University of Michigan, Ann Arbor, MI, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","8393","8399","We present the Reflectance Field Map, a reliable real-time method for detecting shiny surfaces, like glass, metal, and mirrors, with lidar. The Reflectance Field Map combines the theory developed for Light Field Mapping, common in computer graphics, with occupancy grid mapping. Like early methods for sonar-based robot mapping, we show how the addition of angular viewpoint information to a standard 2D grid map enables robust mapping in the presence of specular reflections. However unlike previous approaches, our method works in dynamic environments. Additionally, unlike recent approaches for lidar-based mapping of specular surfaces, our approach is sensor-agnostic and has no reliance on either intensity or multi-return measurements. We demonstrate the ability of the Reflectance Field Map to accurately map a campus environment containing numerous pedestrians and significant plate glass, both straight and curved. The algorithm runs in real-time (75+Hz) on a single core of a standard desktop processor. An open source implementation of the algorithm is available at https://github.com/collinej/reflectance_field_map.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161520","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161520","","Reflectivity;Semiconductor device modeling;Laser radar;Three-dimensional displays;Glass;Robot sensing systems;Real-time systems","","1","","16","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"LODE: Locally Conditioned Eikonal Implicit Scene Completion from Sparse LiDAR","P. Li; R. Zhao; Y. Shi; H. Zhao; J. Yuan; G. Zhou; Y. -Q. Zhang","Institute for AI Industry Research (AIR), Tsinghua University, China; Institute for AI Industry Research (AIR), Tsinghua University, China; Institute for AI Industry Research (AIR), Tsinghua University, China; Institute for AI Industry Research (AIR), Tsinghua University, China; Institute for AI Industry Research (AIR), Tsinghua University, China; Institute for AI Industry Research (AIR), Tsinghua University, China; Institute for AI Industry Research (AIR), Tsinghua University, China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","8269","8276","Scene completion refers to obtaining dense scene representation from an incomplete perception of complex 3D scenes. This helps robots detect multi-scale obstacles and analyse object occlusions in scenarios such as autonomous driving. Recent advances show that implicit representation learning can be leveraged for continuous scene completion and achieved through physical constraints like Eikonal equations. However, former Eikonal completion methods only demonstrate results on watertight meshes at a scale of tens of meshes. None of them are successfully done for non-watertight LiDAR point clouds of open large scenes at a scale of thousands of scenes. In this paper, we propose a novel Eikonal formulation that conditions the implicit representation on localized shape priors which function as dense boundary value constraints, and demonstrate it works on SemanticKITTI and SemanticPOSS. It can also be extended to semantic Eikonal scene completion with only small modifications to the network architecture. With extensive quantitative and qualitative results, we demonstrate the benefits and drawbacks of existing Eikonal methods, which naturally leads to the new locally conditioned formulation. Notably, we improve IoU from 31.7% to 51.2% on SemanticKITTI and from 40.5% to 48.7% on SemanticPOSS. We extensively ablate our methods and demonstrate that the proposed formulation is robust to a wide spectrum of implementation hyper-parameters. Codes and models are publicly available at https://github.com/AIR-DISCOVER/LODE","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160552","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160552","","Point cloud compression;Representation learning;Laser radar;Three-dimensional displays;Shape;Roads;Semantics","","11","","49","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Place Recognition under Occlusion and Changing Appearance via Disentangled Representations","Y. Chen; X. Chen; Y. Li","Xi'an Jiaotong University, China; Xi'an Jiaotong University, China; McMaster University, Canada",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","1882","1888","Place recognition is a critical and challenging task for mobile robots, aiming to retrieve an image captured at the same place as a query image from a database. Existing methods tend to fail while robots move autonomously under occlusion (e.g., car, bus, truck) and changing appearance (e.g., illumination changes, seasonal variation). Because they encode the image into only one code, entangling place features with appearance and occlusion features. To overcome this limitation, we propose PROCA, an unsupervised approach to decompose the image representation into three codes: a place code used as a descriptor to retrieve images, an appearance code that captures appearance properties, and an occlusion code that encodes occlusion content. Extensive experiments show that our model outperforms the state-of-the-art methods. Our code and data are available at https://github.com/rover-xingyu/PROCA.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160506","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160506","","Codes;Image recognition;Automation;Databases;Lighting;Image representation;Mobile robots","","","","29","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"OpTaS: An Optimization-based Task Specification Library for Trajectory Optimization and Model Predictive Control","C. E. Mower; J. Moura; N. Z. Behabadi; S. Vijayakumar; T. Vercauteren; C. Bergeles","School of Biomedical Engineering & Imaging Sciences, King's College, London, UK; School of Informatics, University of Edinburgh, UK; NA; School of Informatics, University of Edinburgh, UK; School of Biomedical Engineering & Imaging Sciences, King's College, London, UK; School of Biomedical Engineering & Imaging Sciences, King's College, London, UK",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","9118","9124","This paper presents OpTaS, a task specification Python library for Trajectory Optimization (TO) and Model Predictive Control (MPC) in robotics. Both TO and MPC are increasingly receiving interest in optimal control and in particular handling dynamic environments. While a flurry of software libraries exists to handle such problems, they either provide interfaces that are limited to a specific problem formulation (e.g. TracIK, CHOMP), or are large and statically specify the problem in configuration files (e.g. EXOTica, eTaSL). OpTaS, on the other hand, allows a user to specify custom nonlinear constrained problem formulations in a single Python script allowing the controller parameters to be modified during execution. The library provides interface to several open source and commercial solvers (e.g. IPOPT, SNOPT, KNITRO, SciPy) to facilitate integration with established workflows in robotics. Further benefits of OpTaS are highlighted through a thorough comparison with common libraries. An additional key advantage of OpTaS is the ability to define optimal control tasks in the joint-space, task-space, or indeed simultaneously. The code for OpTaS is easily installed via pip, and the source code with examples can be found at github.com/cmower/optas.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161272","Wellcome; EPSRC(grant numbers:WT203148/Z/16/Z,NS/A000049/1); Medtronic; ERC(grant numbers:714562); Alan Turing Institute, UK; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161272","","Codes;Software libraries;Source coding;Optimal control;Planning;Task analysis;Trajectory optimization","","4","","34","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"A Sequential Quadratic Programming Approach to the Solution of Open-Loop Generalized Nash Equilibria","E. L. Zhu; F. Borrelli","Department of Mechanical Engineering, University of California, Berkeley, CA, USA; Department of Mechanical Engineering, University of California, Berkeley, CA, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","3211","3217","In this work, we propose a numerical method for the solution of local generalized Nash equilibria (GNE) for the class of open-loop general-sum dynamic games for agents with nonlinear dynamics and constraints. In particular, we formulate a sequential quadratic programming (SQP) approach which requires only the solution of a single convex quadratic program at each iteration and is locally convergent. Central to the effectiveness of our approach is a non-monotonic line search method and a novel merit function for SQP step acceptance which helps to improve solver convergence beyond the local neighborhood of a GNE. We demonstrate the effectiveness of the algorithm in the context of car racing, where we see up to 32% improvement of success rate when comparing against a recent solution approach for dynamic games. We also make our code available at https://github.com/zhu-edward/DGSQP.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160799","National Science Foundation(grant numbers:1931853); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160799","","Codes;Automation;Heuristic algorithms;Search methods;Games;Nonlinear dynamical systems;Quadratic programming","","4","","25","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Reuse your features: unifying retrieval and feature-metric alignment","J. Morlana; J. M. M. Montiel","Instituto de Investigación en Ingeniergía de Aragón (I3A), Universidad de Zaragoza, Zaragoza, Spain; Instituto de Investigación en Ingeniergía de Aragón (I3A), Universidad de Zaragoza, Zaragoza, Spain",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","6072","6079","We propose a compact pipeline to unify all the steps of Visual Localization: image retrieval, candidate re-ranking and initial pose estimation, and camera pose refinement. Our key assumption is that the deep features used for these individual tasks share common characteristics, so we should reuse them in all the procedures of the pipeline. Our DRAN (Deep Retrieval and image Alignment Network) is able to extract global descriptors for efficient image retrieval, use intermediate hierarchical features to re-rank the retrieval list and produce an initial pose guess, which is finally refined by means of a feature-metric optimization based on learned deep multi-scale dense features. DRAN is the first single network able to produce the features for the three steps of visual localization. DRAN achieves competitive performance in terms of robustness and accuracy under challenging conditions in public benchmarks, outperforming other unified approaches and consuming lower computational and memory cost than its counterparts using multiple networks. Code and models will be publicly available at github.com/jmorlana/DRAN.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160501","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160501","","Location awareness;Training;Visualization;Simultaneous localization and mapping;Pipelines;Image retrieval;Feature extraction","","","","44","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"A Framework for the Unsupervised Inference of Relations Between Sensed Object Spatial Distributions and Robot Behaviors","C. Morse; L. Feng; M. Dwyer; S. Elbaum","University of Virginia, USA; University of Virginia, USA; University of Virginia, USA; University of Virginia, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","901","908","The spatial distribution of sensed objects strongly influences the behavior of mobile robots. Yet, as robots evolve in complexity to operate in increasingly rich environments, it becomes much more difficult to specify the underlying relations between sensed object spatial distributions and robot behaviors. We aim to address this challenge by leveraging system trace data to automatically infer relations that help to better characterize these spatial associations. In particular, we introduce SpRinG, a framework for the unsupervised inference of system specifications from traces that characterize the spatial relationships under which a robot operates. Our method builds on a parameterizable notion of reachability to encode relationships of spatial neighborship, which are used to instantiate a language of patterns. These patterns provide the structure to infer, from system traces, the connection between such relationships and robot behaviors. We show that SpRinG can automatically infer spatial relations over two distinct domains: autonomous vehicles in traffic and a surgical robot. Our results demonstrate the power and expressiveness of SpRinG, in its ability to learn existing specifications as machine-checkable first-order logic, uncover previously unstated specifications that are rich and insightful, and reveal contextual differences between executions.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161071","NSF(grant numbers:1924777); AFOSR(grant numbers:FA9550-21-1-0164); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161071","","Point cloud compression;Graphical models;Medical robotics;Robot sensing systems;Behavioral sciences;Planning;Mobile robots","","2","","25","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Code as Policies: Language Model Programs for Embodied Control","J. Liang; W. Huang; F. Xia; P. Xu; K. Hausman; B. Ichter; P. Florence; A. Zeng",NA; NA; NA; NA; NA; NA; NA; NA,2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","9493","9500","Large language models (LLMs) trained on code-completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g., from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (‘faster’) depending on context (i.e., behavioral commonsense). This paper presents Code as Policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160591","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160591","","Feedback loop;Codes;Natural languages;Process control;Detectors;Libraries;Impedance","","140","","60","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Weakly Supervised Referring Expression Grounding via Target-Guided Knowledge Distillation","J. Mi; S. Tang; Z. Ma; D. Liu; Q. Li; J. Zhang","Institute of Machine Intelligence (IMI), University of Shanghai for Science and Technology, China; Institute of Machine Intelligence (IMI), University of Shanghai for Science and Technology, China; Institute of Machine Intelligence (IMI), University of Shanghai for Science and Technology, China; Institute of Machine Intelligence (IMI), University of Shanghai for Science and Technology, China; Institute of Machine Intelligence (IMI), University of Shanghai for Science and Technology, China; Department of Informatics, Technical Aspects of Multimodal Systems (TAMS), University of Hamburg, Germany",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","8299","8305","Weakly supervised referring expression grounding aims to train a model without the manual labels between image regions and referring expressions during the training phase. Current predominant models often adopt deep structures to reconstruct the region-expression correspondence. A crucial deficiency of the existing approaches lies in that these models neglect to exploit potential valuable information to further improve their grounding performance. To address this issue, we leverage knowledge distillation as a unique scheme to excavate and transfer helpful information for acquiring a better model. Specifically, we propose a target-guided knowledge distillation framework that accounts for region-expression pairs reconstruction and matching. We reactivate the target-related prediction information learned by a pre-trained teacher model and transfer the target-related prediction knowledge from the teacher to guide the training process and boost the performance of the student model. We conduct extensive experiments on three benchmark datasets, i.e., RefCOCO, RefCOCO+, and RefCOCOg. Without bells and whistles, our approach achieves state-of-the-art results on several splits of benchmark datasets. The implementation codes and trained models are available at: https://github.com/dami23/WREG_KD.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161294","National Natural Science Foundation(grant numbers:92048205); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161294","","Training;Codes;Automation;Grounding;Manuals;Predictive models;Benchmark testing","","2","","42","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"CAROM Air - Vehicle Localization and Traffic Scene Reconstruction from Aerial Videos","D. Lu; E. Eaton; M. Weg; W. Wang; S. Como; J. Wishart; H. Yu; Y. Yang",Rider University; Rider University; Rider University; Arizona State University; Arizona State University; Arizona State University; Arizona State University; Arizona State University,2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","10666","10673","Road traffic scene reconstruction from videos has been desirable by road safety regulators, city planners, researchers, and autonomous driving technology developers. However, it is expensive and unnecessary to cover every mile of the road with cameras mounted on the road infrastructure. This paper presents a method that can process aerial videos to vehicle trajectory data so that a traffic scene can be automatically reconstructed and accurately re-simulated using computers. On average, the vehicle localization error is about 0.1 m to 0.3 m using a consumer-grade drone flying at 120 meters. This project also compiles a dataset of 50 reconstructed road traffic scenes from about 100 hours of aerial videos to enable various downstream traffic analysis applications and facilitate further road traffic related research. The dataset is available at https://github.com/duolu/CAROM.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160502","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160502","","Location awareness;Urban areas;Transportation;Training data;Cameras;Road traffic;Road safety","","5","","61","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"From Semi-supervised to Omni-supervised Room Layout Estimation Using Point Clouds","H. -a. Gao; B. Tian; P. Li; X. Chen; H. Zhao; G. Zhou; Y. Chen; H. Zha","Department of Computer Science and Technology, Tsinghua University, China; Department of Computer Science and Technology, Tsinghua University, China; Department of Computer Science and Technology, Tsinghua University, China; Department of Computer Science and Technology, Tsinghua University, China; Institute for AI Industry Research (AIR), Tsinghua University, China; Institute for AI Industry Research (AIR), Tsinghua University, China; Intel Labs, China; Peking University, China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","2803","2810","Room layout estimation is a long-existing robotic vision task that benefits both environment sensing and motion planning. However, layout estimation using point clouds (PCs) still suffers from data scarcity due to annotation difficulty. As such, we address the semi-supervised setting of this task based upon the idea of model exponential moving averaging. But adapting this scheme to the state-of-the-art (SOTA) solution for PC-based layout estimation is not straightforward. To this end, we define a quad set matching strategy and several consistency losses based upon metrics tailored for layout quads. Besides, we propose a new online pseudo-label harvesting algorithm that decomposes the distribution of a hybrid distance measure between quads and PC into two components. This technique does not need manual threshold selection and intuitively encourages quads to align with reliable layout points. Surprisingly, this framework also works for the fully-supervised setting, achieving a new SOTA on the ScanNet benchmark. Last but not least, we also push the semi-supervised setting to the realistic omni-supervised setting, demonstrating significantly promoted performance on a newly annotated ARKitScenes testing set. Our codes, data and models are made publicly available**Code: https://github.com/AIR-DISCOVER/Omni-PQ.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161273","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161273","","Point cloud compression;Training;Measurement;Layout;Estimation;Robot sensing systems;Sensors","","10","","60","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Sonicverse: A Multisensory Simulation Platform for Embodied Household Agents that See and Hear","R. Gao; H. Li; G. Dharan; Z. Wang; C. Li; F. Xia; S. Savarese; L. Fei-Fei; J. Wu",Stanford University; Stanford University; Stanford University; Stanford University; Stanford University; Stanford University; Stanford University; Stanford University; Stanford University,2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","704","711","Developing embodied agents in simulation has been a key research topic in recent years. Exciting new tasks, algorithms, and benchmarks have been developed in various simulators. However, most of them assume deaf agents in silent environments, while we humans perceive the world with multiple senses. We introduce Sonicverse, a multisensory simulation platform with integrated audio-visual simulation for training household agents that can both see and hear. Sonicverse models realistic continuous audio rendering in 3D environments in real-time. Together with a new audio-visual VR interface that allows humans to interact with agents with audio, Sonicverse enables a series of embodied AI tasks that need audio-visual perception. For semantic audio-visual navigation in particular, we also propose a new multi-task learning model that achieves state-of-the-art performance. In addition, we demonstrate Sonicverse's realism via sim-to-real transfer, which has not been achieved by other simulators: an agent trained in Sonicverse can successfully perform audio-visual navigation in real-world environments. Sonicverse is available at: https://github.com/StanfordVL/Sonicverse.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160461","ONR; MURI(grant numbers:N00014-22-1-2740); NSF(grant numbers:2120095); Amazon; Bosch; Salesforce; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160461","","Training;Solid modeling;Three-dimensional displays;Navigation;Semantics;Rendering (computer graphics);Multitasking","","3","","62","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Ground then Navigate: Language-guided Navigation in Dynamic Scenes","K. Jain; V. Chhangani; A. Tiwari; K. M. Krishna; V. Gandhi","Kohli Center on Intelligent Systems, International Institute of Information Technology, Hyderabad, India; Kohli Center on Intelligent Systems, International Institute of Information Technology, Hyderabad, India; Kohli Center on Intelligent Systems, International Institute of Information Technology, Hyderabad, India; Kohli Center on Intelligent Systems, International Institute of Information Technology, Hyderabad, India; Kohli Center on Intelligent Systems, International Institute of Information Technology, Hyderabad, India",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","4113","4120","We investigate the Vision-and-Language Navigation (VLN) problem in the context of autonomous driving in outdoor settings. We solve the problem by explicitly grounding the navigable regions corresponding to the textual command. At each timestamp, the model predicts a segmentation mask corresponding to the intermediate or the final navigable region. Our work contrasts with existing efforts in VLN, which pose this task as a node selection problem, given a discrete connected graph corresponding to the environment. We do not assume the availability of such a discretised map. Our work moves towards continuity in action space, provides interpretability through visual feedback and allows VLN on commands requiring finer manoeuvres like “park between the two cars”. Furthermore, we propose a novel meta-dataset CARLA-NAV to allow efficient training and validation. The dataset comprises pre-recorded training sequences and a live environment for validation and testing. We provide extensive qualitative and quantitative em-pirical results to validate the efficacy of the proposed approach. Code is available at https://github.com/kanji95/carla_nav.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160614","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160614","","Training;Visualization;Navigation;Grounding;Predictive models;Multitasking;Trajectory","","12","","45","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Learning to Walk by Steering: Perceptive Quadrupedal Locomotion in Dynamic Environments","M. Seo; R. Gupta; Y. Zhu; A. Skoutnev; L. Sentis; Y. Zhu",The University of Texas at Austin; The University of Texas at Austin; The University of Texas at Austin; Vanderbilt University; The University of Texas at Austin; The University of Texas at Austin,2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","5099","5105","We tackle the problem of perceptive locomotion in dynamic environments. In this problem, a quadrupedal robot must exhibit robust and agile walking behaviors in response to environmental clutter and moving obstacles. We present a hierarchical learning framework, named PRELUDE, which decomposes the problem of perceptive locomotion into high-level decision-making to predict navigation commands and low-level gait generation to realize the target commands. In this framework, we train the high-level navigation controller with imitation learning on human demonstrations collected on a steerable cart and the low-level gait controller with reinforcement learning (RL). Therefore, our method can acquire complex navigation behaviors from human supervision and discover versatile gaits from trial and error. We demonstrate the effectiveness of our approach in simulation and with hardware experiments. Videos and code can be found at the project page: https://ut-austin-rpl.github.io/PRELUDE.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161302","National Science Foundation(grant numbers:1955523,2145283); Office of Naval Research(grant numbers:N00014-22-1-2204); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161302","","Legged locomotion;Deep learning;Navigation;Decision making;Reinforcement learning;Hardware;Behavioral sciences","","1","","44","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Perceiving Unseen 3D Objects by Poking the Objects","L. Chen; Y. Song; H. Bao; X. Zhou","State Key Lab of CAD&CG, Zhejiang University; State Key Lab of CAD&CG, Zhejiang University; State Key Lab of CAD&CG, Zhejiang University; State Key Lab of CAD&CG, Zhejiang University",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","4834","4841","We present a novel approach to interactive 3D object perception for robots. Unlike previous perception algorithms that rely on known object models or a large amount of annotated training data, we propose a poking-based approach that automatically discovers and reconstructs 3D objects. The poking process not only enables the robot to discover unseen 3D objects but also produces multi-view observations for 3D reconstruction of the objects. The reconstructed objects are then memorized by neural networks with regular supervised learning and can be recognized in new test images. The experiments on real-world data show that our approach could unsupervisedly discover and reconstruct unseen 3D objects with high quality, and facilitate real-world applications such as robotic grasping. The code and supplementary materials are available at the project page: https://zju3dv.github.io/poking_perception/.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160338","National Key Research and Development Program of China(grant numbers:2020AAA0108901); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160338","","Solid modeling;Three-dimensional displays;Neural networks;Supervised learning;Training data;Grasping;Object recognition","","2","","51","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Lossless SIMD Compression of LiDAR Range and Attribute Scan Sequences","J. Ford; J. Ford","ComplexIQ, Atlanta, GA, USA; Department of Electrical Engineering, Carnegie Mellon University, Pittsburgh, PA, USA",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","9155","9161","As LiDAR sensors have become ubiquitous, the need for an efficient LiDAR data compression algorithm has increased. Modern LiDARs produce gigabytes of scan data per hour (Fig. 1) and are often used in applications with limited compute, bandwidth, and storage resources. We present a fast, lossless compression algorithm for Li-DAR range and attribute scan sequences including multiple-return range, signal, reflectivity, and ambient infrared. Our algorithm—dubbed “Jiffy”—achieves substantial compression by exploiting spatiotemporal redundancy and sparsity. Speed is accomplished by maximizing use of single-instruction-multiple-data (SIMD) instructions. In autonomous driving, infrastructure monitoring, drone inspection, and handheld mapping benchmarks, the Jiffy algorithm consistently outcompresses competing lossless codecs while operating at speeds in excess of 65M points/sec on a single core. In a typical autonomous vehicle use case, single-threaded Jiffy achieves 6x compression of centimeter-precision range scans at 500+ scans per second. To ensure reproducibility and enable adoption, the software is freely available as an open source library33Software is available here: http://github.com/jsford64/jiffy-compression.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160500","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160500","","Laser radar;Image coding;Codecs;Software algorithms;Benchmark testing;Spatiotemporal phenomena;Velocity measurement","","1","","27","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Orbeez-SLAM: A Real-time Monocular Visual SLAM with ORB Features and NeRF-realized Mapping","C. -M. Chung; Y. -C. Tseng; Y. -C. Hsu; X. -Q. Shi; Y. -H. Hua; J. -F. Yeh; W. -C. Chen; Y. -T. Chen; W. H. Hsu",National Taiwan University; National Taiwan University; National Taiwan University; National Taiwan University; National Taiwan University; National Taiwan University; National Taiwan University; National Yang Ming Chiao Tung University; National Taiwan University,2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","9400","9406","A spatial AI that can perform complex tasks through visual signals and cooperate with humans is highly anticipated. To achieve this, we need a visual SLAM that easily adapts to new scenes without pre-training and generates dense maps for downstream tasks in real-time. None of the previous learning-based and non-learning-based visual SLAMs satisfy all needs due to the intrinsic limitations of their components. In this work, we develop a visual SLAM named Orbeez-SLAM, which successfully collaborates with implicit neural representation and visual odometry to achieve our goals. Moreover, Orbeez-SLAM can work with the monocular camera since it only needs RGB inputs, making it widely applicable to the real world. Results show that our SLAM is up to 800x faster than the strong baseline with superior rendering outcomes. Code link: https://github.com/MarvinChung/Orbeez-SLAM.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160950","National Science and Technology Council(grant numbers:110-2634-F-002-051); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160950","","Visualization;Simultaneous localization and mapping;Codes;Automation;Cameras;Rendering (computer graphics);Real-time systems","","44","","35","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Zero-Shot Object Goal Visual Navigation","Q. Zhao; L. Zhang; B. He; H. Qiao; Z. Liu","State Key Laboratory of Multimodal Artificial Intelligence System, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence System, Institute of Automation, Chinese Academy of Sciences, Beijing, China; College of Electronic and Information Engineering, Tongji University, China; State Key Laboratory of Multimodal Artificial Intelligence System, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence System, Institute of Automation, Chinese Academy of Sciences, Beijing, China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","2025","2031","Object goal visual navigation is a challenging task that aims to guide a robot to find the target object based on its visual observation, and the target is limited to the classes pre-defined in the training stage. However, in real households, there may exist numerous target classes that the robot needs to deal with, and it is hard for all of these classes to be contained in the training stage. To address this challenge, we study the zero-shot object goal visual navigation task, which aims at guiding robots to find targets belonging to novel classes without any training samples. To this end, we also propose a novel zero-shot object navigation framework called semantic similarity network (SSNet). Our framework use the detection results and the cosine similarity between semantic word embeddings as input. Such type of input data has a weak correlation with classes and thus our framework has the ability to generalize the policy to novel classes. Extensive experiments on the AI2-THOR platform show that our model outperforms the baseline models in the zero-shot object navigation task, which proves the generalization ability of our model. Our code is available at: https://github.com/pioneer-innovation/Zero-Shot-Object-Navigation.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161289","National Key Research and Development Plan of China(grant numbers:2020AAA0108902); Chinese Academy of Science(grant numbers:XDB32050100); NSFC, China(grant numbers:61627808,62206288); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161289","","Training;Visualization;Correlation;Codes;Automation;Navigation;Semantics","","12","","33","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"MVTrans: Multi-View Perception of Transparent Objects","Y. R. Wang; Y. Zhao; H. Xu; S. Eppel; A. Aspuru-Guzik; F. Shkurti; A. Garg",University of Toronto & Vector Institute; University of Waterloo; University of Toronto & Vector Institute; University of Toronto & Vector Institute; University of Toronto & Vector Institute; University of Toronto & Vector Institute; University of Toronto & Vector Institute,2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","3771","3778","Transparent object perception is a crucial skill for applications such as robot manipulation in household and laboratory settings. Existing methods utilize RGB-D or stereo inputs to handle a subset of perception tasks including depth and pose estimation. However transparent object perception remains to be an open problem. In this paper, we forgo the unreliable depth map from RGB-D sensors and extend the stereo based method. Our proposed method, MVTrans, is an end-to-end multi-view architecture with multiple perception capabilities, including depth estimation, segmentation, and pose estimation. Additionally, we establish a novel procedural photo-realistic dataset generation pipeline and create a large-scale transparent object detection dataset, Syn-TODD, which is suitable for training networks with all three modalities, RGB-D, stereo and multi-view RGB. https://ac-rad.github.io/MVTrans/","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161089","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161089","","Training;Automation;Three-dimensional displays;Pose estimation;Pipelines;Object detection;Robot sensing systems","","8","","39","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"CalibDepth: Unifying Depth Map Representation for Iterative LiDAR-Camera Online Calibration","J. Zhu; J. Xue; P. Zhang","National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, China; National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, China; National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, China",2023 IEEE International Conference on Robotics and Automation (ICRA),"4 Jul 2023","2023","","","726","733","LiDAR-Camera online calibration is of great significance for building a stable autonomous driving perception system. For online calibration, a key challenge lies in constructing a unified and robust representation between multi-modal sensor data. Most methods extract features manually or implicitly with an end-to-end deep learning method. The former suffers poor robustness, while the latter has poor interpretability. In this paper, we propose CalibDepth, which uses depth maps as the unified representation for image and LiDAR point cloud. CalibDepth introduces a sub-network for monocular depth estimation to assist online calibration tasks. To further improve the performance, we regard online calibration as a sequence prediction problem, and introduce global and local losses to optimize the calibration results. CalibDepth shows excellent performance in different experimental setups. Code is open-sourced at https://github.com/Brickzhuantou/CalibDepth.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161575","NSFC(grant numbers:62036008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161575","","Training;Point cloud compression;Laser radar;Multimodal sensors;Estimation;Feature extraction;Robustness","","7","","43","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"SocialGAIL: Faithful Crowd Simulation for Social Robot Navigation","B. Ling; Y. Lyu; D. Li; G. Gao; Y. Shi; X. Xu; W. Wu","Southeast University, Nanjing, China; Southeast University, Nanjing, China; Southeast University, Nanjing, China; Nanjing University of Science and Technology, Nanjing, China; Southeast University, Nanjing, China; North Information Control Research Academy Group Co., Ltd., Nanjing, China; Southeast University, Nanjing, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","16873","16880","Navigation through crowded human environments is challenging for social robots. While reinforcement learning has been adopted for its capacity to capture complex interactions, the training process often relies on simulators to replicate realistic crowd behaviors, ensuring cost-efficiency. Existing crowd simulation methods typically rely on either handcrafted rules, which may lead to overly aggressive navigation, or learning from human trajectory demonstrations, which can be challenging to generalize effectively. In this paper, we introduce a data-driven crowd simulation method called SocialGAIL, which leverages Generative Adversarial Imitation Learning (GAIL) to emulate real pedestrian navigation in crowded environments. SocialGAIL utilizes an attention-based graph neural network to encode observations and employs a generator-discriminator architecture to closely mimic pedestrian behavior. We propose a set of metrics to evaluate the faithfulness of crowd simulation. Experimental results demonstrate that SocialGAIL outperforms baseline methods in terms of goal-reaching, intermediate state faithfulness, trajectory faithfulness, and adherence to global trajectory patterns. The code of our approach is available at https://github.com/William-island/SocialGAIL.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610371","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610371","","Measurement;Training;Pedestrians;Navigation;Imitation learning;Social robots;Graph neural networks","","","","44","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"VPE-SLAM: Neural Implicit Voxel-permutohedral Encoding for SLAM","Z. Zhang; Y. Zhang; Y. Shen; L. Rong; S. Wang; X. Ouyang; Y. Li","College of Information Science and Engineering, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","5104","5110","NeRF can reconstruct incredibly realistic environmental maps in dense simultaneous localization and mapping, providing robots with more comprehensive scene map information. However, NeRF often struggles with geometric distortions in indoor reconstructions. To correct geometric distortions, we develop VPE-SLAM, based on the proposed voxel-permutohedral encoding, which can incrementally reconstruct maps of unknown scenes. Specifically, voxel-permutohedral encoding combines a sparse voxel feature grid created by an octree and multi-resolution permutohedral tetrahedral feature grids to represent the scene effectively. Especially when dealing with object edges, our method can effectively encode the geometry and texture of edges by the hybrid structural grid. We propose a novel local bundle adjustment module that utilizes a sliding window mechanism to manage adjacent keyframes requiring optimization. Furthermore, the proposed method establishes local map consistency by repeatedly optimizing keyframes that were initially under-optimized through a compensation strategy. The consistency of the local map can enhance the adaptability of our method to challenging scenes. Extensive experiments demonstrate that our method can achieve accurate camera tracking and produce high-quality reconstruction results on the Replica and ScanNet datasets. The source code will be available at https://github.com/NeuCV-IRMI/VPE-SLAM.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610865","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610865","","Geometry;Bundle adjustment;Simultaneous localization and mapping;Accuracy;Source coding;Robot vision systems;Distortion","","","","30","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Vision-based Wearable Steering Assistance for People with Impaired Vision in Jogging","X. Liu; B. Wang; Z. Li","Institute of Advanced Technology, University of Science and Technology of China, Hefei, China; School of Information and Electronic, Beijing Institute of Technology, Beijing, China; School of Mechanical Engineering, Tongji University, Shanghai, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","15270","15275","Outdoor sports pose a challenge for people with impaired vision. The demand for higher-speed mobility inspired us to develop a vision-based wearable steering assistance. To ensure broad applicability, we focused on a representative sports environment, the athletics track. Our efforts centered on improving the speed and accuracy of perception, enhancing planning adaptability for the real world, and providing swift and safe assistance for people with impaired vision. In perception, we engineered a lightweight multitask network capable of simultaneously detecting track lines and obstacles. Additionally, due to the limitations of existing datasets for supporting multi-task detection in athletics tracks, we diligently collected and annotated a new dataset (MAT) containing 1000 images. In planning, we integrated the methods of sampling and spline curves, addressing the planning challenges of curves. Meanwhile, we utilized the positions of the track lines and obstacles as constraints to guide people with impaired vision safely along the current track. Our system is deployed on an embedded device, Jetson Orin NX. Through outdoor experiments, it demonstrated adaptability in different sports scenarios, assisting users in achieving free movement of 400meter at an average speed of 1.34 m/s, meeting the level of normal people in jogging. Our MAT dataset is publicly available from https://github.com/snoopy-l/MAT","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610660","National Natural Science Foundation of China; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610660","","Visualization;Accuracy;Tracking;Multitasking;Planning;Splines (mathematics);Robotics and automation","","","","24","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Learning to Design 3D Printable Adaptations on Everyday Objects for Robot Manipulation","M. Guo; Z. Liu; S. Tian; Z. Xie; J. Wu; C. K. Liu",Stanford University; Stanford University; Stanford University; Stanford University; Stanford University; Stanford University,2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","824","830","Advancements in robot learning for object manipulation have shown promising results, yet certain everyday objects remain challenging for robots to effectively interact with. This discrepancy arises from the fact that human-designed objects are optimized for human use rather than robot manipulation. To address this gap, we propose a framework to automatically design 3D printable adaptations that can be attached to hard-to-use objects, thus improving ""robot ergonomics"". Our learning-based framework formulates the adaptation design and control as a dual Markov decision process and is able to improve robot-object interactions for various robot end effectors and objects. We further validate our designs in the real world with a Franka Panda robot. Please see the supplementary video and https://object-adaptation.github.io for additional visualizations.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610268","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610268","","Visualization;Three-dimensional displays;Ergonomics;Markov decision processes;Affordances;Collaboration;Robot learning","","","","31","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"WayIL: Image-based Indoor Localization with Wayfinding Maps","O. Kwon; D. Jung; Y. Kim; S. Ryu; S. Yeon; S. Oh; D. Lee","Department of Electrical and Computer Engineering and ASRI, Seoul National University, Seoul, South Korea; NAVER LABS, Seongnam, South Korea; NAVER LABS, Seongnam, South Korea; NAVER LABS, Seongnam, South Korea; NAVER LABS, Seongnam, South Korea; Department of Electrical and Computer Engineering and ASRI, Seoul National University, Seoul, South Korea; NAVER LABS, Seongnam, South Korea",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","6274","6281","This paper tackles a localization problem in large-scale indoor environments with wayfinding maps. A wayfinding map abstractly portrays the environment, and humans can localize themselves based on the map. However, when it comes to using it for robot localization, large geometrical discrepancies between the wayfinding map and the real world make it hard to use conventional localization methods. Our objective is to estimate a robot pose within a wayfinding map, utilizing RGB images from perspective cameras. We introduce two different imagination modules which are inspired by how humans can comprehend and interpret their surroundings for localization purposes. These modules jointly learn how to effectively observe the first-person-view (FPV) world to interpret bird-eye-view (BEV) maps. Providing explicit guidance to the two imagination modules significantly improves the precision of the localization system. We demonstrate the effectiveness of the proposed approach using real-world datasets, which are collected from various large-scale crowded indoor environments. The experimental results show that, in 85% of scenarios, the proposed localization system can estimate its pose within 3m in large indoor spaces. Project Site: https://rllab-snu.github.io/projects/WayIL/","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610480","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610480","","Location awareness;Accuracy;Navigation;Semantics;Robot vision systems;Robot localization;Particle filters","","","","43","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"ZS6D: Zero-shot 6D Object Pose Estimation using Vision Transformers","P. Ausserlechner; D. Haberger; S. Thalhammer; J. -B. Weibel; M. Vincze","Vision for Robotics Laboratory, Automation and Control Institute, TU Wien, Austria; Vision for Robotics Laboratory, Automation and Control Institute, TU Wien, Austria; Industrial Engineering Department, University of Applied Sciences Technikum Vienna, Austria; Vision for Robotics Laboratory, Automation and Control Institute, TU Wien, Austria; Vision for Robotics Laboratory, Automation and Control Institute, TU Wien, Austria",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","463","469","As robotic systems increasingly encounter complex and unconstrained real-world scenarios, there is a demand to recognize diverse objects. The state-of-the-art 6D object pose estimation methods rely on object-specific training and therefore do not generalize to unseen objects. Recent novel object pose estimation methods are solving this issue using task-specific fine-tuned CNNs for deep template matching. This adaptation for pose estimation still requires expensive data rendering and training procedures. MegaPose for example is trained on a dataset consisting of two million images showing 20,000 different objects to reach such generalization capabilities. To overcome this shortcoming we introduce ZS6D, for zero-shot novel object 6D pose estimation. Visual descriptors, extracted using pre-trained Vision Transformers (ViT), are used for matching rendered templates against query images of objects and for establishing local correspondences. These local correspondences enable deriving geometric correspondences and are used for estimating the object's 6D pose with RANSAC- based PnP. This approach showcases that the image descriptors extracted by pre-trained ViTs are well-suited to achieve a notable improvement over two state-of-the-art novel object 6D pose estimation methods, without the need for task-specific fine-tuning. Experiments are performed on LMO, YCBV, and TLESS. In comparison to MegaPose, we improve the Average Recall on all three datasets and compared to OSOP we improve on two datasets. The code is available at https://github.com/PhilippAuss/ZS6D.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611464","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611464","","Training;Computer vision;Visualization;Codes;Computational modeling;Pose estimation;Transformers","","6","","40","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"OSSAR: Towards Open-Set Surgical Activity Recognition in Robot-assisted Surgery","L. Bai; G. Wang; J. Wang; X. Yang; H. Gao; X. Liang; A. Wang; M. Islam; H. Ren","Dept. of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, China; Dept. of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, China; Beijing Institute of Technology, Beijing, China; Qilu Hospital of Shandong University, Jinan, China; Dept. of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, China; Tongji University, Shanghai, China; Dept. of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, China; Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS), University College London, London, UK; Dept. of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","14622","14629","In the realm of automated robotic surgery and computer-assisted interventions, understanding robotic surgical activities stands paramount. Existing algorithms dedicated to surgical activity recognition predominantly cater to pre-defined closed-set paradigms, ignoring the challenges of real-world open-set scenarios. Such algorithms often falter in the presence of test samples originating from classes unseen during training phases. To tackle this problem, we introduce an innovative Open-Set Surgical Activity Recognition (OSSAR) framework. Our solution leverages the hyperspherical reciprocal point strategy to enhance the distinction between known and unknown classes in the feature space. Additionally, we address the issue of over-confidence in the closed set by refining model calibration, avoiding misclassification of unknown classes as known ones. To support our assertions, we establish an open-set surgical activity benchmark utilizing the public JIGSAWS dataset. Besides, we also collect a novel dataset on endoscopic submucosal dissection for surgical activity tasks. Extensive comparisons and ablation experiments on these datasets demonstrate the significant outperformance of our method over existing state-of-the-art approaches. Our proposed solution can effectively address the challenges of real-world surgical scenarios. Our code is publicly accessible at github.com/longbai1006/OSSAR.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610246","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610246","","Training;Medical robotics;Codes;Automation;Refining;Surgery;Activity recognition","","2","","46","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Human-Robot Interactive Creation of Artistic Portrait Drawings","F. Gao; L. Dai; J. Zhu; M. Du; Y. Zhang; M. Qiao; C. Xia; N. Wang; P. Li","The Hangzhou Institute of Technology, Xidian University, Hangzhou, China; AiSektcher Technology, Hangzhou, China; AiSektcher Technology, Hangzhou, China; Hangzhou Dianzi University, Hangzhou, China; AiSektcher Technology, Hangzhou, China; The University of Technology, Sydney, Australia; The University of Sydney, Australia; The ISN State Key Laboratry, Xidian University, Xian, China; The Institute of Software, Chinese Academy of Sciences, Beijing, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","11297","11304","In this paper, we present a novel system for Human-Robot Interactive Creation of Artworks (HRICA). Different from previous robot painters, HRICA allows a human user and a robot to alternately draw strokes on a canvas, to collaboratively create a portrait drawing through frequent interactions. The key is to enable the robot to understand human intentions, during the interactive creation process. We here formulate this as a mask-free image inpainting problem, and propose a novel method to estimate the complete version of a portrait drawing, after the human user has drawn some initial strokes. In this way, the robot can select some complementary strokes and draw them on the canvas. To train and evaluate our inpainting method, we construct a novel large-scale portrait drawing dataset, CelebLine, which composes of high-quality portrait line-drawings, with dense labels of both 2D semantic parsing masks and 3D depth maps. Finally, we develop a human-robot interactive drawing system with low-cost hardware, user-friendly interface, and interesting creation experience. Experiments show that our robot can stably cooperate with human users to create diverse styles of portrait drawings. In addition, our portrait drawing inpainting method significantly outperforms previous advanced methods. The code and dataset have been released at: https://github.com/fei-aiart/HRICA.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611451","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611451","","Three-dimensional displays;Codes;Semantic segmentation;Semantics;Estimation;Entertainment industry;Hardware","","","","70","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"SKT-Hang: Hanging Everyday Objects via Object-Agnostic Semantic Keypoint Trajectory Generation","C. -L. Kuo; Y. -W. Chao; Y. -T. Chen",National Yang Ming Chiao Tung University; NVIDIA; National Yang Ming Chiao Tung University,2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","15433","15439","We study the problem of hanging a wide range of grasped objects on diverse supporting items. Hanging objects is a ubiquitous task that is encountered in numerous aspects of our everyday lives. However, both the objects and supporting items can exhibit substantial variations in their shapes and structures, bringing two challenging issues: (1) determining the task-relevant geometric structures across different objects and supporting items, and (2) identifying a robust action sequence to accommodate the shape variations of supporting items. To this end, we propose Semantic Keypoint Trajectory (SKT), an object-agnostic representation that is highly versatile and applicable to various everyday objects. We also propose Shape-conditioned Trajectory Deformation Network (SCTDN), a model that learns to generate SKT by deforming a template trajectory based on the task-relevant geometric structure features of the supporting items. We conduct extensive experiments and demonstrate substantial improvements in our framework over existing robot hanging methods in the success rate and inference time. Finally, our simulation-trained framework shows promising hanging results in the real world. For videos and supplementary materials, please visit our project webpage: https://hcis-lab.github.io/SKT-Hang/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610266","National Science and Technology Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610266","","Deformable models;Shape;Deformation;Semantics;Trajectory;Object recognition;Task analysis","","","","27","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"BaSAL: Size-Balanced Warm Start Active Learning for LiDAR Semantic Segmentation","J. Wei; Y. Lin; H. Caesar","Intelligent Vehicles Group, Delft University of Technology, the Netherlands; Intelligent Vehicles Group, Delft University of Technology, the Netherlands; Intelligent Vehicles Group, Delft University of Technology, the Netherlands",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","18258","18264","Active learning strives to reduce the need for costly data annotation, by repeatedly querying an annotator to label the most informative samples from a pool of unlabeled data, and then training a model from these samples. We identify two problems with existing active learning methods for LiDAR semantic segmentation. First, they overlook the severe class imbalance inherent in LiDAR semantic segmentation datasets. Second, to bootstrap the active learning loop when there is no labeled data available, they train their initial model from randomly selected data samples, leading to low performance. This situation is referred to as the cold start problem. To address these problems we propose BaSAL, a size-balanced warm start active learning model, based on the observation that each object class has a characteristic size. By sampling object clusters according to their size, we can thus create a size-balanced dataset that is also more class-balanced. Furthermore, in contrast to existing information measures like entropy or CoreSet, size-based sampling does not require a pretrained model, thus addressing the cold start problem effectively. Results show that we are able to improve the performance of the initial model by a large margin. Combining warm start and size-balanced sampling with established information measures, our approach achieves comparable performance to training on the entire SemanticKITTI dataset, despite using only 5% of the annotations, outperforming existing active learning methods. We also match the existing state-of-the-art in active learning on nuScenes. Our code is available at: https://github.com/Tony-WJR/BaSAL.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611122","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611122","","Training;Laser radar;Annotations;Fuses;Semantic segmentation;Supervised learning;Size measurement","","1","","49","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Learning to Grasp in Clutter with Interactive Visual Failure Prediction","M. Murray; A. Gupta; M. Cakmak","Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA, USA; Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA, USA; Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA, USA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","18172","18178","Modern warehouses process millions of unique objects which are often stored in densely packed containers. To automate tasks in this environment, a robot must be able to pick diverse objects from highly cluttered scenes. Real-world learning is a promising approach, but executing picks in the real world is time-consuming, can induce costly failures, and often requires extensive human intervention, which causes operational burden and limits the scope of data collection and deployments. In this work, we leverage interactive probes to visually evaluate grasps in clutter without fully executing picks, a capability we refer to as Interactive Visual Failure Prediction (IVFP). This enables autonomous verification of grasps during execution to avoid costly downstream failures as well as autonomous reward assignment, providing supervision to continuously shape and improve grasping behavior as the robot gathers experience in the real world, without constantly requiring human intervention. Through experiments on a Stretch RE1 robot, we study the effect that IVFP has on performance - both in terms of effective data throughput and success rate, and show that this approach leads to grasping policies that outperform policies trained with human supervision alone, while requiring significantly less human intervention. Code, datasets, and videos available at https://robo-ivfp.github.io","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611363","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611363","","Visualization;Grasping;Data collection;Throughput;Behavioral sciences;Clutter;Task analysis","","","","25","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Model-Based Runtime Monitoring with Interactive Imitation Learning","H. Liu; S. Dass; R. Martín-Martín; Y. Zhu",The University of Texas at Austin; The University of Texas at Austin; The University of Texas at Austin; The University of Texas at Austin,2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","4154","4161","Robot learning methods have recently made great strides, but generalization and robustness challenges still hinder their widespread deployment. Failing to detect and address potential failures renders state-of-the-art learning systems not combat-ready for high-stakes tasks. Recent advances in interactive imitation learning have presented a promising framework for human-robot teaming, enabling the robots to operate safely and continually improve their performances over long-term deployments. Nonetheless, existing methods typically require constant human supervision and preemptive feedback, limiting their practicality in realistic domains. This work aims to endow a robot with the ability to monitor and detect errors during task execution. We introduce a model-based runtime monitoring algorithm that learns from deployment data to detect system anomalies and anticipate failures. Unlike prior work that cannot foresee future failures or requires failure experiences for training, our method learns a latent-space dynamics model and a failure classifier, enabling our method to simulate future action outcomes and detect out-of-distribution and high-risk states preemptively. We train our method within an interactive imitation learning framework, where it continually updates the model from the experiences of the human-robot team collected using trustworthy deployments. Consequently, our method reduces the human workload needed over time while ensuring reliable task execution. Our method outperforms the baselines across system-level and unit-test metrics, with 23% and 40% higher success rates in simulation and on physical hardware, respectively. More information at https://ut-austin-rpl.github.io/sirius-runtime-monitor/","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611038","National Science Foundation; Office of Naval Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611038","","Training;Measurement;Learning systems;Runtime;Limiting;Imitation learning;Robustness","","","","51","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"DroneMOT: Drone-based Multi-Object Tracking Considering Detection Difficulties and Simultaneous Moving of Drones and Objects","P. Wang; Y. Wang; D. Li","Department of Computer Science, School of Information, Renmin University of China, Beijing, China; Department of Computer Science, School of Information, Renmin University of China, Beijing, China; Department of Computer Science, School of Information, Renmin University of China, Beijing, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","7397","7404","Multi-object tracking (MOT) on static platforms, such as by surveillance cameras, has achieved significant progress, with various paradigms providing attractive performances. However, the effectiveness of traditional MOT methods is significantly reduced when it comes to dynamic platforms like drones. This decrease is attributed to the distinctive challenges in the MOT-on-drone scenario: (1) objects are generally small in the image plane, blurred, and frequently occluded, making them challenging to detect and recognize; (2) drones move and see objects from different angles, causing the unreliability of the predicted positions and feature embeddings of the objects. This paper proposes DroneMOT, which firstly proposes a Dual-domain Integrated Attention (DIA) module that considers the fast movements of drones to enhance the drone-based object detection and feature embedding for small-sized, blurred, and occluded objects. Then, an innovative Motion-Driven Association (MDA) scheme is introduced, considering the concurrent movements of both the drone and the objects. Within MDA, an Adaptive Feature Synchronization (AFS) technique is presented to update the object features seen from different angles. Additionally, a Dual Motion-based Prediction (DMP) method is employed to forecast the object positions. Finally, both the refined feature embeddings and the predicted positions are integrated to enhance the object association. Comprehensive evaluations on VisDrone2019-MOT and UAVDT datasets show that DroneMOT provides substantial performance improvements over the state-of-the-art in the domain of MOT on drones. The code will be available at https://github.com/PenK1nG/DroneMOT.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610941","National Natural Science Foundation of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610941","","Heating systems;Image recognition;Codes;Surveillance;Object detection;Feature extraction;Synchronization","","","","69","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Legible and Proactive Robot Planning for Prosocial Human-Robot Interactions","J. Geldenbott; K. Leung","Department of Aeronautics and Astronautics, University of Washington; Department of Aeronautics and Astronautics, University of Washington",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","13397","13403","Humans have a remarkable ability to fluently engage in joint collision avoidance in crowded navigation tasks despite the complexities and uncertainties inherent in human behavior. Underlying these interactions is a mutual understanding that (i) individuals are prosocial, that is, there is equitable responsibility in avoiding collisions, and (ii) individuals should behave legibly, that is, move in a way that clearly conveys their intent to reduce ambiguity in how they intend to avoid others. Toward building robots that can safely and seamlessly interact with humans, we propose a general robot trajectory planning framework for synthesizing legible and proactive behaviors and demonstrate that our robot planner naturally leads to prosocial interactions. Specifically, we introduce the notion of a markup factor to incentivize legible and proactive behaviors and an inconvenience budget constraint to ensure equitable collision avoidance responsibility. We evaluate our approach against well-established multi-agent planning algorithms and show that using our approach produces safe, fluent, and prosocial interactions. We demonstrate the real-time feasibility of our approach with human-in-the-loop simulations. Project page can be found at https://uw-ctrl.github.io/phri/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611294","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611294","","Uncertainty;Trajectory planning;Navigation;Human-robot interaction;Real-time systems;Human in the loop;Planning","","1","","44","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"GAM-Depth: Self-Supervised Indoor Depth Estimation Leveraging a Gradient-Aware Mask and Semantic Constraints","A. Cheng; Z. Yang; H. Zhu; K. Mao","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Singapore Institute of Manufacturing Technology (SIMTech), Agency for Science, Technology and Research (A*STAR), Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","5367","5374","Self-supervised depth estimation has evolved into an image reconstruction task that minimizes a photometric loss. While recent methods have made strides in indoor depth estimation, they often produce inconsistent depth estimation in textureless areas and unsatisfactory depth discrepancies at object boundaries. To address these issues, in this work, we propose GAM-Depth, developed upon two novel components: gradient-aware mask and semantic constraints. The gradient-aware mask enables adaptive and robust supervision for both key areas and textureless regions by allocating weights based on gradient magnitudes. The incorporation of semantic constraints for indoor self-supervised depth estimation improves depth discrepancies at object boundaries, leveraging a co-optimization network and proxy semantic labels derived from a pretrained segmentation model. Experimental studies on three indoor datasets, including NYUv2, ScanNet, and InteriorNet, show that GAM-Depth outperforms existing methods and achieves state-of-the-art performance, signifying a meaningful step forward in indoor depth estimation. Our code will be available at https://github.com/AnqiCheng1234/GAM-Depth.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610653","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610653","","Training;Adaptation models;Codes;Semantics;Pipelines;Estimation;Resource management","","2","","48","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"See to Touch: Learning Tactile Dexterity through Visual Incentives","I. Guzey; Y. Dai; B. Evans; S. Chintala; L. Pinto",New York University; New York University; New York University; Meta AI Research; New York University,2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","13825","13832","Equipping multi-fingered robots with tactile sensing is crucial for achieving the precise, contact-rich, and dexterous manipulation that humans excel at. However, relying solely on tactile sensing fails to provide adequate cues for reasoning about objects’ spatial configurations, limiting the ability to correct errors and adapt to changing situations. In this paper, we present Tactile Adaptation from Visual Incentives (TAVI), a new framework that enhances tactile-based dexterity by optimizing dexterous policies using vision-based rewards. First, we use a contrastive-based objective to learn visual representations. Next, we construct a reward function using these visual representations through optimal-transport based matching on one human demonstration. Finally, we use online reinforcement learning on our robot to optimize tactile-based policies that maximize the visual reward. On six challenging tasks, such as peg pick-and-place, unstacking bowls, and flipping slender objects, TAVI achieves a success rate of 73% using our four-fingered Allegro robot hand. The increase in performance is 108% higher than policies using tactile and vision-based rewards and 135% higher than policies without tactile observational input. Robot videos are best viewed on our project website: https://see-to-touch.github.io/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611407","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611407","","Visualization;Limiting;Reinforcement learning;Robot sensing systems;Cognition;Sensors;Task analysis","","1","","67","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"SeqTrack3D: Exploring Sequence Information for Robust 3D Point Cloud Tracking","Y. Lin; Z. Li; Y. Cui; Z. Fang","Faculty of Robot Science and Engineering, Northeastern University, Shenyang, China; Faculty of Robot Science and Engineering, Northeastern University, Shenyang, China; Faculty of Robot Science and Engineering, Northeastern University, Shenyang, China; Faculty of Robot Science and Engineering, Northeastern University, Shenyang, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","6959","6965","3D single object tracking (SOT) is an important and challenging task for the autonomous driving and mobile robotics. Most existing methods perform tracking between two consecutive frames while ignoring the motion patterns of the target over a series of frames, which would cause performance degradation in the scenes with sparse points. To break through this limitation, we introduce ""Sequence-to-Sequence"" tracking paradigm and a tracker named SeqTrack3D to capture target motion across continuous frames. Unlike previous methods that primarily adopted three strategies: matching two consecutive point clouds, predicting relative motion, or utilizing sequential point clouds to address feature degradation, our SeqTrack3D combines both historical point clouds and bounding box sequences. This novel method ensures robust tracking by leveraging location priors from historical boxes, even in scenes with sparse points. Extensive experiments conducted on large-scale datasets show that SeqTrack3D achieves new state-of-the-art performances, improving by 6.00% on NuScenes and 14.13% on Waymo dataset. The code will be made public at https://github.com/aron-lin/seqtrack3d.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611238","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611238","","Point cloud compression;Degradation;Target tracking;Three-dimensional displays;Codes;Transformers;Object tracking","","","","37","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"RGBManip: Monocular Image-based Robotic Manipulation through Active Object Pose Estimation","B. An; Y. Geng; K. Chen; X. Li; Q. Dou; H. Dong","Hyperplane Lab, School of CS, Peking University and National Key Laboratory for Multimedia Information Processing; Hyperplane Lab, School of CS, Peking University and National Key Laboratory for Multimedia Information Processing; Department of Computer Science and Engineering, The Chinese University of Hong Kong; Hyperplane Lab, School of CS, Peking University and National Key Laboratory for Multimedia Information Processing; Department of Computer Science and Engineering, The Chinese University of Hong Kong; Hyperplane Lab, School of CS, Peking University and National Key Laboratory for Multimedia Information Processing",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","7748","7755","Robotic manipulation requires accurate perception of the environment, which poses a significant challenge due to its inherent complexity and constantly changing nature. In this context, RGB image and point-cloud observations are two commonly used modalities in visual-based robotic manipulation, but each of these modalities have their own limitations. Commercial point-cloud observations often suffer from issues like sparse sampling and noisy output due to the limits of the emission-reception imaging principle. On the other hand, RGB images, while rich in texture information, lack essential depth and 3D information crucial for robotic manipulation. To mitigate these challenges, we propose an image-only robotic manipulation framework that leverages an eye-on-hand monocular camera installed on the robot’s parallel gripper. By moving with the robot gripper, this camera gains the ability to actively perceive the object from multiple perspectives during the manipulation process. This enables the estimation of 6D object poses, which can be utilized for manipulation. While, obtaining images from more and diverse viewpoints typically improves pose estimation, it also increases the manipulation time. To address this trade-off, we employ a reinforcement learning policy to synchronize the manipulation strategy with active perception, achieving a balance between 6D pose accuracy and manipulation efficiency. Our experimental results in both simulated and real-world environments showcase the state-of-the-art effectiveness of our approach. We believe that our method will inspire further research on real-world-oriented robotic manipulation. See https://rgbmanip.github.io/ for more details.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610690","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610690","","Accuracy;Three-dimensional displays;Pose estimation;Robot vision systems;Cameras;Sensors;Synchronization","","2","","36","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Learning Barrier-Certified Polynomial Dynamical Systems for Obstacle Avoidance with Robots","M. Schonger; H. T. M. Kussaba; L. Chen; L. Figueredo; A. Swikir; A. Billard; S. Haddadin","Munich Institute of Robotics and Machine Intelligence (MIRMI), Technical University of Munich (TUM), Germany; Munich Institute of Robotics and Machine Intelligence (MIRMI), Technical University of Munich (TUM), Germany; Munich Institute of Robotics and Machine Intelligence (MIRMI), Technical University of Munich (TUM), Germany; School of Computer Science, University of Nottingham, UK; Munich Institute of Robotics and Machine Intelligence (MIRMI), Technical University of Munich (TUM), Germany; Learning Algorithms and Systems Laboratory, EPFL, Switzerland; Munich Institute of Robotics and Machine Intelligence (MIRMI), Technical University of Munich (TUM), Germany",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","17201","17207","Established techniques that enable robots to learn from demonstrations are based on learning a stable dynamical system (DS). To increase the robots’ resilience to perturbations during tasks that involve static obstacle avoidance, we propose incorporating barrier certificates into an optimization problem to learn a stable and barrier-certified DS. Such optimization problem can be very complex or extremely conservative when the traditional linear parameter-varying formulation is used. Thus, different from previous approaches in the literature, we propose to use polynomial representations for DSs, which yields an optimization problem that can be tackled by sum-of-squares techniques. Finally, our approach can handle obstacle shapes that fall outside the scope of assumptions typically found in the literature concerning obstacle avoidance within the DS learning framework. Supplementary material can be found at the project webpage: https://martinschonger.github.io/abc-ds","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610828","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610828","","Shape;Scalability;Perturbation methods;Polynomials;Collision avoidance;Dynamical systems;Task analysis","","","","35","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Neural Informed RRT*: Learning-based Path Planning with Point Cloud State Representations under Admissible Ellipsoidal Constraints","Z. Huang; H. Chen; J. Pohovey; K. Driggs-Campbell","Department of Electrical and Computer Engineering, University of Illinois, Urbana-Champaign; Department of Electrical and Computer Engineering, University of Illinois, Urbana-Champaign; Department of Electrical and Computer Engineering, University of Illinois, Urbana-Champaign; Department of Electrical and Computer Engineering, University of Illinois, Urbana-Champaign",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","8742","8748","Sampling-based planning algorithms like Rapidly-exploring Random Tree (RRT) are versatile in solving path planning problems. RRT* offers asymptotic optimality but requires growing the tree uniformly over the free space, which leaves room for efficiency improvement. To accelerate convergence, rule-based informed approaches sample states in an admissible ellipsoidal subset of the space determined by the current path cost. Learning-based alternatives model the topology of the free space and infer the states close to the optimal path to guide planning. We propose Neural Informed RRT* to combine the strengths from both sides. We define point cloud representations of free states. We perform Neural Focus, which constrains the point cloud within the admissible ellipsoidal subset from Informed RRT*, and feeds into PointNet++ for refined guidance state inference. In addition, we introduce Neural Connect to build connectivity of the guidance state set and further boost performance in challenging planning problems. Our method surpasses previous works in path planning benchmarks while preserving probabilistic completeness and asymptotic optimality. We deploy our method on a mobile robot and demonstrate real world navigation around static obstacles and dynamic humans. Code is available at https://github.com/tedhuang96/nirrt_star.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611099","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611099","","Point cloud compression;Training;Navigation;Probabilistic logic;Path planning;Planning;Topology","","1","","34","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"CppFlow: Generative Inverse Kinematics for Efficient and Robust Cartesian Path Planning","J. Morgan; D. Millard; G. S. Sukhatme","Department of Computer Science, University of Southern California; Department of Computer Science, University of Southern California; Amazon Scholar, USC",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","12279","12785","In this work we present CppFlow - a novel and performant planner for the Cartesian Path Planning problem, which finds valid trajectories up to 129x faster than current methods, while also succeeding on more difficult problems where others fail. At the core of the proposed algorithm is the use of a learned, generative Inverse Kinematics solver, which is able to efficiently produce promising entire candidate solution trajectories on the GPU. Precise, valid solutions are then found through classical approaches such as differentiable programming, global search, and optimization. In combining approaches from these two paradigms we get the best of both worlds - efficient approximate solutions from generative AI which are made exact using the guarantees of traditional planning and optimization. We evaluate our system against other state of the art methods on a set of established baselines as well as new ones introduced in this work and find that our method significantly outperforms others in terms of the time to find a valid solution and planning success rate, and performs comparably in terms of trajectory length over time. Additional results and an open source implementation is available at https://jstmn.github.io/cppflow-website/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611724","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611724","","Adaptation models;Generative AI;Graphics processing units;Kinematics;Programming;Trajectory;Planning","","2","","17","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Zero-training LiDAR-Camera Extrinsic Calibration Method Using Segment Anything Model","Z. Luo; G. Yan; X. Cai; B. Shi","Autonomous Driving Group, Shanghai Artificial Intelligence Laboratory, Shanghai, China; Autonomous Driving Group, Shanghai Artificial Intelligence Laboratory, Shanghai, China; Autonomous Driving Group, Shanghai Artificial Intelligence Laboratory, Shanghai, China; Autonomous Driving Group, Shanghai Artificial Intelligence Laboratory, Shanghai, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","14472","14478","Extrinsic calibration for LiDAR and camera is an essential prerequisite for sensor fusion. Recently, automatic and target-less extrinsic calibration has become the mainstream of academic research. However, geometric feature-based methods still have requirements on the scene. Deep learning methods, while achieving high accuracy and good adaptability, rely on large annotated dataset and need additional training. We propose a novel LiDAR-camera calibration method by using the Segment Anything Model(SAM) without additional training. With the automatically generated masks, we optimize the extrinsic parameters by maximizing the consistency score of the point attributes that fall on each mask. The point cloud attributes include intensity, normal vector and segmentation class. Experiments on different real-world dataset demonstrate the accuracy and robustness of our proposed method. The code is available at https://github.com/OpenCalib/CalibAnything.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610983","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610983","","Training;Point cloud compression;Solid modeling;Adaptation models;Accuracy;Sensor fusion;Vectors","","3","","41","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"FSD: Fast Self-Supervised Single RGB-D to Categorical 3D Objects","M. Lunayach; S. Zakharov; D. Chen; R. Ambrus; Z. Kira; M. Z. Irshad",Georgia Institute of Technology; Toyota Research Institute; Toyota Research Institute; Toyota Research Institute; Georgia Institute of Technology; Georgia Institute of Technology,2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","14630","14637","In this work, we address the challenging task of 3D object recognition without the reliance on real-world 3D labeled data. Our goal is to predict the 3D shape, size, and 6D pose of objects within a single RGB-D image, operating at the category level and eliminating the need for CAD models during inference. While existing self-supervised methods have made strides in this field, they often suffer from inefficiencies arising from non-end-to-end processing, reliance on separate models for different object categories, and slow surface extraction during the training of implicit reconstruction models; thus hindering both the speed and real-world applicability of the 3D recognition process. Our proposed method leverages a multi-stage training pipeline, designed to efficiently transfer synthetic performance to the real-world domain. This approach is achieved through a combination of 2D and 3D supervised losses during the synthetic domain training, followed by the incorporation of 2D supervised and 3D self-supervised losses on real-world data in two additional learning stages. By adopting this comprehensive strategy, our method successfully overcomes the aforementioned limitations and outperforms existing self-supervised 6D pose and size estimation baselines on the NOCS test-set with a 16.4% absolute improvement in mAP for 6D pose estimation while running in near real-time at 5 Hz. Project page: fsd6d.github.io","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611012","","Training;Solid modeling;Surface reconstruction;Three-dimensional displays;Shape;Pose estimation;Predictive models","","4","","52","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"SiLVR: Scalable Lidar-Visual Reconstruction with Neural Radiance Fields for Robotic Inspection","Y. Tao; Y. Bhalgat; L. F. T. Fu; M. Mattamala; N. Chebrolu; M. Fallon","Dept. of Eng. Science, Oxford Robotics Inst., Univ. of Oxford, UK; Dept. of Eng. Science, Visual Geometry Group, Univ. of Oxford, UK; Dept. of Eng. Science, Oxford Robotics Inst., Univ. of Oxford, UK; Dept. of Eng. Science, Oxford Robotics Inst., Univ. of Oxford, UK; Dept. of Eng. Science, Oxford Robotics Inst., Univ. of Oxford, UK; Dept. of Eng. Science, Oxford Robotics Inst., Univ. of Oxford, UK",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","17983","17989","We present a neural-field-based large-scale reconstruction system that fuses lidar and vision data to generate high-quality reconstructions that are geometrically accurate and capture photo-realistic textures. This system adapts the state-of-the-art neural radiance field (NeRF) representation to also incorporate lidar data which adds strong geometric constraints on the depth and surface normals. We exploit the trajectory from a real-time lidar SLAM system to bootstrap a Structure-from-Motion (SfM) procedure to both significantly reduce the computation time and to provide metric scale which is crucial for lidar depth loss. We use submapping to scale the system to large-scale environments captured over long trajectories. We demonstrate the reconstruction system with data from a multi-camera, lidar sensor suite onboard a legged robot, hand-held while scanning building scenes for 600 metres, and onboard an aerial robot surveying a multi-storey mock disaster site-building. Website: https://ori-drs.github.io/projects/silvr/","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611278","Horizon Europe; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611278","","Surface reconstruction;Laser radar;Three-dimensional displays;Simultaneous localization and mapping;Inspection;Sensor fusion;Neural radiance field","","4","","43","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"RAPIDFlow: Recurrent Adaptable Pyramids with Iterative Decoding for Efficient Optical Flow Estimation","H. Morimitsu; X. Zhu; R. M. Cesar; X. Ji; X. -C. Yin","School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China; Institute of Mathematics and Statistics, University of São Paulo, São Paulo, Brazil; Department of Automation, Tsinghua University, Beijing, China; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","2946","2952","Extracting motion information from videos with optical flow estimation is vital in multiple practical robot applications. Current optical flow approaches show remarkable accuracy, but top-performing methods have high computational costs and are unsuitable for embedded devices. Although some previous works have focused on developing low-cost optical flow strategies, their estimation quality has a noticeable gap with more robust methods. In this paper, we develop a novel method to efficiently estimate high-quality optical flow in embedded devices. Our proposed RAPIDFlow model combines efficient NeXt1D convolution blocks with a fully recurrent structure based on feature pyramids to decrease computational costs without significantly impacting estimation accuracy. The adaptable recurrent encoder produces multi-scale features with a single shared block, which allows us to adjust the pyramid length at inference time and make it more robust to changes in input size. Also, it enables our model to offer multiple tradeoffs between accuracy and speed to suit different applications. Experiments using a Jetson Orin NX embedded system on the MPI-Sintel and KITTI public benchmarks show that RAPIDFlow outperforms previous approaches by significant margins at faster speeds. Our code is available at https://github.com/hmorimitsu/ptlflow/tree/main/ptlflow/models/rapidflow.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610277","Research and Development; Fundamental Research Funds for the Central Universities; National Science Fund for Distinguished Young Scholars; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610277","","Adaptation models;Accuracy;Embedded systems;Computational modeling;Estimation;Computational efficiency;Optical flow","","","","46","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Improving Autonomous Driving Safety with POP: A Framework for Accurate Partially Observed Trajectory Predictions","S. Wang; Y. Chen; J. Cheng; X. Mei; R. Xin; Y. Song; M. Liu","Robotics and Autonomous Systems, Division of Emerging Interdisciplinary Areas (EMIA) Under Interdisciplinary Programs Office (IPO), The Hong Kong University of Science and Technology, Hong Kong SAR, China; Robotics and Autonomous Systems, Division of Emerging Interdisciplinary Areas (EMIA) Under Interdisciplinary Programs Office (IPO), The Hong Kong University of Science and Technology, Hong Kong SAR, China; Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong SAR, China; Computer Science, The Hong Kong University of Science and Technology, Hong Kong SAR, China; Robotics and Autonomous Systems, Division of Emerging Interdisciplinary Areas (EMIA) Under Interdisciplinary Programs Office (IPO), The Hong Kong University of Science and Technology, Hong Kong SAR, China; Lotus Technology Ltd, China; Robotics and Autonomous Systems Thrust, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","14450","14456","Accurate trajectory prediction is crucial for safe and efficient autonomous driving, but handling partial observations presents significant challenges. To address this, we propose a novel trajectory prediction framework called Partial Observations Prediction (POP) for congested urban road scenarios. The framework consists of two key stages: self-supervised learning (SSL) and feature distillation. POP first employs SLL to help the model learn to reconstruct history representations, and then utilizes feature distillation as the fine-tuning task to transfer knowledge from the teacher model, which has been pre-trained with complete observations, to the student model, which has only few observations. POP achieves comparable results to topperforming methods in open-loop experiments and outperforms the baseline method in closed-loop simulations, including safety metrics. Qualitative results illustrate the superiority of POP in providing reasonable and safe trajectory predictions. Demo videos and code are available at https://chantsss.github.io/POP/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610154","Hong Kong University of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610154","","Accuracy;Roads;Self-supervised learning;Predictive models;Trajectory;History;Task analysis","","1","","28","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Eliminating Cross-modal Conflicts in BEV Space for LiDAR-Camera 3D Object Detection","J. Fu; C. Gao; Z. Wang; L. Yang; X. Wang; B. Mu; S. Liu","Institute of Artificial Intelligence and Hangzhou Innovation Institute, Beihang University; Institute of Artificial Intelligence and Hangzhou Innovation Institute, Beihang University; Institute of Artificial Intelligence and Hangzhou Innovation Institute, Beihang University; Meituan Inc.; Meituan Inc.; Meituan Inc.; Institute of Artificial Intelligence and Hangzhou Innovation Institute, Beihang University",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","16381","16387","Recent 3D object detectors typically utilize multi-sensor data and unify multi-modal features in the shared bird’s-eye view (BEV) representation space. However, our empirical findings indicate that previous methods have limitations in generating fusion BEV features free from cross-modal conflicts. These conflicts encompass extrinsic conflicts caused by BEV feature construction and inherent conflicts stemming from heterogeneous sensor signals. Therefore, we propose a novel Eliminating Conflicts Fusion (ECFusion) method to explicitly eliminate the extrinsic/inherent conflicts in BEV space and produce improved multi-modal BEV features. Specifically, we devise a Semantic-guided Flow-based Alignment (SFA) module to resolve extrinsic conflicts via unifying spatial distribution in BEV space before fusion. Moreover, we design a Dissolved Query Recovering (DQR) mechanism to remedy inherent conflicts by preserving objectness clues that are lost in the fusion BEV feature. In general, our method maximizes the effective information utilization of each modality and leverages inter-modal complementarity. Our method achieves state-of-the-art performance in the highly competitive nuScenes 3D object detection dataset. The code is released at https://github.com/fjhzhixi/ECFusion.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610230","National Natural Science Foundation of China; Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610230","","Three-dimensional displays;Graphical models;Object detection;Interference;Robot sensing systems;Feature extraction;Spatial resolution","","1","","57","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Kinematic-aware Prompting for Generalizable Articulated Object Manipulation with LLMs","W. Xia; D. Wang; X. Pang; Z. Wang; B. Zhao; D. Hu; X. Li","Gaoling School of Artificial Intelligence, Renmin University of China; Shanghai Artificial Intelligence Laboratory; Gaoling School of Artificial Intelligence, Renmin University of China; Shanghai Artificial Intelligence Laboratory; Shanghai Artificial Intelligence Laboratory; Gaoling School of Artificial Intelligence, Renmin University of China; Shanghai Artificial Intelligence Laboratory",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","2073","2080","Generalizable articulated object manipulation is essential for home-assistant robots. Recent efforts focus on imitation learning from demonstrations or reinforcement learning in simulation, however, due to the prohibitive costs of real-world data collection and precise object simulation, it still remains challenging for these works to achieve broad adaptability across diverse articulated objects. Recently, many works have tried to utilize the strong in-context learning ability of Large Language Models (LLMs) to achieve generalizable robotic manipulation, but most of these researches focus on high-level task planning, sidelining low-level robotic control. In this work, building on the idea that the kinematic structure of the object determines how we can manipulate it, we propose a kinematic-aware prompting framework that prompts LLMs with kinematic knowledge of objects to generate low-level motion trajectory waypoints, supporting various object manipulation. To effectively prompt LLMs with the kinematic structure of different objects, we design a unified kinematic knowledge parser, which represents various articulated objects as a unified textual description containing kinematic joints and contact location. Building upon this unified description, a kinematic-aware planner model is proposed to generate precise 3D manipulation waypoints via a designed kinematic-aware chain-of-thoughts prompting method. Our evaluation spanned 48 instances across 16 distinct categories, revealing that our framework not only outperforms traditional methods on 8 seen categories but also shows a powerful zero-shot capability for 8 unseen articulated object categories with only 17 demonstrations. Moreover, the real-world experiments on 7 different object categories prove our framework’s adaptability in practical scenarios. Code is released at https://github.com/GeWu-Lab/LLM_articulated_object_manipulation.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610744","National Natural Science Foundation of China; National Natural Science Foundation of China; Renmin University of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610744","","Solid modeling;Three-dimensional displays;Large language models;Imitation learning;Kinematics;Reinforcement learning;Trajectory","","3","","45","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"DiPPeR: Diffusion-based 2D Path Planner applied on Legged Robots","J. Liu; M. Stamatopoulou; D. Kanoulas","Department of Computer Science, RPL Lab, University College London, London, UK; Department of Computer Science, RPL Lab, University College London, London, UK; Department of Computer Science, RPL Lab, University College London, London, UK",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","9264","9270","In this work, we present DiPPeR, a novel and fast 2D path planning framework for quadrupedal locomotion, leveraging diffusion-driven techniques. Our contributions include a scalable dataset generator for map images and corresponding trajectories, an image-conditioned diffusion planner for mobile robots, and a training/inference pipeline employing CNNs. We validate our approach in several mazes, as well as in real-world deployment scenarios on Boston Dynamic’s Spot and Unitree’s Go1 robots. DiPPeR performs on average 23 times faster for trajectory generation against both search based and data driven path planning algorithms with an average of 87% consistency in producing feasible paths of various length in maps of variable size, and obstacle structure. Website: https://rpl-cs-ucl.github.io/DiPPeR/","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610013","","Training;Legged locomotion;Heuristic algorithms;Pipelines;Network architecture;Transformers;Generators","","3","","32","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Design and Evaluation of Motion Planners for Quadrotors in Environments with Varying Complexities","Y. S. Shao; Y. Wu; L. Jarin-Lipschitz; P. Chaudhari; V. Kumar","GRASP Laboratory, University of Pennsylvania, Philadelphia, PA, USA; GRASP Laboratory, University of Pennsylvania, Philadelphia, PA, USA; GRASP Laboratory, University of Pennsylvania, Philadelphia, PA, USA; GRASP Laboratory, University of Pennsylvania, Philadelphia, PA, USA; GRASP Laboratory, University of Pennsylvania, Philadelphia, PA, USA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","10033","10039","Motion planning techniques for quadrotors have advanced significantly over the past decade. Most successful planners have two stages: a front-end that determines a path that incorporates geometric (or kinematic or input) constraints and specifies the homotopy class of the trajectory, and a back-end that optimizes this path to respect dynamics and input constraints. While there are many different choices for each stage, the eventual performance depends critically not only on these choices, but also on the environment. Given a new environment, it is difficult to decide a priori how one should design a motion planner. In this work, we develop (i) a procedure to construct parametrized environments, (ii) metrics that characterize the difficulty of motion planning in these environments, and (iii) an open-source software stack that can be used to combine a wide variety of two-stage planners seamlessly. We perform experiments in simulations and a real platform. We find, somewhat conveniently, that geometric front-ends are sufficient for environments with varying complexities if combined with dynamics-aware backends. The metrics we designed faithfully capture the planning difficulty in a given environment. All code is available at https://github.com/KumarRobotics/kr_mp_design.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610207","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610207","","Measurement;Navigation;Software algorithms;Kinematics;Planning;Complexity theory;Trajectory","","1","","43","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Unifying Local and Global Multimodal Features for Place Recognition in Aliased and Low-Texture Environments","A. García-Hernández; R. Giubilato; K. H. Strobl; J. Civera; R. Triebel","Institute of Robotics and Mechatronics, German Aerospace Center (DLR); Institute of Robotics and Mechatronics, German Aerospace Center (DLR); Institute of Robotics and Mechatronics, German Aerospace Center (DLR); I3A, Universidad de Zaragoza, Spain; Institute of Robotics and Mechatronics, German Aerospace Center (DLR)",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","3991","3998","Perceptual aliasing and weak textures pose significant challenges to the task of place recognition, hindering the performance of Simultaneous Localization and Mapping (SLAM) systems. This paper presents a novel model, called UMF (standing for Unifying Local and Global Multimodal Features) that 1) leverages multi-modality by cross-attention blocks between vision and LiDAR features, and 2) includes a re-ranking stage that re-orders based on local feature matching the top-k candidates retrieved using a global representation. Our experiments, particularly on sequences captured on a planetary-analogous environment, show that UMF outperforms significantly previous baselines in those challenging aliased environments. Since our work aims to enhance the reliability of SLAM in all situations, we also explore its performance on the widely used RobotCar dataset, for broader applicability. Code and models are available at https://github.com/DLR-RM/UMF.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611563","Helmholtz Association; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611563","","Visualization;Simultaneous localization and mapping;Laser radar;Codes;Fuses;Transformers;Data models","","2","","53","Crown","8 Aug 2024","","","IEEE","IEEE Conferences"
"Stereo-NEC: Enhancing Stereo Visual-Inertial SLAM Initialization with Normal Epipolar Constraints","W. Wang; C. Chou; G. Sevagamoorthy; K. Chen; Z. Chen; Z. Feng; Y. Xia; F. Cai; Y. Xu; P. Mordohai","Stevens Institute of Technology, Hoboken, NJ, USA; OPPO US Research Center, Palo Alto, CA, USA; OPPO US Research Center, Palo Alto, CA, USA; OPPO US Research Center, Palo Alto, CA, USA; OPPO US Research Center, Palo Alto, CA, USA; OPPO US Research Center, Palo Alto, CA, USA; OPPO US Research Center, Palo Alto, CA, USA; Stony Brook University, Stony Brook, NY; OPPO US Research Center, Palo Alto, CA, USA; Stevens Institute of Technology, Hoboken, NJ, USA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","2691","2697","We propose an accurate and robust initialization approach for stereo visual-inertial SLAM systems. Unlike the current state-of-the-art method, which heavily relies on the accuracy of a pure visual SLAM system to estimate inertial variables without updating camera poses, potentially compromising accuracy and robustness, our approach offers a different solution. We realize the crucial impact of precise gyroscope bias estimation on rotation accuracy. This, in turn, affects trajectory accuracy due to the accumulation of translation errors. To address this, we first independently estimate the gyroscope bias and use it to formulate a maximum a posteriori problem for further refinement. After this refinement, we proceed to update the rotation estimation by performing IMU integration with gyroscope bias removed from gyroscope measurements. We then leverage robust and accurate rotation estimates to enhance translation estimation via 3-DoF bundle adjustment. Moreover, we introduce a novel approach for determining the success of the initialization by evaluating the residual of the normal epipolar constraint. Extensive evaluations on the EuRoC dataset illustrate that our method excels in accuracy and robustness. It outperforms ORB-SLAM3, the current leading stereo visual-inertial initialization method, in terms of absolute trajectory error and relative rotation error, while maintaining competitive computational speed. Notably, even with 5 keyframes for initialization, our method consistently surpasses the state-of-the-art approach using 10 keyframes in rotation accuracy. The open source code is available at https://github.com/ApdowJN/Stereo-NEC.git.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611458","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611458","","Bundle adjustment;Visualization;Accuracy;Simultaneous localization and mapping;Source coding;3-DOF;Estimation","","1","","25","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"SONIC: Sonar Image Correspondence using Pose Supervised Learning for Imaging Sonars","S. Gode; A. Hinduja; M. Kaess","Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","3766","3772","In this paper, we address the challenging problem of data association for underwater SLAM through a novel method for sonar image correspondence using learned features. We introduce SONIC (SONar Image Correspondence), a pose-supervised network designed to yield robust feature correspondence capable of withstanding viewpoint variations. The inherent complexity of the underwater environment stems from the dynamic and frequently limited visibility conditions, restricting vision to a few meters of often featureless expanses. This makes camera-based systems suboptimal in most open water application scenarios. Consequently, multibeam imaging sonars emerge as the preferred choice for perception sensors. However, they too are not without their limitations. While imaging sonars offer superior long-range visibility compared to cameras, their measurements can appear different from varying viewpoints. This inherent variability presents formidable challenges in data association, particularly for feature-based methods. Our method demonstrates significantly better performance in generating correspondences for sonar images which will pave the way for more accurate loop closure constraints and sonar-based place recognition. Code as well as simulated and real-world datasets are made public on https://github.com/rpl-cmu/sonic to facilitate further development in the field.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611678","Office of Naval Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611678","","Meters;Image sensors;Simultaneous localization and mapping;Image recognition;Sonar measurements;Supervised learning;Sonar","","1","","37","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"LIO-EKF: High Frequency LiDAR-Inertial Odometry using Extended Kalman Filters","Y. Wu; T. Guadagnino; L. Wiesmann; L. Klingbeil; C. Stachniss; H. Kuhlmann","Institute of Geodesy and Geoinformation, University of Bonn, Bonn, Germany; Institute of Geodesy and Geoinformation, University of Bonn, Bonn, Germany; Institute of Geodesy and Geoinformation, University of Bonn, Bonn, Germany; Institute of Geodesy and Geoinformation, University of Bonn, Bonn, Germany; Department of Engineering Science, University of Oxford, UK; Institute of Geodesy and Geoinformation, University of Bonn, Bonn, Germany",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","13741","13747","Odometry estimation is crucial for every autonomous system requiring navigation in an unknown environment. In modern mobile robots, 3D LiDAR-inertial systems are often used for this task. By fusing LiDAR scans and IMU measurements, these systems can reduce the accumulated drift caused by sequentially registering individual LiDAR scans and provide a robust pose estimate. Although effective, LiDAR-inertial odometry systems require proper parameter tuning to be deployed. In this paper, we propose LIO-EKF, a tightly-coupled LiDAR-inertial odometry system based on point-to-point registration and the classical extended Kalman filter scheme. We propose an adaptive data association that considers the relative pose uncertainty, the map discretization errors, and the LiDAR noise. In this way, we can substantially reduce the parameters to tune for a given type of environment. The experimental evaluation suggests that the proposed system performs on par with the state-of-the-art LiDAR-inertial odometry pipelines but is significantly faster in computing the odometry. The source code of our implementation is publicly available (https://github.com/YibinWu/LIO-EKF).","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610667","Deutsche Forschungsgemeinschaft; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610667","","Laser radar;Uncertainty;Three-dimensional displays;Source coding;Noise;Robot sensing systems;Odometry","","3","","31","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Semi-Supervised Learning for Visual Bird’s Eye View Semantic Segmentation","J. Zhu; L. Liu; Y. Tang; F. Wen; W. Li; Y. Liu","Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; China Mobile Research Institute, Beijing, China; Noah’s Ark Lab, Huawei Technologies, Beijing, China; Noah’s Ark Lab, Huawei Technologies, Beijing, China; Noah’s Ark Lab, Huawei Technologies, Beijing, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","9079","9085","Visual bird’s eye view (BEV) semantic segmentation helps autonomous vehicles understand the surrounding environment only from front-view (FV) images, including static elements (e.g., roads) and dynamic elements (e.g., vehicles, pedestrians). However, the high cost of annotation procedures of full-supervised methods limits the capability of the visual BEV semantic segmentation, which usually needs HD maps, 3D object bounding boxes, and camera extrinsic matrixes. In this paper, we present a novel semi-supervised framework for visual BEV semantic segmentation to boost performance by exploiting unlabeled images during the training. A consistency loss that makes full use of unlabeled data is then proposed to constrain the model on not only semantic prediction but also the BEV feature. Furthermore, we propose a novel and effective data augmentation method named conjoint rotation which reasonably augments the dataset while maintaining the geometric relationship between the FV images and the BEV semantic segmentation. Extensive experiments on the nuScenes dataset show that our semi-supervised framework can effectively improve prediction accuracy. To the best of our knowledge, this is the first work that explores improving visual BEV semantic segmentation performance using unlabeled data. The code is available at https://github.com/Junyu-Z/Semi-BEVseg.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611420","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611420","","Training;Visualization;Three-dimensional displays;Semantic segmentation;Semantics;Data augmentation;Transformers","","2","","38","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"VOLoc: Visual Place Recognition by Querying Compressed Lidar Map","X. Cai; Y. Wang; Z. Huang; Y. Shao; D. Li","Department of Computer Science, School of Information, Renmin University of China, Beijing, China; Department of Computer Science, School of Information, Renmin University of China, Beijing, China; Department of Computer Science, School of Information, Renmin University of China, Beijing, China; Department of Computer Science, School of Information, Renmin University of China, Beijing, China; Department of Computer Science, School of Information, Renmin University of China, Beijing, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","10192","10199","The availability of city-scale Lidar maps enables the potential of city-scale place recognition using mobile cameras. However, the city-scale Lidar maps generally need to be compressed for storage efficiency, which increases the difficulty of direct visual place recognition in compressed Lidar maps. This paper proposes VOLoc, an accurate and efficient visual place recognition method that exploits geometric similarity to directly query the compressed Lidar map via the real-time captured image sequence. In the offline phase, VOLoc compresses the Lidar maps using a Geometry-Preserving Compressor (GPC), in which the compression is reversible, a crucial requirement for the downstream 6DoF pose estimation. In the online phase, VOLoc proposes an online Geometric Recovery Module (GRM), which is composed of online Visual Odometry (VO) and a point cloud optimization module, such that the local scene structure around the camera is online recovered to build the Querying Point Cloud (QPC). Then the QPC is compressed by the same GPC, and is aggregated into a global descriptor by an attentionbased aggregation module, to query the compressed Lidar map in the vector space. A transfer learning mechanism is also proposed to improve the accuracy and the generality of the aggregation network. Extensive evaluations show that VOLoc provides localization accuracy even better than the Lidar-toLidar place recognition, setting up a new record for utilizing the compressed Lidar map by low-end mobile cameras. The code are publicly available at https://github.com/Master-cai/VOLoc.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610530","National Natural Science Foundation of China; National Natural Science Foundation of China; Renmin University of China; Renmin University of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610530","","Point cloud compression;Visualization;Laser radar;Image coding;Image recognition;Accuracy;Transfer learning","","","","54","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Block-Map-Based Localization in Large-Scale Environment","Y. Feng; Z. Jiang; Y. Shi; Y. Feng; X. Chen; H. Zhao; G. Zhou","Institute for AI Industry Research (AIR), Tsinghua University; Institute for AI Industry Research (AIR), Tsinghua University; QiYuan Lab; ShanghaiTech University; Institute for AI Industry Research (AIR), Tsinghua University; Institute for AI Industry Research (AIR), Tsinghua University; Institute for AI Industry Research (AIR), Tsinghua University",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","1709","1715","Accurate localization is an essential technology for the flexible navigation of robots in large-scale environments. Both SLAM-based and map-based localization will increase the computing load due to the increase in map size, which will affect downstream tasks such as robot navigation and services. To this end, we propose a localization system based on Block Maps (BMs) to reduce the computational load caused by maintaining large-scale maps. Firstly, we introduce a method for generating block maps and the corresponding switching strategies, ensuring that the robot can estimate the state in large-scale environments by loading local map information. Secondly, global localization according to Branch-and-Bound Search (BBS) in the 3D map is introduced to provide the initial pose. Finally, a graph-based optimization method is adopted with a dynamic sliding window that determines what factors are being marginalized whether a robot is exposed to a BM or switching to another one, which maintains the accuracy and efficiency of pose tracking. Comparison experiments are performed on publicly available large-scale datasets. Results show that the proposed method can track the robot pose even though the map scale reaches more than 6 kilometers, while efficient and accurate localization is still guaranteed on NCLT [6] and M2DGR [35]. Codes and data will be publicly available on https://github.com/YixFeng/blocklocalization.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610122","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610122","","Location awareness;Accuracy;Uncertainty;Three-dimensional displays;Navigation;Optimization methods;Switches","","1","","38","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"AYDIV: Adaptable Yielding 3D Object Detection via Integrated Contextual Vision Transformer","T. Dam; S. B. Dharavath; S. Alam; N. Lilith; S. Chakraborty; M. Feroskhan","Saab-NTU Joint Lab, Nanyang Technological University, Singapore; Indian Institute of Technology, Kharagpur, India; Saab-NTU Joint Lab, Nanyang Technological University, Singapore; Saab-NTU Joint Lab, Nanyang Technological University, Singapore; Indian Institute of Technology, Kharagpur, India; Saab-NTU Joint Lab, Nanyang Technological University, Singapore",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","10657","10664","Combining LiDAR and camera data has shown potential in enhancing short-distance object detection in autonomous driving systems. Yet, the fusion encounters difficulties with extended distance detection due to the contrast between LiDAR’s sparse data and the dense resolution of cameras. Besides, discrepancies in the two data representations further complicate fusion methods. We introduce AYDIV, a novel framework integrating a tri-phase alignment process specifically designed to enhance long-distance detection even amidst data discrepancies. AYDIV consists of the Global Contextual Fusion Alignment Transformer (GCFAT), which improves the extraction of camera features and provides a deeper understanding of large-scale patterns; the Sparse Fused Feature Attention (SFFA), which fine-tunes the fusion of LiDAR and camera details; and the Volumetric Grid Attention (VGA) for a comprehensive spatial data fusion. AYDIV’s performance on the Waymo Open Dataset (WOD) with an improvement of 1.24% in mAPH value(L2 difficulty) and the Argoverse2 Dataset with a performance improvement of 7.40% in AP value demonstrates its efficacy in comparison to other existing fusion-based methods. Our code is publicly available at https://github.com/sanjay-810/AYDIV2","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610908","School of Mechanical and Aerospace Engineering; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610908","GCFAT;SFFA;VGA;Multi-modal fusion;3D object detection","Three-dimensional displays;Laser radar;Object detection;Transformers;Cameras;Feature extraction;Robustness","","","","55","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"WayEx: Waypoint Exploration using a Single Demonstration","M. Levy; N. Saini; A. Shrivastava","University of Maryland, College Park; University of Maryland, College Park; University of Maryland, College Park",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","15009","15016","We propose WayEx, a new method for learning complex goal-conditioned robotics tasks from a single demonstration. Our approach distinguishes itself from existing imitation learning methods by demanding fewer expert examples and eliminating the need for information about the actions taken during the demonstration. This is accomplished by introducing a new reward function and employing a knowledge expansion technique. We demonstrate the effectiveness of WayEx, our waypoint exploration strategy, across six diverse tasks, showcasing its applicability in various environments. Notably, our method significantly reduces training time by ∼50% as compared to traditional reinforcement learning methods. WayEx obtains a higher reward than existing imitation learning methods given only a single demonstration. Furthermore, we demonstrate its success in tackling complex environments where standard approaches fall short. Appendix is available at: https://waypoint-ex.github.io.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611088","","Training;Imitation learning;Reinforcement learning;Task analysis;Standards;Robots","","","","32","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Unsupervised Spike Depth Estimation via Cross-modality Cross-domain Knowledge Transfer","J. Liu; Q. Zhang; X. Li; J. Li; G. Wang; M. Lu; T. Huang; S. Zhang","National Key Laboratory for Multimedia Information Processing, School of CS, Peking University; National Key Laboratory for Multimedia Information Processing, School of CS, Peking University; National Key Laboratory for Multimedia Information Processing, School of CS, Peking University; National Key Laboratory for Multimedia Information Processing, School of CS, Peking University; National Key Laboratory for Multimedia Information Processing, School of CS, Peking University; National Key Laboratory for Multimedia Information Processing, School of CS, Peking University; National Key Laboratory for Multimedia Information Processing, School of CS, Peking University; National Key Laboratory for Multimedia Information Processing, School of CS, Peking University",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","9109","9116","Neuromorphic spike data, an upcoming modality with high temporal resolution, has shown promising potential in autonomous driving by mitigating the challenges posed by high-velocity motion blur. However, training the spike depth estimation network holds significant challenges in two aspects: sparse spatial information for pixel-wise tasks and difficulties in achieving paired depth labels for temporally intensive spike streams. Therefore, we introduce open-source RGB data to support spike depth estimation, leveraging its annotations and spatial information. The inherent differences in modalities and data distribution make it challenging to directly apply transfer learning from open-source RGB to target spike data. To this end, we propose a cross-modality cross-domain (BiCross) framework to realize unsupervised spike depth estimation by introducing simulated mediate source spike data. Specifically, we design a Coarse-to-Fine Knowledge Distillation (CFKD) approach to facilitate comprehensive cross-modality knowledge transfer while preserving the unique strengths of both modalities, utilizing a spike-oriented uncertainty scheme. Then, we propose a Self-Correcting Teacher-Student (SCTS) mechanism to screen out reliable pixel-wise pseudo labels and ease the domain shift of the student model, which avoids error accumulation in target spike data. To verify the effectiveness of BiCross, we conduct extensive experiments on four scenarios, including Synthetic to Real, Extreme Weather, Scene Changing, and Real Spike. Our method achieves state-of-the-art (SOTA) performances, compared with RGB-oriented unsupervised depth estimation methods. Code and dataset: https://github.com/Theia-4869/BiCross.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610511","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610511","","Training;Uncertainty;Annotations;Transfer learning;Estimation;Task analysis;Spatial resolution","","","","62","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Object Permanence Filter for Robust Tracking with Interactive Robots","S. Peng; M. X. Wang; J. A. Shah; N. Figueroa","University of Pennsylvania, Philadelphia, USA; Massachusetts Institute of Technology, Cambridge, USA; Massachusetts Institute of Technology, Cambridge, USA; University of Pennsylvania, Philadelphia, USA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","4909","4915","Object permanence, which refers to the concept that objects continue to exist even when they are no longer perceivable through the senses, is a crucial aspect of human cognitive development. In this work, we seek to incorporate this understanding into interactive robots by proposing a set of assumptions and rules to represent object permanence in multi-object, multi-agent interactive scenarios. We integrate these rules into the particle filter, resulting in the Object Permanence Filter (OPF). For multi-object scenarios, we propose an ensemble of K interconnected OPFs, where each filter predicts plausible object tracks that are resilient to missing, noisy, and kinematically or dynamically infeasible measurements, thus bringing perceptional robustness. Through several interactive scenarios, we demonstrate that the proposed OPF approach provides robust tracking in human-robot interactive tasks agnostic to measurement type, even in the presence of prolonged and complete occlusion. Webpage: https://opfilter.github.io/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611528","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611528","","Atmospheric measurements;Robot sensing systems;Particle measurements;Information filters;Robustness;Particle filters;Noise measurement","","1","","20","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Semantically Guided Feature Matching for Visual SLAM","O. Ilter; I. Armeni; M. Pollefeys; D. Barath","Computer Vision and Geometry Group, ETH Zurich, Zurich, Switzerland; Computer Vision and Geometry Group, ETH Zurich, Zurich, Switzerland; Computer Vision and Geometry Group, ETH Zurich, Zurich, Switzerland; Computer Vision and Geometry Group, ETH Zurich, Zurich, Switzerland",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","12013","12019","We introduce a new algorithm that utilizes semantic information to enhance feature matching in visual SLAM pipelines. The proposed method constructs a high-dimensional semantic descriptor for each detected ORB feature. When integrated with traditional visual ones, these descriptors aid in establishing accurate tentative point correspondences between consecutive frames. Additionally, our semantic descriptors enrich 3D map points, enhancing loop closure detection by providing deeper insights into the underlying map regions. Experiments on public large-scale datasets demonstrate that our technique surpasses the accuracy of established methods. Importantly, given its detector-agnostic nature, our algorithm also amplifies the efficacy of modern keypoint detectors, such as SuperPoint. The implementation of our algorithm can be found on Github 3.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610238","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610238","","Visualization;Accuracy;Simultaneous localization and mapping;Three-dimensional displays;Semantics;Pipelines;Feature extraction","","","","30","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Scene Action Maps: Behavioural Maps for Navigation without Metric Information","J. Loo; D. Hsu","School of Computing & Smart Systems Institute, National University of Singapore; School of Computing & Smart Systems Institute, National University of Singapore",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","6354","6360","Humans are remarkable in their ability to navigate without metric information. We can read abstract 2D maps, such as floor-plans or hand-drawn sketches, and use them to navigate in unseen rich 3D environments, without requiring prior traversals to map out these scenes in detail. We posit that this is enabled by the ability to represent the environment abstractly as interconnected navigational behaviours, e.g., ""follow the corridor"" or ""turn right"", while avoiding detailed, accurate spatial information at the metric level. We introduce the Scene Action Map (SAM), a behavioural topological graph, and propose a learnable map-reading method, which parses a variety of 2D maps into SAMs. Map-reading extracts salient information about navigational behaviours from the overlooked wealth of pre-existing, abstract and inaccurate maps, ranging from floor-plans to sketches. We evaluate the performance of SAMs for navigation, by building and deploying a behavioural navigation stack on a quadrupedal robot. Videos and more information is available at: https://scene-action-maps.github.io.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610489","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610489","","Accuracy;Three-dimensional displays;Navigation;Affordances;Buildings;Dynamic range;Distance measurement","","","","37","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"SpawnNet: Learning Generalizable Visuomotor Skills from Pre-trained Network","X. Lin; J. So; S. Mahalingam; F. Liu; P. Abbeel",UC Berkeley; UC Berkeley; UC Berkeley; UC Berkeley; UC Berkeley,2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","4781","4787","The existing internet-scale image and video datasets cover a wide range of everyday objects and tasks, bringing the potential of learning policies that generalize in diverse scenarios. Prior works have explored visual pre-training with different self-supervised objectives. Still, the generalization capabilities of the learned policies and the advantages over well-tuned baselines remain unclear from prior studies. In this work, we present a focused study of the generalization capabilities of the pre-trained visual representations at the categorical level. We identify the key bottleneck in using a frozen pre-trained visual backbone for policy learning and then propose SpawnNet, a novel two-stream architecture that learns to fuse pre-trained multi-layer representations into a separate network to learn a robust policy. Through extensive simulated and real experiments, we show significantly better categorical generalization compared to prior approaches in imitation learning settings. Open-sourced code and videos can be found on our website: https://xingyu-lin.github.io/spawnnet/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610356","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610356","","Training;Visualization;Systematics;Codes;Fuses;Imitation learning;Task analysis","","3","","35","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Lightning NeRF: Efficient Hybrid Scene Representation for Autonomous Driving","J. Cao; Z. Li; N. Wang; C. Ma","MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China; Tusimple, Beijing, China; Tusimple, Beijing, China; MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","16803","16809","Recent studies have highlighted the promising application of NeRF in autonomous driving contexts. However, the complexity of outdoor environments, combined with the restricted viewpoints in driving scenarios, complicates the task of precisely reconstructing scene geometry. Such challenges often lead to diminished quality in reconstructions and extended durations for both training and rendering. To tackle these challenges, we present Lightning NeRF. It uses an efficient hybrid scene representation that effectively utilizes the geometry prior from LiDAR in autonomous driving scenarios. Lightning NeRF significantly improves the novel view synthesis performance of NeRF and reduces computational overheads. Through evaluations on real-world datasets, such as KITTI-360, Argoverse2, and our private dataset, we demonstrate that our approach not only exceeds the current state-of-the-art in novel view synthesis quality but also achieves a five-fold increase in training speed and a ten-fold improvement in rendering speed. Codes are available at https://github.com/VISION-SJTU/Lightning-NeRF.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611130","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611130","","Training;Point cloud compression;Geometry;Image color analysis;Lightning;Neural radiance field;Rendering (computer graphics)","","","","50","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Learning Agile Locomotion and Adaptive Behaviors via RL-augmented MPC","Y. Chen; Q. Nguyen","Department of Aerospace and Mechanical Engineering, University of Southern California, Los Angeles, CA; Department of Aerospace and Mechanical Engineering, University of Southern California, Los Angeles, CA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","11436","11442","In the context of legged robots, adaptive behavior involves adaptive balancing and adaptive swing foot reflection. While adaptive balancing counteracts perturbations to the robot, adaptive swing foot reflection helps the robot to navigate intricate terrains without foot entrapment. In this paper, we manage to bring both aspects of adaptive behavior to quadruped locomotion by combining RL and MPC while improving the robustness and agility of blind legged locomotion. This integration leverages MPC’s strength in predictive capabilities and RL’s adeptness in drawing from past experiences. Unlike traditional locomotion controls that separate stance foot control and swing foot trajectory, our innovative approach unifies them, addressing their lack of synchronization. At the heart of our contribution is the synthesis of stance foot control with swing foot reflection, improving agility and robustness in locomotion with adaptive behavior. A hallmark of our approach is robust blind stair climbing through swing foot reflection. Moreover, we intentionally designed the learning module as a general plugin for different robot platforms. We trained the policy and implemented our approach on the Unitree A1 robot, achieving impressive results: a peak turn rate of 8.5 rad/s, a peak running speed of 3 m/s, and steering at a speed of 2.5 m/s. Remarkably, this framework also allows the robot to maintain stable locomotion while bearing an unexpected load of 10 kg, or 83% of its body mass. We further demonstrate the generalizability and robustness of the same policy where it realizes zero-shot transfer to different robot platforms like Go1 and AlienGo robots for load carrying. Code is made available for the use of the research community at https://github.com/DRCL-USC/RL_augmented_MPC.git","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610453","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610453","","Legged locomotion;Perturbation methods;Reinforcement learning;Stairs;Robustness;Reflection;Trajectory","","1","","38","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Looking Inside Out: Anticipating Driver Intent From Videos","Y. -C. Kung; A. Zhang; J. Wang; J. Biswas","Walker Department Mechanical Engineering, the University of Texas, Austin, Texas, USA; Department of Computer Science, the University of Texas, Austin, Texas, USA; Walker Department Mechanical Engineering, the University of Texas, Austin, Texas, USA; Department of Computer Science, the University of Texas, Austin, Texas, USA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","5608","5614","Anticipating driver intention is an important task when vehicles of mixed and varying levels of human/machine autonomy share roadways. Driver intention can be leveraged to improve road safety, such as warning surrounding vehicles in the event the driver is attempting a dangerous maneuver. In this work, we propose a novel method of utilizing both in-cabin and external camera data to improve state-of-the-art performance in predicting future driver actions. Compared to existing methods, our approach explicitly extracts object and road-level features from external camera data, which we demonstrate are important features for predicting driver intention. Using our handcrafted features as inputs for both a transformer and a long-short-term-memory-based architecture, we empirically show that jointly utilizing in-cabin and external features improves performance compared to using in-cabin features alone. Furthermore, our models predict driver maneuvers more accurately and sooner than existing approaches, with an accuracy of 87.5% and an average prediction time of 4.35 seconds before the maneuver takes place. We release our model configurations and training scripts on https://github.com/ykung83/Driver-Intent-Prediction.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610257","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610257","","Training;Accuracy;Predictive models;Feature extraction;Cameras;Transformers;Vehicle dynamics","","","","19","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"LPS-Net: Lightweight Parameter-shared Network for Point Cloud-based Place Recognition","C. Liu; G. Chen; R. Song","School of Control Science and Engineering, Shandong University, Shandong, China; School of Control Science and Engineering, Shandong University, Shandong, China; School of Control Science and Engineering, Shandong University, Shandong, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","448","454","With innovation in fields such as autonomous driving and augmented reality, point cloud-based place recognition has gained significant attention. Many methods try to address this problem by extracting and matching global descriptors in a database, but they often must balance the extraction of comprehensive contextual information and large model sizes. To overcome this challenge, we propose a lightweight parameter-shared network (LPS-Net), which includes multiple bidirectional perception units (BPUs) to extract multiscale long-range contextual information and parameter-shared NetVLADs (PS-VLADs) to aggregate descriptors. A BPU includes a parameter-shared convolution module (SharedConv) that significantly compresses the model and enhances its ability to capture informative features. In PS-VLADs, we replace half the parameters used in the original NetVLAD with trainable scalars, which further reduces the model size, and theoretically prove their equivalence. Experimental results demonstrate that LPS-Net achieves state-of-the-art performance at the task of point cloud-based place recognition while maintaining a small model size. Code and supplementary materials can be found at https://github.com/Yavinr/LPS-Net.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610758","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610758","","Technological innovation;Codes;Databases;Convolution;Feature extraction;Data mining;Task analysis","","","","27","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Towards Robotic Tree Manipulation: Leveraging Graph Representations","C. H. Kim; M. Lee; O. Kroemer; G. Kantor","Carnegie Mellon University, Pittsburgh, PA, USA; Carnegie Mellon University, Pittsburgh, PA, USA; Carnegie Mellon University, Pittsburgh, PA, USA; Carnegie Mellon University, Pittsburgh, PA, USA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","11884","11890","There is growing interest in automating agricultural tasks that require intricate and precise interaction with specialty crops, such as trees and vines. However, developing robotic solutions for crop manipulation remains a difficult challenge due to complexities involved in modeling their deformable behavior. In this study, we present a framework for learning the deformation behavior of tree-like crops under contact interaction. Our proposed method involves encoding the state of a spring-damper modeled tree crop as a graph. This representation allows us to employ graph networks to learn both a forward model for predicting resulting deformations, and a contact policy for inferring actions to manipulate tree crops. We conduct a comprehensive set of experiments in a simulated environment and demonstrate generalizability of our method on previously unseen trees. Videos can be found on the project website: https://kantor-lab.github.io/tree_gnn","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611327","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611327","","Deformable models;Deformation;Crops;Reinforcement learning;Predictive models;System identification;Task analysis","","","","29","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"IFFNeRF: Initialisation Free and Fast 6DoF pose estimation from a single image and a NeRF model","M. Bortolon; T. Tsesmelis; S. James; F. Poiesi; A. D. Bue","Pattern Analysis and Computer Vision (PAVIS), Fondazione Istituto Italiano di Tecnologia (IIT), Genoa, Italy; Pattern Analysis and Computer Vision (PAVIS), Fondazione Istituto Italiano di Tecnologia (IIT), Genoa, Italy; Pattern Analysis and Computer Vision (PAVIS), Fondazione Istituto Italiano di Tecnologia (IIT), Genoa, Italy; Technologies of Vision (TeV), Fondazione Bruno Kessler (FBK), Trento, Italy; Pattern Analysis and Computer Vision (PAVIS), Fondazione Istituto Italiano di Tecnologia (IIT), Genoa, Italy",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","1985","1991","We introduce IFFNeRF to estimate the six degrees-of-freedom (6DoF) camera pose of a given image, building on the Neural Radiance Fields (NeRF) formulation. IFFNeRF is specifically designed to operate in real-time and eliminates the need for an initial pose guess that is proximate to the sought solution. IFFNeRF utilizes the Metropolis-Hasting algorithm to sample surface points from within the NeRF model. From these sampled points, we cast rays and deduce the color for each ray through pixel-level view synthesis. The camera pose can then be estimated as the solution to a Least Squares problem by selecting correspondences between the query image and the resulting bundle. We facilitate this process through a learned attention mechanism, bridging the query image embedding with the embedding of parameterized rays, thereby matching rays pertinent to the image. Through synthetic and real evaluation settings, we show that our method can improve the angular and translation error accuracy by 80.1% and 67.3%, respectively, compared to iNeRF while performing at 34fps on consumer hardware and not requiring the initial pose guess. Project page: https://mbortolon97.github.io/frenerf/","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610425","Horizon Europe; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610425","","Accuracy;Image color analysis;Robot vision systems;Pose estimation;Memory management;Neural radiance field;Cameras","","1","","26","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"VeloVox: A Low-Cost and Accurate 4D Object Detector with Single-Frame Point Cloud of Livox LiDAR","T. Ma; Z. Zheng; H. Zhou; X. Cai; X. Yang; Y. Li; B. Shi; H. Li","Multimedia Laboratory, The Chinese University of Hong Kong; University of California, Berkeley; Shanghai AI Laboratory; Shanghai AI Laboratory; Shanghai AI Laboratory; Shanghai AI Laboratory; Shanghai AI Laboratory; Multimedia Laboratory, The Chinese University of Hong Kong",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","1992","1998","Combining motion prediction in LiDAR-based 3D object detection is an effective method for improving overall accuracy, especially the downstream autonomous driving tasks. The recent development of low-cost LiDARs (e.g. Livox LiDAR) enables us to explore such 4D perception systems with a lower budget and higher performance. In this paper, we propose a 4D object detector, VeloVox, to establish accurate object detection and velocity estimation with a single-frame point cloud of Livox LiDAR. Based on the non-repetitive scanning pattern and point-level temporal nature, we propose a two-stage module to enhance the spatial-temporal point feature interaction along the time dimension. The aggregated feature also benefits a more accurate proposal refinement. To demonstrate the performance, comparison of VeloVox with several SOTA detector based baselines is evaluated on our in-house dataset and synthesized dataset built under Carla simulation. Code will be released at https://github.com/PJLab-ADG/VeloVox.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610427","Science and Technology Commission of Shanghai Municipality; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610427","","Point cloud compression;Laser radar;Accuracy;Three-dimensional displays;Estimation;Object detection;Detectors","","","","40","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Augmenting Lane Perception and Topology Understanding with Standard Definition Navigation Maps","K. Z. Luo; X. Weng; Y. Wang; S. Wu; J. Li; K. Q. Weinberger; Y. Wang; M. Pavone","Computer Information Sciences Department, Cornell University; NVIDIA; NVIDIA; NVIDIA; NVIDIA; Computer Information Sciences Department, Cornell University; Department of Computer Science, University of Southern California; Department of Aeronautics and Astronautics, Stanford University",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","4029","4035","Autonomous driving has traditionally relied heavily on costly and labor-intensive High Definition (HD) maps, hindering scalability. In contrast, Standard Definition (SD) maps are more affordable and have worldwide coverage, offering a scalable alternative. In this work, we systematically explore the effect of SD maps for real-time lane-topology understanding. We propose a novel framework to integrate SD maps into online map prediction and propose a Transformer-based encoder, SD Map Encoder Representations from transFormers, to leverage priors in SD maps for the lane-topology prediction task. This enhancement consistently and significantly boosts (by up to 60%) lane detection and topology prediction on current state-of-the-art online map prediction methods without bells and whistles and can be immediately incorporated into any Transformer-based lane-topology method. Code is available at https://github.com/NVlabs/SMERF.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610276","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610276","","Navigation;Lane detection;Scalability;Prediction methods;Transformers;Real-time systems;Topology","","2","","34","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"CalliRewrite: Recovering Handwriting Behaviors from Calligraphy Images without Supervision","Y. Luo; Z. Wu; Z. Lian","Wangxuan Institute of Computer Technology, Peking University; Wangxuan Institute of Computer Technology, Peking University; Wangxuan Institute of Computer Technology, Peking University",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","8671","8678","Human-like planning skills and dexterous manipulation have long posed challenges in the fields of robotics and artificial intelligence (AI). The task of reinterpreting calligraphy presents a formidable challenge, as it involves the decomposition of strokes and dexterous utensil control. Previous efforts have primarily focused on supervised learning of a single instrument, limiting the performance of robots in the realm of cross-domain text replication. To address these challenges, we propose CalliRewrite: a coarse-to-fine approach for robot arms to discover and recover plausible writing orders from diverse calligraphy images without requiring labeled demonstrations. Our model achieves fine-grained control of various writing utensils. Specifically, an unsupervised image-to-sequence model decomposes a given calligraphy glyph to obtain a coarse stroke sequence. Using an RL algorithm, a simulated brush is fine-tuned to generate stylized trajectories for robotic arm control. Evaluation in simulation and physical robot scenarios reveals that our method successfully replicates unseen fonts and styles while achieving integrity in unknown characters. To access our code and supplementary materials, please visit our project page: https://luoprojectpage.github.io/callirewrite/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610332","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610332","","Training;Limiting;Instruments;Supervised learning;Writing;Manipulators;Trajectory","","","","51","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Grasp Anything: Combining Teacher-Augmented Policy Gradient Learning with Instance Segmentation to Grasp Arbitrary Objects","M. Mosbach; S. Behnke","Autonomous Intelligent Systems Group, Computer Science Institute VI – Intelligent Systems and Robotics – and the Center for Robotics and the Lamarr Institute for Machine Learning and Artificial Intelligence, University of Bonn, Germany; Autonomous Intelligent Systems Group, Computer Science Institute VI – Intelligent Systems and Robotics – and the Center for Robotics and the Lamarr Institute for Machine Learning and Artificial Intelligence, University of Bonn, Germany",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","7515","7521","Interactive grasping from clutter, akin to human dexterity, is one of the longest-standing problems in robot learning. Challenges stem from the intricacies of visual perception, the demand for precise motor skills, and the complex interplay between the two. In this work, we present Teacher-Augmented Policy Gradient (TAPG), a novel two-stage learning framework that synergizes reinforcement learning and policy distillation. After training a teacher policy to master the motor control based on object pose information, TAPG facilitates guided, yet adaptive, learning of a sensorimotor policy, based on object segmentation. We zero-shot transfer from simulation to a real robot by using Segment Anything Model for promptable object segmentation. Our trained policies adeptly grasp a wide variety of objects from cluttered scenarios in simulation and the real world based on human-understandable prompts. Furthermore, we show robust zero-shot transfer to novel objects. Videos of our experiments are available at https://maltemosbach.github.io/grasp_anything.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610700","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610700","","Training;Instance segmentation;Motor drives;Object segmentation;Reinforcement learning;Robot sensing systems;Motors","","","","27","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Transformer-CNN Cohort: Semi-supervised Semantic Segmentation by the Best of Both Students","X. Zheng; Y. Luo; C. Fu; K. Liu; L. Wang","AI Thrust, HKUST(GZ), Guangzhou, China; Department of Computer Science, Brown University, USA; Department of Computer Science and Engineering, Northeastern University, Shenyang, China; SMMG/ROAS Thrust, HKUST(GZ); Dept. of CSE, AI/CMA Thrust, HKUST(GZ), HKUST, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","11147","11154","The popular methods for semi-supervised semantic segmentation mostly adopt a unitary network model using convolutional neural networks (CNNs) and enforce consistency of the model’s predictions over perturbations applied to the inputs or model. However, such a learning paradigm suffers from two critical limitations: a) learning the discriminative features for the unlabeled data; b) learning both global and local information from the whole image. In this paper, we propose a novel Semi-supervised Learning (SSL) approach, called Transformer-CNN Cohort (TCC), that consists of two students with one based on the vision transformer (ViT) and the other based on the CNN. Our method subtly incorporates the multi-level consistency regularization on the predictions and the heterogeneous feature spaces via pseudo-labeling for the unlabeled data. First, as the inputs of the ViT student are image patches, the feature maps extracted encode crucial class-wise statistics. To this end, we propose class-aware feature consistency distillation (CFCD) that first leverages the outputs of each student as the pseudo labels and generates class-aware feature (CF) maps for knowledge transfer between the two students. Second, as the ViT student has more uniform representations for all layers, we propose consistency-aware cross distillation (CCD) to transfer knowledge between the pixel-wise predictions from the cohort. We validate the TCC framework on Cityscapes and Pascal VOC 2012 datasets, which outperforms existing SSL methods by a large margin. Project page: https://vlislab22.github.io/TCC/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611036","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611036","","Semantic segmentation;Perturbation methods;Predictive models;Semisupervised learning;Transformers;Feature extraction;Convolutional neural networks","","3","","61","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"On camera model conversions","E. Goichon; G. Caron; P. Vasseur; F. Kanehiro","MIS Laboratory, Université de Picardie Jules Verne, Amiens, France; MIS Laboratory, Université de Picardie Jules Verne, Amiens, France; MIS Laboratory, Université de Picardie Jules Verne, Amiens, France; CNRS-AIST JRL(Joint Robotics Laboratory), IRL, National Institute of Advanced Industrial Science and Technology (AIST), Tsukuba, Japan",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","12262","12268","On the one hand, cameras of conventional field-of-view usually considered in computer vision and robotics are very often modeled as a pinhole plus possibly a distortion model. On the other hand, there is a large variety of models for panoramic cameras. Many camera models have been proposed for fisheye cameras, catadioptric cameras, and super fisheye cameras. But in both cases, few models offer the possibility of converting them into another model.This paper contributes to filling this gap in, to allow an algorithm designed with a projection model to accept data of a camera calibrated with another model. So, a pre-existing data set can be used without having to recalibrate the camera. We provide the methodology and mathematical developments for three conversions considering three different types of cameras that are evaluated with respect to calibration and within a visual Simultaneous Localization And Mapping benchmark. The source code of the camera model conversions studied in this paper is shared within the libPeR library for Perception in Robotics: https://github.com/PerceptionRobotique/libPeRbase.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610009","","Visualization;Simultaneous localization and mapping;Computational modeling;Source coding;Robot vision systems;Cameras;Mathematical models","","","","27","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Talk2BEV: Language-enhanced Bird’s-eye View Maps for Autonomous Driving","T. Choudhary; V. Dewangan; S. Chandhok; S. Priyadarshan; A. Jain; A. K. Singh; S. Srivastava; K. M. Jatavallabhula; K. M. Krishna",IIIT Hyderabad; IIIT Hyderabad; University of British Columbia; IIIT Hyderabad; IIIT Hyderabad; University of Tartu; TensorTour Inc.; MIT; IIIT Hyderabad,2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","16345","16352","This work introduces Talk2BEV, a large vision-language model (LVLM)1 interface for bird’s-eye view (BEV) maps commonly used in autonomous driving. While existing perception systems for autonomous driving scenarios have largely focused on a pre-defined (closed) set of object categories and driving scenarios, Talk2BEV eliminates the need for BEV-specific training, relying instead on well-performing pre-trained LVLMs. This enables a single system to cater to a variety of autonomous driving tasks encompassing visual and spatial reasoning, predicting the intents of traffic actors, and decision-making based on visual cues. We extensively evaluate Talk2BEV on a large number of scene understanding tasks that rely on both the ability to interpret freeform natural language queries, and in grounding these queries to the visual context embedded into the language-enhanced BEV map. To enable further research in LVLMs for autonomous driving scenarios, we develop and release Talk2BEV-Bench, a benchmark encompassing 1000 human-annotated BEV scenarios, with more than 20,000 questions and ground-truth responses from the NuScenes dataset. We encourage the reader to view the demos on our project page: https://llmbev.github.io/talk2bev/","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611485","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611485","","Training;Visualization;Trajectory planning;Pipelines;Natural languages;Benchmark testing;Cognition","","","","48","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Open-Fusion: Real-time Open-Vocabulary 3D Mapping and Queryable Scene Representation","K. Yamazaki; T. Hanyu; K. Vo; T. Pham; M. Tran; G. Doretto; A. Nguyen; N. Le","Department of EECS, AICV Lab, University of Arkansas, USA; Department of EECS, AICV Lab, University of Arkansas, USA; Department of EECS, AICV Lab, University of Arkansas, USA; Department of EECS, AICV Lab, University of Arkansas, USA; Department of EECS, AICV Lab, University of Arkansas, USA; Department of CSCE, West Virginia University, USA; Department of CS, University of Liverpool, UK; Department of EECS, AICV Lab, University of Arkansas, USA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","9411","9417","Precise 3D environmental mapping with semantics is essential in robotics. Existing methods often rely on pre-defined concepts during training or are time-intensive when generating semantic maps. This paper presents Open-Fusion, an approach for real-time open-vocabulary 3D mapping and queryable scene representation using RGB-D data. Open-Fusion harnesses the power of a pretrained vision-language foundation model (VLFM) for open-set semantic comprehension and employs the Truncated Signed Distance Function (TSDF) for swift 3D scene reconstruction. By leveraging the VLFM, we extract region-based embeddings and their associated confidence maps. These are then integrated with the 3D knowledge from TSDF using an enhanced Hungarian-based feature-matching mechanism. In particular, Open-Fusion delivers outstanding annotation-free 3D segmentation for open vocabulary query without the need for additional 3D training. Benchmark tests on the ScanNet dataset against leading zero-shot methods highlight Open-Fusion’s superiority. Furthermore, it seamlessly combines the strengths of region-based VLFM and TSDF, facilitating real-time 3D scene comprehension that includes object concepts and open-world semantics. We encourage the readers to view the demos on our project page: https://uark-aicv.github.io/OpenFusion","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610193","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610193","","Training;Vocabulary;Three-dimensional displays;Semantics;Benchmark testing;Feature extraction;Real-time systems","","2","","33","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language Models","Z. Long; G. Killick; R. McCreadie; G. Aragon-Camarasa","School of Computing Science, University of Glasgow, United Kingdom; School of Computing Science, University of Glasgow, United Kingdom; School of Computing Science, University of Glasgow, United Kingdom; School of Computing Science, University of Glasgow, United Kingdom",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","12428","12435","Robotic vision applications often necessitate a wide range of visual perception tasks, such as object detection, segmentation, and identification. While there have been substantial advances in these individual tasks, integrating specialized models into a unified vision pipeline presents significant engineering challenges and costs. Recently, Multimodal Large Language Models (MLLMs) have emerged as novel backbones for various downstream tasks. We argue that leveraging the pre-training capabilities of MLLMs enables the creation of a simplified framework, thus mitigating the need for task-specific encoders. Specifically, the large-scale pretrained knowledge in MLLMs allows for easier fine-tuning to downstream robotic vision tasks and yields superior performance. We introduce the RoboLLM framework, equipped with a BEiT-3 backbone, to address all visual perception tasks in the ARMBench challenge—a large-scale robotic manipulation dataset about real-world warehouse scenarios. RoboLLM not only outperforms existing baselines but also substantially reduces the engineering burden associated with model selection and tuning. All the code used in this paper can be found in https://github.com/longkukuhi/RoboLLM.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610797","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610797","","Knowledge engineering;Large language models;Pipelines;Object segmentation;Object detection;Object recognition;Task analysis","","3","","46","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"CrackNex: a Few-shot Low-light Crack Segmentation Model Based on Retinex Theory for UAV Inspections","Z. Yao; J. Xu; S. Hou; M. C. Chuah","Department of Computer Science and Engineering, P.C. Rossin College of Engineering and Applied Science, Lehigh University, Bethlehem, PA, USA; Department of Computer Science and Engineering, P.C. Rossin College of Engineering and Applied Science, Lehigh University, Bethlehem, PA, USA; Department of Computer Science and Engineering, P.C. Rossin College of Engineering and Applied Science, Lehigh University, Bethlehem, PA, USA; Department of Computer Science and Engineering, P.C. Rossin College of Engineering and Applied Science, Lehigh University, Bethlehem, PA, USA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","11155","11162","Routine visual inspections of concrete structures are imperative for upholding the safety and integrity of critical infrastructure. Such visual inspections sometimes happen under low-light conditions, e.g., checking for bridge health. Crack segmentation under such conditions is challenging due to the poor contrast between cracks and their surroundings. However, most deep learning methods are designed for well-illuminated crack images and hence their performance drops dramatically in low-light scenes. In addition, conventional approaches require many annotated low-light crack images which is time-consuming. In this paper, we address these challenges by proposing CrackNex, a framework that utilizes reflectance information based on Retinex Theory to learn a unified illumination-invariant representation. Furthermore, we utilize few-shot segmentation to solve the inefficient training data problem. In CrackNex, both a support prototype and a reflectance prototype are extracted from the support set. Then, a prototype fusion module is designed to integrate the features from both prototypes. CrackNex outperforms the SOTA methods on multiple datasets. Additionally, we present the first benchmark dataset, LCSD, for low-light crack segmentation. LCSD consists of 102 well-illuminated crack images and 41 low-light crack images. The dataset and code are available at https://github.com/zy1296/CrackNex.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611660","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611660","","Reflectivity;Training;Measurement;Image segmentation;Visualization;Prototypes;Training data","","4","","59","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"An Extrinsic Calibration Method between LiDAR and GNSS/INS for Autonomous Driving","J. Pi; G. Yan; C. Wang; X. Cai; B. Shi","Autonomous Driving Group, Shanghai AI Laboratory, China; Autonomous Driving Group, Shanghai AI Laboratory, China; Autonomous Driving Group, Shanghai AI Laboratory, China; Autonomous Driving Group, Shanghai AI Laboratory, China; Autonomous Driving Group, Shanghai AI Laboratory, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","14520","14526","Accurate and reliable sensor calibration is critical for fusing LiDAR and inertial measurements in autonomous driving. This paper proposes a novel three-stage extrinsic calibration method between LiDAR and GNSS/INS for autonomous driving. The first stage can quickly calibrate the extrinsic parameters between the sensors through point cloud surface features so that the extrinsic can be narrowed from a large initial error to a small error range in little time. The second stage can further calibrate the extrinsic parameters based on LiDAR-mapping space occupancy while removing motion distortion. In the final stage, the z-axis (the vertical direction relative to the ground plane) errors caused by the plane motion of the autonomous vehicle are corrected, and an accurate extrinsic parameter is finally obtained. Specifically, This method utilizes the planar features in the environment, making it possible to quickly carry out calibration. Experimental results on real-world datasets demonstrate the reliability and accuracy of our method. The codes are open-sourced on the Github website. The code link is https://github.com/OpenCalib/LiDAR2INS.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610541","Science and Technology Commission of Shanghai Municipality; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610541","","Point cloud compression;Laser radar;Accuracy;Codes;Calibration;Reliability;Robotics and automation","","","","30","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"RoboKeyGen: Robot Pose and Joint Angles Estimation via Diffusion-based 3D Keypoint Generation","Y. Tian; J. Zhang; G. Huang; B. Wang; P. Wang; J. Pang; H. Dong","CFCS, School of CS, Peking University and National Key Laboratory for Multimedia Information Processing; CFCS, School of CS, Peking University and National Key Laboratory for Multimedia Information Processing; Huawei; Huawei; School of Software & Microelectronics and National Engineering Research Center for Software Engineering, Peking University; Chinese University of Hong Kong; CFCS, School of CS, Peking University and National Key Laboratory for Multimedia Information Processing",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","5375","5381","Estimating robot pose and joint angles is significant in advanced robotics, enabling applications like robot collaboration and online hand-eye calibration. However, the introduction of unknown joint angles makes prediction more complex than simple robot pose estimation, due to its higher dimensionality. Previous methods either regress 3D keypoints directly or utilise a render&compare strategy. These approaches often falter in terms of performance or efficiency and grapple with the cross-camera gap problem. This paper presents a novel framework that bifurcates the high-dimensional prediction task into two manageable subtasks: 2D keypoints detection and lifting 2D keypoints to 3D. This separation promises enhanced performance without sacrificing the efficiency innate to keypoint-based techniques. A vital component of our method is the lifting of 2D keypoints to 3D keypoints. Common deterministic regression methods may falter when faced with uncertainties from 2D detection errors or self-occlusions. Leveraging the robust modeling potential of diffusion models, we reframe this issue as a conditional 3D keypoints generation task. To bolster cross-camera adaptability, we introduce the Normalised Camera Coordinate Space (NCCS), ensuring alignment of estimated 2D keypoints across varying camera intrinsics. Experimental results demonstrate that the proposed method outperforms the state-of-the-art render&compare method and achieves higher inference speed. Furthermore, the tests accentuate our method’s robust cross-camera generalisation capabilities. We intend to release both the dataset and code in https://nimolty.github.io/Robokeygen/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611423","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611423","","Solid modeling;Three-dimensional displays;Uncertainty;Robot kinematics;Robot vision systems;Pose estimation;Collaboration","","1","","38","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"DRIVE: Data-driven Robot Input Vector Exploration","D. Baril; S. -P. Deschênes; L. Coupal; C. Goffin; J. Lépine; P. Giguère; F. Pomerleau","Northern Robotics Laboratory, Université Laval, Quebec City, Quebec, Canada; Northern Robotics Laboratory, Université Laval, Quebec City, Quebec, Canada; Northern Robotics Laboratory, Université Laval, Quebec City, Quebec, Canada; Northern Robotics Laboratory, Université Laval, Quebec City, Quebec, Canada; Northern Robotics Laboratory, Université Laval, Quebec City, Quebec, Canada; Northern Robotics Laboratory, Université Laval, Quebec City, Quebec, Canada; Northern Robotics Laboratory, Université Laval, Quebec City, Quebec, Canada",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","5829","5836","An accurate motion model is a fundamental component of most autonomous navigation systems. While much work has been done on improving model formulation, no standard protocol exists for gathering empirical data required to train models. In this work, we address this issue by proposing Data-driven Robot Input Vector Exploration (DRIVE), a protocol that enables characterizing uncrewed ground vehicles (UGVs) input limits and gathering empirical model training data. We also propose a novel learned slip approach outperforming similar acceleration learning approaches. Our contributions are validated through an extensive experimental evaluation, cumulating over 7km and 1.8h of driving data over three distinct UGVs and four terrain types. We show that our protocol offers increased predictive performance over common human-driven data-gathering protocols. Furthermore, our protocol converges with 46 s of training data, almost four times less than the shortest human dataset gathering protocol. We show that the operational limit for our model is reached in extreme slip conditions encountered on surfaced ice. DRIVE is an efficient way of characterizing UGV motion in its operational conditions. Our code and dataset are both available online at this link: https://github.com/norlab-ulaval/DRIVE.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611172","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611172","","Training;Adaptation models;Protocols;Training data;Predictive models;Vectors;Data models","","1","","25","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"OCC-VO: Dense Mapping via 3D Occupancy-Based Visual Odometry for Autonomous Driving","H. Li; Y. Duan; X. Zhang; H. Liu; J. Ji; Y. Zhang","School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","17961","17967","Visual Odometry (VO) plays a pivotal role in autonomous systems, with a principal challenge being the lack of depth information in camera images. This paper introduces OCC-VO, a novel framework that capitalizes on recent advances in deep learning to transform 2D camera images into 3D semantic occupancy, thereby circumventing the traditional need for concurrent estimation of ego poses and landmark locations. Within this framework, we utilize the TPV-Former to convert surround view cameras’ images into 3D semantic occupancy. Addressing the challenges presented by this transformation, we have specifically tailored a pose estimation and mapping algorithm that incorporates Semantic Label Filter, Dynamic Object Filter, and finally, utilizes Voxel PFilter for maintaining a consistent global semantic map. Evaluations on the Occ3D-nuScenes not only showcase a 20.6% improvement in Success Ratio and a 29.6% enhancement in trajectory accuracy against ORB-SLAM3, but also emphasize our ability to construct a comprehensive map. Our implementation is open-sourced and available at: https://github.com/USTCLH/OCC-VO.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611516","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611516","","Three-dimensional displays;Accuracy;Simultaneous localization and mapping;Heuristic algorithms;Semantics;Filtering algorithms;Cameras","","2","","31","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Uncertainty-aware hybrid paradigm of nonlinear MPC and model-based RL for offroad navigation: Exploration of transformers in the predictive model","F. Lotfi; K. Virji; F. Faraji; L. Berry; A. Holliday; D. Meger; G. Dudek","Mobile Robotics Lab (MRL), Faculty of Computer Science, McGill University, Montreal, Canada; Mobile Robotics Lab (MRL), Faculty of Computer Science, McGill University, Montreal, Canada; Mobile Robotics Lab (MRL), Faculty of Computer Science, McGill University, Montreal, Canada; Mobile Robotics Lab (MRL), Faculty of Computer Science, McGill University, Montreal, Canada; Mobile Robotics Lab (MRL), Faculty of Computer Science, McGill University, Montreal, Canada; Mobile Robotics Lab (MRL), Faculty of Computer Science, McGill University, Montreal, Canada; Mobile Robotics Lab (MRL), Faculty of Computer Science, McGill University, Montreal, Canada",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","2925","2931","In this paper, we investigate a hybrid scheme that combines nonlinear model predictive control (MPC) and model-based reinforcement learning (RL) for navigation planning of an autonomous model car across offroad, unstructured terrains without relying on predefined maps. Our innovative approach takes inspiration from BADGR, an LSTM-based network that primarily concentrates on environment modeling, but distinguishes itself by substituting LSTM modules with transformers to greatly elevate the performance of our model. Addressing uncertainty within the system, we train an ensemble of predictive models and estimate the mutual information between model weights and outputs, facilitating dynamic horizon planning through the introduction of variable speeds. Further enhancing our methodology, we incorporate a nonlinear MPC controller that accounts for the intricacies of the vehicle’s model and states. The model-based RL facet produces steering angles and quantifies inherent uncertainty. At the same time, the nonlinear MPC suggests optimal throttle settings, striking a balance between goal attainment speed and managing model uncertainty influenced by velocity. In the conducted studies, our approach excels over the existing baseline by consistently achieving higher metric values in predicting future events and seamlessly integrating the vehicle’s kinematic model for enhanced decision-making. The code and the evaluation data are available at (Github-repo).","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610452","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610452","Model-based RL;transformers;nonlinear MPC;uncertainty-aware planning;offroad navigation","Uncertainty;Navigation;Reinforcement learning;Predictive models;Transformers;Planning;Trajectory","","","","30","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Surgical Gym: A high-performance GPU-based platform for reinforcement learning with surgical robots","S. Schmidgall; A. Krieger; J. Eshraghian","Department of Electrical Engineering, Johns Hopkins University, Baltimore, MD, USA; Department of Mechanical Engineering, Johns Hopkins University, Baltimore, MD, USA; Department of Electrical Engineering, University of California, Santa Cruz, Santa Cruz, CA, USA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","13354","13361","Recent advances in robot-assisted surgery have resulted in progressively more precise, efficient, and minimally invasive procedures, sparking a new era of robotic surgical intervention. This enables doctors, in collaborative interaction with robots, to perform traditional or minimally invasive surgeries with improved outcomes through smaller incisions. Recent efforts are working toward making robotic surgery more autonomous which has the potential to reduce variability of surgical outcomes and reduce complication rates. Deep reinforcement learning methodologies offer scalable solutions for surgical automation, but their effectiveness relies on extensive data acquisition due to the absence of prior knowledge in successfully accomplishing tasks. Due to the intensive nature of simulated data collection, previous works have focused on making existing algorithms more efficient. In this work, we focus on making the simulator more efficient, making training data much more accessible than previously possible. We introduce Surgical Gym, an open-source high performance platform for surgical robot learning where both the physics simulation and reinforcement learning occur directly on the GPU. We demonstrate between 100-5000× faster training times compared with previous surgical learning platforms. The code is available at: https://github.com/SamuelSchmidgall/SurgicalGym.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610448","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610448","","Training;Medical robotics;Minimally invasive surgery;Torque;Training data;Hardware;PD control","","1","","50","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Contact Energy Based Hindsight Experience Prioritization","E. Sayar; Z. Bing; C. D’Eramo; O. S. Oguz; A. Knoll",Technical University of Munich; Technical University of Munich; University of Würzburg; Bilkent University; Technical University of Munich,2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","5434","5440","Multi-goal robot manipulation tasks with sparse rewards are difficult for reinforcement learning (RL) algorithms due to the inefficiency in collecting successful experiences. Recent algorithms such as Hindsight Experience Replay (HER) expedite learning by taking advantage of failed trajectories and replacing the desired goal with one of the achieved states so that any failed trajectory can be utilized as a contribution to learning. However, HER uniformly chooses failed trajectories, without taking into account which ones might be the most valuable for learning. In this paper, we address this problem and propose a novel approach Contact Energy Based Prioritization (CEBP) to select the samples from the replay buffer based on rich information due to contact, leveraging the touch sensors in the gripper of the robot and object displacement. Our prioritization scheme favors sampling of contact-rich experiences, which are arguably the ones providing the largest amount of information. We evaluate our proposed approach on various sparse reward robotic tasks and compare it with the state-of-the-art methods. We show that our method surpasses or performs on par with those methods on robot manipulation tasks. Finally, we deploy the trained policy from our method to a real Franka robot for a pick-and-place task. We observe that the robot can solve the task successfully. The videos and code are publicly available at: https://erdiphd.github.io/HER_force/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610910","Horizon 2020 Framework Programme; Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610910","","Training;Codes;Friction;Catalysts;Tactile sensors;Reinforcement learning;Trajectory","","","","41","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"LiteTrack: Layer Pruning with Asynchronous Feature Extraction for Lightweight and Efficient Visual Tracking","Q. Wei; B. Zeng; J. Liu; L. He; G. Zeng","School of Computer, Guangdong University of Technology, Guangzhou, China; School of Computer, Guangdong University of Technology, Guangzhou, China; School of Computer, Guangdong University of Technology, Guangzhou, China; Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen, China; School of Computer, Guangdong University of Technology, Guangzhou, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","4968","4975","The recent advancements in transformer-based visual trackers have led to significant progress, attributed to their strong modeling capabilities. However, as performance improves, running latency correspondingly increases, presenting a challenge for real-time robotics applications, especially on edge devices with computational constraints. In response to this, we introduce LiteTrack, an efficient transformer-based tracking model optimized for high-speed operations across various devices. It achieves a more favorable trade-off between accuracy and efficiency than the other lightweight trackers. The main innovations of LiteTrack encompass: 1) asynchronous feature extraction and interaction between the template and search region for better feature fushion and cutting redundant computation, and 2) pruning encoder layers from a heavy tracker to refine the balnace between performance and speed. As an example, our fastest variant, LiteTrack-B4, achieves 65.2% AO on the GOT-10k benchmark, surpassing all preceding efficient trackers, while running over 100 fps with ONNX on the Jetson Orin NX edge device. Moreover, our LiteTrack-B9 reaches competitive 72.2% AO on GOT-10k and 82.4% AUC on TrackingNet, and operates at 171 fps on an NVIDIA 2080Ti GPU. The code and demo materials will be available at https://github.com/TsingWei/LiteTrack.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610022","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610022","","Performance evaluation;Visualization;Technological innovation;Accuracy;Computational modeling;Feature extraction;Transformers","","1","","55","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"GelRoller: A Rolling Vision-based Tactile Sensor for Large Surface Reconstruction Using Self-Supervised Photometric Stereo Method","Z. Zhang; H. Ma; Y. Zhou; J. Ji; H. Yang","State Key Laboratory of Intelligent Manufacturing Equipment and Technology (SKL-IMET), School of Mechanical Science and Engineering, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Intelligent Manufacturing Equipment and Technology (SKL-IMET), School of Mechanical Science and Engineering, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Intelligent Manufacturing Equipment and Technology (SKL-IMET), School of Mechanical Science and Engineering, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Intelligent Manufacturing Equipment and Technology (SKL-IMET), School of Mechanical Science and Engineering, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Intelligent Manufacturing Equipment and Technology (SKL-IMET), School of Mechanical Science and Engineering, Huazhong University of Science and Technology, Wuhan, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","7961","7967","Accurate perception of the surrounding environment stands as a primary objective for robots. Through tactile interaction, vision-based tactile sensors provide the capability to capture high-resolution and multi-modal surface information of objects, thereby facilitating robots in achieving more dexterous manipulations. However, the prevailing GelSight sensors entail intricate calibration procedures, posing challenges in their application on curved surfaces and requiring the maintenance of stable lighting conditions throughout experimentation. Additionally, constrained by shape and structure, current vision-based tactile sensors are predominantly applied to measurements within a limited area. In this study, we design a novel cylindrical vision-based tactile sensor that enables continuous and swift perception of large-scale object surfaces through rolling. To tackle the challenges posed by laborious calibration processes, we propose a self-supervised photometric stereo method based on deep learning, which eliminates pre-calibration requirements and enables the derivation of surface normals from a single image without relying on stable lighting conditions. Finally, we perform surface reconstruction from normal and point cloud registration on the multiple frames of images obtained by rolling the cylindrical sensor, resulting in large surface reconstruction. We compare our method with the representative lookup table method in the GelSight sensors. The results show that the proposed method enhances both reconstruction accuracy and robustness, thereby demonstrating the potential of the proposed sensor in large-scale surface reconstruction. Codes and mechanical structures are available at: https://github.com/ZhangZhiyuanZhang/GelRoller","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610417","National Natural Science Foundation of China; Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610417","","Mechanical sensors;Surface reconstruction;Accuracy;Shape;Shape measurement;Tactile sensors;Lighting","","","","27","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"MoPA: Multi-Modal Prior Aided Domain Adaptation for 3D Semantic Segmentation","H. Cao; Y. Xu; J. Yang; P. Yin; S. Yuan; L. Xie","Centre for Advanced Robotics Technology Innovation (CARTIN), School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Department of Electrical and Computer Engineering, National University of Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Centre for Advanced Robotics Technology Innovation (CARTIN), School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Centre for Advanced Robotics Technology Innovation (CARTIN), School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Centre for Advanced Robotics Technology Innovation (CARTIN), School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","9463","9470","Multi-modal unsupervised domain adaptation (MM-UDA) for 3D semantic segmentation is a practical solution to embed semantic understanding in autonomous systems without expensive point-wise annotations. While previous MM-UDA methods can achieve overall improvement, they suffer from significant class-imbalanced performance, restricting their adoption in real applications. This imbalanced performance is mainly caused by: 1) self-training with imbalanced data and 2) the lack of pixel-wise 2D supervision signals. In this work, we propose Multi-modal Prior Aided (MoPA) domain adaptation to improve the performance of rare objects. Specifically, we develop Valid Ground-based Insertion (VGI) to rectify the imbalance supervision signals by inserting prior rare objects collected from the wild while avoiding introducing artificial artifacts that lead to trivial solutions. Meanwhile, our SAM consistency loss leverages the 2D prior semantic masks from SAM as pixel-wise supervision signals to encourage consistent predictions for each object in the semantic mask. The knowledge learned from modal-specific prior is then shared across modalities to achieve better rare object segmentation. Extensive experiments show that our method achieves state-of-the-art performance on the challenging MM-UDA benchmark. Code will be available at https://github.com/AronCao49/MoPA.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610316","National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610316","","Three-dimensional displays;Codes;Accuracy;Autonomous systems;Annotations;Semantic segmentation;Semantics","","1","","52","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"TRTM: Template-based Reconstruction and Target-oriented Manipulation of Crumpled Cloths","W. Wang; G. Li; M. Zamora; S. Coros","ETH Zurich, Switzerland; ETH Zurich, Switzerland; ETH Zurich, Switzerland; ETH Zurich, Switzerland",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","12522","12528","Precise reconstruction and manipulation of the crumpled cloths is challenging due to the high dimensionality of cloth models, as well as the limited observation at self-occluded regions. We leverage the recent progress in the field of single-view reconstruction to template-based reconstruct the crumpled cloths from their top-view depth observations only, with our proposed sim-real registration protocols. In contrast to previous implicit cloth representations, our reconstruction mesh explicitly describes the positions and visibilities of the entire cloth mesh vertices, enabling more efficient dual-arm and single-arm target-oriented manipulations. Experiments demonstrate that our TRTM system can be applied to daily cloths that have similar topologies as our template mesh, but with different shapes, sizes, patterns, and physical properties. Videos, datasets, pre-trained models, and code can be downloaded from our project website: https://wenbwa.github.io/TRTM/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10609868","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10609868","","Protocols;Codes;Shape;Topology;Robotics and automation;Videos","","","","36","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Exploitation-Guided Exploration for Semantic Embodied Navigation","J. Wasserman; G. Chowdhary; A. Gupta; U. Jain",University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; Carnegie Mellon University; Carnegie Mellon University,2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","2901","2908","In the recent progress in embodied navigation and sim-to-robot transfer, modular policies have emerged as a de facto framework. However, there is more to compositionality beyond the decomposition of the learning load into modular components. In this work, we investigate a principled way to syntactically combine these components. Particularly, we propose Exploitation-Guided Exploration (XgX) where separate modules for exploration and exploitation come together in a novel and intuitive manner. We configure the exploitation module to take over in the deterministic final steps of navigation i.e. when the goal becomes visible. Crucially, an exploitation module teacher-forces the exploration module and continues driving an overridden policy optimization. XgX, with effective decomposition and novel guidance, improves the state-of-the-art performance on the challenging object navigation task from 70% to 73%. Along with better accuracy, through targeted analysis, we show that XgX is also more efficient at goal-conditioned exploration. Finally, we show sim-to-real transfer to robot hardware and XgX performs over two-fold better than the best baseline from simulation benchmarking. Project page: xgxvisnav.github.io","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610117","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610117","","Accuracy;Navigation;Semantics;Benchmark testing;Hardware;Task analysis;Robots","","","","78","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"A 3D Vector Field and Gaze Data Fusion Framework for Hand Motion Intention Prediction in Human-Robot Collaboration","M. Jayasuriya; G. Hu; D. D. K. Le; K. Ang; S. Sankaran; D. Liu","Robotics Institute (RI), University of Technology, Sydney, Australia; Robotics Institute (RI), University of Technology, Sydney, Australia; Robotics Institute (RI), University of Technology, Sydney, Australia; Robotics Institute (RI), University of Technology, Sydney, Australia; Robotics Institute (RI), University of Technology, Sydney, Australia; Robotics Institute (RI), University of Technology, Sydney, Australia",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","5637","5643","In human-robot collaboration (HRC) settings, hand motion intention prediction (HMIP) plays a pivotal role in ensuring prompt decision-making, safety, and an intuitive collaboration experience. Precise and robust HMIP with low computational resources remains a challenge due to the stochastic nature of hand motion and the diversity of HRC tasks. This paper proposes a framework that combines hand trajectories and gaze data to foster robust, real-time HMIP with minimal to no training. A novel 3D vector field method is introduced for hand trajectory representation, leveraging minimum jerk trajectory predictions to discern potential hand motion endpoints. This is statistically combined with gaze fixation data using a weighted Naive Bayes Classifier (NBC). Acknowledging the potential variances in saccadic eye motion due to factors like fatigue or inattentiveness, we incorporate stationary gaze entropy to gauge visual concentration, thereby adjusting the contribution of gaze fixation to the HMIP. Empirical experiments substantiate that the proposed framework robustly predicts intended endpoints of hand motion before at least 50% of the trajectory is completed. It also successfully exploits gaze fixations when the human operator is attentive and mitigates its influence when the operator loses focus. A real-time implementation in a construction HRC scenario (collaborative tiling) showcases the intuitive nature and potential efficiency gains to be leveraged by introducing the proposed HMIP into HRC contexts. The opens-ource implementation of the framework is made available at https://github.com/maleenj/hmip_ros.git.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10609996","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10609996","","Training;Visualization;Three-dimensional displays;Collaboration;Data integration;Benchmark testing;Fatigue","","","","26","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"RH20T: A Comprehensive Robotic Dataset for Learning Diverse Skills in One-Shot","H. -S. Fang; H. Fang; Z. Tang; J. Liu; C. Wang; J. Wang; H. Zhu; C. Lu",Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University,2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","653","660","A key challenge for robotic manipulation in open domains is how to acquire diverse and generalizable skills for robots. Recent progress in one-shot imitation learning and robotic foundation models have shown promise in transferring trained policies to new tasks based on demonstrations. This feature is attractive for enabling robots to acquire new skills and improve their manipulative ability. However, due to limitations in the training dataset, the current focus of the community has mainly been on simple cases, such as push or pick-place tasks, relying solely on visual guidance. In reality, there are many complex skills, some of which may even require both visual and tactile perception to solve. This paper aims to unlock the potential for an agent to generalize to hundreds of real-world skills with multi-modal perception. To achieve this, we have collected a dataset comprising over 110,000 contact-rich robot manipulation sequences across diverse skills, contexts, robots, and camera viewpoints, all collected in the real world. Each sequence in the dataset includes visual, force, audio, and action information. Moreover, we also provide a corresponding human demonstration video and a language description for each robot sequence. We have invested significant efforts in calibrating all the sensors and ensuring a high-quality dataset. The dataset is made publicly available on our website: rh20t.github.io.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611615","Research and Development; Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611615","","Training;Visualization;Imitation learning;Robot vision systems;Force;Cameras;Sensors","","10","","46","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Jacquard V2: Refining Datasets using the Human In the Loop Data Correction Method","Q. Li; S. Yuan","College of Robotics Science and Engineering, Northeastern University, Shenyang City, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","7932","7938","In the context of rapid advancements in industrial automation, vision-based robotic grasping plays an increasingly crucial role. In order to enhance visual recognition accuracy, the utilization of large-scale datasets is imperative for training models to acquire implicit knowledge related to the handling of various objects. Creating datasets from scratch is a time and labor-intensive process. Moreover, existing datasets often contain errors due to automated annotations aimed at expediency, making the improvement of these datasets a substantial research challenge. Consequently, several issues have been identified in the annotation of grasp bounding boxes within the popular Jacquard Grasp Dataset [1]. We propose utilizing a Human-In-The-Loop(HIL) method to enhance dataset quality. This approach relies on backbone deep learning networks to predict object positions and orientations for robotic grasping. Predictions with Intersection over Union (IOU) values below 0.2 undergo an assessment by human operators. After their evaluation, the data is categorized into False Negatives(FN) and True Negatives(TN). FN are then subcategorized into either missing annotations or catastrophic labeling errors. Images lacking labels are augmented with valid grasp bounding box information, whereas images afflicted by catastrophic labeling errors are completely removed. The open-source tool Labelbee was employed for 53,026 iterations of HIL dataset enhancement, leading to the removal of 2,884 images and the incorporation of ground truth information for 30,292 images. The enhanced dataset, named the Jacquard V2 Grasping Dataset, served as the training data for a range of neural networks. We have empirically demonstrated that these dataset improvements significantly enhance the training and prediction performance of the same network, resulting in an increase of 7.1% across most popular detection architectures for ten iterations. This refined dataset will be accessible on One Drive and Baidu Netdisk, while the associated tools, source code, and benchmarks will be made available on GitHub (https://github.com/lqh12345/Jacquard_V2).","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611652","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611652","dataset refinement;human in the loop;robotic vision grasping;Pseudo-Label generation","Training;Visualization;Accuracy;Annotations;Web and internet services;Training data;Grasping","","1","","32","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"MF-MOS: A Motion-Focused Model for Moving Object Segmentation","J. Cheng; K. Zeng; Z. Huang; X. Tang; J. Wu; C. Zhang; X. Chen; R. Fan","School of Electronic and Information Engineering, South China Normal University, Foshan, China; School of Electronic and Information Engineering, South China Normal University, Foshan, China; Department of Computer Science, Aberystwyth University, Aberystwyth, U.K.; School of Electronic and Information Engineering, South China Normal University, Foshan, China; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, China; School of Internet of Things Engineering, Jiangnan University, Wuxi, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Electronics & Information Engineering, Shanghai Research Institute for Intelligent Autonomous Systems, the State Key Laboratory of Intelligent Autonomous Systems, and Frontiers Science Center for Intelligent Autonomous Systems, Tongji University, Shanghai, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","12499","12505","Moving object segmentation (MOS) provides a reliable solution for detecting traffic participants and thus is of great interest in the autonomous driving field. Dynamic capture is always critical in the MOS problem. Previous methods capture motion features from the range images directly. Differently, we argue that the residual maps provide greater potential for motion information, while range images contain rich semantic guidance. Based on this intuition, we propose MF-MOS, a novel motion-focused model with a dual-branch structure for LiDAR moving object segmentation. Novelly, we decouple the spatial-temporal information by capturing the motion from residual maps and generating semantic features from range images, which are used as movable object guidance for the motion branch. Our straightforward yet distinctive solution can make the most use of both range images and residual maps, thus greatly improving the performance of the LiDAR-based MOS task. Remarkably, our MF-MOS achieved a leading IoU of 76.7% on the MOS leaderboard of the SemanticKITTI dataset upon submission, demonstrating the current state-of-the-art performance. The implementation of our MF-MOS has been released at https://github.com/SCNU-RISLAB/MF-MOS.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611400","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611400","","Laser radar;Semantics;Dynamics;Object segmentation;Data augmentation;Data models;Reliability","","7","","23","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"PointSSC: A Cooperative Vehicle-Infrastructure Point Cloud Benchmark for Semantic Scene Completion","Y. Yan; B. Liu; J. Ai; Q. Li; R. Wan; J. Pu","Institute of Science and Technology for Brain-Inspired Intelligence, Fudan University; Institute of Science and Technology for Brain-Inspired Intelligence, Fudan University; Mogo Auto Intelligence and Telematics Information Technology Company Ltd; Mogo Auto Intelligence and Telematics Information Technology Company Ltd; Mogo Auto Intelligence and Telematics Information Technology Company Ltd; Institute of Science and Technology for Brain-Inspired Intelligence, Fudan University",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","17027","17034","Semantic Scene Completion (SSC) aims to jointly generate space occupancies and semantic labels for complex 3D scenes. Most existing SSC models focus on volumetric representations, which are memory-inefficient for large outdoor spaces. Point clouds provide a lightweight alternative but existing benchmarks lack outdoor point cloud scenes with semantic labels. To address this, we introduce PointSSC, the first cooperative vehicle-infrastructure point cloud benchmark for semantic scene completion. These scenes exhibit long-range perception and minimal occlusion. We develop an automated annotation pipeline leveraging Semantic Segment Anything to efficiently assign semantics. To benchmark progress, we propose a LiDAR-based model with a Spatial-Aware Transformer for global and local feature extraction and a Completion and Segmentation Cooperative Module for joint completion and segmentation. PointSSC provides a challenging testbed to drive advances in semantic point cloud completion for real-world navigation. The code and datasets are available at https://github.com/yyxssm/PointSSC.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610043","","Point cloud compression;Three-dimensional displays;Navigation;Semantics;Pipelines;Benchmark testing;Transformers","","1","","50","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Orbit-Surgical: An Open-Simulation Framework for Learning Surgical Augmented Dexterity","Q. Yu; M. Moghani; K. Dharmarajan; V. Schorp; W. C. -H. Panitch; J. Liu; K. Hari; H. Huang; M. Mittal; K. Goldberg; A. Garg","University of Toronto; University of Toronto; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of Toronto; University of California, Berkeley; University of California, Berkeley; ETH Zurich; University of California, Berkeley; University of Toronto",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","15509","15516","Physics-based simulations have accelerated progress in robot learning for driving, manipulation, and locomotion. Yet, a fast, accurate, and robust surgical simulation environment remains a challenge. In this paper, we present Orbit-Surgical, a physics-based surgical robot simulation framework with photorealistic rendering in NVIDIA Omniverse. We provide 14 benchmark surgical tasks for the da Vinci Research Kit (dVRK) and Smart Tissue Autonomous Robot (STAR) which represent common subtasks in surgical training. Orbit-Surgical leverages GPU parallelization to train reinforcement learning and imitation learning algorithms to facilitate study of robot learning to augment human surgical skills. Orbit-Surgical also facilitates realistic synthetic data generation for active perception tasks. We demonstrate Orbit-Surgical sim-to-real transfer of learned policies onto a physical dVRK robot.Project website: orbit-surgical.github.io","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611637","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611637","","Training;Medical robotics;Imitation learning;Stars;Reinforcement learning;Rendering (computer graphics);Orbits","","2","","48","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"OmniLRS: A Photorealistic Simulator for Lunar Robotics","A. Richard; J. Kamohara; K. Uno; S. Santra; D. van der Meer; M. Olivares-Mendez; K. Yoshida","The Space Robotics Research Group at the Interdisciplinary Research Center for Security Reliability and Trust (SnT), University of Luxembourg; Department of Aerospace Engineering, Space Robotics Lab. (SRL), Graduate School of Engineering, Tohoku University, Sendai, Japan; Department of Aerospace Engineering, Space Robotics Lab. (SRL), Graduate School of Engineering, Tohoku University, Sendai, Japan; Department of Aerospace Engineering, Space Robotics Lab. (SRL), Graduate School of Engineering, Tohoku University, Sendai, Japan; The Space Robotics Research Group at the Interdisciplinary Research Center for Security Reliability and Trust (SnT), University of Luxembourg; The Space Robotics Research Group at the Interdisciplinary Research Center for Security Reliability and Trust (SnT), University of Luxembourg; Department of Aerospace Engineering, Space Robotics Lab. (SRL), Graduate School of Engineering, Tohoku University, Sendai, Japan",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","16901","16907","Developing algorithms for extra-terrestrial robotic exploration has always been challenging. Along with the complexity associated with these environments, one of the main issues remains the evaluation of said algorithms. With the regained interest in lunar exploration, there is also a demand for quality simulators that will enable the development of lunar robots. In this paper, we propose Omniverse Lunar Robotic-Sim (OmniLRS) that is a photorealistic Lunar simulator based on Nvidia’s robotic simulator. This simulation provides fast procedural environment generation, multi-robot capabilities, along with synthetic data pipeline for machine-learning applications. It comes with ROS1 and ROS2 bindings to control not only the robots, but also the environments. This work also performs sim-to-real rock instance segmentation to show the effectiveness of our simulator for image-based perception. Trained on our synthetic data, a yolov8 model achieves performance close to a model trained on real-world data, with 5% performance gap. When finetuned with real data, the model achieves 14% higher average precision than the model trained on real-world data, demonstrating our simulator’s photorealism. The code is fully open-source, accessible here: https://github.com/AntoineRichard/OmniLRS, and comes with demonstrations.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610026","","Instance segmentation;Training;Photorealism;Moon;Pipelines;Machine learning;Rocks","","2","","39","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"DMSA - Dense Multi Scan Adjustment for LiDAR Inertial Odometry and Global Optimization","D. Skuddis; N. Haala","Institute for Photogrammetry and Geoinformatics, University of Stuttgart, Stuttgart, Germany; Institute for Photogrammetry and Geoinformatics, University of Stuttgart, Stuttgart, Germany",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","12027","12033","We propose a new method for fine registering multiple point clouds simultaneously. The approach is characterized by being dense, therefore point clouds are not reduced to pre-selected features in advance. Furthermore, the approach is robust against small overlaps and dynamic objects, since no direct correspondences are assumed between point clouds. Instead, all points are merged into a global point cloud, whose scattering is then iteratively reduced. This is achieved by dividing the global point cloud into uniform grid cells whose contents are subsequently modeled by normal distributions. We show that the proposed approach can be used in a sliding window continuous trajectory optimization combined with IMU measurements to obtain a highly accurate and robust LiDAR inertial odometry estimation. Furthermore, we show that the proposed approach is also suitable for large scale keyframe optimization to increase accuracy. We provide the source code and some experimental data on https://github.com/davidskdds/DMSA_LiDAR_SLAM.git.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610818","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610818","","Point cloud compression;Accuracy;Laser radar;Source coding;Estimation;Scattering;Gaussian distribution","","1","","32","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"The LuViRA Dataset: Synchronized Vision, Radio, and Audio Sensors for Indoor Localization","I. Yaman; G. Tian; M. Larsson; P. Persson; M. Sandra; A. Dürr; E. Tegler; N. Challa; H. Garde; F. Tufvesson; K. Åström; O. Edfors; S. Malkowsky; L. Liu","Department of Electrical and Information Technology, Lund University; Department of Electrical and Information Technology, Lund University; Centre for Mathematical Sciences, Lund University; Centre for Mathematical Sciences, Lund University; Department of Electrical and Information Technology, Lund University; Department of Computer Science, Lund University; Centre for Mathematical Sciences, Lund University; Department of Electrical and Information Technology, Lund University; Humanities Lab, Lund University; Department of Electrical and Information Technology, Lund University; Centre for Mathematical Sciences, Lund University; Department of Electrical and Information Technology, Lund University; Department of Electrical and Information Technology, Lund University; Department of Electrical and Information Technology, Lund University",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","11920","11926","We present a synchronized multisensory dataset for accurate and robust indoor localization: the Lund University Vision, Radio, and Audio (LuViRA) Dataset. The dataset includes color images, corresponding depth maps, inertial measurement unit (IMU) readings, channel response between a 5G massive multiple-input and multiple-output (MIMO) testbed and user equipment, audio recorded by 12 microphones, and accurate six degrees of freedom (6DOF) pose ground truth of 0.5 mm. We synchronize these sensors to ensure that all data is recorded simultaneously. A camera, speaker, and transmit antenna are placed on top of a slowly moving service robot, and 89 trajectories are recorded. Each trajectory includes 20 to 50 seconds of recorded sensor data and ground truth labels. Data from different sensors can be used separately or jointly to perform localization tasks, and data from the motion capture (mocap) system is used to verify the results obtained by the localization algorithms. The main aim of this dataset is to enable research on sensor fusion with the most commonly used sensors for localization tasks. Moreover, the full dataset or some parts of it can also be used for other research areas such as channel estimation, image classification, etc. Our dataset is available at: https://github.com/ilaydayaman/LuViRA_Dataset","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610237","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610237","","Location awareness;Accuracy;Service robots;5G mobile communication;Sensor fusion;Sensors;Trajectory","","1","","25","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"FC-Planner: A Skeleton-guided Planning Framework for Fast Aerial Coverage of Complex 3D Scenes","C. Feng; H. Li; M. Zhang; X. Chen; B. Zhou; S. Shen","Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; School of Artificial Intelligence, Sun Yat-Sen University, Zhuhai, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; School of Artificial Intelligence, Sun Yat-Sen University, Zhuhai, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","8686","8692","3D coverage path planning for UAVs is a crucial problem in diverse practical applications. However, existing methods have shown unsatisfactory system simplicity, computation efficiency, and path quality in large and complex scenes. To address these challenges, we propose FC-Planner, a skeleton-guided planning framework that can achieve fast aerial coverage of complex 3D scenes without pre-processing. We decompose the scene into several simple subspaces by a skeleton-based space decomposition (SSD). Additionally, the skeleton guides us to effortlessly determine free space. We utilize the skeleton to efficiently generate a minimal set of specialized and informative viewpoints for complete coverage. Based on SSD, a hierarchical planner effectively divides the large planning problem into independent sub-problems, enabling parallel planning for each subspace. The carefully designed global and local planning strategies are then incorporated to guarantee both high quality and efficiency in path generation. We conduct extensive benchmark and real-world tests, where FC-Planner computes over 10 times faster compared to state-of-the-art methods with shorter path and more complete coverage. The source code will be made publicly available to benefit the community3. Project page: https://hkust-aerial-robotics.github.io/FC-Planner.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610621","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610621","","Three-dimensional displays;Source coding;Robot vision systems;Skeleton;Planning;Trajectory;Computational efficiency","","2","","27","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"LHMap-loc: Cross-Modal Monocular Localization Using LiDAR Point Cloud Heat Map","X. Wu; J. Xu; P. Hu; G. Wang; H. Wang","Department of Automation, Key Laboratory of System Control and Information Processing of Ministry of Education, Key Laboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai Engineering Research Center of Intelligent Control and Management, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Key Laboratory of System Control and Information Processing of Ministry of Education, Key Laboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai Engineering Research Center of Intelligent Control and Management, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Key Laboratory of System Control and Information Processing of Ministry of Education, Key Laboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai Engineering Research Center of Intelligent Control and Management, Shanghai Jiao Tong University, Shanghai, China; Department of Engineering, University of Cambridge, Cambridge, U.K; Department of Automation, Key Laboratory of System Control and Information Processing of Ministry of Education, Key Laboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai Engineering Research Center of Intelligent Control and Management, Shanghai Jiao Tong University, Shanghai, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","8500","8506","Localization using a monocular camera in the pre-built LiDAR point cloud map has drawn increasing attention in the field of autonomous driving and mobile robotics. However, there are still many challenges (e.g. difficulties of map storage, poor localization robustness in large scenes) in accurately and efficiently implementing cross-modal localization. To solve these problems, a novel pipeline termed LHMap-loc is proposed, which achieves accurate and efficient monocular localization in LiDAR maps. Firstly, feature encoding is carried out on the original LiDAR point cloud map by generating offline heat point clouds, by which the size of the original LiDAR map is compressed. Then, an end-to-end online pose regression network is designed based on optical flow estimation and spatial attention to achieve real-time monocular visual localization in a pre-built map. In addition, a series of experiments have been conducted to prove the effectiveness of the proposed method. Our code is available at: https://github.com/IRMVLab/LHMap-loc.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610718","Science and Technology Commission of Shanghai Municipality; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610718","","Location awareness;Point cloud compression;Visualization;Laser radar;Accuracy;Robot vision systems;Real-time systems","","","","31","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"TSCM: A Teacher-Student Model for Vision Place Recognition Using Cross-Metric Knowledge Distillation","Y. Shen; M. Liu; H. Lu; X. Chen","Faculty of Robot Science and Engineering, Northeastern University, China; SIASUN Robot& Automation Co., Ltd, China; College of Intelligence Science and Technology, National University of Defense Technology, China; College of Intelligence Science and Technology, National University of Defense Technology, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","1789","1795","Visual place recognition (VPR) plays a pivotal role in autonomous exploration and navigation of mobile robots within complex outdoor environments. While cost-effective and easily deployed, camera sensors are sensitive to lighting and weather changes, and even slight image alterations can greatly affect VPR efficiency and precision. Existing methods overcome this by exploiting powerful yet large networks, leading to significant consumption of computational resources. In this paper, we propose a high-performance teacher and lightweight student distillation framework called TSCM. It exploits our devised cross-metric knowledge distillation to narrow the performance gap between the teacher and student models, maintaining superior performance while enabling minimal computational load during deployment. We conduct comprehensive evaluations on large-scale datasets, namely Pittsburgh30k and Pittsburgh250k. Experimental results demonstrate the superiority of our method over baseline models in terms of recognition accuracy and model parameter efficiency. Moreover, our ablation studies show that the proposed knowledge distillation technique surpasses other counterparts. The code of our method has been released at https://github.com/nubot-nudt/TSCM.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611612","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611612","","Visualization;Accuracy;Navigation;Computational modeling;Robot vision systems;Real-time systems;Sensors","","","","39","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Better Monocular 3D Detectors with LiDAR from the Past","Y. You; C. P. Phoo; C. Andres Diaz-Ruiz; K. Z. Luo; W. -L. Chao; M. Campbell; B. Hariharan; K. Q. Weinberger","Computer Science Department, Cornell University; Computer Science Department, Cornell University; Mechanical and Aerospace Engineering Department, Cornell University; Computer Science Department, Cornell University; Department of Computer Science and Engineering, Ohio State University; Mechanical and Aerospace Engineering Department, Cornell University; Computer Science Department, Cornell University; Computer Science Department, Cornell University",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","6634","6641","Accurate 3D object detection is crucial to autonomous driving. Though LiDAR-based detectors have achieved impressive performance, the high cost of LiDAR sensors precludes their widespread adoption in affordable vehicles. Camera-based detectors are cheaper alternatives but often suffer inferior performance compared to their LiDAR-based counterparts due to inherent depth ambiguities in images. In this work, we seek to improve monocular 3D detectors by leveraging unlabeled historical LiDAR data. Specifically, at inference time, we assume that the camera-based detectors have access to multiple unlabeled LiDAR scans from past traversals at locations of interest (potentially from other high-end vehicles equipped with LiDAR sensors). Under this setup, we proposed a novel, simple, and end-to-end trainable framework, termed AsyncDepth, to effectively extract relevant features from asynchronous LiDAR traversals of the same location for monocular 3D detectors. We show consistent and significant performance gain (up to 9 AP) across multiple state-of-the-art models and datasets with a negligible additional latency of 9.66 ms and a small storage cost. Our code can be found at https://github.com/YurongYou/AsyncDepth.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610444","Cornell Center for Materials Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610444","","Three-dimensional displays;Laser radar;Costs;Detectors;Object detection;Performance gain;Feature extraction","","","","55","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Doduo: Learning Dense Visual Correspondence from Unsupervised Semantic-Aware Flow","Z. Jiang; H. Jiang; Y. Zhu","Department of Computer Science, the University of Texas at Austin; Department of Computer Science, the University of Texas at Austin; Department of Computer Science, the University of Texas at Austin",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","12420","12427","Dense visual correspondence plays a vital role in robotic perception. This work focuses on establishing the dense correspondence between a pair of images that captures dynamic scenes undergoing substantial transformations. We introduce Doduo to learn general dense visual correspondence from in-the-wild images and videos without ground truth supervision. Given a pair of images, it estimates the dense flow field encoding the displacement of each pixel in one image to its corresponding pixel in the other image. Doduo uses flow-based warping to acquire supervisory signals for the training. Incorporating semantic priors with self-supervised flow training, Doduo produces accurate dense correspondence robust to the dynamic changes of the scenes. Trained on an in-the-wild video dataset, Doduo illustrates superior performance on point-level correspondence estimation over existing self-supervised correspondence learning baselines. We also apply Doduo to articulation estimation and zero-shot goal-conditioned manipulation, underlining its practical applications in robotics. Code and additional visualizations are available at https://ut-austin-rpl.github.io/Doduo/","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611587","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611587","","Training;Representation learning;Visualization;Accuracy;Semantics;Estimation;Self-supervised learning","","1","","66","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Improving Radial Imbalances with Hybrid Voxelization and RadialMix for LiDAR 3D Semantic Segmentation","J. Li; H. Dai; Y. Wang; G. Cao; C. Luo; Y. Ding",Zhejiang University; University of Glasgow; YUNJI Technology; YUNJI Technology; YUNJI Technology; Zhejiang University,2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","7710","7717","Huge progress has been made in LiDAR 3D semantic segmentation, but there are two under-explored imbalances on the radial axis: points are unevenly concentrated on the near side, and the distribution of foreground object instances is skewed to the near side. This leads the training of the model to favor semantics at the near side with the majority of points and object instances. Both the cylindrical and the spherical voxelizations aim to address the problem of imbalanced point distribution by increasing the volume of voxels along the radial distance to include fewer near-side points in a smaller voxel and more far-side points in a bigger voxel. However, this causes a problem of the receptive field enlarging along the radial distance, which is not desirable in LiDAR 3D segmentation. This can be addressed in cubic voxelization which has a fixed volume of voxels. Thus, we propose a new LiDAR 3D semantic segmentation network (Hi-VoxelNet) with Hybrid Voxelization that leverages the advantages of cubic, cylindrical, and spherical voxelizations for hybrid voxel feature learning. To address the radial imbalance of object instances, we propose a novel data augmentation technique termed as RadialMix that uses radial sample duplication to increase the number of distant foreground object instances and mixes the radial duplication with another point cloud for enriching the training samples. With the joint improvements of the radial imbalances, our method archives state-of-the-art performance on nuScenes and SemanticKITTI datasets, and it shows significant improvements along the radial axis. Our code is publicly available at https://github.com/jialeli1/lidarseg3d.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610604","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610604","","Training;Representation learning;Point cloud compression;Three-dimensional displays;Laser radar;Codes;Semantic segmentation","","","","53","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Towards Motion Forecasting with Real-World Perception Inputs: Are End-to-End Approaches Competitive?","Y. Xu; L. Chambon; É. Zablocki; M. Chen; A. Alahi; M. Cord; P. Pérez","Valeo.ai, Paris, France; Valeo.ai, Paris, France; Valeo.ai, Paris, France; Valeo.ai, Paris, France; EPFL, Lausanne, Switzerland; Valeo.ai, Paris, France; Valeo.ai, Paris, France",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","18428","18435","Motion forecasting is crucial in enabling autonomous vehicles to anticipate the future trajectories of surrounding agents. To do so, it requires solving mapping, detection, tracking, and then forecasting problems, in a multi-step pipeline. In this complex system, advances in conventional forecasting methods have been made using curated data, i.e., with the assumption of perfect maps, detection, and tracking. This paradigm, however, ignores any errors from upstream modules. Meanwhile, an emerging end-to-end paradigm, that tightly integrates the perception and forecasting architectures into joint training, promises to solve this issue. However, the evaluation protocols between the two methods were so far incompatible and their comparison was not possible. In fact, conventional forecasting methods are usually not trained nor tested in real-world pipelines (e.g., with upstream detection, tracking, and mapping modules). In this work, we aim to bring forecasting models closer to the real-world deployment. First, we propose a unified evaluation pipeline for forecasting methods with real-world perception inputs, allowing us to compare conventional and end-to-end methods for the first time. Second, our in-depth study uncovers a substantial performance gap when transitioning from curated to perception-based data. In particular, we show that this gap (1) stems not only from differences in precision but also from the nature of imperfect inputs provided by perception modules, and that (2) is not trivially reduced by simply finetuning on perception outputs. Based on extensive experiments, we provide recommendations for critical areas that require improvement and guidance towards more robust motion forecasting in the real world. The evaluation library for benchmarking models under standardized and practical conditions is provided: https://github.com/valeoai/MFEval.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610201","Horizon Europe; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610201","","Training;Protocols;Systematics;Tracking;Pipelines;Predictive models;Benchmark testing","","2","","52","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"DISO: Direct Imaging Sonar Odometry","S. Xu; K. Zhang; Z. Hong; Y. Liu; S. Wang","I-X and Department of Electrical and Electronic Engineering, Imperial College, London, UK; I-X and Department of Electrical and Electronic Engineering, Imperial College, London, UK; School of Engineering and Physical Sciences, Heriot-Watt University, UK; Department of Mechanical Engineering, University College, London, UK; I-X and Department of Electrical and Electronic Engineering, Imperial College, London, UK",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","8573","8579","This paper introduces a novel sonar odometry system that estimates the relative spatial transformation between two sonar image frames. Considering the unique challenges, such as low resolution and high noise, of sonar imagery for odometry and Simultaneous Localization and Mapping (SLAM), the proposed Direct Imaging Sonar Odometry (DISO) system is designed to estimate the relative transformation between two sonar frames by minimizing the aggregated sonar intensity errors of points with high intensity gradients. Moreover, DISO is implemented to incorporate a multi-sensor window optimization technique, a data association strategy and an acoustic intensity outlier rejection algorithm for reliability and accuracy. The effectiveness of DISO is evaluated using both simulated and real-world sonar datasets, showing that it outperforms the existing geometric-only method on localization accuracy and achieves state-of-the-art sonar odometry performance. We release the source codes of the DISO implementation to the community. The source code is available at https://github.com/SenseRoboticsLab/DISO.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611064","","Simultaneous localization and mapping;Accuracy;Three-dimensional displays;Source coding;Sonar;Imaging;Acoustics","","","","19","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"ICGNet: A Unified Approach for Instance-Centric Grasping","R. Zurbrügg; Y. Liu; F. Engelmann; S. Kumar; M. Hutter; V. Patil; F. Yu","ETH Zürich; ETH Zürich; ETH Zürich; Visual Computing, School of PVFA, Texas A&M University; ETH Zürich; ETH Zürich; ETH Zürich",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","4140","4146","Accurate grasping is the key to several robotic tasks including assembly and household robotics. Executing a successful grasp in a cluttered environment requires multiple levels of scene understanding: First, the robot needs to analyze the geometric properties of individual objects to find feasible grasps. These grasps need to be compliant with the local object geometry. Second, for each proposed grasp, the robot needs to reason about the interactions with other objects in the scene. Finally, the robot must compute a collision-free grasp trajectory while taking into account the geometry of the target object. Most grasp detection algorithms directly predict grasp poses in a monolithic fashion, which does not capture the composability of the environment. In this paper, we introduce an end-to-end architecture for object-centric grasping. The method uses pointcloud data from a single arbitrary viewing direction as an input and generates an instance-centric representation for each partially observed object in the scene. This representation is further used for object reconstruction and grasp detection in cluttered table-top scenes. We show the effectiveness of the proposed method by extensively evaluating it against state-of-the-art methods on synthetic datasets, indicating superior performance for grasping and reconstruction. Additionally, we demonstrate real-world applicability by decluttering scenes with varying numbers of objects. Videos and Code icgraspnet.github.io.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611725","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611725","","Geometry;Grasping;Computer architecture;Trajectory;Collision avoidance;Task analysis;Detection algorithms","","5","","40","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"DeFlow: Decoder of Scene Flow Network in Autonomous Driving","Q. Zhang; Y. Yang; H. Fang; R. Geng; P. Jensfelt","Division of Robotics, Perception, and Learning (RPL), KTH Royal Institute of Technology, Stockholm, Sweden; Division of Robotics, Perception, and Learning (RPL), KTH Royal Institute of Technology, Stockholm, Sweden; Division of Robotics, Perception, and Learning (RPL), KTH Royal Institute of Technology, Stockholm, Sweden; System Hub, Hong Kong University of Science and Technology, Guangzhou, China; Division of Robotics, Perception, and Learning (RPL), KTH Royal Institute of Technology, Stockholm, Sweden",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","2105","2111","Scene flow estimation determines a scene’s 3D motion field, by predicting the motion of points in the scene, especially for aiding tasks in autonomous driving. Many networks with large-scale point clouds as input use voxelization to create a pseudo-image for real-time running. However, the voxelization process often results in the loss of point-specific features. This gives rise to a challenge in recovering those features for scene flow tasks. Our paper introduces DeFlow which enables a transition from voxel-based features to point features using Gated Recurrent Unit (GRU) refinement. To further enhance scene flow estimation performance, we formulate a novel loss function that accounts for the data imbalance between static and dynamic points. Evaluations on the Argoverse 2 scene flow task reveal that DeFlow achieves state-of-the-art results on large-scale point cloud data, demonstrating that our network has better performance and efficiency compared to others. The code is available at https://github.com/KTH-RPL/deflow.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610278","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610278","","Point cloud compression;Three-dimensional displays;Estimation;Sensor fusion;Feature extraction;Real-time systems;Sensors","","3","","37","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Detecting and Mitigating System-Level Anomalies of Vision-Based Controllers","A. Gupta; K. Chakraborty; S. Bansal","ECE Department, Indian Institute of Technology (BHU), Varanasi; ECE Department, University of Southern California; ECE Department, University of Southern California",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","9953","9959","Autonomous systems, such as self-driving cars and drones, have made significant strides in recent years by leveraging visual inputs and machine learning for decision-making and control. Despite their impressive performance, these vision-based controllers can make erroneous predictions when faced with novel or out-of-distribution inputs. Such errors can cascade to catastrophic system failures and compromise system safety. In this work, we introduce a run-time anomaly monitor to detect and mitigate such closed-loop, system-level failures. Specifically, we leverage a reachability-based framework to stress-test the vision-based controller offline and mine its system-level failures. This data is then used to train a classifier that is leveraged online to flag inputs that might cause system breakdowns. The anomaly detector highlights issues that transcend individual modules and pertain to the safety of the overall system. We also design a fallback controller that robustly handles these detected anomalies to preserve system safety. We validate the proposed approach on an autonomous aircraft taxiing system that uses a vision-based controller for taxiing. Our results show the efficacy of the proposed approach in identifying and handling system-level anomalies, outperforming methods such as prediction error-based detection, and ensembling, thereby enhancing the overall safety and robustness of autonomous systems. Website: phoenixrider12.github.io/FailureMitigation","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611397","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611397","","Visualization;Autonomous systems;Electric breakdown;Decision making;Machine learning;Detectors;Robustness","","","","25","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"AgriSORT: A Simple Online Real-time Tracking-by-Detection framework for robotics in precision agriculture","L. Saraceni; I. M. Motoi; D. Nardi; T. A. Ciarfuglia",NA; NA; NA; NA,2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","2675","2682","The problem of multi-object tracking (MOT) consists in detecting and tracking all the objects in a video sequence while keeping a unique identifier for each object. It is a challenging and fundamental problem for robotics. In precision agriculture the challenge of achieving a satisfactory solution is amplified by extreme camera motion, sudden illumination changes, and strong occlusions. Most modern trackers rely on the appearance of objects rather than motion for association, which can be ineffective when most targets are static objects with the same appearance, as in the agricultural case. To this end, on the trail of SORT [5], we propose AgriSORT, a simple, online, real-time tracking-by-detection pipeline for precision agriculture based only on motion information that allows for accurate and fast propagation of tracks between frames. The main focuses of AgriSORT are efficiency, flexibility, minimal dependencies, and ease of deployment on robotic platforms. We test the proposed pipeline on a novel MOT benchmark specifically tailored for the agricultural context, based on video sequences taken in a table grape vineyard, particularly challenging due to strong self-similarity and density of the instances. Both the code and the dataset are available for future comparisons at: https://github.com/Lio320/AgriSORT","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610231","European Commission; National Research Centre; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610231","","Precision agriculture;Target tracking;Pipelines;Video sequences;Robot vision systems;Lighting;Cameras","","3","","46","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"OptiState: State Estimation of Legged Robots using Gated Networks with Transformer-based Vision and Kalman Filtering","A. Schperberg; Y. Tanaka; S. Mowlavi; F. Xu; B. Balaji; D. Hong","Robotics and Mechanisms Laboratory, Department of Mechanical and Aerospace Engineering, UCLA, Los Angeles, CA, USA; Robotics and Mechanisms Laboratory, Department of Mechanical and Aerospace Engineering, UCLA, Los Angeles, CA, USA; Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA, USA; Robotics and Mechanisms Laboratory, Department of Mechanical and Aerospace Engineering, UCLA, Los Angeles, CA, USA; Amazon Science, Seattle, WA, USA; Robotics and Mechanisms Laboratory, Department of Mechanical and Aerospace Engineering, UCLA, Los Angeles, CA, USA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","6314","6320","State estimation for legged robots is challenging due to their highly dynamic motion and limitations imposed by sensor accuracy. By integrating Kalman filtering, optimization, and learning-based modalities, we propose a hybrid solution that combines proprioception and exteroceptive information for estimating the state of the robot’s trunk. Leveraging joint encoder and IMU measurements, our Kalman filter is enhanced through a single-rigid body model that incorporates ground reaction force control outputs from convex Model Predictive Control optimization. The estimation is further refined through Gated Recurrent Units, which also considers semantic insights and robot height from a Vision Transformer autoencoder applied on depth images. This framework not only furnishes accurate robot state estimates, including uncertainty evaluations, but can minimize the nonlinear errors that arise from sensor measurements and model simplifications through learning. The proposed methodology is evaluated in hardware using a quadruped robot on various terrains, yielding a 65% improvement on the Root Mean Squared Error compared to our VIO SLAM baseline. Code example: https://github.com/AlexS28/OptiState","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610160","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610160","","Legged locomotion;Accuracy;Filtering;Logic gates;Robot sensing systems;Transformers;Kalman filters","","","","25","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Towards Generalizable Zero-Shot Manipulation via Translating Human Interaction Plans","H. Bharadhwaj; A. Gupta; V. Kumar; S. Tulsiani","Carnegie Mellon University and FAIR, AI at Meta; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","6904","6911","We pursue the goal of developing robots that can interact zero-shot with generic unseen objects via a diverse repertoire of manipulation skills and show how passive human videos can serve as a rich source of data for learning such generalist robots. Unlike typical robot learning approaches which directly learn how a robot should act from interaction data, we adopt a factorized approach that can leverage large-scale human videos to learn how a human would accomplish a desired task (a human ‘plan’), followed by ‘translating’ this plan to the robot’s embodiment. Specifically, we learn a human ‘plan predictor’ that, given a current image of a scene and a goal image, predicts the future hand and object configurations. We combine this with a ‘translation’ module that learns a plan-conditioned robot manipulation policy, and allows following humans plans for generic manipulation tasks in a zero-shot manner with no deployment-time training. Importantly, while the plan predictor can leverage large-scale human videos for learning, the translation module only requires a small amount of in-domain data, and can generalize to tasks not seen during training. We show that our learned system can perform over 16 manipulation skills that generalize to 40 objects, encompassing 100 real-world tasks for table-top manipulation and diverse in-the-wild manipulation. https://homangab.github.io/hopman/","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610288","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610288","","Training;Robot learning;Task analysis;Videos","","3","","64","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud","Z. Pan; F. Ding; H. Zhong; C. X. Lu","Computer Science Research Centre, Royal College of Art, United Kingdom; School of Informatics, University of Edinburgh, United Kingdom; Department of Computer Science and Technology, University of Cambridge, United Kingdom; Department of Computer Science, University College London, United Kingdom",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","4480","4487","Mobile autonomy relies on the precise perception of dynamic environments. Robustly tracking moving objects in 3D world thus plays a pivotal role for applications like trajectory prediction, obstacle avoidance, and path planning. While most current methods utilize LiDARs or cameras for Multiple Object Tracking (MOT), the capabilities of 4D imaging radars remain largely unexplored. Recognizing the challenges posed by radar noise and point sparsity in 4D radar data, we introduce RaTrack, an innovative solution tailored for radar-based tracking. Bypassing the typical reliance on specific object types and 3D bounding boxes, our method focuses on motion segmentation and clustering, enriched by a motion estimation module. Evaluated on the View-of-Delft dataset, RaTrack showcases superior tracking precision of moving objects, largely surpassing the performance of the state of the art. We release our code and model at https://github.com/LJacksonPan/RaTrack.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610368","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610368","","Computer vision;Three-dimensional displays;Motion segmentation;Noise;Radar detection;Radar;Radar imaging","","5","","57","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Complementary Random Masking for RGB-Thermal Semantic Segmentation","U. Shin; K. Lee; I. S. Kweon; J. Oh","Robotics Institute, Carnegie Mellon University, Pittsburgh, Pennsylvania, United States; School of Electrical Engineering, KAIST, Daejeon, Republic of Korea; School of Electrical Engineering, KAIST, Daejeon, Republic of Korea; Robotics Institute, Carnegie Mellon University, Pittsburgh, Pennsylvania, United States",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","11110","11117","RGB-thermal semantic segmentation is one potential solution to achieve reliable semantic scene understanding in adverse weather and lighting conditions. However, the previous studies mostly focus on designing a multi-modal fusion module without consideration of the nature of multi-modality inputs. Therefore, the networks easily become over-reliant on a single modality, making it difficult to learn complementary and meaningful representations for each modality. This paper proposes 1) a complementary random masking strategy of RGB-T images and 2) self-distillation loss between clean and masked input modalities. The proposed masking strategy prevents over-reliance on a single modality. It also improves the accuracy and robustness of the neural network by forcing the network to segment and classify objects even when one modality is partially available. Also, the proposed self-distillation loss encourages the network to extract complementary and meaningful representations from a single modality or complementary masked modalities. We achieve state-of-the-art performance over three RGB-T semantic segmentation benchmarks. Our source code is available at https://github.com/UkcheolShin/CRM_RGBTSeg.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611200","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611200","","Accuracy;Semantic segmentation;Source coding;Neural networks;Semantics;Lighting;Benchmark testing","","6","","49","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Parameter-efficient Prompt Learning for 3D Point Cloud Understanding","H. Sun; Y. Wang; W. Chen; H. Deng; D. Li","Department of Computer Science, School of Information, Renmin University of China, Beijing, China; Department of Computer Science, School of Information, Renmin University of China, Beijing, China; Department of Computer Science, School of Information, Renmin University of China, Beijing, China; Department of Computer Science, School of Information, Renmin University of China, Beijing, China; Department of Computer Science, School of Information, Renmin University of China, Beijing, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","9478","9486","This paper presents a parameter-efficient prompt tuning method, named PPT, to adapt a large multi-modal model for 3D point cloud understanding. Existing strategies are quite expensive in computation and storage, and depend on timeconsuming prompt engineering. We address the problems from three aspects. Firstly, a PromptLearner module is devised to replace hand-crafted prompts with learnable contexts to automate the prompt tuning process. Then, we lock the pre-trained backbone instead of adopting the full fine-tuning paradigm to substantially improve the parameter efficiency. Finally, a lightweight PointAdapter module is arranged near target tasks to enhance prompt tuning for 3D point cloud understanding. Comprehensive experiments are conducted to demonstrate the superior parameter and data efficiency of the proposed method. Meanwhile, we obtain new records on 4 public datasets and multiple 3D tasks, i.e., point cloud recognition, few-shot learning, and part segmentation. The implementation is available at https://github.com/auniquesun/PPT.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610093","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610093","","Point cloud compression;Solid modeling;Adaptation models;Three-dimensional displays;Prompt engineering;Task analysis;Robotics and automation","","","","70","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Scene Informer: Anchor-based Occlusion Inference and Trajectory Prediction in Partially Observable Environments","B. Lange; J. Li; M. J. Kochenderfer","Stanford Intelligent Systems Laboratory, Stanford University, Stanford, CA, USA; Trustworthy Autonomous Systems Laboratory, University of California Riverside, Riverside, CA, USA; Stanford Intelligent Systems Laboratory, Stanford University, Stanford, CA, USA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","14138","14145","Navigating complex and dynamic environments requires autonomous vehicles (AVs) to reason about both visible and occluded regions. This involves predicting the future motion of observed agents, inferring occluded ones, and modeling their interactions based on vectorized scene representations of the partially observable environment. However, prior work on occlusion inference and trajectory prediction have developed in isolation, with the former based on simplified rasterized methods and the latter assuming full environment observability. We introduce the Scene Informer, a unified approach for predicting both observed agent trajectories and inferring occlusions in a partially observable setting. It uses a transformer to aggregate various input modalities and facilitate selective queries on occlusions that might intersect with the AV’s planned path. The framework estimates occupancy probabilities and likely trajectories for occlusions, as well as forecast motion for observed agents. We explore common observability assumptions in both domains and their performance impact. Our approach outperforms existing methods in both occupancy prediction and trajectory prediction in partially observable setting on the Waymo Open Motion Dataset. Our implementation with additional visualizations is available at https://github.com/sisl/SceneInformer.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611060","","Navigation;Predictive models;Transformers;Robot sensing systems;Robustness;Trajectory;Planning","","1","","49","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Fine-Tuning Point Cloud Transformers with Dynamic Aggregation","J. Fei; Z. Deng","Department of Computer Science and Technology, Institute for Artificial Intelligence, Tsinghua University (THUAI), State Key Laboratory of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China; Department of Computer Science and Technology, Institute for Artificial Intelligence, Tsinghua University (THUAI), State Key Laboratory of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","9455","9462","Point clouds play an important role in 3D analysis, which has broad applications in robotics and autonomous driving. The pre-training fine-tuning paradigm has shown great potential in the point cloud domain. Full fine-tuning is generally effective but leads to a heavy storage and computational burden, which becomes inefficient and unacceptable as the size of pre-trained models scales. Although efficient fine-tuning approaches have significant progress in other domains, they generally perform worse for point clouds. To overcome this dilemma, we revisit the official Point-MAE implementation and find the critical role of aggregation in fine-tuning performances. Inspired by such discoveries, we propose a novel dynamic aggregation (DA) method to replace previous static aggregation like mean or max pooling for pre-trained point cloud Transformers. Besides standard metrics such as accuracy or mIoU, we evaluate the number of tunable parameters and additional FLOPs for a fair comparison of our method to different fine-tuning approaches. We construct several DA variants and validate them through extensive experiments. Experimental results demonstrate that DA has competitive performances against full fine-tuning and other efficient fine-tuning approaches. The code is publicly available at https://github.com/JaronTHU/DynamicAggregation.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610767","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610767","","Point cloud compression;Measurement;Three-dimensional displays;Codes;Computational modeling;Aggregates;Transformers","","","","66","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Observation Time Difference: an Online Dynamic Objects Removal Method for Ground Vehicles","R. Wu; C. Pang; X. Wu; Z. Fang","Faculty of Robot Science and Engineering, Northeastern University, Shenyang, China; Faculty of Robot Science and Engineering, Northeastern University, Shenyang, China; Faculty of Robot Science and Engineering, Northeastern University, Shenyang, China; Faculty of Robot Science and Engineering, Northeastern University, Shenyang, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","17997","18003","In the process of urban environment mapping, the sequential accumulations of dynamic objects will leave a large number of traces in the map. These traces will usually have bad influences on the localization accuracy and navigation performance of the robot. Therefore, dynamic objects removal plays an important role for creating clean map. However, conventional dynamic objects removal methods usually run offline. That is, the map is reprocessed after it is constructed, which undoubtedly increases additional time costs. To tackle the problem, this paper proposes a novel method for online dynamic objects removal for ground vehicles. According to the observation time difference between the object and the ground where it is located, dynamic objects are classified into two types: suddenly appear and suddenly disappear. For these two kinds of dynamic objects, we propose downward retrieval and upward retrieval methods to eliminate them respectively. We validate our method on SemanticKITTI dataset and author-collected dataset with highly dynamic objects. Compared with other state-of-the-art methods, our method is more efficient and robust, and reduces the running time per frame by more than 60% on average. Our method will be open-sourced on GitHub 1.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611008","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611008","","Location awareness;Costs;Accuracy;Navigation;Urban areas;Land vehicles;Object recognition","","","","23","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Saturation-Aware Angular Velocity Estimation: Extending the Robustness of SLAM to Aggressive Motions*","S. -P. Deschênes; D. Baril; M. Boxan; J. Laconte; P. Giguère; F. Pomerleau","Northern Robotics Laboratory, Université Laval, Quebec City, Quebec, Canada; Northern Robotics Laboratory, Université Laval, Quebec City, Quebec, Canada; Northern Robotics Laboratory, Université Laval, Quebec City, Quebec, Canada; Northern Robotics Laboratory, Université Laval, Quebec City, Quebec, Canada; Northern Robotics Laboratory, Université Laval, Quebec City, Quebec, Canada; Northern Robotics Laboratory, Université Laval, Quebec City, Quebec, Canada",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","10711","10718","We propose a novel angular velocity estimation method to increase the robustness of Simultaneous Localization And Mapping (SLAM) algorithms against gyroscope saturations induced by aggressive motions. Field robotics expose robots to various hazards, including steep terrains, landslides, and staircases, where substantial accelerations and angular velocities can occur if the robot loses stability and tumbles. These extreme motions can saturate sensor measurements, especially gyroscopes, which are the first sensors to become inoperative. While the structural integrity of the robot is at risk, the robustness of the SLAM framework is oftentimes given little consideration. Consequently, even if the robot is physically capable of continuing the mission, its operation will be compromised due to a corrupted representation of the world. Regarding this problem, we propose a method to estimate the angular velocity using accelerometers during extreme rotations caused by tumbling. We show that our method reduces the median localization error by 71.5 % in translation and 65.5 % in rotation and is robust to mapping failures, which occurred in 37.5 % of the experiments without our method. We also propose the Tumbling-Induced Gyroscope Saturation (TIGS) dataset, which consists of outdoor experiments recording the motion of a mechanical lidar subject to angular velocities four times higher than other similar datasets available. The dataset is available online at https://github.com/norlab-ulaval/Norlab_wiki/wiki/TIGS-Dataset.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610361","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610361","","Location awareness;Simultaneous localization and mapping;Estimation;Angular velocity;Robustness;Motion capture;Gyroscopes","","1","","25","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"3QFP: Efficient neural implicit surface reconstruction using Tri-Quadtrees and Fourier feature Positional encoding","S. Sun; M. Mielle; A. J. Lilienthal; M. Magnusson","AASS MRO Lab, Örebro University, Sweden; Independent Researcher; AASS MRO Lab, Örebro University, Sweden; AASS MRO Lab, Örebro University, Sweden",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","4036","4044","Neural implicit surface representations are currently receiving a lot of interest as a means to achieve high-fidelity surface reconstruction at a low memory cost, compared to traditional explicit representations. However, state-of-the-art methods still struggle with excessive memory usage and non-smooth surfaces. This is particularly problematic in large-scale applications with sparse inputs, as is common in robotics use cases. To address these issues, we first introduce a sparse structure, tri-quadtrees, which represents the environment using learnable features stored in three planar quadtree projections. Secondly, we concatenate the learnable features with a Fourier feature positional encoding. The combined features are then decoded into signed distance values through a small multilayer perceptron. We demonstrate that this approach facilitates smoother reconstruction with a higher completion ratio with fewer holes. Compared to two recent baselines, one implicit and one explicit, our approach requires only 10%–50% as much memory, while achieving competitive quality. The code is released on https://github.com/ljjTYJR/3QFP.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610338","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610338","","Surface reconstruction;Costs;Codes;Memory management;Multilayer perceptrons;Encoding;Robots","","","","45","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Lifelong Robot Library Learning: Bootstrapping Composable and Generalizable Skills for Embodied Control with Language Models","G. Tziafas; H. Kasaei","Department of Artificial Intelligence, University of Groningen, the Netherlands; Department of Artificial Intelligence, University of Groningen, the Netherlands",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","515","522","Large Language Models (LLMs) have emerged as a new paradigm for embodied reasoning and control, most recently by generating robot policy code that utilizes a custom library of vision and control primitive skills. However, prior arts fix their skills library and steer the LLM with carefully handcrafted prompt engineering, limiting the agent to a stationary range of addressable tasks. In this work, we introduce LRLL, an LLM-based lifelong learning agent that continuously grows the robot skill library to tackle manipulation tasks of ever-growing complexity. LRLL achieves this with four novel contributions: 1) a soft memory module that allows dynamic storage and retrieval of past experiences to serve as context, 2) a self-guided exploration policy that proposes new tasks in simulation, 3) a skill abstractor that distills recent experiences into new library skills, and 4) a lifelong learning algorithm for enabling human users to bootstrap new skills with minimal online interaction. LRLL continuously transfers knowledge from the memory to the library, building composable, general and interpretable policies, while bypassing gradient-based optimization, thus relieving the learner from catastrophic forgetting. Empirical evaluation in a simulated tabletop environment shows that LRLL outperforms end-to-end and vanilla LLM approaches in the lifelong setup while learning skills that are transferable to the real world. Project material will become available at the webpage https://gtziafas.github.io/LRLL_project/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611448","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611448","","Limiting;Heuristic algorithms;Large language models;Memory modules;Libraries;Complexity theory;Prompt engineering","","","","70","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Utilizing a Malfunctioning 3D Printer by Modeling Its Dynamics with Machine Learning","R. Caballero; P. Piękos; E. Feron; J. Schmidhuber","Computer, Electrical and Mathematical Sciences and Engineering (CEMSE) Division, King Abdullah University of Science and Technology (KAUST), Thuwal, Kingdom of Saudi Arabia; Computer, Electrical and Mathematical Sciences and Engineering (CEMSE) Division, King Abdullah University of Science and Technology (KAUST), Thuwal, Kingdom of Saudi Arabia; Computer, Electrical and Mathematical Sciences and Engineering (CEMSE) Division, King Abdullah University of Science and Technology (KAUST), Thuwal, Kingdom of Saudi Arabia; Computer, Electrical and Mathematical Sciences and Engineering (CEMSE) Division, King Abdullah University of Science and Technology (KAUST), Thuwal, Kingdom of Saudi Arabia",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","15562","15569","To create a self-repairing 3D printer, it must continue operating even after experiencing corruption. This work focuses on developing a method to effectively utilize a malfunctioning printer for reliable printing. This method can be applied by the printer itself for self-repair and enhance the reliability of commercial 3D printers. We achieve this by modeling the dynamics of the corrupted printer using a machine learning model that by observing one trajectory infers the corrupted printer dynamics to improve its accuracy. Our method is evaluated on a digital twin of the 3D printer, demonstrating its capability to enable the printer to operate reliably, even when encountering new corruptions not encountered during training. The scripts are public on https://github.com/piotrpiekos/adaptive-printer.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611304","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611304","","Training;Solid modeling;Three-dimensional displays;Accuracy;Machine learning;Hardware;Trajectory","","","","52","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"ColonMapper: topological mapping and localization for colonoscopy","J. Morlana; J. D. Tardós; J. M. M. Montiel","Instituto de Investigación en Ingeniería de Aragón (I3A), Universidad de Zaragoza, Zaragoza, Spain; Instituto de Investigación en Ingeniería de Aragón (I3A), Universidad de Zaragoza, Zaragoza, Spain; Instituto de Investigación en Ingeniería de Aragón (I3A), Universidad de Zaragoza, Zaragoza, Spain",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","6329","6336","We propose a topological mapping and localization system able to operate on real human colonoscopies, despite significant shape and illumination changes. The map is a graph where each node codes a colon location by a set of real images, while edges represent traversability between nodes. For close-in-time images, where scene changes are minor, place recognition can be successfully managed with the recent transformers-based local feature matching algorithms. However, under long-term changes –such as different colonoscopies of the same patient– feature-based matching fails. To address this, we train on real colonoscopies a deep global descriptor achieving high recall with significant changes in the scene. The addition of a Bayesian filter boosts the accuracy of long-term place recognition, enabling relocalization in a previously built map. Our experiments show that ColonMapper is able to autonomously build a map and localize against it in two important use cases: localization within the same colonoscopy or within different colonoscopies of the same patient. Code: github.com/jmorlana/ColonMapper.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610426","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610426","","Location awareness;Visualization;Simultaneous localization and mapping;Codes;Shape;Colonoscopy;Transformers","","2","","32","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"SCALE: Self-Correcting Visual Navigation for Mobile Robots via Anti-Novelty Estimation","C. Chen; Y. Liu; Y. Zhuang; S. Mao; K. Xue; S. Zhou","School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen; Huawei Noah’s Ark Lab.; Huawei Noah’s Ark Lab.; Edge Cloud Innovation Lab, Shenzhen Huawei Cloud Computing Technologies Co., Ltd.; Edge Cloud Innovation Lab, Shenzhen Huawei Cloud Computing Technologies Co., Ltd.; Edge Cloud Innovation Lab, Shenzhen Huawei Cloud Computing Technologies Co., Ltd.",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","16360","16366","Although visual navigation has been extensively studied using deep reinforcement learning, online learning for real-world robots remains a challenging task. Recent work directly learned from offline dataset to achieve broader generalization in the real-world tasks, which, however, faces the out-of-distribution (OOD) issue and potential robot localization failures in a given map for unseen observation. This significantly drops the success rates and even induces collision. In this paper, we present a self-correcting visual navigation method, SCALE, that can autonomously prevent the robot from the OOD situations without human intervention. Specifically, we develop an image-goal conditioned offline reinforcement learning method based on implicit Q-learning (IQL). When facing OOD observation, our novel localization recovery method generates the potential future trajectories by learning from the navigation affordance, and estimates the future novelty via random network distillation (RND). A tailored cost function searches for the candidates with the least novelty that can lead the robot to the familiar places. We collect offline data and conduct evaluation experiments in three real-world urban scenarios. Experiment results show that SCALE outperforms the previous state-of-the-art methods for open-world navigation with a unique capability of localization recovery, significantly reducing the need for human intervention. Code is available at https://github.com/KubeEdge4Robotics/ScaleNav.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610847","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610847","","Location awareness;Visualization;Q-learning;Navigation;Robustness;Robot localization;Trajectory","","","","40","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"LOTUS: Continual Imitation Learning for Robot Manipulation Through Unsupervised Skill Discovery","W. Wan; Y. Zhu; R. Shah; Y. Zhu",The University of Texas at Austin; The University of Texas at Austin; The University of Texas at Austin; The University of Texas at Austin,2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","537","544","We introduce LOTUS, a continual imitation learning algorithm that empowers a physical robot to continuously and efficiently learn to solve new manipulation tasks throughout its lifespan. The core idea behind LOTUS is constructing an ever-growing skill library from a sequence of new tasks with a small number of human demonstrations. LOTUS starts with a continual skill discovery process using an open-vocabulary vision model, which extracts skills as recurring patterns presented in unsegmented demonstrations. Continual skill discovery updates existing skills to avoid catastrophic forgetting of previous tasks and adds new skills to solve novel tasks. LOTUS trains a meta-controller that flexibly composes various skills to tackle vision-based manipulation tasks in the lifelong learning process. Our comprehensive experiments show that LOTUS outperforms state-of-the-art baselines by over 11% in success rate, showing its superior knowledge transfer ability compared to prior methods. More results and videos can be found on the project website: https://ut-austin-rpl.github.io/Lotus/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611129","National Science Foundation; Office of Naval Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611129","","Imitation learning;Clustering methods;Memory management;Buildings;Libraries;Task analysis;Robots","","1","","63","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Rethinking Imitation-based Planners for Autonomous Driving","J. Cheng; Y. Chen; X. Mei; B. Yang; B. Li; M. Liu","Hong Kong University of Science and Technology, Hong Kong SAR, China; Hong Kong University of Science and Technology, Hong Kong SAR, China; Hong Kong University of Science and Technology, Hong Kong SAR, China; Hong Kong University of Science and Technology, Hong Kong SAR, China; Lotus Technology Ltd; The Hong Kong University of Science and Technology (Guangzhou), Nansha, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","14123","14130","In recent years, imitation-based driving planners have reported considerable success. However, due to the absence of a standardized benchmark, the effectiveness of various designs remains unclear. The newly released nuPlan addresses this issue by offering a large-scale real-world dataset and a standardized closed-loop benchmark for equitable comparisons. Utilizing this platform, we conduct a comprehensive study on two fundamental yet underexplored aspects of imitation-based planners: the essential features for ego planning and the effective data augmentation techniques to reduce compounding errors. Furthermore, we highlight an imitation gap that has been overlooked by current learning systems. Finally, integrating our findings, we propose a strong baseline model—PlanTF. Our results demonstrate that a well-designed, purely imitation-based planner can achieve highly competitive performance compared to state-of-the-art methods involving hand-crafted rules and exhibit superior generalization capabilities in long-tail cases. Our models and benchmarks are publicly available. Project website https://jchengai.github.io/planTF.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611364","Hong Kong University of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611364","","Training;Adaptation models;System dynamics;Perturbation methods;Benchmark testing;Data augmentation;Planning","","4","","52","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"LiDAR Data Synthesis with Denoising Diffusion Probabilistic Models","K. Nakashima; R. Kurazume","Faculty of Information Science and Electrical Engineering, Kyushu University, Japan; Faculty of Information Science and Electrical Engineering, Kyushu University, Japan",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","14724","14731","Generative modeling of 3D LiDAR data is an emerging task with promising applications for autonomous mobile robots, such as scalable simulation, scene manipulation, and sparse-to-dense completion of LiDAR point clouds. While existing approaches have demonstrated the feasibility of image-based LiDAR data generation using deep generative models, they still struggle with fidelity and training stability. In this work, we present R2DM, a novel generative model for LiDAR data that can generate diverse and high-fidelity 3D scene point clouds based on the image representation of range and reflectance intensity. Our method is built upon denoising diffusion probabilistic models (DDPMs), which have shown impressive results among generative model frameworks in recent years. To effectively train DDPMs in the LiDAR domain, we first conduct an in-depth analysis of data representation, loss functions, and spatial inductive biases. Leveraging our R2DM model, we also introduce a flexible LiDAR completion pipeline based on the powerful capabilities of DDPMs. We demonstrate that our method surpasses existing methods in generating tasks on the KITTI-360 and KITTI-Raw datasets, as well as in the completion task on the KITTI-360 dataset. Our project page can be found at https://kazuto1011.github.io/r2dm.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611480","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611480","","Point cloud compression;Training;Solid modeling;Laser radar;Three-dimensional displays;Semantic segmentation;Pipelines","","3","","41","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential Deep Learning","H. Paat; Q. Lian; W. Yao; T. Zhang","Department of Computer Science and Engineering, the Hong Kong University of Science and Technology (HKUST), Hong Kong, China; Department of Computer Science and Engineering, the Hong Kong University of Science and Technology (HKUST), Hong Kong, China; Autowise.AI; Department of Computer Science and Engineering, the Hong Kong University of Science and Technology (HKUST), Hong Kong, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","13976","13982","Advancements in deep learning-based 3D object detection necessitate the availability of large-scale datasets. However, this requirement introduces the challenge of manual annotation, which is often both burdensome and time-consuming. To tackle this issue, the literature has seen the emergence of several weakly supervised frameworks for 3D object detection which can automatically generate pseudo labels for unlabeled data. Nevertheless, these generated pseudo labels contain noise and are not as accurate as those labeled by humans. In this paper, we present the first approach that addresses the inherent ambiguities present in pseudo labels by introducing an Evidential Deep Learning (EDL) based uncertainty estimation framework. Specifically, we propose MEDL-U, an EDL framework based on MTrans, which not only generates pseudo labels but also quantifies the associated uncertainties. However, applying EDL to 3D object detection presents three key challenges: (1) lower pseudo label quality in comparison to other autolabelers; (2) high evidential uncertainty estimates; and (3) lack of clear interpretability and effective utilization of uncertainties for downstream tasks. We tackle these issues through the introduction of an uncertainty-aware IoU-based loss, an evidence-aware multi-task loss, and the implementation of a post-processing stage for uncertainty refinement. Our experimental results demonstrate that probabilistic detectors trained using the outputs of MEDL-U surpass deterministic detectors trained using outputs from previous 3D annotators on the KITTI val set for all difficulty levels. Moreover, MEDL-U achieves state-of-the-art results on the KITTI official test set compared to existing 3D automatic annotators. Code is publicly available at https://github.com/paathelb/MEDL-U.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610597","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610597","","Deep learning;Three-dimensional displays;Uncertainty;Annotations;Noise;Object detection;Detectors","","","","31","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"AdvGPS: Adversarial GPS for Multi-Agent Perception Attack","J. Li; B. Li; X. Liu; J. Fang; F. Juefei-Xu; Q. Guo; H. Yu","Cleveland State University; Cleveland State University; Cleveland State University; Xi’an Jiaotong University; New York University; Agency for Science, Technology and Research (A*STAR), CFAR and IHPC, Singapore; Cleveland State University",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","18421","18427","The multi-agent perception system collects visual data from sensors located on various agents and leverages their relative poses determined by GPS signals to effectively fuse information, mitigating the limitations of single-agent sensing, such as occlusion. However, the precision of GPS signals can be influenced by a range of factors, including wireless transmission and obstructions like buildings. Given the pivotal role of GPS signals in perception fusion and the potential for various interference, it becomes imperative to investigate whether specific GPS signals can easily mislead the multi-agent perception system. To address this concern, we frame the task as an adversarial attack challenge and introduce ADVGPS, a method capable of generating adversarial GPS signals which are also stealthy for individual agents within the system, significantly reducing object detection accuracy. To enhance the success rates of these attacks in a black-box scenario, we introduce three types of statistically sensitive natural discrepancies: appearance-based discrepancy, distribution-based discrepancy, and task-aware discrepancy. Our extensive experiments on the OPV2V dataset demonstrate that these attacks substantially undermine the performance of state-of-the-art methods, showcasing remarkable transferability across different point cloud based 3D detection systems. This alarming revelation underscores the pressing need to address security implications within multi-agent perception systems, thereby underscoring a critical area of research. The code is available at https://github.com/jinlong17/AdvGPS.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610012","National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610012","","Wireless sensor networks;Three-dimensional displays;Accuracy;Closed box;Pressing;Object detection;Sensors","","1","","26","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"MonoOcc: Digging into Monocular Semantic Occupancy Prediction","Y. Zheng; X. Li; P. Li; Y. Zheng; B. Jin; C. Zhong; X. Long; H. Zhao; Q. Zhang","The State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute for AI Industry Research (AIR), Tsinghua University, China; Institute for AI Industry Research (AIR), Tsinghua University, China; Institute for AI Industry Research (AIR), Tsinghua University, China; The State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute for AI Industry Research (AIR), Tsinghua University, China; Department of Computer Science, the University of Hong Kong; Institute for AI Industry Research (AIR), Tsinghua University, China; The State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","18398","18405","Monocular Semantic Occupancy Prediction aims to infer the complete 3D geometry and semantic information of scenes from only 2D images. It has garnered significant attention, particularly due to its potential to enhance the 3D perception of autonomous vehicles. However, existing methods rely on a complex cascaded framework with relatively limited information to restore 3D scenes, including a dependency on supervision solely on the whole network’s output, single-frame input, and the utilization of a small backbone. These challenges, in turn, hinder the optimization of the framework and yield inferior prediction results, particularly concerning smaller and long-tailed objects. To address these issues, we propose MonoOcc. In particular, we (i) improve the monocular occupancy prediction framework by proposing an auxiliary semantic loss as supervision to the shallow layers of the framework and an image-conditioned cross-attention module to refine voxel features with visual clues, and (ii) employ a distillation module that transfers temporal information and richer knowledge from a larger image backbone to the monocular semantic occupancy prediction framework with low cost of hardware. With these advantages, our method yields state-of-the-art performance on the camera-based SemanticKITTI Scene Completion benchmark. Codes and models can be accessed at https://github.com/ucaszyp/MonoOcc.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611261","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611261","","Geometry;Visualization;Three-dimensional displays;Semantics;Prediction methods;Benchmark testing;Hardware","","3","","49","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Trust Recognition in Human-Robot Cooperation Using EEG","C. Xu; C. Zhang; Y. Zhou; Z. Wang; P. Lu; B. He","Department of Control Science and Engineering, College of Electronics and Information Engineering, Tongji University, Shanghai, China; Department of Control Science and Engineering, College of Electronics and Information Engineering, Tongji University, Shanghai, China; Department of Control Science and Engineering, College of Electronics and Information Engineering, Tongji University, Shanghai, China; Department of Control Science and Engineering, College of Electronics and Information Engineering, Tongji University, Shanghai, China; Department of Control Science and Engineering, College of Electronics and Information Engineering, Tongji University, Shanghai, China; Department of Control Science and Engineering, College of Electronics and Information Engineering, Tongji University, Shanghai, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","7827","7833","Collaboration between humans and robots is becoming increasingly crucial in our daily life. In order to accomplish efficient cooperation, trust recognition is vital, empowering robots to predict human behaviors and make trust-aware decisions. Consequently, there is an urgent need for a generalized approach to recognize human-robot trust. This study addresses this need by introducing an EEG-based method for trust recognition during human-robot cooperation. A human-robot cooperation game scenario is used to stimulate various human trust levels when working with robots. To enhance recognition performance, the study proposes an EEG Vision Transformer model coupled with a 3-D spatial representation to capture the spatial information of EEG, taking into account the topological relationship among electrodes. To validate this approach, a public EEG-based human trust dataset called EEGTrust is constructed. Experimental results indicate the effectiveness of the proposed approach, achieving an accuracy of 74.99% in slice-wise cross-validation and 62.00% in trial-wise cross-validation. This outperforms baseline models in both recognition accuracy and generalization. Furthermore, an ablation study demonstrates a significant improvement in trust recognition performance of the spatial representation. The source code and EEGTrust dataset are available at https://github.com/CaiyueXu/EEGTrust.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610156","National Key Research and Development Program of China; National Natural Science Foundation of China; Science and Technology Commission of Shanghai Municipality; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610156","","Adaptation models;Computer vision;Solid modeling;Accuracy;Source coding;Brain modeling;Transformers","","","","36","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Introducing CEA-IMSOLD: an Industrial Multi-Scale Object Localization Dataset","B. Meden; P. Vega; F. M. De Chamisso; S. Bourgeois","CEA, List, Université Paris-Saclay, Palaiseau, France; CEA, List, Université Paris-Saclay, Palaiseau, France; CEA, List, Université Paris-Saclay, Palaiseau, France; CEA, List, Université Paris-Saclay, Palaiseau, France",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","17020","17026","We introduce the CEA Industrial Multi-Scale Object Localization Dataset (CEA-IMSOLD), a new BOP format dataset for 6-DoF object localization, crucial for robotics. This dataset aims to evaluate the current localization methods with respect to a new difficulty: large variations in observation distance and, consequently, large variations in image appearance. Compared to the other publicly available datasets, our dataset provides both images with objects small and completely visible in the image, and images where objects are observed close enough so they appear larger than the field of view of the camera. We also propose to consider the observation distance in the evaluation process and introduce new metrics to do so. Finally, our dataset contains a large variety of industrial objects, from small and simple objects such as bolts to sizable and complex ones such as large car parts. We provide baseline results and the dataset is made publicly available to support the community at https://cea-list.github.io/CEA-IMSOLD/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10609999","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10609999","","Location awareness;Measurement;Three-dimensional displays;Service robots;Object detection;Fasteners;Robot sensing systems","","","","21","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Stereo-LiDAR Depth Estimation with Deformable Propagation and Learned Disparity-Depth Conversion","A. Li; A. Hu; W. Xi; W. Yu; D. Zou","Shanghai Key Laboratory of Navigation and Location-Based Service, Shanghai Jiao Tong University; Shanghai Key Laboratory of Navigation and Location-Based Service, Shanghai Jiao Tong University; Intelligent Perception Institute, Midea Corporate Research Center; Shanghai Key Laboratory of Navigation and Location-Based Service, Shanghai Jiao Tong University; Shanghai Key Laboratory of Navigation and Location-Based Service, Shanghai Jiao Tong University",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","2729","2736","Accurate and dense depth estimation with stereo cameras and LiDAR is an important task for automatic driving and robotic perception. While sparse hints from LiDAR points have improved cost aggregation in stereo matching, their effectiveness is limited by the low density and non-uniform distribution. To address this issue, we propose a novel stereo-LiDAR depth estimation network with Semi-Dense hint Guidance, named SDG-Depth. Our network includes a deformable propagation module for generating a semi-dense hint map and a confidence map by propagating sparse hints using a learned deformable window. These maps then guide cost aggregation in stereo matching. To reduce the triangulation error in depth recovery from disparity, especially in distant regions, we introduce a disparity-depth conversion module. Our method is both accurate and efficient. The experimental results on benchmark tests show its superior performance. Our code is available at https://github.com/SJTU-ViSYS/SDG-Depth.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611533","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611533","","Laser radar;Costs;Accuracy;Three-dimensional displays;Robot vision systems;Estimation;Reliability","","1","","43","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Sim2Real Manipulation on Unknown Objects with Tactile-based Reinforcement Learning","E. Su; C. Jia; Y. Qin; W. Zhou; A. Macaluso; B. Huang; X. Wang","University of California, San Diego, CA, USA; University of California, San Diego, CA, USA; University of California, San Diego, CA, USA; Carnegie Mellon University, PA, USA; University of California, San Diego, CA, USA; University of Illinois Urbana-Champaign, IL, USA; University of California, San Diego, CA, USA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","9234","9241","Using tactile sensors for manipulation remains one of the most challenging problems in robotics. At the heart of these challenges is generalization: How can we train a tactile-based policy that can manipulate unseen and diverse objects? In this paper, we propose to perform Reinforcement Learning with only visual tactile sensing inputs on diverse objects in a physical simulator. By training with diverse objects in simulation, it enables the policy to generalize to unseen objects. However, leveraging simulation introduces the Sim2Real transfer problem. To mitigate this problem, we study different tactile representations and evaluate how each affects real-robot manipulation results after transfer. We conduct our experiments on diverse real-world objects and show significant improvements over baselines. Our project page is available at https://tactilerl.github.io/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611113","","Training;Heart;Visualization;Tactile sensors;Reinforcement learning;Sensors;Task analysis","","","","53","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"PROGrasp: Pragmatic Human-Robot Communication for Object Grasping","G. -C. Kang; J. Kim; J. Kim; B. -T. Zhang","Interdisciplinary Program in AI, Seoul National University; Interdisciplinary Program in AI, Seoul National University; Interdisciplinary Program in Neuroscience, Seoul National University; Interdisciplinary Program in AI, Seoul National University",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","3304","3310","Interactive Object Grasping (IOG) is the task of identifying and grasping the desired object via human-robot natural language interaction. Current IOG systems assume that a human user initially specifies the target object’s category (e.g., bottle). Inspired by pragmatics, where humans often convey their intentions by relying on context to achieve goals, we introduce a new IOG task, Pragmatic-IOG, and the corresponding dataset, Intention-oriented Multi-modal Dialogue (IM-Dial). In our proposed task scenario, an intention-oriented utterance (e.g., ""I am thirsty"") is initially given to the robot. The robot should then identify the target object by interacting with a human user. Based on the task setup, we propose a new robotic system that can interpret the user’s intention and pick up the target object, Pragmatic Object Grasping (PROGrasp). PROGrasp performs Pragmatic-IOG by incorporating modules for visual grounding, question asking, object grasping, and most importantly, answer interpretation for pragmatic inference. Experimental results show that PROGrasp is effective in offline (i.e., target object discovery) and online (i.e., IOG with a physical robot arm) settings. Code and data are available at https://github.com/gicheonkang/prograsp.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610543","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610543","","Visualization;Codes;Grounding;Natural languages;Grasping;Manipulators;Object recognition","","1","","41","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"DCPT: Darkness Clue-Prompted Tracking in Nighttime UAVs","J. Zhu; H. Tang; Z. -Q. Cheng; J. -Y. He; B. Luo; S. Qiu; S. Li; H. Lu","Dalian University of Technology, Dalian, China; Dalian University of Technology, Dalian, China; Carnegie Mellon University, PA, USA; DAMO Academy, Alibaba Group, Shenzhen, China; DAMO Academy, Alibaba Group, Shenzhen, China; Dalian University of Technology, Dalian, China; Dalian University of Technology, Dalian, China; Dalian University of Technology, Dalian, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","7381","7388","Existing nighttime unmanned aerial vehicle (UAV) trackers follow an ""Enhance-then-Track"" architecture - first using a light enhancer to brighten the nighttime video, then employing a daytime tracker to locate the object. This separate enhancement and tracking fails to build an end-to-end trainable vision system. To address this, we propose a novel architecture called Darkness Clue-Prompted Tracking (DCPT) that achieves robust UAV tracking at night by efficiently learning to generate darkness clue prompts. Without a separate enhancer, DCPT directly encodes anti-dark capabilities into prompts using a darkness clue prompter (DCP). Specifically, DCP iteratively learns emphasizing and undermining projections for darkness clues. It then injects these learned visual prompts into a daytime tracker with fixed parameters across transformer layers. Moreover, a gated feature aggregation mechanism enables adaptive fusion between prompts and between prompts and the base model. Extensive experiments show state-of-the-art performance for DCPT on multiple dark scenario benchmarks. The unified end-to-end learning of enhancement and tracking in DCPT enables a more trainable system. The darkness clue prompting efficiently injects anti-dark knowledge without extra modules. Code is available at https://github.com/bearyi26/DCPT.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610544","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610544","","Adaptation models;Visualization;Codes;Machine vision;Logic gates;Benchmark testing;Autonomous aerial vehicles","","3","","45","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"An Analytic Solution to the 3D CSC Dubins Path Problem","V. M. Baez; N. Navkar; A. T. Becker","Electrical Engineering, University of Houston, TX, USA; Department of Surgery, Hamad Medical Corporation, Doha, Qatar; Electrical Engineering, University of Houston, TX, USA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","7157","7163","We present an analytic solution to the 3D Dubins path problem for paths composed of an initial circular arc, a straight component, and a final circular arc. These are commonly called CSC paths. By modeling the start and goal configurations of the path as the base frame and final frame of an RRPRR manipulator, we treat this as an inverse kinematics problem. The kinematic features of the 3D Dubins path are built into the constraints of our manipulator model. Furthermore, we show that the number of solutions is not constant, with up to seven valid CSC path solutions even in non-singular regions. An implementation of solution is available at https: //github.com/aabecker/dubins3D.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611360","Qatar National Research Fund; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611360","","Solid modeling;Concentric tube robots;Analytical models;Three-dimensional displays;Kinematics;Manipulators;Turning","","1","","39","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Enabling the Deployment of Any-Scale Robotic Applications in Microservice Architectures through Automated Containerization*","J. -P. Busch; L. Reiher; L. Eckstein","Institute for Automotive Engineering (ika), RWTH Aachen University, Germany; Institute for Automotive Engineering (ika), RWTH Aachen University, Germany; Institute for Automotive Engineering (ika)",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","17650","17656","In an increasingly automated world – from ware-house robots to self-driving cars – streamlining the development and deployment process and operations of robotic applications becomes ever more important. Automated DevOps processes and microservice architectures have already proven successful in other domains such as large-scale customer-oriented web services (e.g., Netflix). We recommend to employ similar microservice architectures for the deployment of small- to large-scale robotic applications in order to accelerate development cycles, loosen functional dependence, and improve resiliency and elasticity. In order to facilitate involved DevOps processes, we present and release a tooling suite for automating the development of microservices for robotic applications based on the Robot Operating System (ROS). Our tooling suite covers the automated minimal containerization of ROS applications, a collection of useful machine learning-enabled base container images, as well as a CLI tool for simplified interaction with container images during the development phase. Within the scope of this paper, we embed our tooling suite into the overall context of streamlined robotics deployment and compare it to alternative solutions. We release our tools as open-source software at github.com/ika-rwth-aachen/dorotos.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611586","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611586","","DevOps;Operating systems;Microservice architectures;Computer architecture;Containers;Streaming media;Stakeholders","","","","48","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Sim-to-Real Grasp Detection with Global-to-Local RGB-D Adaptation","H. Ma; R. Qin; M. Shi; B. Gao; D. Huang","State Key Laboratory of Software Development Environment, School of Computer Science and Engineering, Beihang University, Beijing, China; State Key Laboratory of Software Development Environment, School of Computer Science and Engineering, Beihang University, Beijing, China; State Key Laboratory of Software Development Environment, School of Computer Science and Engineering, Beihang University, Beijing, China; Geometry Robotics and the School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; State Key Laboratory of Software Development Environment, School of Computer Science and Engineering, Beihang University, Beijing, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","13910","13917","This paper focuses on the sim-to-real issue of RGB-D grasp detection and formulates it as a domain adaptation problem. In this case, we present a global-to-local method to address hybrid domain gaps in RGB and depth data and insufficient multi-modal feature alignment. First, a self-supervised rotation pre-training strategy is adopted to deliver robust initialization for RGB and depth networks. We then propose a global-to-local alignment pipeline with individual global domain classifiers for scene features of RGB and depth images as well as a local one specifically working for grasp features in the two modalities. In particular, we propose a grasp prototype adaptation module, which aims to facilitate fine-grained local feature alignment by dynamically updating and matching the grasp prototypes from the simulation and real-world scenarios throughout the training process. Due to such designs, the proposed method substantially reduces the domain shift and thus leads to consistent performance improvements. Extensive experiments are conducted on the GraspNet-Planar benchmark and physical environment, and superior results are achieved which demonstrate the effectiveness of our method. Code is available at https://github.com/mahaoxiang822/GL-MSDA.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611165","National Natural Science Foundation of China; State Key Laboratory of Software Development Environment; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611165","","Training;Codes;Pipelines;Prototypes;Benchmark testing;Robustness;Robotics and automation","","","","46","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"SERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning","J. Luo; Z. Hu; C. Xu; Y. L. Tan; J. Berg; A. Sharma; S. Schaal; C. Finn; A. Gupta; S. Levine","Department of EECS, University of California, Berkeley; Department of EECS, University of California, Berkeley; Department of EECS, University of California, Berkeley; Department of EECS, University of California, Berkeley; Department of Computer Science, University of Washington; Department of Computer Science, Stanford University; Intrinsic Innovation LLC; Department of Computer Science, Stanford University; Department of Computer Science, University of Washington; Department of EECS, University of California, Berkeley",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","16961","16969","In recent years, significant progress has been made in the field of robotic reinforcement learning (RL), enabling methods that handle complex image observations, train in the real world, and incorporate auxiliary data, such as demonstrations and prior experience. However, despite these advances, robotic RL remains hard to use. It is acknowledged among practitioners that the particular implementation details of these algorithms are often just as important (if not more so) for performance as the choice of algorithm. We posit that a significant challenge to the widespread adoption of robotic RL, as well as the further development of robotic RL methods, is the comparative inaccessibility of such methods. To address this challenge, we developed a carefully implemented library containing a sample efficient off-policy deep RL method, together with methods for computing rewards and resetting the environment, a high-quality controller for a widely adopted robot, and a number of challenging example tasks. We provide this library as a resource for the community, describe its design choices, and present experimental results. Perhaps surprisingly, we find that our implementation can achieve very efficient learning, acquiring policies for PCB board assembly, cable routing, and object relocation between 25 to 50 minutes of training per policy on average, improving over state-of-the-art results reported for similar tasks in the literature. These policies achieve perfect or near-perfect success rates, extreme robustness even under perturbations, and exhibit emergent recovery and correction behaviors. We hope these promising results and our high-quality open-source implementation will provide a tool for the robotics community to facilitate further developments in robotic RL. Our code, documentation, and videos can be found at https://serl-robot.github.io/","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610040","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610040","","Training;Perturbation methods;Reinforcement learning;Routing;Libraries;Software;Robustness","","3","","63","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"ESP: Extro-Spective Prediction for Long-term Behavior Reasoning in Emergency Scenarios","D. Wang; Z. Lai; Y. Li; Y. Wu; Y. Ma; J. Betz; R. Yang; W. Li","Inceptio Technology, Shanghai, China; Inceptio Technology, Shanghai, China; Inceptio Technology, Shanghai, China; Nanjing University of Posts and Telecommunications, Nanjing, China; ShanghaiTech University, Shanghai, China; Professorship of Autonomous Vehicle Systems, Technical University of Munich, Garching, Germany; Inceptio Technology, Shanghai, China; Inceptio Technology, Shanghai, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","13030","13037","Emergent-scene safety is the key milestone for fully autonomous driving, and reliable on-time prediction is essential to maintain safety in emergency scenarios. However, these emergency scenarios are long-tailed and hard to collect, which restricts the system from getting reliable predictions. In this paper, we build a new dataset, which aims at the longterm prediction with the inconspicuous state variation in history for the emergency event, named the Extro-Spective Prediction (ESP) problem. Based on the proposed dataset, a flexible feature encoder for ESP is introduced to various prediction methods as a seamless plug-in, and its consistent performance improvement underscores its efficacy. Furthermore, a new metric named clamped temporal error (CTE) is proposed to give a more comprehensive evaluation of prediction performance, especially in time-sensitive emergency events of subseconds. Interestingly, as our ESP features can be described in human-readable language naturally, the application of integrating into ChatGPT also shows huge potential. The ESP-dataset and all benchmarks are released at https://dingrui-wang.github.io/ESP-Dataset/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610002","Natural Science Foundation of Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610002","","Measurement;Prediction methods;Benchmark testing;Chatbots;Cognition;Safety;Reliability","","","","43","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Conditionally Combining Robot Skills using Large Language Models","K. R. Zentner; R. Julian; B. Ichter; G. S. Sukhatme",Univ. of Southern California; Google DeepMind; Google DeepMind; Univ. of Southern California,2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","14046","14053","This paper combines two contributions. First, we introduce an extension of the Meta-World benchmark, which we call ""Language-World,"" which allows a large language model to operate in a simulated robotic environment using semi-structured natural language queries and scripted skills described using natural language. By using the same set of tasks as Meta-World, Language-World results can be easily compared to Meta-World results, allowing for a point of comparison between recent methods using Large Language Models (LLMs) and those using Deep Reinforcement Learning. Second, we introduce a method we call Plan Conditioned Behavioral Cloning (PCBC), that allows finetuning the behavior of high-level plans using end-to-end demonstrations. Using Language-World, we show that PCBC is able to achieve strong performance in a variety of few-shot regimes, often achieving task generalization with as little as a single demonstration. We have made Language-World available as open-source software at https://github.com/krzentner/language-world/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611275","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611275","","Large language models;Imitation learning;Natural languages;Cloning;Benchmark testing;Deep reinforcement learning;Task analysis","","","","41","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Increasing SLAM Pose Accuracy by Ground-to-Satellite Image Registration","Y. Zhang; Y. Shi; S. Wang; A. Vora; A. Perincherry; Y. Chen; H. Li","Robotics Institute, University of Technology Sydney, Sydney, Australia; ShanghaiTech University, Shanghai, China; College of Engineering and Computer Science, Australian National University, Canberra, Australia; Ford Motor Company, Dearborn, USA; Ford Motor Company, Dearborn, USA; College of Engineering and Computer Science, Australian National University, Canberra, Australia; College of Engineering and Computer Science, Australian National University, Canberra, Australia",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","8522","8528","Vision-based localization for autonomous driving has been of great interest among researchers. When a pre-built 3D map is not available, the techniques of visual simultaneous localization and mapping (SLAM) are typically adopted. Due to error accumulation, visual SLAM (vSLAM) usually suffers from long-term drift. This paper proposes a framework to increase the localization accuracy by fusing the vSLAM with a deep-learning based ground-to-satellite (G2S) image registration method. In this framework, a coarse (spatial correlation bound check) to fine (visual odometry consistency check) method is designed to select the valid G2S prediction. The selected prediction is then fused with the SLAM measurement by solving a scaled pose graph problem. To further increase the localization accuracy, we provide an iterative trajectory fusion pipeline. The proposed framework is evaluated on two well-known autonomous driving datasets, and the results demonstrate the accuracy and robustness in terms of vehicle localization. The code will be available at https://github.com/YanhaoZhang/SLAM-G2S-Fusion.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611079","visual SLAM;cross-view localization;autonomous driving","Location awareness;Visualization;Image registration;Simultaneous localization and mapping;Accuracy;Three-dimensional displays;Robustness","","1","","40","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"AGRNav: Efficient and Energy-Saving Autonomous Navigation for Air-Ground Robots in Occlusion-Prone Environments","J. Wang; Z. Sun; X. Guan; T. Shen; Z. Zhang; T. Duan; D. Huang; S. Zhao; H. Cui","University of Hong Kong; University of Hong Kong; University of Hong Kong; University of Hong Kong; University of Hong Kong; University of Hong Kong; University of Hong Kong; Huawei Technologies, Co. Ltd.; University of Hong Kong",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","11133","11139","The exceptional mobility and long endurance of air-ground robots are raising interest in their usage to navigate complex environments (e.g., forests and large buildings). However, such environments often contain occluded and unknown regions, and without accurate prediction of unobserved obstacles, the movement of the air-ground robot often suffers a sub-optimal trajectory under existing mapping-based and learning-based navigation methods. In this work, we present AGRNav, a novel framework designed to search for safe and energy-saving air-ground hybrid paths. AGRNav contains a lightweight semantic scene completion network (SCONet) with self-attention to enable accurate obstacle predictions by capturing contextual information and occlusion area features. The framework subsequently employs a query-based method for low-latency updates of prediction results to the grid map. Finally, based on the updated map, the hierarchical path planner efficiently searches for energy-saving paths for navigation. We validate AGRNav’s performance through benchmarks in both simulated and real-world environments, demonstrating its superiority over classical and state-of-the-art methods. The open-source code is available at https://github.com/jmwang0117/AGRNav.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610829","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610829","","Accuracy;Forests;Navigation;Semantics;Predictive models;Robustness;Air to ground communication","","2","","25","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"FF-LOGO: Cross-Modality Point Cloud Registration with Feature Filtering and Local to Global Optimization","N. Ma; M. Wang; Y. Han; Y. -J. Liu","Faculty of Information Technology, Beijing University of Technology, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China; Department of Computer Science and Technology, BNRist, MOE-Key Laboratory of Pervasive Computing, Tsinghua University, Beijing, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","744","750","Cross-modality point cloud registration is confronted with significant challenges due to inherent differences in modalities between sensors. To deal with this problem, we propose FF-LOGO: a cross-modality point cloud registration framework with Feature Filtering and LOcal-Global Optimization. The cross-modality feature correlation filtering module extracts geometric transformation-invariant features from cross-modality point clouds and achieves point selection by feature matching. We also introduce a cross-modality optimization process, including a local adaptive key region aggregation module and a global modality consistency fusion optimization module. Experimental results demonstrate that our two-stage optimization significantly improves the registration accuracy of the feature association and selection module. Our method achieves a substantial increase in recall rate compared to the current state-of-the-art methods on the 3DCSR dataset, improving from 40.59% to 75.74%. Our code will be available at https://github.com/wangmohan17/FFLOGO.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610549","National Natural Science Foundation of China; Beijing Hospital; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610549","","Point cloud compression;Deep learning;Accuracy;Correlation;Codes;Filtering;Feature extraction","","2","","29","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Language-Conditioned Affordance-Pose Detection in 3D Point Clouds","T. Nguyen; M. N. Vu; B. Huang; T. Van Vo; V. Truong; N. Le; T. Vo; B. Le; A. Nguyen","FPT Software AI Center, Vietnam; Automation & Control Institute, TU Wien, Vienna, Austria; Imperial College London, UK; FPT Software AI Center, Vietnam; FPT Software AI Center, Vietnam; Department of Computer Science & Computer Engineering, University of Arkansas, USA; Faculty of Mathematics and Statistics, Ton Duc Thang University, Ho Chi Minh City, Vietnam; Faculty of Information Technology, University of Science, Ho Chi Minh City, VietNam; Department of Computer Science, University of Liverpool, UK",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","3071","3078","Affordance detection and pose estimation are of great importance in many robotic applications. Their combination helps the robot gain an enhanced manipulation capability, in which the generated pose can facilitate the corresponding affordance task. Previous methods for affodance-pose joint learning are limited to a predefined set of affordances, thus limiting the adaptability of robots in real-world environments. In this paper, we propose a new method for language-conditioned affordance-pose joint learning in 3D point clouds. Given a 3D point cloud object, our method detects the affordance region and generates appropriate 6-DoF poses for any unconstrained affordance label. Our method consists of an open-vocabulary affordance detection branch and a language-guided diffusion model that generates 6-DoF poses based on the affordance text. We also introduce a new high-quality dataset for the task of language-driven affordance-pose joint learning. Intensive experimental results demonstrate that our proposed method works effectively on a wide range of open-vocabulary affordances and outperforms other baselines by a large margin. In addition, we illustrate the usefulness of our method in real-world robotic applications. Our code and dataset are publicly available at https://3DAPNet.github.io.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610008","","Point cloud compression;Three-dimensional displays;Codes;Limiting;Affordances;Pose estimation;Diffusion models","","2","","65","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Efficient Motion Planning for Manipulators with Control Barrier Function-Induced Neural Controller","M. Yu; C. Yu; M. -M. Naddaf-Sh; D. Upadhyay; S. Gao; C. Fan","Department of Aeronautics and Astronautics, Massachusetts Institute of Technology, USA; Department of Computer Science and Engineering, University of California, San Diego, USA; Ford Motor Company, USA; Ford Motor Company, USA; Department of Computer Science and Engineering, University of California, San Diego, USA; Department of Aeronautics and Astronautics, Massachusetts Institute of Technology, USA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","14348","14355","Sampling-based motion planning methods for manipulators in crowded environments often suffer from expensive collision checking and high sampling complexity, which make them difficult to use in real time. To address this issue, we propose a new generalizable control barrier function (CBF)based steering controller to reduce the number of samples needed in a sampling-based motion planner RRT. Our method combines the strength of CBF for real-time collision-avoidance control and RRT for long-horizon motion planning, by using CBF-induced neural controller (CBF-INC) to generate control signals that steer the system towards sampled configurations by RRT. CBF-INC is learned as Neural Networks and has two variants handling different inputs, respectively: state (signed distance) input and point-cloud input from LiDAR. In the latter case, we also study two different settings: fully and partially observed environmental information. Compared to manually crafted CBF which suffers from over-approximating robot geometry, CBF-INC can balance safety and goal-reaching better without being over-conservative. Given state-based input, our neural CBF-induced neural controller-enhanced RRT (CBFINC-RRT) can increase the success rate by 14% while reducing the number of nodes explored by 30%, compared with vanilla RRT on hard test cases. Given LiDAR input where vanilla RRT is not directly applicable, we demonstrate that our CBF-INCRRT can improve the success rate by 10%, compared with planning with other steering controllers. Our project page with supplementary material is at https://mit-realm.github.io/CBFINC-RRT-website/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610785","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610785","","Geometry;Laser radar;Neural networks;Manipulators;Real-time systems;Planning;Safety","","1","","54","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"DTPP: Differentiable Joint Conditional Prediction and Cost Evaluation for Tree Policy Planning in Autonomous Driving","Z. Huang; P. Karkus; B. Ivanovic; Y. Chen; M. Pavone; C. Lv","School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore; NVIDIA Research, NVIDIA Corporation, Santa Clara, CA, USA; NVIDIA Research, NVIDIA Corporation, Santa Clara, CA, USA; NVIDIA Research, NVIDIA Corporation, Santa Clara, CA, USA; NVIDIA Research, NVIDIA Corporation, Santa Clara, CA, USA; School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","6806","6812","Motion prediction and cost evaluation are vital components in the decision-making system of autonomous vehicles. However, existing methods often ignore the importance of cost learning and treat them as separate modules. In this study, we employ a tree-structured policy planner and propose a differentiable joint training framework for both ego-conditioned prediction and cost models, resulting in a direct improvement of the final planning performance. For conditional prediction, we introduce a query-centric Transformer model that performs efficient ego-conditioned motion prediction. For planning cost, we propose a learnable context-aware cost function with latent interaction features, facilitating differentiable joint learning. We validate our proposed approach using the real-world nuPlan dataset and its associated planning test platform. Our framework not only matches state-of-the-art planning methods but outperforms other learning-based methods in planning quality, while operating more efficiently in terms of runtime. We show that joint training delivers significantly better performance than separate training of the two modules. Additionally, we find that tree-structured policy planning outperforms the conventional single-stage planning approach. Code is available: https://github.com/MCZhi/DTPP.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610550","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610550","","Training;Costs;Runtime;Trajectory planning;Predictive models;Transformers;Probabilistic logic","","1","","37","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"SuperFusion: Multilevel LiDAR-Camera Fusion for Long-Range HD Map Generation","H. Dong; W. Gu; X. Zhang; J. Xu; R. Ai; H. Lu; J. Kannala; X. Chen","ETH, Zürich; HAOMO.AI; HAOMO.AI; HAOMO.AI; HAOMO.AI; National University of Defense Technology; Aalto University; National University of Defense Technology",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","9056","9062","High-definition (HD) semantic map generation of the environment is an essential component of autonomous driving. Existing methods have achieved good performance in this task by fusing different sensor modalities, such as LiDAR and camera. However, current works are based on raw data or network feature-level fusion and only consider short-range HD map generation, limiting their deployment to realistic autonomous driving applications. In this paper, we focus on the task of building the HD maps in both short ranges, i.e., within 30m, and also predicting long-range HD maps up to 90m, which is required by downstream path planning and control tasks to improve the smoothness and safety of autonomous driving. To this end, we propose a novel network named SuperFusion, exploiting the fusion of LiDAR and camera data at multiple levels. We use LiDAR depth to improve image depth estimation and use image features to guide long-range LiDAR feature prediction. We benchmark our SuperFusion on the nuScenes dataset and a self-recorded dataset and show that it outperforms the state-of-the-art baseline methods with large margins on all intervals. Additionally, we apply the generated HD map to a downstream path planning task, demonstrating that the long-range HD maps predicted by our method can lead to better path planning for autonomous vehicles. Our code and self-recorded dataset have been released at https://github.com/haomo-ai/SuperFusion.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611320","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611320","","Laser radar;Limiting;Semantics;Estimation;Cameras;Path planning;Safety","","5","","34","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"RenderOcc: Vision-Centric 3D Occupancy Prediction with 2D Rendering Supervision","M. Pan; J. Liu; R. Zhang; P. Huang; X. Li; H. Xie; B. Wang; L. Liu; S. Zhang","National Key Laboratory for Multimedia Information Processing, School of CS, Peking University; National Key Laboratory for Multimedia Information Processing, School of CS, Peking University; CUHK MMLAB; Xiaomi Car; National Key Laboratory for Multimedia Information Processing, School of CS, Peking University; Nanjing University; Nanyang Technological University; Xiaomi Car; National Key Laboratory for Multimedia Information Processing, School of CS, Peking University",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","12404","12411","3D occupancy prediction holds significant promise in the fields of robot perception and autonomous driving, which quantifies 3D scenes into grid cells with semantic labels. Recent works mainly utilize complete occupancy labels in 3D voxel space for supervision. However, the expensive annotation process and sometimes ambiguous labels have severely constrained the usability and scalability of 3D occupancy models. To address this, we present RenderOcc, a novel paradigm for training 3D occupancy models only using 2D labels. Specifically, we extract a NeRF-style 3D volume representation from multi-view images, and employ volume rendering techniques to establish 2D renderings, thus enabling direct 3D supervision from 2D semantics and depth labels. Additionally, we introduce an Auxiliary Ray method to tackle the issue of sparse viewpoints in autonomous driving scenarios, which leverages sequential frames to construct comprehensive 2D rendering for each object. To our best knowledge, RenderOcc is the first attempt to train multi-view 3D occupancy models only using 2D labels, reducing the dependence on costly 3D occupancy annotations. Extensive experiments demonstrate that RenderOcc achieves comparable performance to models fully supervised with 3D labels, underscoring the significance of this approach in real-world applications. Our code is available at https://github.com/pmj110119/RenderOcc.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611537","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611537","","Training;Solid modeling;Three-dimensional displays;Annotations;Scalability;Semantics;Rendering (computer graphics)","","4","","50","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"BEVUDA: Multi-geometric Space Alignments for Domain Adaptive BEV 3D Object Detection","J. Liu; R. Zhang; X. Li; X. Chi; Z. Chen; M. Lu; Y. Guo; S. Zhang","National Key Laboratory for Multimedia Information Processing, School of CS, Peking University; National Key Laboratory for Multimedia Information Processing, School of CS, Peking University; National Key Laboratory for Multimedia Information Processing, School of CS, Peking University; National Key Laboratory for Multimedia Information Processing, School of CS, Peking University; National Key Laboratory for Multimedia Information Processing, School of CS, Peking University; National Key Laboratory for Multimedia Information Processing, School of CS, Peking University; AI2Robotics; National Key Laboratory for Multimedia Information Processing, School of CS, Peking University",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","9487","9494","Vision-centric bird-eye-view (BEV) perception has shown promising potential in autonomous driving. Recent works mainly focus on improving efficiency or accuracy but neglect the challenges when facing environment changing, resulting in severe degradation of transfer performance. For BEV perception, we figure out the significant domain gaps existing in typical real-world cross-domain scenarios and comprehensively solve the Domain Adaption (DA) problem for multi-view 3D object detection. Since BEV perception approaches are complicated and contain several components, the domain shift accumulation on multiple geometric spaces (i.e., 2D, 3D Voxel, BEV) makes BEV DA even challenging. In this paper, we propose a Multi-space Alignment Teacher-Student (MATS) framework to ease the domain shift accumulation, which consists of a Depth-Aware Teacher (DAT) and a Geometric-space Aligned Student (GAS) model. DAT tactfully combines target lidar and reliable depth prediction to construct depth-aware information, extracting target domain-specific knowledge in Voxel and BEV feature spaces. It then transfers the sufficient domain knowledge of multiple spaces to the student model. In order to jointly alleviate the domain shift, GAS projects multi-geometric space features to a shared geometric embedding space and decreases data distribution distance between two domains. To verify the effectiveness of our method, we conduct BEV 3D object detection experiments on three cross-domain scenarios and achieve state-of-the-art performance. Code: https://github.com/liujiaming1996/BEVUDA.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610096","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610096","","Degradation;Three-dimensional displays;Laser radar;Codes;Object detection;Feature extraction;Reliability","","","","62","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"ForceSight: Text-Guided Mobile Manipulation with Visual-Force Goals","J. A. Collins; C. Houff; Y. L. Tan; C. C. Kemp","Institute for Robotics and Intelligent Machines, Georgia Institute of Technology (GT); Institute for Robotics and Intelligent Machines, Georgia Institute of Technology (GT); Institute for Robotics and Intelligent Machines, Georgia Institute of Technology (GT); Institute for Robotics and Intelligent Machines, Georgia Institute of Technology (GT)",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","10874","10880","We present ForceSight, a system for text-guided mobile manipulation that predicts visual-force goals using a text-conditioned vision transformer. Given a single RGBD image and a text prompt, ForceSight determines a target end-effector pose in the camera frame (kinematic goal) and the associated forces (force goal). Together, these two components form a visual-force goal. Prior work has demonstrated that deep models outputting human-interpretable kinematic goals can enable dexterous manipulation by real robots. Forces are critical to manipulation, yet have typically been relegated to low-level execution in these systems. When deployed on a mobile manipulator equipped with an eye-in-hand RGBD camera, ForceSight performed tasks such as precision grasps, drawer opening, and object handovers with an 81% success rate in unseen environments with object instances that differed significantly from the training data. In a separate experiment, relying exclusively on visual servoing and ignoring force goals dropped the success rate from 90% to 45%, demonstrating that force goals can significantly enhance performance. The appendix, videos, code, and trained models are available at https://force-sight.github.io/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611210","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611210","","Force;Training data;Kinematics;Handover;Cameras;Transformers;Visual servoing","","","","41","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Point Cloud Models Improve Visual Robustness in Robotic Learners","S. Peri; I. Lee; C. Kim; L. Fuxin; T. Hermans; S. Lee",Oregon State University; University of Utah; Oregon State University; Oregon State University; University of Utah; Oregon State University,2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","17529","17536","Visual control policies can encounter significant performance degradation when visual conditions like lighting or camera position differ from those seen during training – often exhibiting sharp declines in capability even for minor differences. In this work, we examine robustness to a suite of these types of visual changes for RGB-D and point cloud based visual control policies. To perform these experiments on both model-free and model-based reinforcement learners, we introduce a novel Point Cloud World Model (PCWM) and point cloud based control policies. Our experiments show that policies that explicitly encode point clouds are significantly more robust than their RGB-D counterparts. Further, we find our proposed PCWM significantly outperforms prior works in terms of sample efficiency during training. Taken together, these results suggest reasoning about the 3D scene through point clouds can improve performance, reduce learning time, and increase robustness for robotic learners. Code: https://github.com/pvskand/pcwm","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610710","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610710","point cloud world model;model-based reinforcement learning;vision-based robot control;robustness","Point cloud compression;Training;Degradation;Visualization;Three-dimensional displays;Lighting;Robustness","","","","74","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Unsupervised Learning of Neuro-symbolic Rules for Generalizable Context-aware Planning in Object Arrangement Tasks","S. Sharma; S. Tuli; R. Paul","IIT Delhi; IIT Delhi; Computer Science and Engg. (CSE) and Yardi School of AI (ScAI), IIT Delhi",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","12865","12872","As robots tackle complex object arrangement tasks, it becomes imperative for them to be able to generalize to complex worlds and scale with number of objects. This work postulates that extracting action primitives, such as push operations, their pre-conditions and effects would enable strong generalization to unseen worlds. Hence, we factorize policy learning as inference of such generic rules, which act as strong priors for predicting actions given the world state. Learnt rules act as propositional knowledge and enable robots to reach goals in a zero-shot method by applying the rules independently and incrementally. However, obtaining hand-engineered rules, such as PDDL descriptions is hard, especially for unseen worlds. This work aims to learn generic, sparse, and context-aware rules that govern action primitives in robotic worlds through human demonstrations in simple domains. We demonstrate that our approach, namely RLAP, is able to extract rules without explicit supervision of rule labels and generate goal-reaching plans in complex Sokoban styled domains that scale with number of objects. RLAP furnishes significantly higher goal reaching rate and shorter planning times compared to the state-of-the-art techniques. The code, dataset, and videos are hosted at https://rule-learning-rlap.github.io/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610696","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610696","","Uncertainty;Protocols;Monte Carlo methods;Stacking;Search engines;Planning;Task analysis","","","","31","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Lightweight Event-based Optical Flow Estimation via Iterative Deblurring","Y. Wu; F. Paredes-Vallés; G. C. H. E. de Croon","Micro Air Vehicle Laboratory, Faculty of Aerospace Engineering, Delft University of Technology, Delft, The Netherlands; Micro Air Vehicle Laboratory, Faculty of Aerospace Engineering, Delft University of Technology, Delft, The Netherlands; Micro Air Vehicle Laboratory, Faculty of Aerospace Engineering, Delft University of Technology, Delft, The Netherlands",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","14708","14715","Inspired by frame-based methods, state-of-the-art event-based optical flow networks rely on the explicit construction of correlation volumes, which are expensive to compute and store, rendering them unsuitable for robotic applications with limited compute and energy budget. Moreover, correlation volumes scale poorly with resolution, prohibiting them from estimating high-resolution flow. We observe that the spatiotemporally continuous traces of events provide a natural search direction for seeking pixel correspondences, obviating the need to rely on gradients of explicit correlation volumes as such search directions. We introduce IDNet (Iterative Deblurring Network), a lightweight yet high-performing event-based optical flow network directly estimating flow from event traces without using correlation volumes. We further propose two iterative update schemes: ""ID"" which iterates over the same batch of events, and ""TID"" which iterates over time with streaming events in an online fashion. Our top-performing model (ID) sets a new state of the art on DSEC benchmark. Meanwhile, the base model (TID) is competitive with prior arts while using 80% fewer parameters, consuming 20x less memory footprint and running 40% faster on the NVidia Jetson Xavier NX. Furthermore, the TID scheme is even more efficient offering an additional 5x faster inference speed and 8 ms ultra-low latency at the cost of only a 9% performance drop, making it the only model among current literature capable of real-time operation while maintaining decent performance.Code: https://github.com/tudelft/idnet.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610353","Office of Naval Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610353","","Image motion analysis;Correlation;Memory management;Estimation;Rendering (computer graphics);Real-time systems;Iterative algorithms","","1","","51","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"RIC: Rotate-Inpaint-Complete for Generalizable Scene Reconstruction","I. Kasahara; S. Agrawal; S. Engin; N. Chavan-Dafle; S. Song; V. Isler","Samsung AI Center, New York; Samsung AI Center, New York; Samsung AI Center, New York; Samsung AI Center, New York; Samsung AI Center, New York; Samsung AI Center, New York",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","2713","2720","General scene reconstruction refers to the task of estimating the full 3D geometry and texture of a scene containing previously unseen objects. In many practical applications such as AR/VR, autonomous navigation, and robotics, only a single view of the scene may be available, making the scene reconstruction task challenging. In this paper, we present a method for scene reconstruction by structurally breaking the problem into two steps: rendering novel views via inpainting and 2D to 3D scene lifting. Specifically, we leverage the generalization capability of large visual language models (DALL•E 2) to inpaint the missing areas of scene color images rendered from different views. Next, we lift these inpainted images to 3D by predicting normals of the inpainted image and solving for the missing depth values. By predicting for normals instead of depth directly, our method allows for robustness to changes in depth distributions and scale. With rigorous quantitative evaluation, we show that our method outperforms multiple baselines while providing generalization to novel objects and scenes. Code and data is available at https://samsunglabs.github.io/RIC-project-page/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611694","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611694","","Geometry;Visualization;Three-dimensional displays;Codes;Color;Rendering (computer graphics);Robustness","","1","","40","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Vehicle Behavior Prediction by Episodic-Memory Implanted NDT","P. Shen; J. Fang; H. Yu; J. Xue","Chang’an University, China; Xi’an Jiaotong University, China; Cleveland State University, USA; Xi’an Jiaotong University, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","14177","14183","In autonomous driving, predicting the behavior (turning left, stopping, etc.) of target vehicles is crucial for the self-driving vehicle to make safe decisions and avoid accidents. Existing deep learning-based methods have shown excellent and accurate performance, but the black-box nature makes it untrustworthy to apply them in practical use. In this work, we explore the interpretability of behavior prediction of target vehicles by an Episodic Memory implanted Neural Decision Tree (abbrev. eMem-NDT). The structure of eMem-NDT is constructed by hierarchically clustering the text embedding of vehicle behavior descriptions. eMem-NDT is a neural-backed part of a pre-trained deep learning model by changing the soft-max layer of the deep model to eMem-NDT, for grouping and aligning the memory prototypes of the historical vehicle behavior features in training data on a neural decision tree. Each leaf node of eMem-NDT is modeled by a neural network for aligning the behavior memory prototypes. By eMem-NDT, we infer each instance in behavior prediction of vehicles by bottom-up Memory Prototype Matching (MPM) (searching the appropriate leaf node and the links to the root node) and top-down Leaf Link Aggregation (LLA) (obtaining the probability of future behaviors of vehicles for certain instances). We validate eMem-NDT on BLVD and LOKI datasets, and the results show that our model can obtain a superior performance to other methods with clear explainability. The code is available in https://github.com/JWFangit/eMem-NDT.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610995","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610995","","Neural networks;Prototypes;Training data;Link aggregation;Predictive models;Turning;Data models","","","","41","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Reducing Non-IID Effects in Federated Autonomous Driving with Contrastive Divergence Loss","T. Do; B. X. Nguyen; Q. D. Tran; H. Nguyen; E. Tjiputra; T. -C. Chiu; A. Nguyen","AIOZ, Singapore; AIOZ, Singapore; AIOZ, Singapore; AIOZ, Singapore; AIOZ, Singapore; Department of Computer Science, National Tsing Hua University, Taiwan; Deparment of Computer Science, University of Liverpool, UK",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","2190","2196","Federated learning has been widely applied in autonomous driving since it enables training a learning model among vehicles without sharing users’ data. However, data from autonomous vehicles usually suffer from the non-independent-and-identically-distributed (non-IID) problem, which may cause negative effects on the convergence of the learning process. In this paper, we propose a new contrastive divergence loss to address the non-IID problem in autonomous driving by reducing the impact of divergence factors from transmitted models during the local learning process of each silo. We also analyze the effects of contrastive divergence in various autonomous driving scenarios, under multiple network infrastructures, and with different centralized/distributed learning schemes. Our intensive experiments on three datasets demonstrate that our proposed contrastive divergence loss significantly improves the performance over current state-of-the-art approaches. Our source code is available at https://github.com/aioz-ai/CDL.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611202","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611202","","Training;Federated learning;Source coding;Roads;Benchmark testing;Propagation losses;Data models","","","","65","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"OmniColor: A Global Camera Pose Optimization Approach of LiDAR-360Camera Fusion for Colorizing Point Clouds","B. Liu; G. Zhao; J. Jiao; G. Cai; C. Li; H. Yin; Y. Wang; M. Liu; P. Hui","The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; The Hong Kong University of Science and Technology, Hong Kong SAR, China; The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","6396","6402","A Colored point cloud, as a simple and efficient 3D representation, has many advantages in various fields, including robotic navigation and scene reconstruction. This representation is now commonly used in 3D reconstruction tasks relying on cameras and LiDARs. However, fusing data from these two types of sensors is poorly performed in many existing frameworks, leading to unsatisfactory mapping results, mainly due to inaccurate camera poses. This paper presents Omni-Color, a novel and efficient algorithm to colorize point clouds using an independent 360-degree camera. Given a LiDAR-based point cloud and a sequence of panorama images with initial coarse camera poses, our objective is to jointly optimize the poses of all frames for mapping images onto geometric reconstructions. Our pipeline works in an off-the-shelf manner that does not require any feature extraction or matching process. Instead, we find optimal poses by directly maximizing the photometric consistency of LiDAR maps. In experiments, we show that our method can overcome the severe visual distortion of omnidirectional images and greatly benefit from the wide field of view (FOV) of 360-degree cameras to reconstruct various scenarios with accuracy and stability. The code will be released at https://github.com/liubonan123/OmniColor/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610292","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610292","","Point cloud compression;Visualization;Three-dimensional displays;Laser radar;Accuracy;Robot vision systems;Virtual reality","","","","32","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Degradation Resilient LiDAR-Radar-Inertial Odometry","M. Nissov; N. Khedekar; K. Alexis","Norwegian University of Science and Technology (NTNU), Trondheim, Norway; Norwegian University of Science and Technology (NTNU), Trondheim, Norway; Norwegian University of Science and Technology (NTNU), Trondheim, Norway",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","8587","8594","Enabling autonomous robots to operate robustly in challenging environments is necessary in a future with increased autonomy. For many autonomous systems, estimation and odometry remains a single point of failure, from which it can often be difficult, if not impossible, to recover. As such robust odometry solutions are of key importance. In this work a method for tightly-coupled LiDAR-Radar-Inertial fusion for odometry is proposed, enabling the mitigation of the effects of LiDAR degeneracy by leveraging a complementary perception modality while preserving the accuracy of LiDAR in well-conditioned environments. The proposed approach combines modalities in a factor graph-based windowed smoother with sensor information-specific factor formulations which enable, in the case of degeneracy, partial information to be conveyed to the graph along the non-degenerate axes. The proposed method is evaluated in real-world tests on a flying robot experiencing degraded conditions including geometric self-similarity as well as obscurant occlusion. For the benefit of the community we release the datasets presented: https://github.com/ntnu-arl/lidar_degeneracy_datasets.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611444","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611444","","Degradation;Laser radar;Prevention and mitigation;Estimation;Robot sensing systems;Sensors;Odometry","","2","","54","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Residual-NeRF: Learning Residual NeRFs for Transparent Object Manipulation","B. P. Duisterhof; Y. Mao; S. H. Teng; J. Ichnowski","The Robotics Institute, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","13918","13924","Transparent objects are ubiquitous in industry, pharmaceuticals, and households. Grasping and manipulating these objects is a significant challenge for robots. Existing methods have difficulty reconstructing complete depth maps for challenging transparent objects, leaving holes in the depth reconstruction. Recent work has shown neural radiance fields (NeRFs) work well for depth perception in scenes with transparent objects, and these depth maps can be used to grasp transparent objects with high accuracy. NeRF-based depth reconstruction can still struggle with especially challenging transparent objects and lighting conditions. In this work, we propose Residual-NeRF, a method to improve depth perception and training speed for transparent objects. Robots often operate in the same area, such as a kitchen. By first learning a background NeRF of the scene without transparent objects to be manipulated, we reduce the ambiguity faced by learning the changes with the new object. We propose training two additional networks: a residual NeRF learns to infer residual RGB values and densities, and a Mixnet learns how to combine background and residual NeRFs. We contribute synthetic and real experiments that suggest Residual-NeRF improves depth perception of transparent objects. The results on synthetic data suggest Residual-NeRF outperforms the baselines with a 46.1 % lower RMSE and a 29.5 % lower MAE. Real-world qualitative experiments suggest Residual-NeRF leads to more robust depth maps with less noise and fewer holes. Website: https://residual-nerf.github.io","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611224","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611224","","Training;Industries;Accuracy;Service robots;Noise;Lighting;Grasping","","","","43","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Complementing Onboard Sensors with Satellite Maps: A New Perspective for HD Map Construction","W. Gao; J. Fu; Y. Shen; H. Jing; S. Chen; N. Zheng","National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center for Visual Information and Applications, Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Shaanxi, P.R. China; The Chinese University of Hong Kong, Shatin, Hong Kong; National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center for Visual Information and Applications, Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Shaanxi, P.R. China; National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center for Visual Information and Applications, Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Shaanxi, P.R. China; National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center for Visual Information and Applications, Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Shaanxi, P.R. China; National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center for Visual Information and Applications, Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Shaanxi, P.R. China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","11103","11109","High-definition (HD) maps play a crucial role in autonomous driving systems. Recent methods have attempted to construct HD maps in real-time using vehicle onboard sensors. Due to the inherent limitations of onboard sensors, which include sensitivity to detection range and susceptibility to occlusion by nearby vehicles, the performance of these methods significantly declines in complex scenarios and long-range detection tasks. In this paper, we explore a new perspective that boosts HD map construction through the use of satellite maps to complement onboard sensors. We initially generate the satellite map tiles for each sample in nuScenes and release a complementary dataset for further research. To enable better integration of satellite maps with existing methods, we propose a hierarchical fusion module, which includes feature-level fusion and BEV-level fusion. The feature-level fusion, composed of a mask generator and a masked cross-attention mechanism, is used to refine the features from onboard sensors. The BEV-level fusion mitigates the coordinate differences between features obtained from onboard sensors and satellite maps through an alignment module. The experimental results on the augmented nuScenes showcase the seamless integration of our module into three existing HD map construction methods. The satellite maps and our proposed module notably enhance their performance in both HD map semantic segmentation and instance detection tasks. Our code will be available at https://github.com/xjtu-csgao/SatforHDMap.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611611","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611611","","Cloud computing;Satellites;Sensitivity;Tiles;Semantic segmentation;Sensor phenomena and characterization;Sensor fusion","","3","","37","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"RaSim: A Range-aware High-fidelity RGB-D Data Simulation Pipeline for Real-world Applications","X. Liu; C. Zhang; G. Wang; R. Zhang; X. Ji","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Lab for High Technology, Tsinghua University; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","17057","17064","In robotic vision, a de-facto paradigm is to learn in simulated environments and then transfer to real-world applications, which poses an essential challenge in bridging the sim-to-real domain gap. While mainstream works tackle this problem in the RGB domain, we focus on depth data synthesis and develop a Range-aware RGB-D data Simulation pipeline (RaSim). In particular, high-fidelity depth data is generated by imitating the imaging principle of real-world sensors. A range-aware rendering strategy is further introduced to enrich data diversity. Extensive experiments show that models trained with RaSim can be directly applied to real-world scenarios without any finetuning and excel at downstream RGB-D perception tasks. Data and code are available at https://github.com/shanice-l/RaSim.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611208","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611208","","Image sensors;Bridges;Solid modeling;Three-dimensional displays;Codes;Pipelines;Rendering (computer graphics)","","","","61","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Barrier Functions Inspired Reward Shaping for Reinforcement Learning","Nilaksh; A. Ranjan; S. Agrawal; A. Jain; P. Jagtap; S. Kolathaya","Indian Institute of Technology (IIT), Kharagpur; Indian Institute of Science (IISc), Bangalore; Indian Institute of Science (IISc), Bangalore; Indian Institute of Technology (IIT), Kharagpur; RBCCPS, IISc; CSA & RBCCPS, IISc",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","10807","10813","Reinforcement Learning (RL) has progressed from simple control tasks to complex real-world challenges with large state spaces. While RL excels in these tasks, training time remains a limitation. Reward shaping is a popular solution, but existing methods often rely on value functions, which face scalability issues. This paper presents a novel safety-oriented reward-shaping framework inspired by barrier functions, offering simplicity and ease of implementation across various environments and tasks. To evaluate the effectiveness of the proposed reward formulations, we conduct simulation experiments on CartPole, Ant, and Humanoid environments, along with real-world deployment on the Unitree Go1 quadruped robot. Our results demonstrate that our method leads to 1.4-2.8 times faster convergence and as low as 50-60% actuation effort compared to the vanilla reward. In a sim-to-real experiment with the Go1 robot, we demonstrated better control and dynamics of the bot with our reward framework. We have open-sourced our code at https://github.com/Safe-RL-IISc/barrier_shaping.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610391","Google; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610391","","Training;System dynamics;Scalability;Humanoid robots;Reinforcement learning;Hardware;Safety","","2","","34","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Toward Optimal Tabletop Rearrangement with Multiple Manipulation Primitives","B. Huang; X. Zhang; J. Yu","Department of Computer Science, Rutgers, State University of New Jersey, Piscataway, NJ, USA; Southern University of Science and Technology, China; Department of Computer Science, Rutgers, State University of New Jersey, Piscataway, NJ, USA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","10860","10866","In practice, many types of manipulation actions (e.g., pick-n-place and push) are needed to accomplish real-world manipulation tasks. Yet, limited research exists that explores the synergistic integration of different manipulation actions for optimally solving long-horizon task-and-motion planning problems. In this study, we propose and investigate planning high-quality action sequences for solving long-horizon tabletop rearrangement tasks in which multiple manipulation primitives are required. Denoting the problem rearrangement with multiple manipulation primitives (REMP), we develop two algorithms, hierarchical best-first search (HBFS) and parallel Monte Carlo tree search for multi-primitive rearrangement (PMMR) toward optimally resolving the challenge. Extensive simulation and real robot experiments demonstrate that both methods effectively tackle REMP, with HBFS excelling in planning speed and P M MR producing human-like, high-quality solutions with a nearly 100% success rate. Source code and supplementary materials will be available at https://github.com/arc-l/remp.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610565","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610565","","Monte Carlo methods;Source coding;Search problems;Planning;Task analysis;Robots","","","","30","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Quadcopter Trajectory Time Minimization and Robust Collision Avoidance via Optimal Time Allocation","Z. Xu; K. Shimada","Department of Mechanical Engineering, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Mechanical Engineering, Carnegie Mellon University, Pittsburgh, PA, USA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","16531","16537","Autonomous navigation requires robots to generate trajectories for collision avoidance efficiently. Although plenty of previous works have proven successful in generating smooth and spatially collision-free trajectories, their solutions often suffer from suboptimal time efficiency and potential un-safety, particularly when accounting for uncertainties in robot perception and control. To address this issue, this paper presents the Robust Optimal Time Allocation (ROTA) framework. This framework is designed to optimize the time progress of the trajectories temporally, serving as a post-processing tool to enhance trajectory time efficiency and safety under uncertainties. In this study, we begin by formulating a non-convex optimization problem aimed at minimizing trajectory execution time while incorporating constraints on collision probability as the robot approaches obstacles. Subsequently, we introduce the concept of the trajectory braking zone and adopt the chance-constrained formulation for robust collision avoidance in the braking zones. Finally, the non-convex optimization problem is reformulated into a second-order cone programming problem to achieve real-time performance. Through simulations and physical flight experiments, we demonstrate that the proposed approach effectively reduces trajectory execution time while enabling robust collision avoidance in complex environments. Our software1 is available on GitHub, along with the developed autonomy framework2, as open-source ROS packages.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610290","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610290","","Uncertainty;Real-time systems;Trajectory;Safety;Resource management;Time factors;Collision avoidance","","","","33","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Open X-Embodiment: Robotic Learning Datasets and RT-X Models : Open X-Embodiment Collaboration0","A. O’Neill; A. Rehman; A. Maddukuri; A. Gupta; A. Padalkar; A. Lee; A. Pooley; A. Gupta; A. Mandlekar; A. Jain; A. Tung; A. Bewley; A. Herzog; A. Irpan; A. Khazatsky; A. Rai; A. Gupta; A. Wang; A. Singh; A. Garg; A. Kembhavi; A. Xie; A. Brohan; A. Raffin; A. Sharma; A. Yavary; A. Jain; A. Balakrishna; A. Wahid; B. Burgess-Limerick; B. Kim; B. Schölkopf; B. Wulfe; B. Ichter; C. Lu; C. Xu; C. Le; C. Finn; C. Wang; C. Xu; C. Chi; C. Huang; C. Chan; C. Agia; C. Pan; C. Fu; C. Devin; D. Xu; D. Morton; D. Driess; D. Chen; D. Pathak; D. Shah; D. Büchler; D. Jayaraman; D. Kalashnikov; D. Sadigh; E. Johns; E. Foster; F. Liu; F. Ceola; F. Xia; F. Zhao; F. Stulp; G. Zhou; G. S. Sukhatme; G. Salhotra; G. Yan; G. Feng; G. Schiavi; G. Berseth; G. Kahn; G. Wang; H. Su; H. -S. Fang; H. Shi; H. Bao; H. Ben Amor; H. I. Christensen; H. Furuta; H. Walke; H. Fang; H. Ha; I. Mordatch; I. Radosavovic; I. Leal; J. Liang; J. Abou-Chakra; J. Kim; J. Drake; J. Peters; J. Schneider; J. Hsu; J. Bohg; J. Bingham; J. Wu; J. Gao; J. Hu; J. Wu; J. Wu; J. Sun; J. Luo; J. Gu; J. Tan; J. Oh; J. Wu; J. Lu; J. Yang; J. Malik; J. Silvério; J. Hejna; J. Booher; J. Tompson; J. Yang; J. Salvador; J. J. Lim; J. Han; K. Wang; K. Rao; K. Pertsch; K. Hausman; K. Go; K. Gopalakrishnan; K. Goldberg; K. Byrne; K. Oslund; K. Kawaharazuka; K. Black; K. Lin; K. Zhang; K. Ehsani; K. Lekkala; K. Ellis; K. Rana; K. Srinivasan; K. Fang; K. P. Singh; K. -H. Zeng; K. Hatch; K. Hsu; L. Itti; L. Y. Chen; L. Pinto; L. Fei-Fei; L. Tan; L. J. Fan; L. Ott; L. Lee; L. Weihs; M. Chen; M. Lepert; M. Memmel; M. Tomizuka; M. Itkina; M. G. Castro; M. Spero; M. Du; M. Ahn; M. C. Yip; M. Zhang; M. Ding; M. Heo; M. K. Srirama; M. Sharma; M. J. Kim; N. Kanazawa; N. Hansen; N. Heess; N. J. Joshi; N. Suenderhauf; N. Liu; N. Di Palo; N. M. M. Shafiullah; O. Mees; O. Kroemer; O. Bastani; P. R. Sanketi; P. T. Miller; P. Yin; P. Wohlhart; P. Xu; P. D. Fagan; P. Mitrano; P. Sermanet; P. Abbeel; P. Sundaresan; Q. Chen; Q. Vuong; R. Rafailov; R. Tian; R. Doshi; R. Martín-Martín; R. Baijal; R. Scalise; R. Hendrix; R. Lin; R. Qian; R. Zhang; R. Mendonca; R. Shah; R. Hoque; R. Julian; S. Bustamante; S. Kirmani; S. Levine; S. Lin; S. Moore; S. Bahl; S. Dass; S. Sonawani; S. Song; S. Xu; S. Haldar; S. Karamcheti; S. Adebola; S. Guist; S. Nasiriany; S. Schaal; S. Welker; S. Tian; S. Ramamoorthy; S. Dasari; S. Belkhale; S. Park; S. Nair; S. Mirchandani; T. Osa; T. Gupta; T. Harada; T. Matsushima; T. Xiao; T. Kollar; T. Yu; T. Ding; T. Davchev; T. Z. Zhao; T. Armstrong; T. Darrell; T. Chung; V. Jain; V. Vanhoucke; W. Zhan; W. Zhou; W. Burgard; X. Chen; X. Wang; X. Zhu; X. Geng; X. Liu; X. Liangwei; X. Li; Y. Lu; Y. J. Ma; Y. Kim; Y. Chebotar; Y. Zhou; Y. Zhu; Y. Wu; Y. Xu; Y. Wang; Y. Bisk; Y. Cho; Y. Lee; Y. Cui; Y. Cao; Y. -H. Wu; Y. Tang; Y. Zhu; Y. Zhang; Y. Jiang; Y. Li; Y. Li; Y. Iwasawa; Y. Matsuo; Z. Ma; Z. Xu; Z. J. Cui; Z. Zhang; Z. Lin","University of California, Berkeley; University of Edinburgh; University of Texas at Austin; University of Washington; German Aerospace Center; University of California, Berkeley; Google DeepMind; Stanford University; NVIDIA; Intrinsic LLC; Stanford University; Google DeepMind; Google DeepMind; Google DeepMind; Stanford University; New York University; Meta AI; University of California, Berkeley; Google DeepMind; Georgia Institute of Technology; Allen Institute for AI; Stanford University; Google DeepMind; German Aerospace Center; Stanford University; University of California, Davis; University of Washington; Toyota Research Institute; Google DeepMind; Queensland University of Technology; Korea Advanced Institute of Science & Technology; Max Planck Institute; Toyota Research Institute; Google DeepMind; Shanghai Jiao Tong University; University of California, Berkeley; University of California, Berkeley; Google DeepMind; Stanford University; University of California, Berkeley; Columbia University; University of Freiburg; Google DeepMind; Stanford University; Stanford University; Google DeepMind; Google DeepMind; Georgia Institute of Technology; Stanford University; Google DeepMind; University of Washington; Carnegie Mellon University; University of California, Berkeley; Max Planck Institute; University of Pennsylvania; Google DeepMind; Google DeepMind; Imperial College London; Stanford University; University of California, Berkeley; Istituto Italiano di Tecnologia; Google DeepMind; IO-AI TECH; German Aerospace Center; New York University; University of Southern California; University of Southern California; University of California, San Diego; University of California, Berkeley; ETH Zürich; University of Montreal; University of California, Berkeley; California Institute of Technology; University of California, San Diego; Shanghai Jiao Tong University; Stanford University; University of Southern California; Arizona State University; University of California, San Diego; The University of Tokyo; University of California, Berkeley; Shanghai Jiao Tong University; Columbia University; Google DeepMind; University of California, Berkeley; Google DeepMind; Google DeepMind; Queensland University of Technology; Korea Advanced Institute of Science & Technology; University of California, Berkeley; Technische Universität Darmstadt; Max Planck Institute; Google DeepMind; Stanford University; Google DeepMind; University of California, Berkeley; Stanford University; The University of Texas at Austin; Stanford University; Google Research; Stanford University; University of California, Berkeley; University of California, San Diego; Google DeepMind; The University of Tokyo; Princeton University; University of California, San Diego; Stanford University; University of California, Berkeley; German Aerospace Center; Stanford University; Stanford University; Google DeepMind; Stanford University; Allen Institute for AI; Korea Advanced Institute of Science & Technology; Korea Advanced Institute of Science & Technology; University of California, San Diego; Google DeepMind; University of California, Berkeley; Google DeepMind; Intrinsic LLC; Google DeepMind; University of California, Berkeley; Google DeepMind; Google DeepMind; The University of Tokyo; University of California, Berkeley; Stanford University; Carnegie Mellon University; Allen Institute for AI; University of Southern California; University of Montreal; Queensland University of Technology; Stanford University; University of California, Berkeley; EPFL; Allen Institute for AI; Toyota Research Institute; Stanford University; University of Southern California; University of California, Berkeley; New York University; Stanford University; University of California, Berkeley; NVIDIA; ETH Zürich; Google DeepMind; Allen Institute for AI; IO-AI TECH; Stanford University; University of Washington; University of California, Berkeley; Toyota Research Institute; University of Washington; Stanford University; Stanford University; Google DeepMind; University of California, San Diego; University of Illinois Urbana-Champaign; University of California, Berkeley; Korea Advanced Institute of Science & Technology; Carnegie Mellon University; Carnegie Mellon University; Stanford University; The University of Tokyo; University of California, San Diego; Google DeepMind; Google DeepMind; Queensland University of Technology; IO-AI TECH; Imperial College London; New York University; University of Freiburg; Carnegie Mellon University; University of Pennsylvania; Google DeepMind; Toyota Research Institute; University of Washington; Google DeepMind; Google DeepMind; University of Edinburgh; University of Michigan; Google DeepMind; University of California, Berkeley; Stanford University; University of Washington; Google DeepMind; Google DeepMind; University of California, Berkeley; University of California, Berkeley; The University of Texas at Austin; University of Washington; University of Washington; Allen Institute for AI; University of California, Berkeley; IO-AI TECH; Stanford University; Carnegie Mellon University; The University of Texas at Austin; University of California, Berkeley; Google DeepMind; German Aerospace Center; Google DeepMind; Google DeepMind; University of California, San Diego; Google DeepMind; Carnegie Mellon University; University of Southern California; Arizona State University; Columbia University; Google DeepMind; New York University; Stanford University; University of California, Berkeley; Max Planck Institute; The University of Texas at Austin; Intrinsic LLC; Google DeepMind; Stanford University; University of Edinburgh; Carnegie Mellon University; Stanford University; Korea Advanced Institute of Science & Technology; Toyota Research Institute; Stanford University; The University of Tokyo; Allen Institute for AI; The University of Tokyo; The University of Tokyo; Google DeepMind; Toyota Research Institute; Google DeepMind; Google DeepMind; Google DeepMind; Stanford University; Google DeepMind; University of California, Berkeley; University of California, Berkeley; Google DeepMind; Google DeepMind; University of California, Berkeley; Google DeepMind; University of Technology, Nuremberg; Google DeepMind; University of California, San Diego; University of California, Berkeley; University of California, Berkeley; IO-AI TECH; IO-AI TECH; University of California, San Diego; Google DeepMind; University of Pennsylvania; Allen Institute for AI; Google DeepMind; Arizona State University; The University of Texas at Austin; Carnegie Mellon University; Google DeepMind; University of Illinois Urbana-Champaign; Carnegie Mellon University; Korea Advanced Institute of Science & Technology; University of California, Berkeley; Stanford University; IO-AI TECH; University of California, San Diego; Google DeepMind; The University of Texas at Austin; University of Washington; Stanford University; University of Pennsylvania; University of Illinois Urbana-Champaign; The University of Tokyo; The University of Tokyo; University of California, Berkeley; Google DeepMind; New York University; Allen Institute for AI; University of California, Berkeley",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","6892","6903","Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train ""generalist"" X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms. The project website is robotics-transformer-x.github.io.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611477","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611477","","Learning systems;Adaptation models;Computer vision;Computational modeling;Collaboration;Data models;Task analysis","","1","","134","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Robust and Remote Center of Cyclic Motion Control for Redundant Robots with Partially Unknown Structure","L. Jin; K. Liu; M. Liu","School of Information Science and Engineering, Lanzhou University, Lanzhou, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","3492","3498","Remote center of motion (RCM) describes a robot with a rod-like end-effector operating through a hole in the interface separating the internal space from the external space. Considering that the control of RCM may be influenced by perturbations (noises) and that the end-effector is frequently replaced to complete different tasks, the structural information related to the robot manipulator and its rod-like end-effector may contain errors. This paper proposes an acceleration-level remote center of cyclic motion (ARC2M) control scheme, which takes into account the cyclic motion index and the physical limitations of robot manipulators to achieve repetitive motion planning and RCM control at the acceleration level. Additionally, a parameter calculation method is proposed to compute unknown parameters of the end-effector under the influence of noise. Kalman filter and a neural dynamics-based method are employed to address noises effects, and related theoretical analyses are given. To validate the proposed ARC2M scheme, simulations and physical experiments are carried out. The source code is available at https://github.com/LongJin-lab/ARCM.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611145","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; Natural Science Foundation of Gansu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611145","","Trajectory tracking;Source coding;Perturbation methods;Noise;Aerospace electronics;End effectors;Real-time systems","","","","20","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Tight Motion Planning by Riemannian Optimization for Sliding and Rolling with Finite Number of Contact Points","D. Livnat; M. M. Bilevich; D. Halperin","Blavatnik School of Computer Science, Tel-Aviv University, Israel; Blavatnik School of Computer Science, Tel-Aviv University, Israel; Blavatnik School of Computer Science, Tel-Aviv University, Israel",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","14333","14340","We address a challenging problem in motion planning where robots must navigate through narrow passages in their configuration space. Our novel approach leverages optimization techniques to facilitate sliding and rolling movements across critical regions, which represent semi-free configurations, where the robot and the obstacles are in contact. Our algorithm seamlessly traverses widely free regions, follows semi-free paths in narrow passages, and smoothly transitions between the two types. We specifically focus on scenarios resembling 3D puzzles, intentionally designed to be complex for humans by requiring intricate simultaneous translations and rotations. Remarkably, these complexities also present computational challenges. Our contributions are threefold: First, we solve previously unsolved problems; second, we outperform state-of-the-art algorithms on certain problem types; and third, we present a rigorous analysis supporting the consistency of the algorithm. In the Supplementary Material we provide theoretical foundations for our approach. The Supplementary Material and our open source software are available at https://github.com/TAU-CGL/tr-rrt-public. This research sheds light on effective approaches to address motion planning difficulties in intricate 3D puzzle-like scenarios.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611716","Israel Science Foundation; Ministry of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611716","","Three-dimensional displays;Navigation;Software algorithms;Planning;Complexity theory;Robots;Optimization","","","","52","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Robust Collaborative Perception against Temporal Information Disturbance","X. He; Y. Li; T. Cui; M. Wang; T. Liu; Y. Yue","School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","16207","16213","Collaborative perception facilitates a more comprehensive representation of the environment by leveraging complementary information shared among various agents and sensors. However, practical applications often encounter information disturbance which includes perception packet loss and time delays, and a comprehensive framework that can simultaneously address such issues is absent. In addition, the feature extraction process prior to fusion is not sufficient, as it lacks exploration of the local semantics and context dependencies of individual features. To enhance both accuracy and robustness, this paper introduces a novel framework named Robust Collaborative Perception against Temporal Information Disturbance, which predicts perception information when disturbance occurs. Specifically, the Historical Frame Prediction (HFP) module is introduced to make compensation for information loss with temporal association excavation of historical features. Based on the predicted features generated by the HFP module, the Pyramid Attention Integration (PAI) module is introduced to augment local semantics and incorporate global long-range dependencies through multi-scale window attention. Compared with existing methods on the publicly available dataset OPV2V, our approach exhibits superior performance and expanded robustness in the 3D object detection task. The code will be publicly available at https://github.com/hexunjie/Ro-temd.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611481","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611481","","Three-dimensional displays;Accuracy;Delay effects;Semantics;Collaboration;Packet loss;Object detection","","1","","24","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"MOSAIC: Learning Unified Multi-Sensory Object Property Representations for Robot Learning via Interactive Perception","G. Tatiya; J. Francis; H. -H. Wu; Y. Bisk; J. Sinapov","Department of Computer Science, Tufts University; Bosch Center for AI; Bosch Center for AI; Carnegie Mellon University; Department of Computer Science, Tufts University",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","15381","15387","A holistic understanding of object properties across diverse sensory modalities (e.g., visual, audio, and haptic) is essential for tasks ranging from object categorization to complex manipulation. Drawing inspiration from cognitive science studies that emphasize the significance of multi-sensory integration in human perception, we introduce MOSAIC (Multimodal Object property learning with Self-Attention and Interactive Comprehension), a novel framework designed to facilitate the learning of unified multi-sensory object property representations. While it is undeniable that visual information plays a prominent role, we acknowledge that many fundamental object properties extend beyond the visual domain to encompass attributes like texture, mass distribution, or sounds, which significantly influence how we interact with objects. In MOSAIC, we leverage this profound insight by distilling knowledge from multimodal foundation models and aligning these representations not only across vision but also haptic and auditory sensory modalities. Through extensive experiments on a dataset where a humanoid robot interacts with 100 objects across 10 exploratory behaviors, we demonstrate the versatility of MOSAIC in two task families: object categorization and object-fetching tasks. Our results underscore the efficacy of MOSAIC's unified representations, showing competitive performance in category recognition through a simple linear probe setup and excelling in the fetch object task under zero-shot transfer conditions. This work pioneers the application of sensory grounding in foundation models for robotics, promising a significant leap in multi-sensory perception capabilities for autonomous systems. We have released the code, datasets, and additional results: https://github.com/gtatiya/MOSAIC.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10609998","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10609998","","Visualization;Grounding;Humanoid robots;Robot sensing systems;Robot learning;Distance measurement;Haptic interfaces","","1","","35","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Diving into the Depths of Spotting Text in Multi-Domain Noisy Scenes","A. Das; S. Biswas; U. Pal; J. Lladós","Computer Vision and Pattern Recognition Unit, Indian Statistical Institute, Kolkata, India; Computer Science Department, Computer Vision Center, Universitat Autónoma de Barcelona, Barcelona, Spain; Computer Vision and Pattern Recognition Unit, Indian Statistical Institute, Kolkata, India; Computer Science Department, Computer Vision Center, Universitat Autónoma de Barcelona, Barcelona, Spain",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","410","417","When used in a real-world noisy environment, the capacity to generalize to multiple domains is essential for any autonomous scene text spotting system. However, existing state-of-the-art methods employ pretraining and fine-tuning strategies on natural scene datasets, which do not exploit the feature interaction across other complex domains. In this work, we explore and investigate the problem of domain-agnostic scene text spotting, i.e., training a model on multi-domain source data such that it can directly generalize to target domains rather than being specialized for a specific domain or scenario. In this regard, we present the community a text spotting validation benchmark called Under-Water Text (UWT) for noisy underwater scenes to establish an important case study. Moreover, we also design an efficient super-resolution based end-to-end transformer baseline called DA-TextSpotter which achieves comparable or superior performance over existing text spotting architectures for both regular and arbitrary-shaped scene text spotting benchmarks in terms of both accuracy and model efficiency. The dataset, code and pre-trained models have been released in our Github.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611120","Generalitat de Catalunya; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611120","","Training;Codes;Accuracy;Superresolution;Benchmark testing;Transformers;Data models","","2","","60","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"RoboHop: Segment-based Topological Map Representation for Open-World Visual Navigation","S. Garg; K. Rana; M. Hosseinzadeh; L. Mares; N. Sünderhauf; F. Dayoub; I. Reid","The University of Adelaide, Australia; Queensland University of Technology, Australia; The University of Adelaide, Australia; The University of Adelaide, Australia; Queensland University of Technology, Australia; The University of Adelaide, Australia; The University of Adelaide, Australia",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","4090","4097","Mapping is crucial for spatial reasoning, planning and robot navigation. Existing approaches range from metric, which require precise geometry-based optimization, to purely topological, where image-as-node based graphs lack explicit object-level reasoning and interconnectivity. In this paper, we propose a novel topological representation of an environment based on , which are semantically meaningful and open-vocabulary queryable, conferring several advantages over previous works based on pixel-level features. Unlike 3D scene graphs, we create a purely topological graph with segments as nodes, where edges are formed by a) associating segment-level descriptors between pairs of consecutive images and b) connecting neighboring segments within an image using their pixel centroids. This unveils a continuous sense of a place, defined by inter-image persistence of segments along with their intra-image neighbours. It further enables us to represent and update segment-level descriptors through neighborhood aggregation using graph convolution layers, which improves robot localization based on segment-level retrieval. Using real-world data, we show how our proposed map representation can be used to i) generate navigation plans in the form of hops over segments and ii) search for target objects using natural language queries describing spatial relations of objects. Furthermore, we quantitatively analyze data association at the segment level, which underpins inter-image connectivity during mapping and segment-level localization when revisiting the same place. Finally, we show preliminary trials on segment-level ‘hopping’ based zero-shot real-world navigation. Project page with supplementary details: oravus.github.io/RoboHop/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610234","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610234","","Image segmentation;Visualization;Three-dimensional displays;Navigation;Search problems;Cognition;Robot localization","","5","","70","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"LIKO: LiDAR, Inertial, and Kinematic Odometry for Bipedal Robots","Q. Zhao; M. Li; Y. Shi; X. Chen; Z. Yu; L. Han; Z. Fu; J. Zhang; C. Li; Y. Zhang; Q. Huang","School of Mechatronical Engineering, Beijing Institute of Technology (BIT), Beijing, China; School of Mechatronical Engineering, Beijing Institute of Technology (BIT), Beijing, China; Qiyuan Lab; School of Mechatronical Engineering, Beijing Institute of Technology (BIT), Beijing, China; School of Mechatronical Engineering, Beijing Institute of Technology (BIT), Beijing, China; School of Mechatronical Engineering, Beijing Institute of Technology (BIT), Beijing, China; School of Mechatronical Engineering, Beijing Institute of Technology (BIT), Beijing, China; School of Mechatronical Engineering, Beijing Institute of Technology (BIT), Beijing, China; School of Mechatronical Engineering, Beijing Institute of Technology (BIT), Beijing, China; School of Mechatronical Engineering, Beijing Institute of Technology (BIT), Beijing, China; School of Mechatronical Engineering, Beijing Institute of Technology (BIT), Beijing, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","1180","1185","High-frequency and accurate state estimation is crucial for biped robots. This paper presents a tightly-coupled LiDAR-Inertial-Kinematic Odometry (LIKO) for biped robot state estimation based on an iterated extended Kalman filter. Beyond state estimation, the foot contact position is also modeled and estimated. This allows for both position and velocity updates from kinematic measurement. Additionally, the use of kinematic measurement results in an increased output state frequency of about 1kHz. This ensures temporal continuity of the estimated state and makes it practical for control purposes of biped robots. We also announce a biped robot dataset consisting of LiDAR, inertial measurement unit (IMU), joint encoders, force/torque (F/T) sensors, and motion capture ground truth to evaluate the proposed method. The dataset is collected during robot locomotion, and our approach reached the best quantitative result among other LIO-based methods and biped robot state estimation algorithms. The dataset and source code will be available at https://github.com/Mr-Zqr/LIKO.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610222","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610222","","Visualization;Laser radar;Accuracy;Source coding;Robot sensing systems;Sensors;Frequency measurement","","1","","32","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Noisy Few-shot 3D Point Cloud Scene Segmentation","H. Huang; S. Yuan; C. Wen; Y. Hao; Y. Fang","Center for Artificial Intelligence and Robotics (CAIR), New York University, Abu Dhabi, UAE; Center for Artificial Intelligence and Robotics (CAIR), New York University, Abu Dhabi, UAE; Center for Artificial Intelligence and Robotics (CAIR), New York University, Abu Dhabi, UAE; Center for Artificial Intelligence and Robotics (CAIR), New York University, Abu Dhabi, UAE; Center for Artificial Intelligence and Robotics (CAIR), New York University, Abu Dhabi, UAE",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","11070","11077","3D scene semantic segmentation plays a crucial role in robotics by enabling robots to understand and interpret their environment in a detailed and context-aware manner, facilitating tasks such as navigation, object manipulation, and interaction within complex spaces. A preponderance of methodology predominantly adopts a fully supervised framework for 3D point cloud scene semantic segmentation. Such paradigms exhibit an intrinsic dependency on extensive labeled datasets, presenting challenges in acquisition and exhibiting incapacity to segment novel classes, especially when the training data are contaminated by noisy samples. To address these limitations, this study introduces a novel few-shot segmentation approach to robustly segment 3D point cloud scenes with noisy labels using a meta-learning scheme. Specifically, we first build a multi-prototype graph and then suppress samples with noisy labels based on the graph structure. A subgraph bagging scheme is then proposed to conduct semi-supervised transductive learning to propagate labels. To optimize the graph structure to learn discriminative prototype features, we design a triplet contrastive loss to increase the compactness of these subgraphs. We evaluated our method on two widely used 3D point cloud scene segmentation benchmarks within few-shot (i.e., 2/3-way 5-shot) segmentation settings with noisy samples. Experimental results demonstrate the improvement of our method over the compared baselines, illustrating the robustness of our method in few-shot 3D scene segmentation against noisy samples. The code is available at: https://github.com/hhuang-code/Noisy_Fewshot_Segmentation.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611583","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611583","","Point cloud compression;Three-dimensional displays;Semantic segmentation;Noise;Training data;Benchmark testing;Robustness","","","","76","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"A Biomorphic Whisker Sensor for Aerial Tactile Applications","C. Ye; G. De Croon; S. Hamaza","Dept. of Control & Operations, Biomorphic Intelligence Laboratory & MAVlab, Faculty of Aerospace Engineering, TU Delft, The Netherlands; Dept. of Control & Operations, Biomorphic Intelligence Laboratory & MAVlab, Faculty of Aerospace Engineering, TU Delft, The Netherlands; Dept. of Control & Operations, Biomorphic Intelligence Laboratory & MAVlab, Faculty of Aerospace Engineering, TU Delft, The Netherlands",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","5257","5263","Unmanned air vehicles (UAVs) have traditionally been considered as ""eyes in the sky"", that can move in three dimensions and need to avoid any contact with their environment. On the contrary, contact should not be considered as a problem, but as an opportunity to expand the range of UAVs applications. In this paper, we designed, fabricated, and characterized a whisker sensor unit based on MEMS barometers suitable for tactile localization on UAVs, featuring lightweight, low stiffness, high sensitivity, a broad sensing range, and scalability. Then, for the challenging task of contact point localization, we propose a Recurrent Multi-output Network (RMN) for predicting 3D contact points under continuous contact conditions to address the problems of non-linearity, hysteresis, and non-injective mapping between signals and contact points by considering time series. In addition, we propose an azimuth prediction loss function which reduces the RMSE by 3.24◦ compared to L1 loss. Finally, we conduct experiments on a linear stage to validate the 3D contact point localization capability of the proposed whisker system and model. The results show that our localization can achieve excellent performance, with an inference time of 1.4 ms and a mean error of only 9.18 mm in Euclidean distance within 3D space, laying a robust foundation for future implementation of tactile localization on UAVs. The design files, dataset, and source code are available on: https://github.com/BioMorphic-Intelligence-Lab/Whisker-3D-Localization.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610850","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610850","","Location awareness;Three-dimensional displays;Sensitivity;Azimuth;Scalability;Euclidean distance;Robot sensing systems","","","","25","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"A 3D Mixed Reality Interface for Human-Robot Teaming","J. Chen; B. Sun; M. Pollefeys; H. Blum","Computer Vision and Geometry Lab, ETH Zürich; Computer Vision and Geometry Lab, ETH Zürich; Computer Vision and Geometry Lab, ETH Zürich; Computer Vision and Geometry Lab, ETH Zürich",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","11327","11333","This paper presents a mixed-reality human-robot teaming system. It allows human operators to see in real-time where robots are located, even if they are not in line of sight. The operator can also visualize the map that the robots create of their environment and can easily send robots to new goal positions. The system mainly consists of a mapping and a control module. The mapping module is a real-time multi-agent visual SLAM system that co-localizes all robots and mixed-reality devices to a common reference frame. Visualizations in the mixed-reality device then allow operators to see a virtual life-sized representation of the cumulative 3D map overlaid onto the real environment. As such, the operator can effectively ""see through"" walls into other rooms. To control robots and send them to new locations, we propose a drag-and-drop interface. An operator can grab any robot hologram in a 3D mini map and drag it to a new desired goal pose. We validate the proposed system through a user study and real-world deployments. We make the mixed-reality application publicly available at github.com/cvg/HoloLens_ros.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611017","","Visualization;Three-dimensional displays;Simultaneous localization and mapping;Mixed reality;Virtual reality;Real-time systems;Robots","","1","","31","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"3D-OAE: Occlusion Auto-Encoders for Self-Supervised Learning on Point Clouds","J. Zhou; X. Wen; B. Ma; Y. -S. Liu; Y. Gao; Y. Fang; Z. Han","School of Software, BNRist, Tsinghua University, Beijing, China; NVIDIA; School of Software, Tsinghua University and Beijing Academy of Artificial Intelligence, Beijing, China; School of Software, BNRist, Tsinghua University, Beijing, China; School of Software, BNRist, Tsinghua University, Beijing, China; New York University; Department of Computer Science, Wayne State University, USA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","15416","15423","The manual annotation for large-scale point clouds is still tedious and unavailable for many harsh real-world tasks. Self-supervised learning, which is used on raw and unlabeled data to pre-train deep neural networks, is a promising approach to address this issue. Existing works usually take the common aid from auto-encoders to establish the self-supervision by the self-reconstruction schema. However, the previous auto-encoders merely focus on the global shapes and do not distinguish the local and global geometric features apart. To address this problem, we present a novel and efficient self-supervised point cloud representation learning framework, named 3D Occlusion Auto-Encoder (3D-OAE), to facilitate the detailed supervision inherited in local regions and global shapes. We propose to randomly occlude some local patches of point clouds and establish the supervision via inpainting the occluded patches using the remaining ones. Specifically, we design an asymmetrical encoder-decoder architecture based on standard Transformer, where the encoder operates only on the visible subset of patches to learn local patterns, and a lightweight decoder is designed to leverage these visible patterns to infer the missing geometries via self-attention. We find that occluding a very high proportion of the input point cloud (e.g. 75%) will still yield a nontrivial self-supervisory performance, which enables us to achieve 3-4 times faster during training but also improve accuracy. Experimental results show that our approach outperforms the state-of-the-art on a diverse range of down-stream discriminative and generative tasks. Code is available at https://github.com/junshengzhou/3D-OAE.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610588","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610588","","Point cloud compression;Training;Representation learning;Three-dimensional displays;Shape;Self-supervised learning;Transformers","","4","","54","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"6-DoF Grasp Pose Evaluation and Optimization via Transfer Learning from NeRFs","G. Sóti; X. Huang; C. Wurll; B. Hein","Institute of Robotics and Autonomous Systems, Karlsruhe University of Applied Sciences, Karlsruhe, Germany; Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Karlsruhe, Germany; Institute of Robotics and Autonomous Systems, Karlsruhe University of Applied Sciences, Karlsruhe, Germany; Institute of Robotics and Autonomous Systems, Karlsruhe University of Applied Sciences, Karlsruhe, Germany",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","9495","9501","We address the problem of robotic grasping of known and unknown objects using implicit behavior cloning. We train a grasp evaluation model from a small number of demonstrations that outputs higher values for grasp candidates that are more likely to succeed in grasping. This evaluation model serves as an objective function, that we maximize to identify successful grasps. Key to our approach is the utilization of learned implicit representations of visual and geometric features derived from a pre-trained NeRF. Though trained exclusively in a simulated environment with simplified objects and 4-DoF topdown grasps, our evaluation model and optimization procedure demonstrate generalization to 6-DoF grasps and novel objects both in simulation and in real-world settings, without the need for additional data. Supplementary material is available at: https://gergely-soti.github.io/grasp","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610402","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610402","","Training;Visualization;Evaluation models;Transfer learning;Grasping;Neural radiance field;6-DOF","","","","31","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"An Onboard Framework for Staircases Modeling Based on Point Clouds","C. Qing; R. Zeng; X. Wu; Y. Shi; G. Ma","Sino-German College of Intelligent Manufacturing, Shenzhen Technology University, China; Sino-German College of Intelligent Manufacturing, Shenzhen Technology University, China; Sino-German College of Intelligent Manufacturing, Shenzhen Technology University, China; Institute for AI Industry Research (AIR), Tsinghua University, China; Sino-German College of Intelligent Manufacturing, Shenzhen Technology University, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","1759","1765","The detection of traversable regions on staircases and the physical modeling constitutes pivotal aspects of the mobility of legged robots. This paper presents an onboard framework tailored to the detection of traversable regions and the modeling of physical attributes of staircases by point cloud data. To mitigate the influence of illumination variations and the overfitting due to the dataset diversity, a series of data augmentations are introduced to enhance the training of the fundamental network. A curvature suppression cross-entropy(CSCE) loss is proposed to reduce the ambiguity of prediction on the boundary between traversable and non-traversable regions. Moreover, a measurement correction based on the pose estimation of stairs is introduced to calibrate the output of raw modeling that is influenced by tilted perspectives. Lastly, we collect a dataset pertaining to staircases and introduce new evaluation criteria. Through a series of rigorous experiments conducted on this dataset, we substantiate the superior accuracy and generalization capabilities of our proposed method. Codes, models, and datasets will be available at https://github.com/szturobotics/Stair-detection-and-modeling-project.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610407","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610407","","Training;Point cloud compression;Legged locomotion;Pose estimation;Lighting;Stairs;Data augmentation","","","","25","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Multi-objective Cross-task Learning via Goal-conditioned GPT-based Decision Transformers for Surgical Robot Task Automation","J. Fu; Y. Long; K. Chen; W. Wei; Q. Dou","Department of Computer Science and Engineering, The Chinese University of Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","13362","13368","Surgical robot task automation has been a promising research topic for improving surgical efficiency and quality. Learning-based methods have been recognized as an interesting paradigm and been increasingly investigated. However, existing approaches encounter difficulties in long-horizon goal-conditioned tasks due to the intricate compositional structure, which requires decision-making for a sequence of sub-steps and understanding of inherent dynamics of goal-reaching tasks. In this paper, we propose a new learning-based framework by leveraging the strong reasoning capability of the GPT-based architecture to automate surgical robotic tasks. The key to our approach is developing a goal-conditioned decision transformer to achieve sequential representations with goal-aware future indicators in order to enhance temporal reasoning. Moreover, considering to exploit a general understanding of dynamics inherent in manipulations, thus making the model’s reasoning ability to be task-agnostic, we also design a cross-task pretraining paradigm that uses multiple training objectives associated with data from diverse tasks. We have conducted extensive experiments on 10 tasks using the surgical robot learning simulator SurRoL [1]. The results show that our new approach achieves promising performance and task versatility compared to existing methods. The learned trajectories can be deployed on the da Vinci Research Kit (dVRK) for validating its practicality in real surgical robot settings. Our project website is at: https://med-air.github.io/SurRoL.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611051","Innovation and Technology Fund; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611051","","Training;Learning systems;Medical robotics;Automation;Decision making;Transformers;Cognition","","","","39","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"GrainGrasp: Dexterous Grasp Generation with Fine-grained Contact Guidance","F. Zhao; D. Tsetserukou; Q. Liu","Department of Computer Science and Technology, Dalian University of Technology, China; Skolkovo Institute of Science and Technology, Moscow, Russia; Department of Computer Science and Technology, Dalian University of Technology, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","6470","6476","One goal of dexterous robotic grasping is to allow robots to handle objects with the same level of flexibility and adaptability as humans. However, it remains a challenging task to generate an optimal grasping strategy for dexterous hands, especially when it comes to delicate manipulation and accurate adjustment the desired grasping poses for objects of varying shapes and sizes. In this paper, we propose a novel dexterous grasp generation scheme called GrainGrasp that provides fine-grained contact guidance for each fingertip. In particular, we employ a generative model to predict separate contact maps for each fingertip on the object point cloud, effectively capturing the specifics of finger-object interactions. In addition, we develop a new dexterous grasping optimization algorithm that solely relies on the point cloud as input, eliminating the necessity for complete mesh information of the object. By leveraging the contact maps of different fingertips, the proposed optimization algorithm can generate precise and determinable strategies for human-like object grasping. Experimental results confirm the efficiency of the proposed scheme. Our code is available at https://github.com/wmtlab/GrainGrasp.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610035","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610035","","Point cloud compression;Codes;Accuracy;Shape;Optimization methods;Grasping;Predictive models","","1","","39","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"ReorientDiff: Diffusion Model based Reorientation for Object Manipulation","U. A. Mishra; Y. Chen","Institute for Robotics and Intelligent Machines (IRIM), Georgia Institute of Technology; Institute for Robotics and Intelligent Machines (IRIM), Georgia Institute of Technology",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","10867","10873","The ability to manipulate objects in desired configurations is a fundamental requirement for robots to complete various practical applications. While certain goals can be achieved by picking and placing the objects of interest directly, object reorientation is needed for precise placement in most of the tasks. In such scenarios, the object must be reoriented and re-positioned into intermediate poses that facilitate accurate placement at the target pose. To this end, we propose a reorientation planning method, ReorientDiff, that utilizes a diffusion model-based approach. The proposed method employs both visual inputs from the scene, and goal-specific language prompts to plan intermediate reorientation poses. Specifically, the scene and language-task information are mapped into a joint scene-task representation feature space, which is subsequently leveraged to condition the diffusion model. The diffusion model samples intermediate poses based on the representation using classifier-free guidance and then uses gradients of learned feasibility-score models for implicit iterative pose-refinement. The proposed method is evaluated using a set of YCB-objects and a suction gripper, demonstrating a success rate of 95.2% in simulation. Overall, we present a promising approach to address the reorientation challenge in manipulation by learning a conditional distribution, which is an effective way to move towards generalizable object manipulation. More results can be found on our website: https://utkarshmishra04.github.io/ReorientDiff.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610749","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610749","","Visualization;Uncertainty;Accuracy;Predictive models;Diffusion models;Planning;Iterative methods","","2","","34","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Merging Decision Transformers: Weight Averaging for Forming Multi-Task Policies","D. Lawson; A. H. Qureshi","Department of Computer Science, Purdue University, West Lafayette, IN, USA; Department of Computer Science, Purdue University, West Lafayette, IN, USA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","12942","12948","Recent work has shown the promise of creating generalist, transformer-based, models for language, vision, and sequential decision-making problems. To create such models, we generally require centralized training objectives, data, and compute. It is of interest if we can more flexibly create generalist policies by merging together multiple, task-specific, individually trained policies. In this work, we take a preliminary step in this direction through merging, or averaging, subsets of Decision Transformers in parameter space trained on different MuJoCo locomotion problems, forming multi-task models without centralized training. We also demonstrate the importance of various methodological choices when merging policies, such as utilizing common pre-trained initializations, increasing model capacity, and utilizing Fisher information for weighting parameter importance. In general, we believe research in this direction could help democratize and distribute the process that forms multi-task robotics policies. Our implementation is available at https://github.com/daniellawson9999/merging-decision-transformer.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610919","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610919","","Training;Computational modeling;Merging;Decision making;Transformers;Multitasking;Data models","","","","38","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"MMAUD: A Comprehensive Multi-Modal Anti-UAV Dataset for Modern Miniature Drone Threats","S. Yuan; Y. Yang; T. H. Nguyen; T. -M. Nguyen; J. Yang; F. Liu; J. Li; H. Wang; L. Xie","The School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; The School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; The School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; The School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; The School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; The School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; The School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; The School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; The School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","2745","2751","In response to the evolving challenges posed by small unmanned aerial vehicles (UAVs), which possess the potential to transport harmful payloads or independently cause damage, we introduce MMAUD: a comprehensive Multi-Modal Anti-UAV Dataset. MMAUD addresses a critical gap in contemporary threat detection methodologies by focusing on drone detection, UAV-type classification, and trajectory estimation. MMAUD stands out by combining diverse sensory inputs, including stereo vision, various Lidars, Radars, and audio arrays. It offers a unique overhead aerial detection vital for addressing real-world scenarios with higher fidelity than datasets captured on specific vantage points using thermal and RGB. Additionally, MMAUD provides accurate Leica-generated ground truth data, enhancing credibility and enabling confident refinement of algorithms and models, which has never been seen in other datasets. Most existing works do not disclose their datasets, making MMAUD an invaluable resource for developing accurate and efficient solutions. Our proposed modalities are cost-effective and highly adaptable, allowing users to experiment and implement new UAV threat detection tools. Our dataset closely simulates real-world scenarios by incorporating ambient heavy machinery sounds. This approach enhances the dataset’s applicability, capturing the exact challenges faced during proximate vehicular operations. It is expected that MMAUD can play a pivotal role in advancing UAV threat detection, classification, trajectory estimation capabilities, and beyond. Our dataset, codes, and designs will be available in https://ntu-aris.github.io/MMAUD.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610957","National Research Foundation; Delta; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610957","UAV;LIDAR;Audio;video fusion;Detection;Classification;Trajectory Estimation","Accuracy;Laser radar;Estimation;Autonomous aerial vehicles;Robot sensing systems;Threat assessment;Trajectory","","3","","42","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"VOOM: Robust Visual Object Odometry and Mapping using Hierarchical Landmarks","Y. Wang; C. Jiang; X. Chen","School of Mechanical Engineering, Beijing Institute of Technology, China; School of Mechanical Engineering, Beijing Institute of Technology, China; College of Intelligence Science and Technology, National University of Defense Technology, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","10298","10304","In recent years, object-oriented simultaneous localization and mapping (SLAM) has attracted increasing attention due to its ability to provide high-level semantic information while maintaining computational efficiency. Some researchers have attempted to enhance localization accuracy by integrating the modeled object residuals into bundle adjustment. However, few have demonstrated better results than feature-based visual SLAM systems, as the generic coarse object models, such as cuboids or ellipsoids, are less accurate than feature points. In this paper, we propose a Visual Object Odometry and Mapping framework VOOM using high-level objects and low-level points as the hierarchical landmarks in a coarse-to-fine manner instead of directly using object residuals in bundle adjustment. Firstly, we introduce an improved observation model and a novel data association method for dual quadrics, employed to represent physical objects. It facilitates the creation of a 3D map that closely reflects reality. Next, we use object information to enhance the data association of feature points and consequently update the map. In the visual object odometry backend, the updated map is employed to further optimize the camera pose and the objects. Meanwhile, local bundle adjustment is performed utilizing the objects and points-based covisibility graphs in our visual object mapping process. Experiments show that VOOM outperforms both object-oriented SLAM and feature points SLAM systems such as ORB-SLAM2 in terms of localization. The implementation of our method is available at https://github.com/yutongwangBIT/VOOM.git.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611684","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611684","","Location awareness;Bundle adjustment;Visualization;Simultaneous localization and mapping;Accuracy;Object oriented modeling;Computational modeling","","2","","29","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"AirExo: Low-Cost Exoskeletons for Learning Whole-Arm Manipulation in the Wild","H. Fang; H. -S. Fang; Y. Wang; J. Ren; J. Chen; R. Zhang; W. Wang; C. Lu","School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University; School of Mechanical Engineering, Shanghai Jiao Tong University; School of Mechanical Engineering, Shanghai Jiao Tong University; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University; School of Mechanical Engineering, Shanghai Jiao Tong University; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","15031","15038","While humans can use parts of their arms other than the hands for manipulations like gathering and supporting, whether robots can effectively learn and perform the same type of operations remains relatively unexplored. As these manipulations require joint-level control to regulate the complete poses of the robots, we develop AirExo, a low-cost, adaptable, and portable dual-arm exoskeleton, for teleoperation and demonstration collection. As collecting teleoperated data is expensive and time-consuming, we further leverage AirExo to collect cheap in-the-wild demonstrations at scale. Under our in-the-wild learning framework, we show that with only 3 minutes of the teleoperated demonstrations, augmented by diverse and extensive in-the-wild data collected by AirExo, robots can learn a policy that is comparable to or even better than one learned from teleoperated demonstrations lasting over 20 minutes. Experiments demonstrate that our approach enables the model to learn a more general and robust policy across the various stages of the task, enhancing the success rates in task completion even with the presence of disturbances. Project website: airexo.github.io.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610799","Research and Development; Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610799","","Costs;Atmospheric modeling;Exoskeletons;Manipulators;Robustness;Task analysis;Robots","","2","","48","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Fault Tolerant Neural Control Barrier Functions for Robotic Systems under Sensor Faults and Attacks","H. Zhang; L. Niu; A. Clark; R. Poovendran","Electrical and Systems Engineering Department, McKelvey School of Engineering, Washington University in St. Louis, St. Louis, MO; Department of Electrical and Computer Engineering, Network Security Lab, University of Washington, Seattle, WA; Electrical and Systems Engineering Department, McKelvey School of Engineering, Washington University in St. Louis, St. Louis, MO; Department of Electrical and Computer Engineering, Network Security Lab, University of Washington, Seattle, WA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","9901","9907","Safety is a fundamental requirement of many robotic systems. Control barrier function (CBF)-based approaches have been proposed to guarantee the safety of robotic systems. However, the effectiveness of these approaches highly relies on the choice of CBFs. Inspired by the universal approximation power of neural networks, there is a growing trend toward representing CBFs using neural networks, leading to the notion of neural CBFs (NCBFs). Current NCBFs, however, are trained and deployed in benign environments, making them ineffective for scenarios where robotic systems experience sensor faults and attacks. In this paper, we study safety-critical control synthesis for robotic systems under sensor faults and attacks. Our main contribution is the development and synthesis of a new class of CBFs that we term fault tolerant neural control barrier function (FT-NCBF). We derive the necessary and sufficient conditions for FT-NCBFs to guarantee safety, and develop a data-driven method to learn FT-NCBFs by minimizing a loss function constructed using the derived conditions. Using the learned FT-NCBF, we synthesize a control input and formally prove the safety guarantee provided by our approach. We demonstrate our proposed approach using two case studies: obstacle avoidance problem for an autonomous mobile robot and spacecraft rendezvous problem, with code available via https://github.com/HongchaoZhang-HZ/FTNCBF.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610491","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610491","","Space vehicles;Sufficient conditions;Fault tolerance;Neural networks;Fault tolerant systems;Robot sensing systems;Safety","","","","42","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Grasp-Anything: Large-scale Grasp Dataset from Foundation Models","A. D. Vuong; M. N. Vu; H. Le; B. Huang; H. T. T. Binh; T. Vo; A. Kugi; A. Nguyen","FPT Software AI Center, Vietnam; Automation & Control Institute, TU Wien, Vienna, Austria; FPT Software AI Center, Vietnam; Imperial College London, UK; Hanoi University of Science and Technology, Vietnam; Faculty of Mathematics and Statistics, Ton Duc Thang University, Ho Chi Minh City, Vietnam; Automation & Control Institute, TU Wien, Vienna, Austria; Department of Computer Science, University of Liverpool, UK",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","14030","14037","Foundation models such as ChatGPT have made significant strides in robotic tasks due to their universal representation of real-world domains. In this paper, we leverage foundation models to tackle grasp detection, a persistent challenge in robotics with broad industrial applications. Despite numerous grasp datasets, their object diversity remains limited compared to real-world figures. Fortunately, foundation models possess an extensive repository of real-world knowledge, including objects we encounter in our daily lives. As a consequence, a promising solution to the limited representation in previous grasp datasets is to harness the universal knowledge embedded in these foundation models. We present Grasp-Anything, a new large-scale grasp dataset synthesized from foundation models to implement this solution. Grasp-Anything excels in diversity and magnitude, boasting 1M samples with text descriptions and more than 3M objects, surpassing prior datasets. Empirically, we show that Grasp-Anything successfully facilitates zero-shot grasp detection on vision-based tasks and real-world robotic experiments. Our dataset and code are available at https://airvlab.github.io/grasp-anything/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611277","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611277","","Codes;Service robots;Chatbots;Task analysis","","4","","67","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Physical Priors Augmented Event-Based 3D Reconstruction","J. Wang; J. He; Z. Zhang; R. Xu","MICS Thrust, HKUST(GZ), Guangzhou, China; MICS Thrust, HKUST(GZ), Guangzhou, China; MICS Thrust, HKUST(GZ), Guangzhou, China; MICS Thrust, HKUST(GZ), Guangzhou, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","16810","16817","3D neural implicit representations play a significant component in many robotic applications. However, reconstructing neural radiance fields (NeRF) from realistic event data remains a challenge due to the sparsities and the lack of information when only event streams are available. In this paper, we utilize motion, geometry, and density priors behind event data to impose strong physical constraints to augment NeRF training. The proposed novel pipeline can directly benefit from those priors to reconstruct 3D scenes without additional inputs. Moreover, we present a novel density-guided patch-based sampling strategy for robust and efficient learning, which not only accelerates training procedures but also conduces to expressions of local geometries. More importantly, we establish the first large dataset for event-based 3D reconstruction, which contains 101 objects with various materials and geometries, along with the groundtruth of images and depth maps for all camera viewpoints, which significantly facilitates other research in the related fields. The code and dataset will be publicly available at https://github.com/Mercerai/PAEv3d.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611153","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611153","","Geometry;Training;Three-dimensional displays;Pipelines;Neural radiance field;Noise measurement;Image reconstruction","","2","","56","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Rank2Reward: Learning Shaped Reward Functions from Passive Video","D. Yang; D. Tjia; J. Berg; D. Damen; P. Agrawal; A. Gupta","Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Computer Science, University of Washington, Seattle, WA, USA; Department of Computer Science, University of Washington, Seattle, WA, USA; School of Computer Science, University of Bristol, Bristol, UK; Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Computer Science, University of Washington, Seattle, WA, USA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","2806","2813","Teaching robots novel skills with demonstrations via human-in-the-loop data collection techniques like kinesthetic teaching or teleoperation puts a heavy burden on human supervisors. In contrast to this paradigm, it is often significantly easier to provide raw, action-free visual data of tasks being performed. Moreover, this data can even be mined from video datasets or the web. Ideally, this data can serve to guide robot learning for new tasks in novel environments, informing both ""what"" to do and ""how"" to do it. A powerful way to encode both the ""what"" and the ""how"" is to infer a well-shaped reward function for reinforcement learning. The challenge is determining how to ground visual demonstration inputs into a well-shaped and informative reward function. We propose a technique Rank2Reward for learning behaviors from videos of tasks being performed without access to any low-level states and actions. We do so by leveraging the videos to learn a reward function that measures incremental ""progress"" through a task by learning how to temporally rank the video frames in a demonstration. By inferring an appropriate ranking, the reward function is able to guide reinforcement learning by indicating when task progress is being made. This ranking function can be integrated into an adversarial imitation learning scheme resulting in an algorithm that can learn behaviors without exploiting the learned reward function. We demonstrate the effectiveness of Rank2Reward at learning behaviors from raw video on a number of tabletop manipulation tasks in both simulations and on a real-world robotic arm. We also demonstrate how Rank2Reward can be easily extended to be applicable to web-scale video datasets. Code and videos are available at https://rank2reward.github.io","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610873","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610873","","Visualization;Codes;Imitation learning;Education;Reinforcement learning;Data collection;Manipulators","","","","67","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"A Novel, Efficient and Accurate Method for Lidar Camera Calibration","Z. Huang; X. Zhang; A. Garcia; X. Huang","Worcester Polytechnic Institute, Worcester, MA, USA; Worcester Polytechnic Institute, Worcester, MA, USA; Worcester Polytechnic Institute, Worcester, MA, USA; Worcester Polytechnic Institute, Worcester, MA, USA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","14513","14519","As autonomous systems evolve, the precise calibration of lidar and camera sensors remains a pivotal concern. Among the myriad of available techniques, target-based calibration methods, which employ planar boards with distinct geometry and image patterns, have been a popular choice. These methods simplify the task of extracting corresponding features between the image and lidar point cloud. But many of these approaches also face a significant challenge, which is their sensitivity to lidar resolution and Field of View (FOV), which may degrade the reliability of the calibration results. Therefore, our research introduces a novel calibration method using a uniquely designed acrylic checkerboard which allows the lidar beam to pass through the white grids and reflect back from the black grids. This innovative technique sidesteps the common challenges associated with lidar feature extraction. Our method’s distinct advantage lies in its ability to perform accurate calibrations at close distances, owing to the efficient feature extraction from both lidar and camera sensors. This novel, efficient, and accurate method can provide state-of-the-art results for camera lidar calibration in the field. Please also check our Github repository: https://github.com/WPI-APA-Lab/Acrylic-Board-Lidar-Camera-Calibration","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611162","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611162","","Laser radar;Accuracy;Three-dimensional displays;Feature extraction;Cameras;Calibration;Sensors","","1","","32","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Learning to Catch Reactive Objects with a Behavior Predictor","K. Lu; J. -X. Zhong; B. Yang; B. Wang; A. Markham","Department of Computer Science, University of Oxford, Oxford, UK; Department of Computer Science, University of Oxford, Oxford, UK; Department of Computing, vLAR Group; Department of Aeronautical and Aviation Engineering, Hong Kong Polytechnic University, HKSAR; Department of Computer Science, University of Oxford, Oxford, UK",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","9205","9211","Tracking and catching moving objects is an important ability for robots in a dynamic world. Whilst some objects have highly predictable state evolution e.g., the ballistic trajectory of a tennis ball, reactive targets alter their behavior in response to motion of the manipulator. Reactive applications range from gently capturing living animals such as snakes or fish for biological investigations, to smoothly interacting with and assisting a person. Existing works for dynamic catching usually perform target prediction followed by planning, but seldom account for highly non-linear reactive behaviors. Alternatively, Reinforcement Learning (RL) based methods simply treat the target and its motion as part of the observation of the world-state, but perform poorly due to the weak reward signal. In this work, we blend the approach of an explicit, yet learned, target state predictor with RL. We further show how a tightly coupled predictor which ‘observes’ the state of the robot leads to significantly improved anticipatory action, especially with targets that seek to evade the robot following a simple policy. Experiments show that our method achieves an 86.4% (open plane area) and a 73.8% (room) success rate on evasive objects, outperforming monolithic reinforcement learning and other techniques. We also demonstrate the efficacy of our approach across varied targets and trajectories. All code, data, and additional videos are at this GitHub link: https://kl-research.github.io/dyncatch.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611106","","Target tracking;Sports equipment;Dynamics;Reinforcement learning;Trajectory;Planning;Task analysis","","1","","42","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Learning Force Control for Legged Manipulation","T. Portela; G. B. Margolis; Y. Ji; P. Agrawal",Improbable AI Lab; Improbable AI Lab; Improbable AI Lab; Improbable AI Lab,2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","15366","15372","Controlling the contact force during interactions is an inherent requirement for locomotion and manipulation tasks. Current reinforcement learning approaches to locomotion and manipulation rely implicitly on forceful interaction to accomplish tasks but do not explicitly regulate it. This paper proposes a reinforcement learning task specification that focuses on matching desired contact force levels. Integrating force control with the coordination of a robot’s body and arm, we present an end-to-end policy for legged manipulator control. Force control enables us to realize compliant gripper and whole-body pulling movements that have not been previously demonstrated using a learned policy. It also facilitates a characterization of the force-tracking performance of learned policies in simulation and the real world, indicating their performance potential for force-critical tasks. Video is available at the project website: https://tif-twirl-13.github.io/learning-compliance.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611066","National Science Foundation; United States Air Force; Arm; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611066","","Legged locomotion;Robot kinematics;Force;Reinforcement learning;Manipulators;Task analysis;Force control","","2","","40","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"LiDAR-CS Dataset: LiDAR Point Cloud Dataset with Cross-Sensors for 3D Object Detection","J. Fang; D. Zhou; J. Zhao; C. Wu; C. Tang; C. -Z. Xu; L. Zhang","State Key Lab of IOTSC, CIS, University of Macau; Robotics and Autonomous Driving Laboratory, Baidu Research; Robotics and Autonomous Driving Laboratory, Baidu Research; Robotics and Autonomous Driving Laboratory, Baidu Research; University of California, Irvine; State Key Lab of IOTSC, CIS, University of Macau; Robotics and Autonomous Driving Laboratory, Baidu Research",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","14822","14829","Over the past few years, there has been remarkable progress in research on 3D point clouds and their use in autonomous driving scenarios has become widespread. However, deep learning methods heavily rely on annotated data and often face domain generalization issues. Unlike 2D images whose domains usually pertain to the texture information present in them, the features derived from a 3D point cloud are affected by the distribution of the points. The lack of a 3D domain adaptation benchmark leads to the common practice of training a model on one benchmark (e.g. Waymo) and then assessing it on another dataset (e.g. KITTI). This setting results in two distinct domain gaps: scenarios and sensors, making it difficult to analyze and evaluate the method accurately. To tackle this problem, this paper presents ${\color{Red}\text{LiDAR}}$ Dataset with ${\color{Red}\text{C}}{\text{ross}} - {\color{Red}\text{S}}{\text{ensors}}$ (LiDAR-CS Dataset), which contains large-scale annotated LiDAR point cloud under six groups of different sensors but with the same corresponding scenarios, captured from hybrid realistic LiDAR simulator. To our knowledge, LiDAR-CS Dataset is the first dataset that addresses the sensor-related gaps in the domain of 3D object detection in real traffic. Furthermore, we evaluate and analyze the performance using various baseline detectors and demonstrated its potential applications. Project page: https://opendriving.github.io/lidar-cs.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611136","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611136","","Point cloud compression;Training;Solid modeling;Three-dimensional displays;Laser radar;Object detection;Detectors","","2","","50","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"DynaInsRemover: A Real-time Dynamic Instance-Aware Static 3D LiDAR Mapping Framework for Dynamic Environment","H. Zhao; M. Yao; X. Xiao; B. Zheng","Intelligent Robotics Lab (IRL), School of Artificial Intelligence, Jilin University, Changchun, China; Intelligent Robotics Lab (IRL), School of Artificial Intelligence, Jilin University, Changchun, China; CVIR Lab, Changchun University of Science and Technology, Changchun, China; Shanghai Aerospace Control Technology Institute, Shanghai, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","1803","1809","Dynamic objects diversify the distribution of point cloud in the map, degrading the performance of the robotic downstream tasks. To address this problem, we present a novel real-time dynamic instance-aware static mapping framework called DynaInsRemover, which exploits the geometric discrepancies between instances to efficiently remove dynamic objects and preserve more details of static map. It contains the Instance Occupancy Check module for initial dynamic instance proposal and the Instance Belief Update module for reverting false positives. We quantitatively evaluate our approach performance on the SemanticKITTI dataset and validate it in a real-world environment. Experimental evaluations show that our method achieves very promising results in dynamic environments. The implementation of our method is available as open source at: https://github.com/Zhaohuanfeng/DynaInsRemover.git.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610211","National Natural Science Foundation of China; Natural Science Foundation of Jilin Province; Natural Science Foundation of Shanghai; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610211","","Point cloud compression;Three-dimensional displays;Laser radar;Real-time systems;Windows;Proposals;Object recognition","","","","40","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Adaptive Outlier Thresholding for Bundle Adjustment in Visual SLAM","A. Fontan; J. Civera; M. Milford","QUT Centre for Robotics, Queensland University of Technology, Brisbane, Australia; School of Engineering, University of Zaragoza, Zaragoza, Spain; QUT Centre for Robotics, Queensland University of Technology, Brisbane, Australia",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","3969","3976","State-of-the-art V-SLAM pipelines utilize robust cost functions and outlier rejection techniques to remove incorrect correspondences. However, these methods are typically fine-tuned to overfit certain benchmarks and struggle to adapt effectively to changes in the application domain or environmental conditions. This renders them impractical for many robotic applications in which robustness in a wide variety of conditions is essential. In this paper we introduce a novel distribution-based approach for online outlier rejection that reduces the necessity for scene-specific fine-tuning while simultaneously improving the overall SLAM performance. Through experiments across 3 different public datasets, we show that our approach consistently outperforms state-of-the-art methods in various real-world settings. Our code is available at https://github.com/alejandrofontan/ORB_SLAM2_Distribution","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610433","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610433","","Bundle adjustment;Training;Gamma distribution;Visualization;Simultaneous localization and mapping;Pipelines;Training data","","","","49","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Fast and Robust Point Cloud Registration with Tree-based Transformer","G. Chen; M. Wang; Y. Yang; L. Yuan; Y. Yue","School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Electrical and Computer Engineering, Peking University & Pecheng Lab, Shenzhen, China; School of Automation, Beijing Institute of Technology, Beijing, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","773","780","Point cloud registration is essential in computer vision and robotics. Recently, transformer-based methods have achieved advanced point cloud registration performance. However, the standard attention mechanism utilized in these methods considers many low-relevance points, and it has difficulty focusing its attention weights on sparse and meaningful points, leading to limited local structure modeling capabilities and quadratic computational complexity. To address these limitations, we present the Tree-based Transformer (TrT), which is able to extract abundant local and global features with linear computational complexity. Specifically, the TrT builds coarse-to-dense feature trees, and a novel Tree-based Attention (TrA) is proposed to guide the progressive convergence of the attended regions toward meaningful points and to structurize point clouds following tree structures. In each layer, the top ${\mathcal{S}}$ key points with the highest attention scores are selected, such that in the next layer, attention is evaluated only within the specified high-relevance regions, corresponding to the child points of these selected ${\mathcal{S}}$ points. Additionally, coarse features containing high-level semantic information are incorporated into the child points to guide the feature extraction process, facilitating local structure modeling and multiscale information integration. Consequently, TrA enables the model to focus on critical local structures and extract rich local information with linear computational complexity. Experiments demonstrate that our method achieves state-of-the-art performance on 3DMatch and KITTI benchmarks. The code for our method is publicly available at https://github.com/CGuangyan-BIT/TrT.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610004","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610004","","Point cloud compression;Computational modeling;Semantics;Focusing;Benchmark testing;Feature extraction;Transformers","","","","43","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"PeLiCal: Targetless Extrinsic Calibration via Penetrating Lines for RGB-D Cameras with Limited Co-visibility","J. Shin; S. Yun; A. Kim","Dept. of Mechanical Engineering, SNU, Seoul, S. Korea; Dept. of Mechanical Engineering, SNU, Seoul, S. Korea; Dept. of Mechanical Engineering, SNU, Seoul, S. Korea",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","14506","14512","RGB-D cameras are crucial in robotic perception, given their ability to produce images augmented with depth data. However, their limited field of view (FOV) often requires multiple cameras to cover a broader area. In multi-camera RGB-D setups, the goal is typically to reduce camera overlap, optimizing spatial coverage with as few cameras as possible. The extrinsic calibration of these systems introduces additional complexities. Existing methods for extrinsic calibration either necessitate specific tools or highly depend on the accuracy of camera motion estimation. To address these issues, we present PeLiCal, a novel line-based calibration approach for RGB-D camera systems exhibiting limited overlap. Our method leverages long line features from surroundings, and filters out outliers with a novel convergence voting algorithm, achieving targetless, real-time, and outlier-robust performance compared to existing methods. We open source our implementation on https://github.com/joomeok/PeLiCal.git.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611415","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611415","","Three-dimensional displays;Robot kinematics;Motion estimation;Robot vision systems;Pose estimation;Merging;Cameras","","","","20","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"CNS: Correspondence Encoded Neural Image Servo Policy","A. Chen; H. Yu; Y. Wang; R. Xiong","Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","17410","17416","Image servo is an indispensable technique in robotic applications that helps to achieve high precision positioning. The intermediate representation of image servo policy is important to sensor input abstraction and policy output guidance. Classical approaches achieve high precision but require clean keypoint correspondence, and suffer from limited convergence basin or weak feature error robustness. Recent learning-based methods achieve moderate precision and large convergence basin on specific scenes but face issues when generalizing to novel environments. In this paper, we encode keypoints and correspondence into a graph and use graph neural network as architecture of controller. This design utilizes both advantages: generalizable intermediate representation from keypoint correspondence and strong modeling ability from neural network. Other techniques including realistic data generation, feature clustering and distance decoupling are proposed to further improve efficiency, precision and generalization. Experiments in simulation and real-world verify the effectiveness of our method in speed (maximum 40fps along with observer), precision (<0.3° and sub-millimeter accuracy) and generalization (sim-to-real without fine-tuning). Project homepage (full paper with supplementary text, video and code): https://hhcaz.github.io/CNS-home.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611185","Nature; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611185","","Learning systems;Training;Three-dimensional displays;Robot sensing systems;Graph neural networks;Visual servoing;Robustness","","","","33","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"RGB-based Category-level Object Pose Estimation via Decoupled Metric Scale Recovery","J. Wei; X. Song; W. Liu; L. Kneip; H. Li; P. Ji","Smart Robotics Lab, CIT, Technical University of Munich; Tencent; Tencent; Mobile Perception Lab, SIST, ShanghaiTech University; Australian National University; Tencent",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","2036","2042","While showing promising results, recent RGB-D camera-based category-level object pose estimation methods have restricted applications due to the heavy reliance on depth sensors. RGB-only methods provide an alternative to this problem yet suffer from inherent scale ambiguity stemming from monocular observations. In this paper, we propose a novel pipeline that decouples the 6D pose and size estimation to mitigate the influence of imperfect scales on rigid transformations. Specifically, we leverage a pre-trained monocular estimator to extract local geometric information, mainly facilitating the search for inlier 2D-3D correspondence. Meanwhile, a separate branch is designed to directly recover the metric scale of the object based on category-level statistics. Finally, we advocate using the RANSAC-PnP algorithm to robustly solve for 6D object pose. Extensive experiments have been conducted on both synthetic and real datasets, demonstrating the superior performance of our method over previous state-of-the-art RGB-based approaches, especially in terms of rotation accuracy. Code: https://github.com/goldoak/DMSR.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611723","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611723","","Measurement;Codes;Accuracy;Pose estimation;Pipelines;Sensors;Reliability","","1","","31","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"iRoCo: Intuitive Robot Control From Anywhere Using a Smartwatch","F. C. Weigend; X. Liu; S. Sonawani; N. Kumar; V. Vasudevan; H. Ben Amor","SCAI, Arizona State University; SCAI, Arizona State University; SCAI, Arizona State University; Corporate Functions-R&D, Procter and Gamble; Corporate Functions-R&D, Procter and Gamble; SCAI, Arizona State University",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","17800","17806","This paper introduces iRoCo (intuitive Robot Control) – a framework for ubiquitous human-robot collaboration using a single smartwatch and smartphone. By integrating probabilistic differentiable filters, iRoCo optimizes a combination of precise robot control and unrestricted user movement from ubiquitous devices. We demonstrate and evaluate the effectiveness of iRoCo in practical teleoperation and drone piloting applications. Comparative analysis shows no significant difference between task performance with iRoCo and gold-standard control systems in teleoperation tasks. Additionally, iRoCo users complete drone piloting tasks 32% faster than with a traditional remote control and report less frustration in a subjective load index questionnaire. Our findings strongly suggest that iRoCo is a promising new approach for intuitive robot control through smartwatches and smart-phones from anywhere, at any time. The code is available at www.github.com/wearable-motion-capture","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610805","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610805","","Performance evaluation;Wearable Health Monitoring Systems;Robot control;Collaboration;Probabilistic logic;Motion capture;Kalman filters","","","","30","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Occluded Part-aware Graph Convolutional Networks for Skeleton-based Action Recognition","M. H. Kim; M. Ju Kim; S. B. Yoo","Department of Artificial Intelligence Convergence, Chonnam National University, Gwangju, South Korea; Department of Artificial Intelligence Convergence, Chonnam National University, Gwangju, South Korea; Department of Artificial Intelligence Convergence, Chonnam National University, Gwangju, South Korea",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","7310","7317","Recognizing human action is one of the most critical factors in the visual perception of robots. Specifically, skeletonbased action recognition has been actively researched to enhance recognition performance at a lower cost. However, action recognition in occlusion situations, where body parts are not visible, is still challenging.We propose an occluded part-aware graph convolutional network (OP-GCN) to address this challenge using the optimal occluded body parts. The proposed model uses an occluded part detector to identify occluded body parts within a human skeleton. It is based on an autoencoder trained on a nonoccluded human skeleton and exploits the symmetry and angular information of the skeleton. Then, we select an optimal group constructed considering the occluded body parts. Each group comprises five sets of joint nodes, focusing on the body parts, excluding the occluded ones. Finally, to enhance interaction within the selected groups, we apply an interpart association module, considering the fusion of global and local elements. The experimental results reveal that the proposed model outperforms others on the occluded datasets. These comparative experiments demonstrate the effectiveness of the study in addressing the challenge of action recognition in occlusion situations. Our code is publicly available at https://github.com/MJ-Kor/OP-GCN.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610972","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610972","","Costs;Codes;Graph convolutional networks;Human-robot interaction;Focusing;Detectors;Skeleton","","2","","57","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"FLTRNN: Faithful Long-Horizon Task Planning for Robotics with Large Language Models","J. Zhang; L. Tang; Y. Song; Q. Meng; H. Qian; J. Shao; W. Song; S. Zhu; J. Gu","Zhejiang University; University of Chinese Academy of Sciences; Zhejiang University; Research Center for Intelligent Robotics, Zhejiang Lab; Zhejiang University; Zhejiang University; Research Center for Intelligent Robotics, Zhejiang Lab; Zhejiang University; Research Center for Intelligent Robotics, Zhejiang Lab",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","6680","6686","Recent planning methods based on Large Language Models typically employ the In-Context Learning paradigm. Complex long-horizon planning tasks require more context(including instructions and demonstrations) to guarantee that the generated plan can be executed correctly. However, in such conditions, LLMs may overlook(unfaithful) the rules in the given context, resulting in the generated plans being invalid or even leading to dangerous actions. In this paper, we investigate the faithfulness of LLMs for complex long-horizon tasks. Inspired by human intelligence, we introduce a novel framework named FLTRNN. FLTRNN employs a language-based RNN structure to integrate task decomposition and memory management into LLM planning inference, which could effectively improve the faithfulness of LLMs and make the planner more reliable. We conducted experiments in VirtualHome household tasks. Results show that our model significantly improves faithfulness and success rates for complex long-horizon tasks. Website at https://tannl.github.io/FLTRNN.github.io/","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611663","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611663","","Knowledge engineering;Large language models;Human intelligence;Memory management;Cognition;Planning;Reliability","","","","33","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"GAMMA: Graspability-Aware Mobile MAnipulation Policy Learning based on Online Grasping Pose Fusion","J. Zhang; N. Gireesh; J. Wang; X. Fang; C. Xu; W. Chen; L. Dai; H. Wang","CFCS, School of Computer Science, Peking University; Galbot; Beijing Academy of Artificial Intelligence; Beijing Academy of Artificial Intelligence; Galbot; Beijing Academy of Artificial Intelligence; Tongji University; CFCS, School of Computer Science, Peking University",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","1399","1405","Mobile manipulation constitutes a fundamental task for robotic assistants and garners significant attention within the robotics community. A critical challenge inherent in mobile manipulation is the effective observation of the target while approaching it for grasping. In this work, we propose a graspability-aware mobile manipulation approach powered by an online grasping pose fusion framework that enables a temporally consistent grasping observation. Specifically, the predicted grasping poses are online organized to eliminate the redundant, outlier grasping poses, which can be encoded as a grasping pose observation state for reinforcement learning. Moreover, on-the-fly fusing the grasping poses enables a direct assessment of graspability, encompassing both the quantity and quality of grasping poses. This assessment can subsequently serve as an observe-to-grasp reward, motivating the agent to prioritize actions that yield detailed observations while approaching the target object for grasping. Through extensive experiments conducted on the Habitat and Isaac Gym simulators, we find that our method attains a good balance between observation and manipulation, yielding high performance under various grasping metrics. Furthermore, we discover that the incorporation of temporal information from grasping poses aids in mitigating the sim-to-real gap, leading to robust performance in challenging real-world experiments. Project page: https://pku-epic.github.io/GAMMA/","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610125","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610125","","Measurement;Accuracy;Fuses;Habitats;Grasping;Reinforcement learning;Robustness","","1","","37","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Chasing Day and Night: Towards Robust and Efficient All-Day Object Detection Guided by an Event Camera","J. Cao; X. Zheng; Y. Lyu; J. Wang; R. Xu; L. Wang","MICS Thrust, HKUST(GZ); AI Thrust, HKUST(GZ); AI Thrust, HKUST(GZ); MICS Thrust, HKUST(GZ); MICS Thrust, HKUST(GZ), Guangzhou, China; Dept. of CSE, HKUST, AI Thrust, HKUST(GZ), Guangzhou, Hong Kong SAR, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","9026","9032","The ability to detect objects in all lighting (i.e., normal-, over-, and under-exposed) conditions is crucial for real-world applications, such as self-driving. Traditional RGB-based detectors often fail under such varying lighting conditions. Therefore, recent works utilize novel event cameras to supplement or guide the RGB modality; however, these methods typically adopt asymmetric network structures that rely predominantly on the RGB modality, resulting in limited robustness for all-day detection. In this paper, we propose EOLO, a novel object detection framework that achieves robust and efficient all-day detection by fusing both RGB and event modalities. Our EOLO framework is built based on a lightweight spiking neural network (SNN) to efficiently leverage the asynchronous property of events. Buttressed by it, we first introduce an Event Temporal Attention (ETA) module to learn the high temporal information from events while preserving crucial edge information. Secondly, as different modalities exhibit varying levels of importance under diverse lighting conditions, we propose a novel Symmetric RGB-Event Fusion (SREF) module to effectively fuse RGB-Event features without relying on a specific modality, thus ensuring a balanced and adaptive fusion for all-day detection. In addition, to compensate for the lack of paired RGB-Event datasets for all-day training and evaluation, we propose an event synthesis approach based on the randomized optical flow that allows for directly generating the event frame from a single exposure image. We further build two new datasets, E-MSCOCO and E-VOC based on the popular benchmarks MSCOCO and PASCAL VOC. Extensive experiments demonstrate that our EOLO outperforms the state-of-the-art detectors, e.g., RENet [1], by a substantial margin (+3.74% mAP50) in all lighting conditions. Our code and datasets will be available at https://vlislab22.github.io/EOLO/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611705","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611705","","Training;Lighting;Object detection;Spiking neural networks;Detectors;Feature extraction;Cameras","","2","","40","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Keypoints-guided Lightweight Network for Single-view 3D Human Reconstruction","Y. Chen; C. Wang","School of Automation, Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, Southeast University, China; School of Automation, Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, Southeast University, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","13070","13076","Single-view 3D human reconstruction has been a hot topic due to the potential of wide applications. To achieve high accuracy, existing works usually take computationally intensive models as backbone for exhaustive underlying features and then directly estimate human mesh vertices. These factors lead to redundant parameters, large calculations and low efficiency, while lightweight solutions to address these challenges are relatively scarce. In this work, based on the problems studied above, we propose a keypoints-guided lightweight network with an encoding-decoding framework. As the input is an image, a lightweight backbone named multi-stage and global feature enhanced network is designed for 2D encoding, where some operations of multi-scale fusion and frequency domain filtering are performed to extract more informative but low-resolution features. As the output is mesh of human body, we construct a keypoints-based 3D human template, with which the 2D low-resolution features can be mapped to 3D space to guide the 3D decoding with high efficiency and high accuracy. Extensive experiments on popular benchmarks 3DPW and Human3.6M illustrate the favorable trade-off between the accuracy and complexity of our method. Our code is publicly available at https://github.com/ChrisChenYh/EfficientHuman.git.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610020","National Natural Science Foundation of China; State Grid Jiangsu Electric Power; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610020","","Three-dimensional displays;Accuracy;Image coding;Frequency-domain analysis;Estimation;Benchmark testing;Feature extraction","","","","37","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"SRFNet: Monocular Depth Estimation with Fine-grained Structure via Spatial Reliability-oriented Fusion of Frames and Events","T. Pan; Z. Cao; L. Wang","AI Thrust, HKUST(GZ), Guangzhou, China; AI Thrust, HKUST(GZ), Guangzhou, China; Dept. of CSE, HKUST, AI Thrust, HKUST(GZ), China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","10695","10702","Monocular depth estimation is a crucial task to measure distance relative to a camera, which is important for applications, such as robot navigation and self-driving. Traditional frame-based methods suffer from performance drops due to the limited dynamic range and motion blur. Therefore, recent works leverage novel event cameras to complement or guide the frame modality via frame-event feature fusion. However, event streams exhibit spatial sparsity, leaving some areas unperceived, especially in regions with marginal light changes. Therefore, direct fusion methods, e.g., RAMNet [6], often ignore the contribution of the most confident regions of each modality. This leads to structural ambiguity in the modality fusion process, thus degrading the depth estimation performance. In this paper, we propose a novel Spatial Reliability-oriented Fusion Network (SRFNet), that can estimate depth with fine-grained structure at both daytime and nighttime. Our method consists of two key technical components. Firstly, we propose an attention-based interactive fusion (AIF) module that applies spatial priors of events and frames as the initial masks and learns the consensus regions to guide the inter-modal feature fusion. The fused feature are then fed back to enhance the frame and event feature learning. Meanwhile, it utilizes an output head to generate a fused mask, which is iteratively updated for learning consensual spatial priors. Secondly, we propose the Reliability-oriented Depth Refinement (RDR) module to estimate dense depth with the fine-grained structure based on the fused features and masks. We evaluate the effectiveness of our method on the synthetic and real-world datasets, which shows that, even without pretraining, our method outperforms the prior methods, e.g., RAMNet [6], especially in night scenes. Our project homepage: https://vlislab22.github.io/SRFNet.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610921","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610921","","Representation learning;Robot vision systems;Estimation;Sensor fusion;Cameras;Sensors;Reliability","","2","","42","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"HERO-SLAM: Hybrid Enhanced Robust Optimization of Neural SLAM","Z. Xin; Y. Yue; L. Zhang; C. Wu","Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; Robotics and Autonomous Driving Lab (RAL), Baidu Research; Robotics and Autonomous Driving Lab (RAL), Baidu Research",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","8610","8616","Simultaneous Localization and Mapping (SLAM) is a fundamental task in robotics, driving numerous applications such as autonomous driving and virtual reality. Recent progress on neural implicit SLAM has shown encouraging and impressive results. However, the robustness of neural SLAM, particularly in challenging or data-limited situations, remains an unresolved issue. This paper presents HERO-SLAM, a Hybrid Enhanced Robust Optimization method for neural SLAM, which combines the benefits of neural implicit field and feature-metric optimization. This hybrid method optimizes a multi-resolution implicit field and enhances robustness in challenging environments with sudden viewpoint changes or sparse data collection. Our comprehensive experimental results on benchmarking datasets validate the effectiveness of our hybrid approach, demonstrating its superior performance over existing implicit field-based methods in challenging scenarios. HERO-SLAM provides a new pathway to enhance the stability, performance, and applicability of neural SLAM in real-world scenarios. Project page: https://hero-slam.github.io.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610000","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610000","","Simultaneous localization and mapping;Optimization methods;Virtual reality;Data collection;Benchmark testing;Robustness;Task analysis","","1","","40","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Specifying and Monitoring Safe Driving Properties with Scene Graphs","F. Toledo; T. Woodlief; S. Elbaum; M. B. Dwyer","University of Virginia, USA; University of Virginia, USA; University of Virginia, USA; University of Virginia, USA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","15577","15584","With the proliferation of autonomous vehicles (AVs) comes the need to ensure they abide by safe driving properties. Specifying and monitoring such properties, however, is challenging because of the mismatch between the semantic space over which typical driving properties are asserted (e.g., vehicles, pedestrians, intersections) and the sensed inputs of AVs. Existing efforts either assume for such semantic data to be available or develop bespoke methods for capturing it. Instead, this work introduces a framework that can extract scene graphs (SGs) from sensor inputs to capture the entities related to the AV, and a domain-specific language that enables building propositions over those graphs and composing them through temporal logic. We implemented the framework to monitor for specification violations of 3 top AVs from the CARLA Autonomous Driving Leaderboard, and found that the AVs violated 71% of properties during at least one test. Artifact available at https://github.com/less-lab-uva/SGSM.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610973","U.S. Army; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610973","","Space vehicles;Runtime;Pedestrians;Semantics;Robot sensing systems;Logic;Robotics and automation","","","","63","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"RoboVQA: Multimodal Long-Horizon Reasoning for Robotics","P. Sermanet; T. Ding; J. Zhao; F. Xia; D. Dwibedi; K. Gopalakrishnan; C. Chan; G. Dulac-Arnold; S. Maddineni; N. J. Joshi; P. Florence; W. Han; R. Baruch; Y. Lu; S. Mirchandani; P. Xu; P. Sanketi; K. Hausman; I. Shafran; B. Ichter; Y. Cao",Google DeepMind; Google DeepMind; Google DeepMind; Google DeepMind; Google DeepMind; Google DeepMind; Google DeepMind; Google DeepMind; Google DeepMind; Google DeepMind; Google DeepMind; Google DeepMind; Google DeepMind; Google DeepMind; Google DeepMind; Google DeepMind; Google DeepMind; Google DeepMind; Google DeepMind; Google DeepMind; Google DeepMind,2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","645","652","We present a scalable, bottom-up and intrinsically diverse data collection scheme that can be used for high-level reasoning with long and medium horizons and that has 2.2x higher throughput compared to traditional narrow top-down step-by-step collection. We collect realistic data by performing any user requests within the entirety of 3 office buildings and using multiple embodiments (robot, human, human with grasping tool). With this data, we show that models trained on all embodiments perform better than ones trained on the robot data only, even when evaluated solely on robot episodes. We explore the economics of collection costs and find that for a fixed budget it is beneficial to take advantage of the cheaper human collection along with robot collection. We release a large and highly diverse (29,520 unique instructions) dataset dubbed RoboVQA containing 829,502 (video, text) pairs for robotics-focused visual question answering. We also demonstrate how evaluating real robot experiments with an intervention mechanism enables performing tasks to completion, making it deployable with human oversight even if imperfect while also providing a single performance metric. We demonstrate a single video-conditioned model named RoboVQA-VideoCoCa trained on our dataset that is capable of performing a variety of grounded high-level reasoning tasks in broad realistic settings with a cognitive intervention rate 46% lower than the zeroshot state of the art visual language model (VLM) baseline and is able to guide real robots through long-horizon tasks. The performance gap with zero-shot state-of-the-art models indicates that a lot of grounded data remains to be collected for real-world deployment, emphasizing the critical need for scalable data collection approaches. Finally, we show that video VLMs significantly outperform single-image VLMs with an average error rate reduction of 19% across all VQA tasks. Thanks to video conditioning and dataset diversity, the model can be used as general video value functions (e.g. success and affordance) in situations where actions needs to be recognized rather than states, expanding capabilities and environment understanding for robots. Data and videos are available at robovqa.github.io","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610216","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610216","","Measurement;Visualization;Biological system modeling;Data collection;Throughput;Cognition;Data models","","1","","45","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"InteRACT: Transformer Models for Human Intent Prediction Conditioned on Robot Actions","K. Kedia; A. Bhardwaj; P. Dan; S. Choudhury","Department of Computer Science, Cornell University; Department of Computer Science, Cornell University; Department of Computer Science, Cornell University; Department of Computer Science, Cornell University",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","621","628","In collaborative human-robot manipulation, a robot must predict human intents and adapt its actions accordingly to smoothly execute tasks. However, the human’s intent in turn depends on actions the robot takes, creating a chicken-or-egg problem. Prior methods ignore such inter-dependency and instead train marginal intent prediction models independent of robot actions. This is because training conditional models is hard given a lack of paired human-robot interaction datasets.Can we instead leverage large-scale human-human interaction data that is more easily accessible? Our key insight is to exploit a correspondence between human and robot actions that enables transfer learning from human-human to human-robot data. We propose a novel architecture, InteRACT, that pre-trains a conditional intent prediction model on large human-human datasets and fine-tunes on a small human-robot dataset. We evaluate on a set of real-world collaborative human-robot manipulation tasks and show that our conditional model improves over various marginal baselines. We also introduce new techniques to tele-operate a 7-DoF robot arm and collect a diverse range of human-robot collaborative manipulation data which we open-source. We release our code and datasets at https://portal-cornell.github.io/InteRACT/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610681","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610681","","Training;Adaptation models;Transfer learning;Collaboration;Human-robot interaction;Predictive models;Transformers","","","","51","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Learning Multi-Scale Context Mask-RCNN Network for Slant Angled Aerial Imagery in Instance Segmentation in a Sim2Real setup","Q. Saadiyean; S. P. Samprithi; S. Sundaram","Department of Aerospace Engineering, Indian Institute of Science; Department of Electronics and Communication Engineering, PES University; Department of Aerospace Engineering, Indian Institute of Science",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","13573","13580","While instance segmentation models excel at object detection in satellite imagery, their performance drops when applied to slant-angled aerial images due to occlusion and scale variation. This is mainly caused by a lack of training data for such diverse viewpoints and scales. To address this limitation, we propose the Sim2Real-based Multi-Scale Context Mask-RCNN (MSC-RCNN) network, specifically designed for slant-angled aerial imagery. Sim2Real-based transfer learning is adapted to compensate for the limited availability of real-world slant-angle training data. A synthetic dataset is generated using Unreal Engine, detailing the methodology of replicating the real-world scene, for producing diverse slant-angle drone datasets with various weather conditions and backgrounds. The model leverages two distinct feature pyramid backbones, with one incorporating dilated convolutions to address large-scale objects and the other optimized for regular convolutions. Their outputs are fused to effectively detect objects across various scales and angles. Through experiments, it was demonstrated that incorporating this synthetic data significantly reduces reliance on real data while maintaining high mean Average Precision (mAP) scores. Compared to the baseline Mask R-CNN, the proposed approach with Sim2Real adaptation and the MSC-RCNN architecture achieves a remarkable 7.6% performance improvement in instance segmentation accuracy with only a 6% increase in model size. Code can be found at: https://github.com/MSC-RCNN","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610358","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610358","","Instance segmentation;Convolutional codes;Adaptation models;Accuracy;Surveillance;Transfer learning;Training data","","1","","42","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Offline Goal-Conditioned Reinforcement Learning for Safety-Critical Tasks with Recovery Policy","C. Cao; Z. Yan; R. Lu; J. Tan; X. Wang","Center for Intelligent Control and Telescience, Tsinghua Shenzhen International Graduate School, Shenzhen, China; Center for Intelligent Control and Telescience, Tsinghua Shenzhen International Graduate School, Shenzhen, China; Center for Intelligent Control and Telescience, Tsinghua Shenzhen International Graduate School, Shenzhen, China; Center for Intelligent Control and Telescience, Tsinghua Shenzhen International Graduate School, Shenzhen, China; Center for Intelligent Control and Telescience, Tsinghua Shenzhen International Graduate School, Shenzhen, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","2838","2844","Offline goal-conditioned reinforcement learning (GCRL) aims at solving goal-reaching tasks with sparse rewards from an offline dataset. While prior work has demonstrated various approaches for agents to learn near-optimal policies, these methods encounter limitations when dealing with diverse constraints in complex environments, such as safety constraints. Some of these approaches prioritize goal attainment without considering safety, while others excessively focus on safety at the expense of training efficiency. In this paper, we study the problem of constrained offline GCRL and propose a new method called Recovery-based Supervised Learning (RbSL) to accomplish safety-critical tasks with various goals. To evaluate the method performance, we build a benchmark based on the robot-fetching environment with a randomly positioned obstacle and use expert or random policies to generate an offline dataset. We compare RbSL with three offline GCRL algorithms and one offline safe RL algorithm. As a result, our method outperforms the existing state-of-the-art methods to a large extent. Furthermore, we validate the practicality and effectiveness of RbSL by deploying it on a real Panda manipulator. Code is available at https://github.com/Sunlighted/RbSL.git.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610856","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610856","","Training;Costs;Codes;Supervised learning;Reinforcement learning;Benchmark testing;Manipulators","","","","43","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Ada-Tracker: Soft Tissue Tracking via Inter-Frame and Adaptive-template Matching","J. Guo; J. Wang; Z. Li; T. Jia; Q. Dou; Y. -H. Liu","CUHK T Stone Robotics Institute, The Chinese University of Hong Kong, Hong Kong; CUHK T Stone Robotics Institute, The Chinese University of Hong Kong, Hong Kong; Johns Hopkins University, United States; Faculty of Urology, Third Medical Center, Chinese PLA General Hospital, Beijing, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; CUHK T Stone Robotics Institute, The Chinese University of Hong Kong, Hong Kong",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","15463","15470","Soft tissue tracking is crucial for computer-assisted interventions. Existing approaches mainly rely on extracting discriminative features from the template and videos to recover corresponding matches. However, it is difficult to adopt these techniques in surgical scenes, where tissues are changing in shape and appearance throughout the surgery. To address this problem, we exploit optical flow to naturally capture the pixel-wise tissue deformations and adaptively correct the tracked template. Specifically, we first implement an inter-frame matching mechanism to extract a coarse region of interest based on optical flow from consecutive frames. To accommodate appearance change and alleviate drift, we then propose an adaptive-template matching method, which updates the tracked template based on the reliability of the estimates. Our approach, Ada-Tracker, enjoys both short-term dynamics modeling by capturing local deformations and long-term dynamics modeling by introducing global temporal compensation. We evaluate our approach on the public SurgT benchmark, which is generated from Hamlyn, SCARED, and Kidney boundary datasets. The experimental results show that Ada-Tracker achieves superior accuracy and performs more robustly against prior works. Code is available at https://github.com/wrld/Ada-Tracker.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611030","","Deformable models;Adaptation models;Accuracy;Tracking;Deformation;Biological tissues;Surgery","","","","43","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"EquivAct: SIM(3)-Equivariant Visuomotor Policies beyond Rigid Object Manipulation","J. Yang; C. Deng; J. Wu; R. Antonova; L. Guibas; J. Bohg","Department of Computer Science, Stanford University; Department of Computer Science, Stanford University; Department of Computer Science, Princeton University; Department of Computer Science, Stanford University; Department of Computer Science, Stanford University; Department of Computer Science, Stanford University",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","9249","9255","If a robot masters folding a kitchen towel, we would expect it to master folding a large beach towel. However, existing policy learning methods that rely on data augmentation still don’t guarantee such generalization. Our insight is to add equivariance to both the visual object representation and policy architecture. We propose EquivAct which utilizes SIM(3)-equivariant network structures that guarantee generalization across all possible object translations, 3D rotations, and scales by construction. EquivAct is trained in two phases. We first pre-train a SIM(3)-equivariant visual representation on simulated scene point clouds. Then, we learn a SIM(3)-equivariant visuomotor policy using a small amount of source task demonstrations. We show that the learned policy directly transfers to objects that substantially differ from demonstrations in scale, position, and orientation. We evaluate our method in three manipulation tasks involving deformable and articulated objects, going beyond typical rigid object manipulation tasks considered in prior work. We conduct experiments both in simulation and in reality. For real robot experiments, our method uses 20 human demonstrations of a tabletop task and transfers zero-shot to a mobile manipulation task in a much larger setup. Experiments confirm that our contrastive pre-training procedure and equivariant architecture offer significant improvements over prior work. Project website: equivact.github.io","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611491","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611491","","Learning systems;Point cloud compression;Visualization;Three-dimensional displays;Data augmentation;Task analysis;Robots","","3","","39","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Outram: One-shot Global Localization via Triangulated Scene Graph and Global Outlier Pruning","P. Yin; H. Cao; T. -M. Nguyen; S. Yuan; S. Zhang; K. Liu; L. Xie","Centre for Advanced Robotics Technology Innovation (CARTIN), School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Centre for Advanced Robotics Technology Innovation (CARTIN), School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Centre for Advanced Robotics Technology Innovation (CARTIN), School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Centre for Advanced Robotics Technology Innovation (CARTIN), School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Department of Electronic and Computer Engineering, the Hong Kong University of Science and Technology, Hong Kong SAR, China; Centre for Advanced Robotics Technology Innovation (CARTIN), School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Centre for Advanced Robotics Technology Innovation (CARTIN), School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","13717","13723","One-shot LiDAR localization refers to the ability to estimate the robot pose from one single point cloud, which yields significant advantages in initialization and relocalization processes. In the point cloud domain, the topic has been extensively studied as a global descriptor retrieval (i.e., loop closure detection) and pose refinement (i.e., point cloud registration) problem both in isolation or combined. However, few have explicitly considered the relationship between candidate retrieval and correspondence generation in pose estimation, leaving them brittle to substructure ambiguities. To this end, we propose a hierarchical one-shot localization algorithm called Outram that leverages substructures of 3D scene graphs for locally consistent correspondence searching and global substructure-wise outlier pruning. Such a hierarchical process couples the feature retrieval and the correspondence extraction to resolve the substructure ambiguities by conducting a local-to-global consistency refinement. We demonstrate the capability of Outram in a variety of scenarios in multiple large-scale outdoor datasets. Our implementation is open-sourced: https://github.com/Pamphlett/Outram.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610206","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610206","","Location awareness;Point cloud compression;Three-dimensional displays;Laser radar;Costs;Pose estimation;Feature extraction","","3","","38","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"RoboAgent: Generalization and Efficiency in Robot Manipulation via Semantic Augmentations and Action Chunking","H. Bharadhwaj; J. Vakil; M. Sharma; A. Gupta; S. Tulsiani; V. Kumar","The Robotics Institute, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","4788","4795","The grand aim of having a single robot that can manipulate arbitrary objects in diverse settings is at odds with the paucity of robotics datasets. Acquiring and growing such datasets is strenuous due to manual efforts, operational costs, and safety challenges. A path toward such a universal agent requires an efficient framework capable of generalization but within a reasonable data budget. In this paper, we develop an efficient framework (MT-ACT) for training universal agents capable of multi-task manipulation skills using (a) semantic augmentations that can rapidly multiply existing datasets and (b) action representations that can extract performant policies with small yet diverse multi-modal datasets without overfitting. In addition, reliable task conditioning and an expressive policy architecture enables our agent to exhibit a diverse repertoire of skills in novel situations specified using task commands. Using merely 7500 demonstrations, we are able to train a single policy RoboAgent capable of 12 unique skills, and demonstrate its generalization over 38 tasks spread across common daily activities in diverse kitchen scenes. On average, RoboAgent outperforms prior methods by over 40% in unseen situations while being more sample efficient. See https://robopen.github.io/for video results and appendix.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611293","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611293","","Training;Costs;Semantics;Multitasking;Safety;Reliability;Task analysis","","16","","68","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Air Bumper: A Collision Detection and Reaction Framework for Autonomous MAV Navigation","R. Wang; Z. Guo; Y. Chen; X. Wang; B. M. Chen","Chinese University of Hong Kong, Shatin, N.T., Hong Kong; Chinese University of Hong Kong, Shatin, N.T., Hong Kong; Chinese University of Hong Kong, Shatin, N.T., Hong Kong; Chinese University of Hong Kong, Shatin, N.T., Hong Kong; Chinese University of Hong Kong, Shatin, N.T., Hong Kong",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","15735","15741","Autonomous navigation in unknown environments with obstacles remains challenging for micro aerial vehicles (MAVs) due to their limited onboard computing and sensing resources. Although various collision avoidance methods have been developed, it is still possible for drones to collide with unobserved obstacles due to unpredictable disturbances, sensor limitations, and control uncertainty. Instead of completely avoiding collisions, this article proposes Air Bumper, a collision detection and reaction framework, for fully autonomous flight in 3D environments to improve flight safety. Our framework only utilizes the onboard inertial measurement unit (IMU) to detect and estimate collisions. We further design a collision recovery control for rapid recovery and collision-aware mapping to integrate collision information into general LiDAR-based sensing and planning frameworks. Our simulation and experimental results show that the drone can rapidly detect, estimate, and recover from collisions with obstacles in 3D space and continue the flight smoothly with the help of the collision-aware map. In addition, we will open-source the implementation of Air Bumper on GitHub1.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611410","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611410","","Three-dimensional displays;Uncertainty;Software algorithms;Software;Planning;Sensors;Trajectory","","","","28","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"FE-DeTr: Keypoint Detection and Tracking in Low-quality Image Frames with Events","X. Wang; K. Chen; W. Yang; L. Yu; Y. Xing; H. Yu","EIS, Wuhan University, Wuhan, China; EIS, Wuhan University, Wuhan, China; EIS, Wuhan University, Wuhan, China; EIS, Wuhan University, Wuhan, China; SynSense Tech. Co. Ltd., Chengdu, China; EIS, Wuhan University, Wuhan, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","14638","14644","Keypoint detection and tracking in traditional image frames are often compromised by image quality issues such as motion blur and extreme lighting conditions. Event cameras offer potential solutions to these challenges by virtue of their high temporal resolution and high dynamic range. However, they have limited performance in practical applications due to their inherent noise in event data. This paper advocates fusing the complementary information from image frames and event streams to achieve more robust keypoint detection and tracking. Specifically, we propose a novel keypoint detection network that fuses the textural and structural information from image frames with the high-temporal-resolution motion information from event streams, namely FE-DeTr. The network leverages a temporal response consistency for supervision, ensuring stable and efficient keypoint detection. Moreover, we use a spatio-temporal nearest-neighbor search strategy for robust keypoint tracking. Extensive experiments are conducted on a new dataset featuring both image frames and event data captured under extreme conditions. The experimental results confirm the superior performance of our method over both existing frame-based and event-based methods. Our code, pre-trained models, and dataset are available at https://github.com/yuyangpoi/FE-DeTr.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610579","Natural Science Foundation of Hubei Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610579","","Image quality;Image resolution;Tracking;Fuses;Noise;Lighting;Nearest neighbor methods","","1","","37","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Towards Large-Scale Incremental Dense Mapping using Robot-centric Implicit Neural Representation","J. Liu; H. Chen","School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, P.R. China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, P.R. China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","4045","4051","Large-scale dense mapping is vital in robotics, digital twins, and virtual reality. Recently, implicit neural mapping has shown remarkable reconstruction quality. However, incremental large-scale mapping with implicit neural representations remains problematic due to low efficiency, limited video memory, and the catastrophic forgetting phenomenon. To counter these challenges, we introduce the Robot-centric Implicit Mapping (RIM) technique for large-scale incremental dense mapping. This method employs a hybrid representation, encoding shapes with implicit features via a multi-resolution voxel map and decoding signed distance fields through a shallow MLP. We advocate for a robot-centric local map to boost model training efficiency and curb the catastrophic forgetting issue. A decoupled scalable global map is further developed to archive learned features for reuse and maintain constant video memory consumption. Validation experiments demonstrate our method’s exceptional quality, efficiency, and adaptability across diverse scales and scenes over advanced dense mapping methods using range sensors. Our system’s code will be accessible at https://github.com/HITSZ-NRSL/RIM.git.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611564","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611564","","Training;Solid modeling;Adaptation models;Shape;Memory management;Virtual reality;Sensor phenomena and characterization","","","","30","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Bridging the Sim-to-Real Gap with Dynamic Compliance Tuning for Industrial Insertion","X. Zhang; M. Tomizuka; H. Li","Mechanical Systems Control Lab, UC, Berkeley, USA; Mechanical Systems Control Lab, UC, Berkeley, USA; Autodesk Research, USA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","4356","4363","Contact-rich manipulation tasks often exhibit a large sim-to-real gap. For instance, industrial assembly tasks frequently involve tight insertions where the clearance is less than 0.1 mm and can even be negative when dealing with a deformable receptacle. This narrow clearance leads to complex contact dynamics that are difficult to model accurately in simulation, making it challenging to transfer simulation-learned policies to real-world robots. In this paper, we propose a novel framework for robustly learning manipulation skills for real-world tasks using simulated data only. Our framework consists of two main components: the ""Force Planner"" and the ""Gain Tuner"". The Force Planner plans both the robot motion and desired contact force, while the Gain Tuner dynamically adjusts the compliance control gains to track the desired contact force during task execution. The key insight is that by dynamically adjusting the robot’s compliance control gains during task execution, we can modulate contact force in the new environment, thereby generating trajectories similar to those trained in simulation and narrowing the sim-to-real gap. Experimental results show that our method, trained in simulation on a generic square peg-and-hole task, can generalize to a variety of real-world insertion tasks involving narrow and negative clearances, all without requiring any fine-tuning. Videos are available at https://dynamic-compliance.github.io","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610707","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610707","","Robot motion;Tuners;Tracking;Force;Dynamics;Trajectory;Task analysis","","1","","45","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Toward Grounded Commonsense Reasoning","M. Kwon; H. Hu; V. Myers; S. Karamcheti; A. Dragan; D. Sadigh",Stanford University; Stanford University; UC Berkeley; Stanford University; UC Berkeley; Stanford University,2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","5463","5470","Consider a robot tasked with tidying a desk with a meticulously constructed Lego sports car. A human may recognize that it is not appropriate to disassemble the sports car and put it away as part of the ""tidying."" How can a robot reach that conclusion? Although large language models (LLMs) have recently been used to enable commonsense reasoning, grounding this reasoning in the real world has been challenging. To reason in the real world, robots must go beyond passively querying LLMs and actively gather information from the environment that is required to make the right decision. For instance, after detecting that there is an occluded car, the robot may need to actively perceive the car to know whether it is an advanced model car made out of Legos or a toy car built by a toddler. We propose an approach that leverages an LLM and vision language model (VLM) to help a robot actively perceive its environment to perform grounded commonsense reasoning. To evaluate our framework at scale, we release the MessySurfaces dataset which contains images of 70 real-world surfaces that need to be cleaned. We additionally illustrate our approach with a robot on 2 carefully designed surfaces. We find an average 12.9% improvement on the MessySurfaces benchmark and an average 15% improvement on the robot experiments over baselines that do not use active perception. The dataset, code, and videos of our approach can be found at https://minaek.github.io/grounded_commonsense_reasoning/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611218","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611218","","Pediatrics;Grounding;Large language models;Toy manufacturing industry;Automobiles;Task analysis;Robots","","3","","54","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"A Collision-Aware Cable Grasping Method in Cluttered Environment","L. Zhang; K. Bai; Q. Li; Z. Chen; J. Zhang","Department of Informatics, TAMS (Technical Aspects of Multimodal Systems), Universität Hamburg; Department of Informatics, TAMS (Technical Aspects of Multimodal Systems), Universität Hamburg; Department of Informatics, TAMS (Technical Aspects of Multimodal Systems), Universität Hamburg; Agile Robots AG, Germany; Department of Informatics, TAMS (Technical Aspects of Multimodal Systems), Universität Hamburg",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","2126","2132","We introduce a Cable Grasping-Convolutional Neural Network (CG-CNN) designed to facilitate robust cable grasping in cluttered environments. Utilizing physics simulations, we generate an extensive dataset that mimics the intricacies of cable grasping, factoring in potential collisions between cables and robotic grippers. We employ the Approximate Convex Decomposition technique to dissect the non-convex cable model, with grasp quality autonomously labeled based on simulated grasping attempts. The CG-CNN is refined using this simulated dataset and enhanced through domain randomization techniques. Subsequently, the trained model predicts grasp quality, guiding the optimal grasp pose to the robot’s controller for execution. Grasping efficacy is assessed across both synthetic and real-world settings. Given our model’s implicit collision sensitivity, we achieved commendable success rates of 92.3% for known cables and 88.4% for unknown cables, surpassing contemporary state-of-the-art approaches. Supplementary materials can be found at https://leizhang-public.github.io/cg-cnn/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610559","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610559","","Sensitivity;Neural networks;MIMICs;Grasping;Predictive models;Robot sensing systems;Collision avoidance","","","","31","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Robot Fine-Tuning Made Easy: Pre-Training Rewards and Policies for Autonomous Real-World Reinforcement Learning","J. Yang; M. S. Mark; B. Vu; A. Sharma; J. Bohg; C. Finn","Department of Computer Science, Stanford University; Department of Computer Science, Stanford University; Department of Computer Science, Stanford University; Department of Computer Science, Stanford University; Department of Computer Science, Stanford University; Department of Computer Science, Stanford University",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","4804","4811","The pre-train and fine-tune paradigm in machine learning has had dramatic success in a wide range of domains because the use of existing data or pre-trained models on the internet enables quick and easy learning of new tasks. We aim to enable this paradigm in robotic reinforcement learning, allowing a robot to learn a new task with little human effort by leveraging data and models from the Internet. However, reinforcement learning often requires significant human effort in the form of manual reward specification or environment resets, even if the policy is pre-trained. We introduce RoboFuME, a reset-free fine-tuning system that pre-trains a multi-task manipulation policy from diverse datasets of prior experiences and self-improves online to learn a target task with minimal human intervention. Our insights are to utilize calibrated offline reinforcement learning techniques to ensure efficient online fine-tuning of a pre-trained policy in the presence of distribution shifts and leverage pre-trained vision language models (VLMs) to build a robust reward classifier for autonomously providing reward signals during the online fine-tuning process. In a diverse set of five real robot manipulation tasks, we show that our method can incorporate data from an existing robot dataset collected at a different institution and improve on a target task within as little as 3 hours of autonomous real-world experience. We also demonstrate in simulation experiments that our method outperforms prior works that use different RL algorithms or different approaches for predicting rewards. Project website: https://robofume.github.io","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610421","Toyota Research Institute; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610421","","Reinforcement learning;Manuals;Prediction algorithms;Multitasking;Data models;Internet;Task analysis","","","","54","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"SPOTS: Stable Placement of Objects with Reasoning in Semi-Autonomous Teleoperation Systems","J. Lee; S. Park; J. Park; K. Lee; S. Choi","Department of Artificial Intelligence, Korea University, Seoul, Korea; Department of Artificial Intelligence, Korea University, Seoul, Korea; Department of Artificial Intelligence, Korea University, Seoul, Korea; Department of Artificial Intelligence, Chungang University, Seoul, Korea; Department of Artificial Intelligence, Korea University, Seoul, Korea",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","17786","17792","Pick-and-place is one of the fundamental tasks in robotics research. However, the attention has been mostly focused on the ""pick"" task, leaving the ""place"" task relatively unexplored. In this paper, we address the problem of placing objects in the context of a teleoperation framework. Particularly, we focus on two aspects of the place task: stability robustness and contextual reasonableness of object placements. Our proposed method combines simulation-driven physical stability verification via real-to-sim and the semantic reasoning capability of large language models. In other words, given place context information (e.g., user preferences, object to place, and current scene information), our proposed method outputs a probability distribution over the possible placement candidates, considering the robustness and reasonableness of the place task. Our proposed method is extensively evaluated in two simulation and one real world environments and we show that our method can greatly increase the physical plausibility of the placement as well as contextual soundness while considering user preferences. Code, video, and details are available at: https://joonhyunglee.github.io/spots/","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611613","Korea University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611613","","Feedback loop;Large language models;Semantics;Stability analysis;Cognition;Robustness;Real-time systems","","2","","39","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Close the Sim2real Gap via Physically-based Structured Light Synthetic Data Simulation","K. Bai; L. Zhang; Z. Chen; F. Wan; J. Zhang","Department of Informatics, TAMS (Technical Aspects of Multimodal Systems), Universität Hamburg, Germany; Department of Informatics, TAMS (Technical Aspects of Multimodal Systems), Universität Hamburg, Germany; Agile Robots AG, Munich, Germany; School of Design, Southern University of Science and Technology, Shenzhen, China; Department of Informatics, TAMS (Technical Aspects of Multimodal Systems), Universität Hamburg, Germany",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","17035","17041","Despite the substantial progress in deep learning, its adoption in industrial robotics projects remains limited, primarily due to challenges in data acquisition and labeling. Previous sim2real approaches using domain randomization require extensive scene and model optimization. To address these issues, we introduce an innovative physically-based structured light simulation system, generating both RGB and physically realistic depth images, surpassing previous dataset generation tools. We create an RGBD dataset tailored for robotic industrial grasping scenarios and evaluate it across various tasks, including object detection, instance segmentation, and embedding sim2real visual perception in industrial robotic grasping. By reducing the sim2real gap and enhancing deep learning training, we facilitate the application of deep learning models in industrial settings. Project details are available at https://baikaixin-public.github.io/structured_light_3D_synthesizer/","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611401","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611401","","Deep learning;Training;Adaptation models;Three-dimensional displays;Service robots;Annotations;Transfer learning","","","","36","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"CoBT: Collaborative Programming of Behaviour Trees from One Demonstration for Robot Manipulation","A. Jain; P. Long; V. Villani; J. D. Kelleher; M. Chiara Leva","Technological University Dublin; Atlantic Technological University; ARS Control Lab, University of Modena and Reggio Emilia; ADAPT Research Centre, School of Computer Science and Statistics, Trinity College Dublin; Technological University Dublin",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","12993","12999","Mass customization and shorter manufacturing cycles are becoming more important among small and medium-sized companies. However, classical industrial robots struggle to cope with product variation and dynamic environments. In this paper, we present CoBT, a collaborative programming by demonstration framework for generating reactive and modular behavior trees. CoBT relies on a single demonstration and a combination of data-driven machine learning methods with logic-based declarative learning to learn a task, thus eliminating the need for programming expertise or long development times. The proposed framework is experimentally validated on 7 manipulation tasks and we show that CoBT achieves ≈ 93% success rate overall with an average of 7.5s programming time. We conduct a pilot study with non-expert users to provide feedback regarding the usability of CoBT. More videos and generated behavior trees are available at: https://github.com/jainaayush2006/CoBT.git.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611654","European Regional Development Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611654","","Mass customization;Collaboration;Machine learning;Companies;Programming;Industrial robots;Task analysis","","","","28","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"UncertaintyTrack: Exploiting Detection and Localization Uncertainty in Multi-Object Tracking","C. W. Lee; S. L. Waslander","Institute For Aerospace Studies (UTIAS), University of Toronto, Toronto, Canada; Institute For Aerospace Studies (UTIAS), University of Toronto, Toronto, Canada",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","4946","4953","Multi-object tracking (MOT) methods have seen a significant boost in performance recently, due to strong interest from the research community and steadily improving object detection methods. The majority of tracking methods, which follow the tracking-by-detection (TBD) paradigm, blindly trust the incoming detections with no sense of their associated localization uncertainty. This lack of uncertainty awareness poses a problem in safety-critical tasks such as autonomous driving where passengers could be put at risk due to erroneous detections that have propagated to downstream tasks, including MOT. While there are existing works in probabilistic object detection that predict the localization uncertainty around the boxes, no work in 2D MOT for autonomous driving has studied whether these estimates are meaningful enough to be leveraged effectively in object tracking. We introduce UncertaintyTrack, a collection of extensions that can be applied to multiple TBD trackers to account for localization uncertainty estimates from probabilistic object detectors. Experiments on the Berkeley Deep Drive MOT dataset show that the combination of our method and informative uncertainty estimates reduces the number of ID switches by around 19% and improves mMOTA by 2-3%. The source code is available at https://github.com/TRAILab/UncertaintyTrack","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610458","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610458","","Location awareness;Uncertainty;Three-dimensional displays;Source coding;Object detection;Detectors;Probabilistic logic","","","","46","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Heuristic-based Incremental Probabilistic Roadmap for Efficient UAV Exploration in Dynamic Environments","Z. Xu; C. Suzuki; X. Zhan; K. Shimada","Department of Mechanical Engineering, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Mechanical Engineering, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Mechanical Engineering, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Mechanical Engineering, Carnegie Mellon University, Pittsburgh, PA, USA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","11832","11838","Autonomous exploration in dynamic environments necessitates a planner that can proactively respond to changes and make efficient and safe decisions for robots. Although plenty of sampling-based works have shown success in exploring static environments, their inherent sampling randomness and limited utilization of previous samples often result in sub-optimal exploration efficiency. Additionally, most of these methods struggle with efficient replanning and collision avoidance in dynamic settings. To overcome these limitations, we propose the Heuristic-based Incremental Probabilistic Roadmap Exploration (HIRE) planner for UAVs exploring dynamic environments. The proposed planner adopts an incremental sampling strategy based on the probabilistic roadmap constructed by heuristic sampling toward the unexplored region next to the free space, defined as the heuristic frontier regions. The heuristic frontier regions are detected by applying a lightweight vision-based method to the different levels of the occupancy map. Moreover, our dynamic module ensures that the planner dynamically updates roadmap information based on the environment changes and avoids dynamic obstacles. Simulation and physical experiments prove that our planner can efficiently and safely explore dynamic environments. Our software1 is available on GitHub with the experiment video2.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610462","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610462","","Probabilistic logic;Autonomous aerial vehicles;Collision avoidance;Robots;Software development management","","","","28","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Cross Domain Policy Transfer with Effect Cycle-Consistency","R. Zhu; T. Dai; O. Celiktutan","Department of Engineering, King’s College London; Department of Computing Science, University of Aberdeen; Department of Engineering, King’s College London",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","9471","9477","Training a robotic policy from scratch using deep reinforcement learning methods can be prohibitively expensive due to sample inefficiency. To address this challenge, transferring policies trained in the source domain to the target domain becomes an attractive paradigm. Previous research has typically focused on domains with similar state and action spaces but differing in other aspects. In this paper, our primary focus lies in domains with different state and action spaces, which has broader practical implications, i.e. transfer the policy from robot A to robot B. Unlike prior methods that rely on paired data, we propose a novel approach for learning the mapping functions between state and action spaces across domains using unpaired data. We propose effect cycle-consistency, which aligns the effects of transitions across two domains through a symmetrical optimization structure for learning these mapping functions. Once the mapping functions are learned, we can seamlessly transfer the policy from the source domain to the target domain. Our approach has been tested on three locomotion tasks and two robotic manipulation tasks. The empirical results demonstrate that our method can reduce alignment errors significantly and achieve better performance compared to the state-of-the-art method. Project page: https://ricky-zhu.github.io/effect_cycle_consistency.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611110","China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611110","","Training;Deep reinforcement learning;Task analysis;Robots;Optimization","","","","37","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Advancements in 3D Lane Detection Using LiDAR Point Clouds: From Data Collection to Model Development","R. Zhao; Y. Heng; H. Wang; Y. Gao; S. Liu; C. Yao; J. Chen; W. Cai","School of Computer Science, University of Sydney; Baidu ACG; School of Computer Science, University of Sydney; Baidu ACG; Baidu ACG; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University; Baidu ACG; School of Computer Science, University of Sydney",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","5382","5388","Advanced Driver-Assistance Systems (ADAS) have successfully integrated learning-based techniques into vehicle perception and decision-making. However, their application in 3D lane detection for effective driving environment perception is hindered by the lack of comprehensive LiDAR datasets. The sparse nature of LiDAR point cloud data prevents an efficient manual annotation process. To solve this problem, we present LiSV-3DLane, a large-scale 3D lane dataset that comprises 20k frames of surround-view LiDAR point clouds with enriched semantic annotation. Unlike existing datasets confined to a frontal perspective, LiSV-3DLane provides a full 360-degree spatial panorama around the ego vehicle, capturing complex lane patterns in both urban and highway environments. We leverage the geometric traits of lane lines and the intrinsic spatial attributes of LiDAR data to design a simple yet effective automatic annotation pipeline for generating finer lane labels. To propel future research, we propose a novel LiDAR-based 3D lane detection model, LiLaDet, incorporating the spatial geometry learning of the LiDAR point cloud into Bird’s Eye View (BEV) based lane identification. Experimental results indicate that LiLaDet outperforms existing camera- and LiDAR-based approaches in the 3D lane detection task on the K-Lane dataset and our LiSV-3DLane. The project code will be available at https://github.com/RunkaiZhao/LiLaDet.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610087","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610087","","Point cloud compression;Solid modeling;Three-dimensional displays;Laser radar;Lane detection;Annotations;Semantics","","","","32","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Metrically Scaled Monocular Depth Estimation through Sparse Priors for Underwater Robots","L. Ebner; G. Billings; S. Williams","Robotic Systems Lab, ETH Zurich; Australian Centre for Field Robotics, University of Sydney; Australian Centre for Field Robotics, University of Sydney",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","3751","3757","In this work, we address the problem of real-time dense depth estimation from monocular images for mobile underwater vehicles. We formulate a deep learning model that fuses sparse depth measurements from triangulated features to improve the depth predictions and solve the problem of scale ambiguity. To allow prior inputs of arbitrary sparsity, we apply a dense parameterization method. Our model extends recent state-of-the-art approaches to monocular image based depth estimation, using an efficient encoder-decoder backbone and modern lightweight transformer optimization stage to encode global context. The network is trained in a supervised fashion on the forward-looking underwater dataset, FLSea. Evaluation results on this dataset demonstrate significant improvement in depth prediction accuracy by the fusion of the sparse feature priors. In addition, without any retraining, our method achieves similar depth prediction accuracy on a downward looking dataset we collected with a diver operated camera rig, conducting a survey of a coral reef. The method achieves real-time performance, running at 24 FPS on a NVIDIA Jetson Xavier NX, 160 FPS on a NVIDIA RTX 2080 GPU and 7 FPS on a single Intel i9-9900K CPU core, making it suitable for direct deployment on embedded GPU systems. The implementation of this work is made publicly available at https://github.com/ebnerluca/uw_depth.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611007","Marine Robotics;Computer Vision for Automation;Deep Learning for Visual Perception","Surveys;Deep learning;Accuracy;Fuses;Graphics processing units;Estimation;Predictive models","","1","","34","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Breaking Data Silos: Cross-Domain Learning for Multi-Agent Perception from Independent Private Sources","J. Li; B. Li; X. Liu; R. Xu; J. Ma; H. Yu","Cleveland Vision & AI Lab, Cleveland State University; Cleveland Vision & AI Lab, Cleveland State University; Cleveland Vision & AI Lab, Cleveland State University; UCLA Mobility Lab, University of California, Los Angeles; UCLA Mobility Lab, University of California, Los Angeles; Cleveland Vision & AI Lab, Cleveland State University",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","18414","18420","The diverse agents in multi-agent perception systems may be from different companies. Each company might use the identical classic neural network architecture based encoder for feature extraction. However, the data source to train the various agents is independent and private in each company, leading to the Distribution Gap of different private data for training distinct agents in multi-agent perception system. The data silos by the above Distribution Gap could result in a significant performance decline in multi-agent perception. In this paper, we thoroughly examine the impact of the distribution gap on existing multi-agent perception systems. To break the data silos, we introduce the Feature Distribution-aware Aggregation (FDA) framework for cross-domain learning to mitigate the above Distribution Gap in multi-agent perception. FDA comprises two key components: Learnable Feature Compensation Module and Distribution-aware Statistical Consistency Module, both aimed at enhancing intermediate features to minimize the distribution gap among multi-agent features. Intensive experiments on the public OPV2V and V2XSet datasets underscore FDA’s effectiveness in point cloud-based 3D object detection, presenting it as an invaluable augmentation to existing multi-agent perception systems. The code is available at https://github.com/jinlong17/BDS-V2V.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610591","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610591","","Training;Three-dimensional displays;Codes;Soft sensors;Neural networks;Training data;Companies","","1","","30","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"A Distributed Multi-Robot Framework for Exploration, Information Acquisition and Consensus","A. Patwardhan; A. J. Davison",NA; NA,2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","12062","12068","The distributed coordination of robot teams performing complex tasks is challenging to formulate. The different aspects of a complete task such as local planning for obstacle avoidance, global goal coordination and collaborative mapping are often solved separately, when clearly each of these should influence the others for the most efficient behaviour. In this paper we use the example application of distributed information acquisition as a robot team explores a large space to show that we can formulate the whole problem as a single factor graph with multiple connected layers representing each aspect. We use Gaussian Belief Propagation (GBP) as the inference mechanism, which permits parallel, on-demand or asynchronous computation for efficiency when different aspects are more or less important. This is the first time that a distributed GBP multi-robot solver has been proven to enable intelligent collaborative behaviour rather than just guiding robots to individual, selfish goals. We encourage the reader to view our demos at https://aalpatya.github.io/gbpstack.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610185","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610185","","Robot kinematics;Heuristic algorithms;Scalability;Collaboration;Space exploration;Planning;Computational efficiency","","","","15","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"CoFRIDA: Self-Supervised Fine-Tuning for Human-Robot Co-Painting","P. Schaldenbrand; G. Parmar; J. -Y. Zhu; J. McCann; J. Oh","The Robotics Institute, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","2296","2302","Prior robot painting and drawing work, such as FRIDA, has focused on decreasing the sim-to-real gap and expanding input modalities for users, but the interaction with these systems generally exists only in the input stages. To support interactive, human-robot collaborative painting, we introduce the Collaborative FRIDA (CoFRIDA) robot painting framework, which can co-paint by modifying and engaging with content already painted by a human collaborator. To improve text-image alignment–FRIDA’s major weakness–our system uses pre-trained text-to-image models; however, pre-trained models in the context of real-world co-painting do not perform well because they (1) do not understand the constraints and abilities of the robot and (2) cannot perform co-painting without making unrealistic edits to the canvas and overwriting content. We propose a self-supervised fine-tuning procedure that can tackle both issues, allowing the use of pre-trained state-of-the-art text-image alignment models with robots to enable co-painting in the physical world. Our open-source approach, CoFRIDA, creates paintings and drawings that match the input text prompt more clearly than FRIDA, both from a blank canvas and one with human created work. More generally, our fine-tuning procedure successfully encodes the robot’s constraints and abilities into a foundation model, showcasing promising results as an effective method for reducing sim-to-real gaps. https://pschaldenbrand.github.io/cofrida/","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610618","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610618","","Adaptation models;Impedance matching;Semantics;Collaboration;Text to image;Planning;Noise measurement","","4","","37","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"CoLRIO: LiDAR-Ranging-Inertial Centralized State Estimation for Robotic Swarms","S. Zhong; H. Chen; Y. Qi; D. Feng; Z. Chen; J. Wu; W. Wen; M. Liu","School of Systems Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Systems Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Systems Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Systems Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Systems Science and Engineering, Sun Yat-sen University, Guangzhou, China; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong; Department of Aeronautical and Aviation Engineering, Hong Kong Polytechnic University, Hong Kong; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","3920","3926","Collaborative state estimation using different heterogeneous sensors is a fundamental prerequisite for robotic swarms operating in GPS-denied environments, posing a significant research challenge. In this paper, we introduce a centralized system to facilitate collaborative LiDAR-ranging-inertial state estimation, enabling robotic swarms to operate without the need for anchor deployment. The system efficiently distributes computationally intensive tasks to a central server, thereby reducing the computational burden on individual robots for local odometry calculations. The server back-end establishes a global reference by leveraging shared data and refining joint pose graph optimization through place recognition, global optimization techniques, and removal of outlier data to ensure precise and robust collaborative state estimation. Extensive evaluations of our system, utilizing both publicly available datasets and our custom datasets, demonstrate significant enhancements in the accuracy of collaborative SLAM estimates. Moreover, our system exhibits remarkable proficiency in large-scale missions, seamlessly enabling ten robots to collaborate effectively in performing SLAM tasks. In order to contribute to the research community, we will make our code open-source and accessible at https://github.com/PengYu-team/Co-LRIO.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611672","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611672","","Simultaneous localization and mapping;Accuracy;Scalability;Collaboration;Computational efficiency;Sensors;Servers","","2","","40","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"ASGrasp: Generalizable Transparent Object Reconstruction and 6-DoF Grasp Detection from RGB-D Active Stereo Camera","J. Shi; Y. A; Y. Jin; D. Li; H. Niu; Z. Jin; H. Wang","Samsung R&D Institute, China, Beijing; Samsung R&D Institute, China, Beijing; Samsung R&D Institute, China, Beijing; Samsung R&D Institute, China, Beijing; Samsung R&D Institute, China, Beijing; Samsung R&D Institute, China, Beijing; CFCS, School of Computer Science, Peking University",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","5441","5447","In this paper, we tackle the problem of grasping transparent and specular objects. This issue holds importance, yet it remains unsolved within the field of robotics due to failure of recover their accurate geometry by depth cameras. For the first time, we propose ASGrasp, a 6-DoF grasp detection network that uses an RGB-D active stereo camera. ASGrasp utilizes a two-layer learning-based stereo network for the purpose of transparent object reconstruction, enabling material-agnostic object grasping in cluttered environments. In contrast to existing RGB-D based grasp detection methods, which heavily depend on depth restoration networks and the quality of depth maps generated by depth cameras, our system distinguishes itself by its ability to directly utilize raw IR and RGB images for transparent object geometry reconstruction. We create an extensive synthetic dataset through domain randomization, which is based on GraspNet-1Billion. Our experiments demonstrate that ASGrasp can achieve over 90% success rate for generalizable transparent object grasping in both simulation and the real via seamless sim-to-real transfer. Our method significantly outperforms SOTA networks and even surpasses the performance upper bound set by perfect visible point cloud inputs. Project page: https://pku-epic.github.io/ASGrasp","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611152","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611152","","Geometry;Upper bound;Three-dimensional displays;Robot vision systems;Grasping;Cameras;6-DOF","","","","27","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Ground-Fusion: A Low-cost Ground SLAM System Robust to Corner Cases","J. Yin; A. Li; W. Xi; W. Yu; D. Zou",Shanghai Jiao Tong University; Shanghai Jiao Tong University; Midea Corparate Research Center; Shanghai Jiao Tong University; Shanghai Jiao Tong University,2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","8603","8609","We introduce Ground-Fusion, a low-cost sensor fusion simultaneous localization and mapping (SLAM) system for ground vehicles. Our system features efficient initialization, effective sensor anomaly detection and handling, real-time dense color mapping, and robust localization in diverse environments. We tightly integrate RGB-D images, inertial measurements, wheel odometer and GNSS signals within a factor graph to achieve accurate and reliable localization both indoors and outdoors. To ensure successful initialization, we propose an efficient strategy that comprises three different methods: stationary, visual, and dynamic, tailored to handle diverse cases. Furthermore, we develop mechanisms to detect sensor anomalies and degradation, handling them adeptly to maintain system accuracy. Our experimental results on both public and self-collected datasets demonstrate that Ground-Fusion outperforms existing low-cost SLAM systems in corner cases. We release the code and datasets at https://github.com/SJTU-ViSYS/Ground-Fusion.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610070","","Location awareness;Visualization;Simultaneous localization and mapping;Accuracy;Wheels;Sensor fusion;Land vehicles","","2","","31","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Robot Synesthesia: In-Hand Manipulation with Visuotactile Sensing","Y. Yuan; H. Che; Y. Qin; B. Huang; Z. -H. Yin; K. -W. Lee; Y. Wu; S. -C. Lim; X. Wang","Institute for Interdisciplinary Information Sciences, Tsinghua University; University of California, San Diego; University of California, San Diego; University of Illinois, Urbana-Champaign; University of California, Berkeley; Dongguk University; Institute for Interdisciplinary Information Sciences, Tsinghua University; Dongguk University; University of California, San Diego",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","6558","6565","Executing contact-rich manipulation tasks necessitates the fusion of tactile and visual feedback. However, the distinct nature of these modalities poses significant challenges. In this paper, we introduce a system that leverages visual and tactile sensory inputs to enable dexterous in-hand manipulation. Specifically, we propose Robot Synesthesia, a novel point cloudbased tactile representation inspired by human tactile-visual synesthesia. This approach allows for the simultaneous and seamless integration of both sensory inputs, offering richer spatial information and facilitating better reasoning about robot actions. Comprehensive ablations are performed on how the integration of vision and touch can improve reinforcement learning and Sim2Real performance. Our project page is available at https://yingyuan0414.github.io/visuotactile/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610532","Qualcomm; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610532","","Training;Point cloud compression;Visualization;Optical feedback;Pipelines;Tactile sensors;Reinforcement learning","","7","","65","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"HIO-SDF: Hierarchical Incremental Online Signed Distance Fields","V. Vasilopoulos; S. Garg; J. Huh; B. Lee; V. Isler",Samsung AI Center NY; Samsung AI Center NY; Samsung AI Center NY; Samsung AI Center NY; Samsung AI Center NY,2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","17537","17543","A good representation of a large, complex mobile robot workspace must be space-efficient yet capable of encoding relevant geometric details. When exploring unknown environments, it needs to be updatable incrementally in an online fashion. We introduce HIO-SDF, a new method that represents the environment as a Signed Distance Field (SDF). State of the art representations of SDFs are based on either neural networks or voxel grids. Neural networks are capable of representing the SDF continuously. However, they are hard to update incrementally as neural networks tend to forget previously observed parts of the environment unless an extensive sensor history is stored for training. Voxel-based representations do not have this problem but they are not space-efficient especially in large environments with fine details. HIO-SDF combines the advantages of these representations using a hierarchical approach which employs a coarse voxel grid that captures the observed parts of the environment together with high-resolution local information to train a neural network. HIO-SDF achieves a 46% lower mean global SDF error across all test scenes than a state of the art continuous representation, and a 30% lower error than a discrete representation at the same resolution as our coarse global SDF grid. Videos and code are available at: https://samsunglabs.github.io/HIO-SDF-project-page/","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610367","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610367","","Training;Motion planning;Three-dimensional displays;Neural networks;Robot sensing systems;Planning;Mobile robots","","2","","32","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Uncertainty-aware 3D Object-Level Mapping with Deep Shape Priors","Z. Liao; J. Yang; J. Qian; A. P. Schoellig; S. L. Waslander",University of Toronto Institute for Aerospace Studies; University of Toronto Institute for Aerospace Studies; University of Toronto Institute for Aerospace Studies; University of Toronto Institute for Aerospace Studies; University of Toronto Institute for Aerospace Studies,2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","4082","4089","3D object-level mapping is a fundamental problem in robotics, which is especially challenging when object CAD models are unavailable during inference. We propose a framework that can reconstruct high-quality object-level maps for unknown objects. Our approach takes multiple RGB-D images as input and outputs dense 3D shapes and 9-DoF poses (including 3 scale parameters) for detected objects. The core idea is to leverage a learnt generative model for a category of object shapes as priors and to formulate a probabilistic, uncertainty-aware optimization framework for 3D reconstruction. We derive a probabilistic formulation that propagates shape and pose uncertainty through two novel loss functions. Unlike current state-of-the-art approaches, we explicitly model the uncertainty of the object shapes and poses during our optimization, resulting in a high-quality object-level mapping system. Moreover, the estimated shape and pose uncertainties, which we demonstrate can accurately reflect the true errors of our object maps, can be useful for downstream robotics tasks such as active vision. We perform extensive evaluations on indoor and outdoor real-world datasets, achieving substantial improvements over state-of-the-art methods. Our code is available at https://github.com/TRAILab/UncertainShapePose.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611206","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611206","","Solid modeling;Uncertainty;Three-dimensional displays;Shape;Robot vision systems;Probabilistic logic;Cameras","","1","","41","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"RadarCam-Depth: Radar-Camera Fusion for Depth Estimation with Learned Metric Scale","H. Li; Y. Ma; Y. Gu; K. Hu; Y. Liu; X. Zuo","Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Department of Computing and Mathematical Sciences, California Institute of Technology, USA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","10665","10672","We present a novel approach for metric dense depth estimation based on the fusion of a single-view image and a sparse, noisy Radar point cloud. The direct fusion of heterogeneous Radar and image data, or their encodings, tends to yield dense depth maps with significant artifacts, blurred boundaries, and suboptimal accuracy. To circumvent this issue, we learn to augment versatile and robust monocular depth prediction with the dense metric scale induced from sparse and noisy Radar data. We propose a Radar-Camera framework for highly accurate and fine-detailed dense depth estimation with four stages, including monocular depth prediction, global scale alignment of monocular depth with sparse Radar points, quasi-dense scale estimation through learning the association between Radar points and image patches, and local scale refinement of dense depth using a scale map learner. Our proposed method significantly outperforms the state-of-the-art Radar-Camera depth estimation methods by reducing the mean absolute error (MAE) of depth estimation by 25.6% and 40.2% on the challenging nuScenes dataset and our self-collected ZJU-4DRadarCam dataset, respectively. Our code and dataset will be released at https://github.com/MMOCKING/RadarCam-Depth.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610929","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610929","","Point cloud compression;Image coding;Accuracy;Three-dimensional displays;Robot vision systems;Estimation;Radar","","2","","38","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Ricmonk: A Three-Link Brachiation Robot with Passive Grippers for Energy-Efficient Brachiation","S. S. Grama; M. Javadi; S. Kumar; H. Z. Boroujeni; F. Kirchner","Yardstick Robotics GmbH, Bremen, Germany; Robotics Innovation Center, DFKI GmbH, Bremen, Germany; Robotics Innovation Center, DFKI GmbH, Bremen, Germany; Robotics Innovation Center, DFKI GmbH, Bremen, Germany; Robotics Innovation Center, DFKI GmbH, Bremen, Germany",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","8920","8926","This paper presents the design, analysis, and performance evaluation of RicMonk, a novel three-link brachiation robot equipped with passive hook-shaped grippers. Brachiation, an agile and energy-efficient mode of locomotion observed in primates, has inspired the development of RicMonk to explore versatile locomotion and maneuvers on ladder-like structures. The robot’s anatomical resemblance to gibbons and the integration of a tail mechanism for energy injection contribute to its unique capabilities. The paper discusses the use of the Direct Collocation methodology for optimizing trajectories for the robot’s dynamic behaviors and stabilization of these trajectories using a Time-varying Linear Quadratic Regulator. With RicMonk we demonstrate bidirectional brachiation, and provide comparative analysis with its predecessor, AcroMonk - a two-link brachiation robot, to demonstrate that the presence of a passive tail helps improve energy efficiency. The system design, controllers, and software implementation are publicly available on GitHub at https://github.com/dfki-ric-underactuated-lab/ricmonk and the video demonstration of the experiments can be viewed at https://youtu.be/hOuDQI7CD8w.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611003","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611003","Underactuated robots;biologically-inspired robots;education robotics","Performance evaluation;Regulators;Tail;Energy efficiency;Software;Trajectory;Grippers","","2","","29","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Learning for Deformable Linear Object Insertion Leveraging Flexibility Estimation from Visual Cues","M. Li; C. Choi","Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN; Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","5183","5189","Manipulation of deformable Linear objects (DLOs), including iron wire, rubber, silk, and nylon rope, is ubiquitous in daily life. These objects exhibit diverse physical properties, such as Young’s modulus and bending stiffness. Such diversity poses challenges for developing generalized manipulation policies. However, previous research limited their scope to single-material DLOs and engaged in time-consuming data collection for the state estimation. In this paper, we propose a two-stage manipulation approach consisting of a material property (e.g., flexibility) estimation and policy learning for DLO insertion with reinforcement learning. Firstly, we design a flexibility estimation scheme that characterizes the properties of different types of DLOs. The ground truth flexibility data is collected in simulation to train our flexibility estimation module. During the manipulation, the robot interacts with the DLOs to estimate flexibility by analyzing their visual configurations. Secondly, we train a policy conditioned on the estimated flexibility to perform challenging DLO insertion tasks. Our pipeline trained with diverse insertion scenarios achieves an 85.6% success rate in simulation and 66.67% in real robot experiments. Please refer to our project page: https://lmeee.github.io/DLOInsert/","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610419","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610419","","Visualization;Pipelines;Reinforcement learning;Trajectory;Wire;Rubber;Reliability","","","","36","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Gaussian Process-based Traversability Analysis for Terrain Mapless Navigation","A. Leininger; M. Ali; H. Jardali; L. Liu","Department of Computer Science, Indiana University, Bloomington, IN, USA; Department of Intelligent Systems Engineering, Indiana University, Bloomington; Department of Intelligent Systems Engineering, Indiana University, Bloomington; Department of Intelligent Systems Engineering, Indiana University, Bloomington",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","10925","10931","Efficient navigation through uneven terrain remains a challenging endeavor for autonomous robots. We propose a new geometric-based uneven terrain mapless navigation framework combining a Sparse Gaussian Process (SGP) local map with a Rapidly-Exploring Random Tree* (RRT*) planner. Our approach begins with the generation of a high-resolution SGP local map, providing an interpolated representation of the robot’s immediate environment. This map captures crucial environmental variations, including height, uncertainties, and slope characteristics. Subsequently, we construct a traversability map based on the SGP representation to guide our planning process. The RRT* planner efficiently generates real-time navigation paths, avoiding untraversable terrain in pursuit of the goal. This combination of SGP-based terrain interpretation and RRT* planning enables ground robots to safely navigate environments with varying elevations and steep obstacles. We evaluate the performance of our proposed approach through robust simulation testing, highlighting its effectiveness in achieving safe and efficient navigation compared to existing methods. See the project GitHub 1 for source code and supplementary materials, including a video demonstrating experimental results.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610106","Off-road navigation;Traversability-analysis;Gaussian process (GP)","Uncertainty;Navigation;Source coding;Vegetation;Gaussian processes;Planning;Vehicle dynamics","","","","36","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Refining Pre-Trained Motion Models","X. Sun; A. W. Harley; L. J. Guibas","Computer Science Department, Stanford University; Computer Science Department, Stanford University; Computer Science Department, Stanford University",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","4932","4938","Given the difficulty of manually annotating motion in video, the current best motion estimation methods are trained with synthetic data, and therefore struggle somewhat due to a train/test gap. Self-supervised methods hold the promise of training directly on real video, but typically perform worse. These include methods trained with warp error (i.e., color constancy) combined with smoothness terms, and methods that encourage cycle-consistency in the estimates (i.e., tracking backwards should yield the opposite trajectory as tracking forwards). In this work, we take on the challenge of improving state-of-the-art supervised models with self-supervised training. We find that when the initialization is supervised weights, most existing self-supervision techniques actually make performance worse instead of better, which suggests that the benefit of seeing the new data is overshadowed by the noise in the training signal. Focusing on obtaining a ""clean"" training signal from real-world unlabelled video, we propose to separate label-making and training into two distinct stages. In the first stage, we use the pre-trained model to estimate motion in a video, and then select the subset of motion estimates which we can verify with cycle-consistency. This produces a sparse but accurate pseudo-labelling of the video. In the second stage, we fine-tune the model to reproduce these outputs, while also applying augmentations on the input. We complement this boot-strapping method with simple techniques that densify and re-balance the pseudo-labels, ensuring that we do not merely train on ""easy"" tracks. We show that our method yields reliable gains over fully-supervised methods in real videos, for both short-term (flow-based) and long-range (multiframe) pixel tracking. Our code can be found here: https: //github.com/AlexSunNik/refining-motion-code.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610900","Toyota Research Institute; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610900","","Training;Tracking;Image color analysis;Motion estimation;Refining;Noise;Trajectory","","2","","39","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Follow the Footprints: Self-supervised Traversability Estimation for Off-road Vehicle Navigation based on Geometric and Visual Cues","Y. Jeon; E. I. Son; S. -W. Seo","Seoul National University, Seoul; Seoul National University, Seoul; Seoul National University, Seoul",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","1774","1780","In this study, we address the off-road traversability estimation problem, that predicts areas where a robot can navigate in off-road environments. An off-road environment is an unstructured environment comprising a combination of traversable and non-traversable spaces, which presents a challenge for estimating traversability. This study highlights three primary factors that affect a robot’s traversability in an off-road environment: surface slope, semantic information, and robot platform. We present two strategies for estimating traversability, using a guide filter network (GFN) and footprint supervision module (FSM). The first strategy involves building a novel GFN using a newly designed guide filter layer. The GFN interprets the surface and semantic information from the input data and integrates them to extract features optimized for traversability estimation. The second strategy involves developing an FSM, which is a self-supervision module that utilizes the path traversed by the robot in pre-driving, also known as a footprint. This enables the prediction of traversability that reflects the characteristics of the robot platform. Based on these two strategies, the proposed method overcomes the limitations of existing methods, which require laborious human supervision and lack scalability. Extensive experiments in diverse conditions, including automobiles and unmanned ground vehicles, herbfields, woodlands, and farmlands, demonstrate that the proposed method is compatible for various robot platforms and adaptable to a range of terrains. Code is available at https://github.com/yurimjeon1892/FtFoot.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611198","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611198","","Visualization;Navigation;Scalability;Semantics;Estimation;Self-supervised learning;Information filters","","","","29","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"ViPlanner: Visual Semantic Imperative Learning for Local Navigation","P. Roth; J. Nubert; F. Yang; M. Mittal; M. Hutter","Robotic Systems Lab, ETH Zürich, Zürich, Switzerland; Robotic Systems Lab, ETH Zürich, Zürich, Switzerland; Robotic Systems Lab, ETH Zürich, Zürich, Switzerland; Robotic Systems Lab, ETH Zürich, Zürich, Switzerland; Robotic Systems Lab, ETH Zürich, Zürich, Switzerland",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","5243","5249","Real-time path planning in outdoor environments still challenges modern robotic systems due to differences in terrain traversability, diverse obstacles, and the necessity for fast decision-making. Established approaches have primarily focused on geometric navigation solutions, which work well for structured geometric obstacles but have limitations regarding the semantic interpretation of different terrain types and their affordances. Moreover, these methods fail to identify traversable geometric occurrences, such as stairs. To overcome these issues, we introduce ViPlanner, a learned local path planning approach that generates local plans based on geometric and semantic information. The system is trained using the Imperative Learning paradigm, for which the network weights are optimized end-to-end based on the planning task objective. This optimization uses a differentiable formulation of a semantic costmap, which enables the planner to distinguish between the traversability of different terrains and accurately identify obstacles. The semantic information is represented in 30 classes using an RGB colorspace that can effectively encode the multiple levels of traversability. We show that the planner can adapt to diverse real-world environments without requiring any real-world training. In fact, the planner is trained purely in simulation, enabling a highly scalable training data generation. Experimental results demonstrate resistance to noise, zero-shot sim-to-real transfer, and a decrease of 38.02% in terms of traversability cost compared to purely geometric-based approaches. Code and models are made publicly available: https://github.com/leggedrobotics/viplanner.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610025","Horizon Europe; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610025","","Training;Resistance;Visualization;Navigation;Semantics;Training data;Stairs","","4","","45","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Vision-Language Interpreter for Robot Task Planning","K. Shirai; C. C. Beltran-Hernandez; M. Hamaya; A. Hashimoto; S. Tanaka; K. Kawaharazuka; K. Tanaka; Y. Ushiku; S. Mori","Kyoto University, Kyoto, Japan; OMRON SINIC X Corporation, Tokyo, Japan; OMRON SINIC X Corporation, Tokyo, Japan; OMRON SINIC X Corporation, Tokyo, Japan; OMRON SINIC X Corporation, Tokyo, Japan; University of Tokyo, Tokyo, Japan; OMRON SINIC X Corporation, Tokyo, Japan; OMRON SINIC X Corporation, Tokyo, Japan; University of Tokyo, Tokyo, Japan",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","2051","2058","Large language models (LLMs) are accelerating the development of language-guided robot planners. Meanwhile, symbolic planners offer the advantage of interpretability. This paper proposes a new task that bridges these two trends, namely, multimodal planning problem specification. The aim is to generate a problem description (PD), a machine-readable file used by the planners to find a plan. By generating PDs from language instruction and scene observation, we can drive symbolic planners in a language-guided framework. We propose a Vision-Language Interpreter (ViLaIn), a new framework that generates PDs using state-of-the-art LLM and vision-language models. ViLaIn can refine generated PDs via error message feedback from the symbolic planner. Our aim is to answer the question: How accurately can ViLaIn and the symbolic planner generate valid robot plans? To evaluate ViLaIn, we introduce a novel dataset called the problem description generation (ProDG) dataset. The framework is evaluated with four new evaluation metrics. Experimental results show that ViLaIn can generate syntactically correct problems with more than 99% accuracy and valid plans with more than 58% accuracy. Our code and dataset are available at https://github.com/omron-sinicx/ViLaIn.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611112","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611112","","Measurement;Accuracy;Codes;Large language models;Refining;Linguistics;Market research","","5","","47","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Stein Variational Guided Model Predictive Path Integral Control: Proposal and Experiments with Fast Maneuvering Vehicles","K. Honda; N. Akai; K. Suzuki; M. Aoki; H. Hosogaya; H. Okuda; T. Suzuki","The Department of Mechanical Systems Engineering, Graduate School of Engineering, Nagoya University, Furo-Cho, Chikusa-Ku, Nagoya, Aichi, Japan; The Department of Mechanical Systems Engineering, Graduate School of Engineering, Nagoya University, Furo-Cho, Chikusa-Ku, Nagoya, Aichi, Japan; The Department of Mechanical Systems Engineering, Graduate School of Engineering, Nagoya University, Furo-Cho, Chikusa-Ku, Nagoya, Aichi, Japan; The Department of Mechanical Systems Engineering, Graduate School of Engineering, Nagoya University, Furo-Cho, Chikusa-Ku, Nagoya, Aichi, Japan; The Department of Mechanical Systems Engineering, Graduate School of Engineering, Nagoya University, Furo-Cho, Chikusa-Ku, Nagoya, Aichi, Japan; The Department of Mechanical Systems Engineering, Graduate School of Engineering, Nagoya University, Furo-Cho, Chikusa-Ku, Nagoya, Aichi, Japan; The Department of Mechanical Systems Engineering, Graduate School of Engineering, Nagoya University, Furo-Cho, Chikusa-Ku, Nagoya, Aichi, Japan",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","7020","7026","This paper presents a novel Stochastic Optimal Control (SOC) method based on Model Predictive Path Integral control (MPPI), named Stein Variational Guided MPPI (SVG-MPPI), designed to handle rapidly shifting multimodal optimal action distributions. While MPPI can find a Gaussian-approximated optimal action distribution in closed form, i.e., without iterative solution updates, it struggles with the mul-timodality of the optimal distributions. This is due to the less representative nature of the Gaussian. To overcome this limitation, our method aims to identify a target mode of the optimal distribution and guide the solution to converge to fit it. In the proposed method, the target mode is roughly estimated using a modified Stein Variational Gradient Descent (SVGD) method and embedded into the MPPI algorithm to find a closed-form ""mode-seeking"" solution that covers only the target mode, thus preserving the fast convergence property of MPPI. Our simulation and real-world experimental results demonstrate that SVG-MPPI outperforms both the original MPPI and other state-of-the-art sampling-based SOC algorithms in terms of path-tracking and obstacle-avoidance capabilities. https://github.com/kohonda/proj-svg_mppi","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611021","","Stochastic processes;Optimal control;Predictive models;Prediction algorithms;Proposals;Iterative methods;Robotics and automation","","1","","33","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Zero-Shot Wireless Indoor Navigation through Physics-Informed Reinforcement Learning","M. Yin; T. Li; H. Lei; Y. Hu; S. Rangan; Q. Zhu","Tandon School of Engineering, New York University, NY, USA; Tandon School of Engineering, New York University, NY, USA; Tandon School of Engineering, New York University, NY, USA; Tandon School of Engineering, New York University, NY, USA; Tandon School of Engineering, New York University, NY, USA; Tandon School of Engineering, New York University, NY, USA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","5111","5118","The growing focus on indoor robot navigation utilizing wireless signals has stemmed from the capability of these signals to capture high-resolution angular and temporal measurements. Prior heuristic-based methods, based on radio frequency (RF) propagation, are intuitive and generalizable across simple scenarios, yet fail to navigate in complex environments. On the other hand, end-to-end (e2e) deep reinforcement learning (RL) can explore a rich class of policies, delivering surprising performance when facing complex wireless environments. However, the price to pay is the astronomical amount of training samples, and the resulting policy, without fine-tuning (zero-shot), is unable to navigate efficiently in new scenarios unseen in the training phase. To equip the navigation agent with sample-efficient learning and zero-shot generalization, this work proposes a novel physics-informed RL (PIRL) where a distance-to-target-based cost (standard in e2e) is augmented with physics-informed reward shaping. The key intuition is that wireless environments vary, but physics laws persist. After learning to utilize the physics information, the agent can transfer this knowledge across different tasks and navigate in an unknown environment without fine-tuning. The proposed PIRL is evaluated using a wireless digital twin (WDT) built upon simulations of a large class of indoor environments from the AI Habitat dataset augmented with electromagnetic radiation simulation for wireless signals. It is shown that the PIRL significantly outperforms both e2e RL and heuristic-based solutions in terms of generalization and performance. Source code is available at https://github.com/Panshark/PIRL-WIN.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611229","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611229","","Wireless communication;Training;Indoor navigation;Source coding;Radio navigation;Task analysis;Physics","","","","47","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"A Point-to-distribution Degeneracy Detection Factor for LiDAR SLAM using Local Geometric Models","S. Ji; W. Chen; Z. Su; Y. Guan; J. Li; H. Zhang; H. Zhu","Biomimetic and Intelligent Robotics Lab (BIRL), School of Electromechanical Engineer, Guangdong University of Technology, Guangzhou, China; Biomimetic and Intelligent Robotics Lab (BIRL), School of Electromechanical Engineer, Guangdong University of Technology, Guangzhou, China; Guangdong Key Laboratory of Modern Control Technology, Institute of Intelligent Manufacturing, Guangdong Academy of Sciences, Guangzhou, China; Biomimetic and Intelligent Robotics Lab (BIRL), School of Electromechanical Engineer, Guangdong University of Technology, Guangzhou, China; College of Engineering, South China Agricultural University, China; Shenzhen Key Laboratory of Robotics and Computer Vision, Southern University of Science and Technology, China; Biomimetic and Intelligent Robotics Lab (BIRL), School of Electromechanical Engineer, Guangdong University of Technology, Guangzhou, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","12283","12289","Limited by the working principles, LiDAR-SLAM systems suffer from the degeneration phenomenon in environments such as long corridors and tunnels, due to the lack of sufficient geometric features for frame-to-frame matching. The accuracy and sensitivity of existing degeneracy detection methods need to be further improved. In this paper, we propose a novel method for degeneracy detection using local geometric models based on point-to-distribution matching. To obtain an accurate description of local geometric models, an adaptive adjustment of voxel segmentation according to the point cloud distribution and density is designed. The codes of the proposed method is open-source and available at https://github.com/jisehua/Degenerate-Detection.git. Experiments with public datasets and self-build robots were conducted to evaluate the methods. The results exhibit that our proposed method achieves higher accuracy than the other existing approaches. Applying our proposed method is beneficial for improving the robustness of the LiDAR-SLAM systems.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610340","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610340","","Point cloud compression;Accuracy;Simultaneous localization and mapping;Sensitivity;Laser radar;Geometric modeling;Noise","","1","","30","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"QuadricsNet: Learning Concise Representation for Geometric Primitives in Point Clouds","J. Wu; H. Yu; W. Yang; G. -S. Xia","School of Computer Science, Wuhan University, Wuhan, China; School of Electronic Information, Wuhan University, Wuhan, China; School of Electronic Information, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","4060","4066","This paper presents a novel framework to learn a concise geometric primitive representation for 3D point clouds. Different from representing each type of primitive individually, we focus on the challenging problem of how to achieve a concise and uniform representation robustly. We employ quadrics to represent diverse primitives with only 10 parameters and propose the first end-to-end learning-based framework, namely QuadricsNet, to parse quadrics in point clouds. The relationships between quadrics mathematical formulation and geometric attributes, including the type, scale and pose, are insightfully integrated for effective supervision of QuaidricsNet. Besides, a novel pattern-comprehensive dataset with quadrics segments and objects is collected for training and evaluation. Experiments demonstrate the effectiveness of our concise representation and the robustness of QuadricsNet. Our code is available at https://github.com/MichaelWu99-lab/QuadricsNet.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610722","Natural Science Foundation of Hubei Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610722","","Point cloud compression;Training;Three-dimensional displays;Codes;Robustness;Robotics and automation","","1","","37","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"TreeScope: An Agricultural Robotics Dataset for LiDAR-Based Mapping of Trees in Forests and Orchards","D. Cheng; F. Cladera; A. Prabhu; X. Liu; A. Zhu; P. C. Green; R. Ehsani; P. Chaudhari; V. Kumar","GRASP Laboratory, University of Pennsylvania, Philadelphia, PA, USA; GRASP Laboratory, University of Pennsylvania, Philadelphia, PA, USA; GRASP Laboratory, University of Pennsylvania, Philadelphia, PA, USA; GRASP Laboratory, University of Pennsylvania, Philadelphia, PA, USA; GRASP Laboratory, University of Pennsylvania, Philadelphia, PA, USA; Forest Resources and Environmental Conservation, Virginia Polytechnic Institute and State University, Blacksburg, VA, USA; Department of Mechanical Engineering, University of California, Merced, CA, USA; GRASP Laboratory, University of Pennsylvania, Philadelphia, PA, USA; GRASP Laboratory, University of Pennsylvania, Philadelphia, PA, USA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","14860","14866","Data collection for forestry, timber, and agriculture relies on manual techniques which are labor-intensive and time-consuming. We seek to demonstrate that robotics offers improvements over these techniques and can accelerate agricultural research, beginning with semantic segmentation and diameter estimation of trees in forests and orchards. We present TreeScope v1.0, the first robotics dataset for precision agriculture and forestry addressing the counting and mapping of trees in forestry and orchards. TreeScope provides LiDAR data from agricultural environments collected with robotics platforms, such as UAV and mobile robot platforms carried by vehicles and human operators. In the first release of this dataset, we provide ground-truth data with over 1,800 manually annotated semantic labels for tree stems and field-measured tree diameters. We share benchmark scripts for these tasks that researchers may use to evaluate the accuracy of their algorithms. Finally, we run our open-source diameter estimation and off-the-shelf semantic segmentation algorithms and share our baseline results.The dataset can be found at https://treescope.org, and the data pre-processing and benchmark code is available at https://github.com/KumarRobotics/treescope.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611103","","Precision agriculture;Forests;Laser radar;Semantic segmentation;Semantics;Estimation;Manuals","","3","","33","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"WLST: Weak Labels Guided Self-training for Weakly-supervised Domain Adaptation on 3D Object Detection","T. -L. Tsou; T. -H. Wu; W. H. Hsu",National Taiwan University; National Taiwan University; National Taiwan University,2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","18214","18220","In the field of domain adaptation (DA) on 3D object detection, most of the work is dedicated to unsupervised domain adaptation (UDA). Yet, without any target annotations, the performance gap between the UDA approaches and the fully-supervised approach is still noticeable, which is impractical for real-world applications. On the other hand, weakly-supervised domain adaptation (WDA) is an underexplored yet practical task that only requires few labeling effort on the target domain. To improve the DA performance in a cost-effective way, we propose a general weak labels guided self-training framework, WLST, designed for WDA on 3D object detection. By incorporating autolabeler, which can generate 3D pseudo labels from 2D bounding boxes, into the existing self-training pipeline, our method is able to generate more robust and consistent pseudo labels that would benefit the training process on the target domain. Extensive experiments demonstrate the effectiveness, robustness, and detector-agnosticism of our WLST framework. Notably, it outperforms previous state-of-the-art methods on all evaluation tasks. Code and models are available at https://github.com/jacky121298/WLST. Note that the complete version with appendix is available on arXiv.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610801","National Science and Technology Council; Qualcomm; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610801","","Training;Three-dimensional displays;Codes;Annotations;Pipelines;Object detection;Robustness","","","","30","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"WeatherDepth: Curriculum Contrastive Learning for Self-Supervised Depth Estimation under Adverse Weather Conditions","J. Wang; C. Lin; L. Nie; S. Huang; Y. Zhao; X. Pan; R. Ai","Institute of Information Science, Beijing Jiaotong University, Beijing, China; Institute of Information Science, Beijing Jiaotong University, Beijing, China; Institute of Information Science, Beijing Jiaotong University, Beijing, China; Institute of Information Science, Beijing Jiaotong University, Beijing, China; Institute of Information Science, Beijing Jiaotong University, Beijing, China; Institute of Information Science, Beijing Jiaotong University, Beijing, China; Institute of Information Science, Beijing Jiaotong University, Beijing, China",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","4976","4982","Depth estimation models have shown promising performance on clear scenes but fail to generalize to adverse weather conditions due to illumination variations, weather particles, etc. In this paper, we propose WeatherDepth, a self-supervised robust depth estimation model with curriculum contrastive learning, to tackle performance degradation in complex weather conditions. Concretely, we first present a progressive curriculum learning scheme with three simple-to-complex curricula to gradually adapt the model from clear to relative adverse, and then to adverse weather scenes. It encourages the model to gradually grasp beneficial depth cues against the weather effect, yielding smoother and better domain adaption. Meanwhile, to prevent the model from forgetting previous curricula, we integrate contrastive learning into different curricula. By drawing reference knowledge from the previous course, our strategy establishes a depth consistency constraint between different courses toward robust depth estimation in diverse weather. Besides, to reduce manual intervention and better adapt to different models, we designed an adaptive curriculum scheduler to automatically search for the best timing for course switching. In the experiment, the proposed solution is proven to be easily incorporated into various architectures and demonstrates state-of-the-art (SoTA) performance on both synthetic and real weather datasets. Source code and data are available at https://github.com/wangjiyuan9/WeatherDepth.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611100","","Degradation;Adaptation models;Source coding;Estimation;Contrastive learning;Switches;Manuals","","1","","32","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Towards Unified Interactive Visual Grounding in The Wild","J. Xu; H. Zhang; Q. Si; Y. Li; X. Lan; T. Kong",Xi’an Jiaotong University; ByteDance Research; ByteDance Research; ByteDance Research; Xi’an Jiaotong University; ByteDance Research,2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","3288","3295","Interactive visual grounding in Human-Robot Interaction (HRI) is challenging yet practical due to the inevitable ambiguity in natural languages. It requires robots to disambiguate the user’s input by active information gathering. Previous approaches often rely on predefined templates to ask disambiguation questions, resulting in performance reduction in realistic interactive scenarios. In this paper, we propose TiO, an end-to-end system for interactive visual grounding in human-robot interaction. Benefiting from a unified formulation of visual dialog and grounding, our method can be trained on a joint of extensive public data, and show superior generality to diversified and challenging open-world scenarios. In the experiments, we validate TiO on GuessWhat?! and InViG benchmarks, setting new state-of-the-art performance by a clear margin. Moreover, we conduct HRI experiments on the carefully selected 150 challenging scenes as well as real-robot platforms. Results show that our method demonstrates superior generality to diversified visual and language inputs with a high success rate. Codes and demos are available on https://jxu124.github.io/TiO/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10611354","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611354","","Visualization;Grounding;Natural languages;Human-robot interaction;Training data;Benchmark testing;Transformers","","","","59","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Extreme Parkour with Legged Robots","X. Cheng; K. Shi; A. Agarwal; D. Pathak",Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University,2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","11443","11450","Humans can perform parkour by traversing obstacles in a highly dynamic fashion requiring precise eye-muscle coordination and movement. Getting robots to do the same task requires overcoming similar challenges. Classically, this is done by independently engineering perception, actuation, and control systems to very low tolerances. This restricts them to tightly controlled settings such as a predetermined obstacle course in labs. In contrast, humans are able to learn parkour through practice without significantly changing their underlying biology. In this paper, we take a similar approach to developing robot parkour on a small low-cost robot with imprecise actuation and a single front-facing depth camera for perception which is low-frequency, jittery, and prone to artifacts. We show how a single neural net policy operating directly from a camera image, trained in simulation with large-scale RL, can overcome imprecise sensing and actuation to output highly precise control behavior end-to-end. We show our robot can perform a high jump on obstacles 2x its height, long jump across gaps 2x its length, do a handstand and run across tilted ramps, and generalize to novel obstacle courses with different physical properties. Parkour videos at https: //extreme-parkour.github.io/.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610200","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610200","","Legged locomotion;Robot kinematics;Robot vision systems;Neural networks;Cameras;Control systems;Biology","","27","","47","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Physical and Digital Adversarial Attacks on Grasp Quality Networks","N. W. Alharthi; M. Brandão","King’s College, London, UK; King’s College, London, UK",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","1907","1902","Grasp Quality Networks are important components of grasping-capable autonomous robots, as they allow them to evaluate grasp candidates and select the one with highest chance of success. The widespread use of pick-and-place robots and Grasp Quality Networks raises the question of whether such systems are vulnerable to adversarial attacks, as that could lead to large economic damage. In this paper we propose two kinds of attacks on Grasp Quality Networks, one assuming physical access to the workspace (to place or attach a new object) and another assuming digital access to the camera software (to inject a pixel-intensity change on a single pixel). We then use evolutionary optimization to obtain attacks that simultaneously minimize the noticeability of the attacks and the chance that selected grasps are successful. Our experiments show that both kinds of attack lead to drastic drops in algorithm performance, thus making them important attacks to consider in the cybersecurity of grasping robots. Source code can be found at https://github.com/Naif-W-Alharthi/Physical-and-Digital-Attacks-on-Grasping-Networks","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10610886","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610886","","Economics;Prevention and mitigation;Source coding;Software algorithms;Grasping;Software;Protection","","1","","22","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"An Open-source Sim2Real Approach for Sensor-independent Robot Navigation in a Grid","M. M. Abrar; S. Mondal; M. Hickner","Department of Mechanical Engineering, University of Washington, Seattle, WA, USA; Department of Mechanical Engineering, University of Washington, Seattle, WA, USA; Department of Mechanical Engineering, University of Washington, Seattle, WA, USA",2024 9th International Conference on Robotics and Automation Engineering (ICRAE),"28 Jan 2025","2024","","","106","111","This paper presents a Sim2Real (Simulation to Reality) approach to bridge the gap between a trained agent in a simulated environment and its real-world implementation in navigating a robot in a similar setting. Specifically, we focus on navigating a quadruped robot in a real-world grid-like environment inspired by the Gymnasium Frozen Lake — a highly user-friendly and free Application Programming Interface (API) to develop and test Reinforcement Learning (RL) algorithms. We detail the development of a pipeline to transfer motion policies learned in the Frozen Lake simulation to a physical quadruped robot, thus enabling autonomous navigation and obstacle avoidance in a grid without relying on expensive localization and mapping sensors. The work involves training an RL agent in the Frozen Lake environment and utilizing the resulting Q-table to control a 12 Degrees-of-Freedom (DOF) quadruped robot. In addition to detailing the RL implementation, inverse kinematics-based quadruped gaits, and the transfer policy pipeline, we open-source the project on GitHub and include a demonstration video of our Sim2Real transfer approach. This work provides an accessible, straightforward, and low-cost framework for researchers, students, and hobbyists to explore and implement RL-based robot navigation in real-world grid environments.","","979-8-3315-1830-1","10.1109/ICRAE64368.2024.10851595","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10851595","Sim2Real;Quadruped;Robotics;Sensor-independent Navigation;Grid Navigation;Gymnasium;Q-Learning;Reinforcement Learning;Arduino;Policy Transfer","Training;Navigation;Pipelines;Lakes;Robot sensing systems;Sensors;Quadrupedal robots;Robots;Application programming interfaces;Software development management","","","","20","IEEE","28 Jan 2025","","","IEEE","IEEE Conferences"
"Efficient Autonomous Exploration with Dueling DDQN Enhancing Active SLAM through Reinforcement Learning","X. Li; K. Leong; C. K. Seow; N. Pugeault; Q. Cao","School of Computing Science, University of Glasgow, Glasgow, United Kingdom; School of Computing and Information Systems, Singapore Management University Singapore, Singapore; School of Computing Science, University of Glasgow, Glasgow, United Kingdom; School of Computing Science, University of Glasgow, Glasgow, United Kingdom; School of Computing Science, University of Glasgow, Glasgow, United Kingdom",2024 9th International Conference on Robotics and Automation Engineering (ICRAE),"28 Jan 2025","2024","","","123","130","Active Simultaneous Localization and Mapping (SLAM) is a critical problem in autonomous robotics, enabling robots to navigate to new regions while building an accurate model of their surroundings. This paper presents a novel Active SLAM framework enhanced by Reinforcement Learning (RL) techniques to address challenges in autonomous navigation within unknown environments. The proposed system introduces two key innovations: (1) the integration of Dueling Double Deep Q-Networks (Dueling DDQN) to dynamically select frontiers for exploration based on real-time environmental feedback, and (2) a multi-component reward function that optimally balances exploration and exploitation by incorporating factors such as distance, information gain, and revisitation penalties. These innovations result in a significant performance gain, with the Dueling DDQN method demonstrating up to a 45 percentage points improvement in map completion rates and a 35 percentage points reduction in exploration time compared to traditional SLAM and standard RL approaches. Additionally, a novel evaluation method is introduced, using image-based comparison techniques to assess performance in environments without ground truth data. Experimental results in simulated environments validate the system’s ability to enhance both mapping accuracy and computational efficiency, demonstrating the potential of RL to advance autonomous robotic exploration and mapping. The implementation and source code for this work can be found at https://github.com/LiXin0123/ExplORB-SLAM-Dueling-DDQN.git, providing a comprehensive resource for further experimentation and validation.","","979-8-3315-1830-1","10.1109/ICRAE64368.2024.10851611","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10851611","frontier exploration;reinforcement learning;active SLAM;ORBSLAM;visual-graph SLAM","Technological innovation;Simultaneous localization and mapping;Accuracy;Source coding;Scalability;Reinforcement learning;Performance gain;Transformers;Real-time systems;Standards","","","","18","IEEE","28 Jan 2025","","","IEEE","IEEE Conferences"
