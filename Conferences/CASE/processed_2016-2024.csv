id,citekey,title,authors,year,month,journal,book_title,publisher,institution,volume,number,pages,note,keywords,url,code,pdf,image,thumbnail,doi,external,abstract,isbn,type_id
689.0,,Visual Imitation Learning of Non-Prehensile Manipulation Tasks with Dynamics-Supervised Models,A. Mustafa; R. Hanai; I. G. Ramirez-Alpizar; F. Erich; R. Nakajo; Y. Domae; T. Ogata,2024,,,2024 IEEE 20th International Conference on Automation Science and Engineering (CASE),IEEE,"National Institute of Advanced Industrial Science and Technology (AIST), Japan; National Institute of Advanced Industrial Science and Technology (AIST), Japan; National Institute of Advanced Industrial Science and Technology (AIST), Japan; National Institute of Advanced Industrial Science and Technology (AIST), Japan; National Institute of Advanced Industrial Science and Technology (AIST), Japan; National Institute of Advanced Industrial Science and Technology (AIST), Japan; Graduate School of Fundamental Science and Engineering, Waseda University, Tokyo, Japan",0,0,,,Training;Visualization;Accuracy;Imitation learning;Computer architecture;Predictive models;Feature extraction;Proposals;Manipulator dynamics;Image reconstruction,,https://github.com/Automation-Research-Team/DynamicsMapping-2D,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10711321,,,10.1109/CASE59546.2024.10711321,False,"Unlike quasi-static robotic manipulation tasks like pick-and-place, dynamic tasks such as non-prehensile manipulation pose greater challenges, especially for vision-based control. Successful control requires the extraction of features relevant to the target task. In visual imitation learning settings, these features can be learnt by backpropagating the policy loss through the vision backbone. Yet, this approach tends to learn task-specific features with limited generalizability. Alternatively, learning world models can realize more generalizable vision backbones. Utilizing the learnt features, task-specific policies are subsequently trained. Commonly, these models are trained solely to predict the next RGB state from the current state and action taken. But only-RGB prediction might not fully-capture the task-relevant dynamics. In this work, we hypothesize that direct supervision of target dynamic states (Dynamics Mapping) can learn better dynamics-informed world models. Beside the next RGB reconstruction, the world model is also trained to directly predict position, velocity, and acceleration of environment rigid bodies. To verify our hypothesis, we designed a non-prehensile 2D environment tailored to two tasks: ""Balance-Reaching"" and ""Bin-Dropping"". When trained on the first task, dynamics mapping enhanced the task performance under different training configurations (Decoupled, Joint, End-to-End) and policy architectures (Feedforward, Recurrent). Notably, its most significant impact was for world model pretraining boosting the success rate from 21% to 85%. Although frozen dynamics-informed world models could generalize well to a task with in-domain dynamics, but poorly to a one with out-of-domain dynamics. Code available at: https://github.com/Automation-Research-Team/DynamicsMapping-2D",979-8-3503-5851-3,2
690.0,,DMG6D: A Depth-based Multi-Flow Global Feature Fusion Network for 6D Pose Estimation,Z. Wang; Q. Zhang; X. Sun; J. Zhu; H. Wei,2024,,,2024 IEEE 20th International Conference on Automation Science and Engineering (CASE),IEEE,"College of Automation, Jiangsu University of Science and Technology, Zhenjiang, China; College of Automation, Jiangsu University of Science and Technology, Zhenjiang, China; College of Automation, Jiangsu University of Science and Technology, Zhenjiang, China; College of Automation, Jiangsu University of Science and Technology, Zhenjiang, China; Ocean College, Zhejiang University, Zhoushan, China",0,0,,,Point cloud compression;Geometry;Visualization;Accuracy;Pose estimation;Feature extraction;Robot sensing systems;Reliability;Data mining;Visual perception,,https://github.com/wangzihanggg/DMG6D,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10711760,,,10.1109/CASE59546.2024.10711760,False,"The accurate estimation of the 6-degree-of-freedom pose of a target object in the environment plays a pivotal role in robot perception, providing a foundation for interaction and manipulation between robots and the surrounding environment. Nonetheless, traditional vision sensors are prone to diminished reliability in visual perception due to environmental factors such as lighting conditions and occlusions. Depth sensors, such as Time-of-Flight (TOF) and structured light sensors, offer promising opportunities for reliable target pose estimation. However, accurately determining the pose solely based on a single depth image presents significant challenges due to the limited availability of rich appearance and texture information. To comprehensively address this challenge, we investigate the mechanism of feature extraction and representation using depth images, along with the utilization of normal angle and point cloud information derived from the depth images, to achieve robust estimation of the visual target poses. By exploiting the latent information within the depth images, including normal angles and point clouds, we have developed the DMG6D robust target pose estimation framework. Within the DMG6D framework, we first employ physical methods to infer the normal angle and spatial position of each pixel in the depth image. Subsequently, we introduce a three-branch feature extraction and a global feature fusion network to enable a comprehensive depiction of the target object. Finally, a robust pose estimation for the target object is obtained utilizing the least squares method. Experimental results emphatically demonstrate that the proposed DMG6D surpasses existing algorithms in terms of its ability to estimate 6D poses using depth images, effectively underscoring the efficacy of our designed depth image feature extraction strategy. Access to the code and video is available at https://github.com/wangzihanggg/DMG6D.",979-8-3503-5851-3,2
691.0,,An Examination of Offline-Trained Encoders in Vision-Based Deep Reinforcement Learning for Autonomous Driving,S. Mohammed; A. Argun; N. Bonnotte; G. Ascheid,2024,,,2024 IEEE 20th International Conference on Automation Science and Engineering (CASE),IEEE,"Research Division, Akkodis Germany Solutions GmbH, Sindelfingen; Department of Electrical Engineering, Institute for Communication Technologies and Embedded Systems (ICE), RWTH Aachen University, Aachen, Germany; Research Division, Akkodis Germany Solutions GmbH, Sindelfingen; Department of Electrical Engineering, Institute for Communication Technologies and Embedded Systems (ICE), RWTH Aachen University, Aachen, Germany",0,0,,,Training;Representation learning;Scalability;Zero shot learning;Contrastive learning;Deep reinforcement learning;Transformers;Spatiotemporal phenomena;Collision avoidance;Autonomous vehicles,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10711548,,,10.1109/CASE59546.2024.10711548,False,"Our research investigates the challenges Deep Reinforcement Learning (DRL) faces in complex, Partially Observable Markov Decision Processes (POMDP) such as autonomous driving (AD), and proposes a solution for vision-based navigation in these environments. Partial observability reduces RL performance significantly, and this can be mitigated by augmenting sensor information and data fusion to reflect a more Markovian environment. However, this necessitates an increasingly complex perception module, whose training via RL is complicated due to inherent limitations. As the neural network architecture becomes more complex, the reward function’s effectiveness as an error signal diminishes since the only source of supervision is the reward, which is often noisy, sparse, and delayed. Task-irrelevant elements in images, such as the sky or certain objects, pose additional complexities. Our research adopts an offline-trained encoder to leverage large video datasets through self-supervised learning to learn generalizable representations. Then, we train a head network on top of these representations through DRL to learn to control an ego vehicle in the CARLA AD simulator. This study presents a broad investigation of the impact of different learning schemes for offline-training of encoders on the performance of DRL agents in challenging AD tasks. Furthermore, we show that the features learned by watching BDD100K driving videos can be directly transferred to achieve lane following and collision avoidance in CARLA simulator, in a zero-shot learning fashion. Finally, we explore the impact of various architectural decisions for the RL networks to utilize the transferred representations efficiently. Therefore, in this work, we introduce and validate an optimal way for obtaining suitable representations of the environment, and transferring them to RL networks. We experimentally demonstrate the correlation between RL performance in the CARLA simulator and the quality of representations received by the RL agent. These results highlight the crucial role that feature extraction and state representation play in RL.",979-8-3503-5851-3,2
692.0,,DuHa: a dual-hand action segmentation method for human-robot collaborative assembly,H. Zheng; R. Lee; Y. Lu; X. Xu,2024,,,2024 IEEE 20th International Conference on Automation Science and Engineering (CASE),IEEE,"Department of Mechanical and Mechatronics Engineering, The University of Auckland, New Zealand; Department of Mechanical and Mechatronics Engineering, The University of Auckland, New Zealand; Department of Mechanical and Mechatronics Engineering, The University of Auckland, New Zealand; Department of Mechanical and Mechatronics Engineering, The University of Auckland, New Zealand",0,0,,,Accuracy;Motion segmentation;Taxonomy;Collaboration;Human-robot interaction;Feature extraction;Real-time systems;Timing;Assembly;Robots,,https://github.com/LISMS-AKL-NZ/DuHa,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10711776,,,10.1109/CASE59546.2024.10711776,False,"Human-robot collaboration (HRC) is crucial for enabling mass personalised manufacturing and human-centric manufacturing. The recent advancements in video understanding technology have enabled robots to interpret human actions from videos and discern the appropriate timing and nature of required robot assistance. However, current vision-based HRC systems mainly focus on capturing human overall actions, neglecting the detailed actions of each hand. This restricts robots from providing finer attentive assistance, such as delivering the required tools to a specific hand. Furthermore, these systems predominantly utilise classification-based action recognition techniques rather than reasoning-involved action segmentation techniques, thereby constraining their practical application. To address above limitations, this paper proposes a vision-based dual-hand action segmentation method for human-robot collaborative assembly (HRCA) scenarios. Initially, graph neural networks are used to capture hand-object interactions for both hands across frames, then these features are integrated with scene features, and finally, temporal convolutional networks are applied to capture temporal dynamics of the integrated features over frames. To better serve the application needs of HRCA, we have refined the action segmentation to atomic action level according to the Human-Robot Shared Assembly Taxonomy, facilitating comprehensive collaboration. More details can be found at https://github.com/LISMS-AKL-NZ/DuHa",979-8-3503-5851-3,2
693.0,,"Multimodal Human Detection using RGB, Thermal and LiDAR modalities for Robotic Perception",K. O. S. Mota; L. Garrote; C. Premebida,2024,,,2024 IEEE 20th International Conference on Automation Science and Engineering (CASE),IEEE,"Department of Electrical and Computer Engineering, Institute of Systems and Robotics, University of Coimbra, Portugal; Department of Electrical and Computer Engineering, Institute of Systems and Robotics, University of Coimbra, Portugal; Department of Electrical and Computer Engineering, Institute of Systems and Robotics, University of Coimbra, Portugal",0,0,,,YOLO;Laser radar;Machine learning algorithms;Surveillance;Soft sensors;Thermal engineering;Robot vision systems;Security;Mobile robots;Software development management,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10711565,,,10.1109/CASE59546.2024.10711565,False,"People detection is a relevant research topic in artificial perception with wide range of applications from security, surveillance, robotics to autonomous driving. Overcoming challenges in this field involves advanced algorithms, combination of machine learning approaches, as well as the use of sensory data e.g., from cameras and LiDARs. This work addresses the problem of people detection using YOLO, a state-of-the-art object detection method, trained on three distinct data sources LiDAR, RGB (color) and ‘thermal’ (long-wave infra-red) images. The rationale for combining multiple-sensory representation relies on the assumption that each sensor has its own advantages and disadvantages, but together they normally complement each other - specially in real-world conditions. LiDAR contributes to a physically-interpretable mapping of the environment, providing precise information regarding size/dimension and location of the objects, while RGB and thermal provide relevant textural features. The sensors have been calibrated w.r.t. each other thus, allowing the LiDAR’s point-clouds to be projected into the image plane, followed by an up-sampling step, to create dense-depth maps (DM) that enable direct use of the YOLO framework. To support the experiments, a new multi-sensory dataset has been collected using a mobile robot. Besides single-modality models, this paper also explores early and late-fusion strategies. Finally, the new dataset has been made available in a Github repository 1.",979-8-3503-5851-3,2
694.0,,Automating Deformable Gasket Assembly,S. Adebola; T. Sadjadpour; K. El-Refai; W. Panitch; Z. Ma; R. Lin; T. Qiu; S. Ganti; C. Le; J. Drake; K. Goldberg,2024,,,2024 IEEE 20th International Conference on Automation Science and Engineering (CASE),IEEE,"The AUTOLab, UC Berkeley; The AUTOLab, UC Berkeley; The AUTOLab, UC Berkeley; The AUTOLab, UC Berkeley; The AUTOLab, UC Berkeley; The AUTOLab, UC Berkeley; The AUTOLab, UC Berkeley; The AUTOLab, UC Berkeley; The AUTOLab, UC Berkeley; The AUTOLab, UC Berkeley; The AUTOLab, UC Berkeley",0,0,,,Solid modeling;Computer aided software engineering;Codes;Imitation learning;Gaskets;Surface fitting;Data models;Automobiles;Assembly;Videos,,https://github.com/BerkeleyAutomation/gasket-assembly,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10711836,,,10.1109/CASE59546.2024.10711836,False,"In Gasket Assembly, a deformable gasket must be aligned and pressed into a narrow channel. This task is common for sealing surfaces in the manufacturing of automobiles, appliances, electronics, and other products. Gasket Assembly is a long-horizon, high-precision task and the gasket must align with the channel and be fully pressed in to achieve a secure fit. To compare approaches, we present 4 methods for Gasket Assembly: one policy from deep imitation learning and three procedural algorithms. We evaluate these methods with 100 physical trials. Results suggest that the Binary+ algorithm succeeds in 10/10 on the straight channel whereas the learned policy based on 250 human teleoperated demonstrations succeeds in 8/10 trials and is significantly slower. Code, CAD models, videos, and data can be found at https://berkeleyautomation.github.io/robot-gasket/.",979-8-3503-5851-3,2
695.0,,Conquering the Robotic Software Development Cycle at Scale: Using KubeROS from Simulation to Real-world Deployment,Y. Zhang; F. Pasch; F. Mirus; K. -U. Scholl; C. Wurll; B. Hein,2024,,,2024 IEEE 20th International Conference on Automation Science and Engineering (CASE),IEEE,"Karlsruhe University of Applied Sciences, Karlsruhe, Germany; Intel Labs, Karlsruhe, Germany; Intel Labs, Karlsruhe, Germany; Intel Labs, Karlsruhe, Germany; Karlsruhe University of Applied Sciences, Karlsruhe, Germany; Karlsruhe University of Applied Sciences, Karlsruhe, Germany",0,0,,,Training;Location awareness;Navigation;Scalability;Microservice architectures;Full stack;Production;Software;Mobile robots;Software development management,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10711591,,,10.1109/CASE59546.2024.10711591,False,"Integrating state-of-the-art robotics algorithms into a working system remains a challenge, especially for large-scale deployments. One promising way to address such issues is to apply to robotics the massive cloud and edge computing that already powers AI research from training to deployment. In this paper, we extend the KubeROS platform to serve as a one-stop solution aimed at facilitating robotic development cycles from research to deployment. The closed-loop workflow maintains a single software stack containing many microservice-oriented and containerized software modules across all stages. In addition to the architectural adaptation, we develop a new subsystem called BatchJob to leverage cloud/edge to efficiently run experiments at scale and evaluate the system with statistical significance. Finally, our approach enables a seamless transition to production and closes the cycle for continuous improvement. To demonstrate its applicability and scalability, we choose mobile robot navigation as an example and perform experiments on three levels: comparison of localization approaches with benchmark datasets, testing and evaluation with closed-loop simulation, and evaluation on a real robot. Compared to sequential execution on a single machine, our approach accelerates experiments by at least a factor of 30 on a 9-node edge cluster. Our platform and code to reproduce the experiments are available on Github.1",979-8-3503-5851-3,2
696.0,,Simple But Effective: Rethinking the Ability of Deep Learning in fNIRS to Exclude Abnormal Input,Z. Cao,2024,,,2024 IEEE 20th International Conference on Automation Science and Engineering (CASE),IEEE,"Department of Mathematics, ETH, Zurich, Switzerland",0,0,,,Deep learning;Training;Measurement;Computer aided software engineering;Transformers;Data models;Reliability;Functional near-infrared spectroscopy;Monitoring;Testing,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10711774,,,10.1109/CASE59546.2024.10711774,False,"Functional near-infrared spectroscopy (fNIRS) is a non-invasive technique for monitoring brain activity. To better understand the brain, researchers often use deep learning to address the classification challenges of fNIRS data. Our study shows that while current networks in fNIRS are highly accurate for predictions within their training distribution, they falter at identifying and excluding abnormal data which is out-of-distribution, affecting their reliability. We propose integrating metric learning and supervised methods into fNIRS research to improve networks capability in identifying and excluding out-of-distribution outliers. This method is simple yet effective. In our experiments, it significantly enhanced the performance of various networks in fNIRS, particularly transformer-based one, which shows the great improvement in reliability. We will make our experiment data available on GitHub‡.",979-8-3503-5851-3,2
697.0,,Region of Interest Loss for Anonymizing Learned Image Compression,C. Liebender; R. Bezerra; K. Ohno; S. Tadokoro,2024,,,2024 IEEE 20th International Conference on Automation Science and Engineering (CASE),IEEE,"Graduate School of Information Sciences, Tohoku University, Japan; Graduate School of Information Sciences, Tohoku University, Japan; Graduate School of Information Sciences, Tohoku University, Japan; Graduate School of Information Sciences, Tohoku University, Japan",0,0,,,Training;Data privacy;Image coding;Surveillance;Transform coding;Transforms;Cameras;Information filtering;Faces;Information integrity,,https://github.com/BRN-Hub/anon-compression,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10711721,,,10.1109/CASE59546.2024.10711721,False,"The use of AI in public spaces continually raises concerns about privacy and the protection of sensitive data. An example is the deployment of detection and recognition methods on humans, where images are provided by surveillance cameras. This results in the acquisition of great amounts of sensitive data, since the capture and transmission of images taken by such cameras happens unaltered, for them to be received by a server on the network. However, many applications do not explicitly require the identity of a given person in a scene; An anonymized representation containing information of the person’s position while preserving the context of them in the scene suffices. We show how using a customized loss function on region of interests (ROI) can achieve sufficient anonymization such that human faces become unrecognizable while persons are kept detectable, by training an end-to-end optimized autoencoder for learned image compression that utilizes the flexibility of the learned analysis and reconstruction transforms for the task of mutating parts of the compression result. This approach enables compression and anonymization in one step on the capture device, instead of transmitting sensitive, nonanonymized data over the network. Additionally, we evaluate how this anonymization impacts the average precision of pre-trained foundation models on detecting faces (MTCNN) and humans (YOLOv8) in comparison to non-ANN based methods, while considering compression rate and latency. The code and model weights for this approach, including usage examples are publicly available on GitHub: https://github.com/BRN-Hub/anon-compression",979-8-3503-5851-3,2
698.0,,6D Assembly Pose Estimation by Point Cloud Registration for Robotic Manipulation,K. Samarawickrama; G. Sharma; A. Angleraud; R. Pieters,2024,,,2024 IEEE 20th International Conference on Automation Science and Engineering (CASE),IEEE,"Automation Technology and Mechanical Engineering, Tampere University, Tampere, Finland; Automation Technology and Mechanical Engineering, Tampere University, Tampere, Finland; Automation Technology and Mechanical Engineering, Tampere University, Tampere, Finland; Automation Technology and Mechanical Engineering, Tampere University, Tampere, Finland",0,0,,,Point cloud compression;Measurement;Solid modeling;Three-dimensional displays;Semantic segmentation;Source coding;Pose estimation;Transforms;Assembly;Synthetic data,,https://github.com/KulunuOS/6DAPose,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10711374,,,10.1109/CASE59546.2024.10711374,False,"The demands on robotic manipulation skills to perform challenging tasks have drastically increased in recent times. To perform these tasks with dexterity, robots require perception tools to understand the scene and extract useful information that transforms to robot control inputs. To this end, recent research has introduced various object pose estimation and grasp pose detection methods that yield precise results. Assembly pose estimation is a secondary yet highly desirable skill in robotic assembling as it requires more detailed information on object placement as compared to bin picking and pick-and-place tasks. However, it has been often overlooked in research due to the complexity of integration in an agile framework. To address this issue, we propose an assembly pose estimation method with RGB-D input and 3D CAD models of the associated objects. The framework consists of semantic segmentation of the scene and registering point clouds of local surfaces against target point clouds derived from CAD models to estimate 6D poses. We show that our method can deliver sufficient accuracy for assembling object assemblies using evaluation metrics and demonstrations. The source code and dataset for the work can be found at: https://github.com/KulunuOS/6DAPose",979-8-3503-5851-3,2
699.0,,GraspTrack: Object and Grasp Pose Tracking for Arbitrary Objects,B. Stephan; S. B. Fischedick; D. Seichter; D. Aganian; H. -M. Gross,2024,,,2024 IEEE 20th International Conference on Automation Science and Engineering (CASE),IEEE,"Neuroinformatics and Cognitive Robotics Lab, Technische Universität Ilmenau, Ilmenau, Germany; Neuroinformatics and Cognitive Robotics Lab, Technische Universität Ilmenau, Ilmenau, Germany; Neuroinformatics and Cognitive Robotics Lab, Technische Universität Ilmenau, Ilmenau, Germany; Neuroinformatics and Cognitive Robotics Lab, Technische Universität Ilmenau, Ilmenau, Germany; Neuroinformatics and Cognitive Robotics Lab, Technische Universität Ilmenau, Ilmenau, Germany",0,0,,,Instance segmentation;Point cloud compression;Target tracking;Service robots;Shape;Pipelines;Pose estimation;Grasping;Robustness;Planning,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10711504,,,10.1109/CASE59546.2024.10711504,False,"A necessary skill for robots for human-robot-collaboration in Industry 4.0 scenarios is the manipulation of objects. To manipulate objects, the robot must estimate grasp poses autonomously. Object-agnostic approaches are often realized through frame-based deep-learning methods that take a single frame of information (image or point cloud) as input while discarding previous estimates, even if they were verified by execution. In addition, object-agnostic grasp estimation methods lack the ability to target specific objects in cluttered scenes. To address this, we propose GraspTrack – a class-agnostic pipeline that uses instance segmentation and object pose estimation to track estimated grasps for each object simultaneously. To evaluate our pipeline and its modules, we perform extensive experiments on the GraspNet-1Billion dataset and extend its evaluation to measure grasp quality for each object, capturing the ability to grasp targeted objects more effectively. Our experiments prove our pipeline to be robust against difficult viewing angles and occlusions, outperforming frame-based grasp pose estimation.",979-8-3503-5851-3,2
700.0,,Explainable Online Unsupervised Anomaly Detection for Cyber-Physical Systems via Causal Discovery from Time Series*,D. Meli,2024,,,2024 IEEE 20th International Conference on Automation Science and Engineering (CASE),IEEE,"Department of Computer Science, University of Verona, Italy",0,0,,,Training;Service robots;Time series analysis;Training data;Cyber-physical systems;Benchmark testing;Robot sensing systems;Real-time systems;Safety;Anomaly detection,,https://github.com/Isla-lab/causal_anomaly_detection,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10711445,,,10.1109/CASE59546.2024.10711445,False,"Online unsupervised detection of anomalies is crucial to guarantee the correct operation of cyber-physical systems and the safety of humans interacting with them. State-of-the-art approaches based on deep learning via neural networks achieve outstanding performance at anomaly recognition, evaluating the discrepancy between a normal model of the system (with no anomalies) and the real-time stream of sensor time series. However, large training data and time are typically required, and explainability is still a challenge to identify the root of the anomaly and implement predictive maintainance. In this paper, we use causal discovery to learn a normal causal graph of the system, and we evaluate the persistency of causal links during real-time acquisition of sensor data to promptly detect anomalies. On two benchmark anomaly detection datasets, we show that our method has higher training efficiency, outperforms the accuracy of state-of-the-art neural architectures and correctly identifies the sources of > 10 different anomalies. The code is at https://github.com/Isla-lab/causal_anomaly_detection.",979-8-3503-5851-3,2
701.0,,Enhancing Autonomous Robot Safety through Localization Performance Analysis,U. S. Bingöl; A. Hacınecipoğlu; M. M. Ankaralı,2024,,,2024 IEEE 20th International Conference on Automation Science and Engineering (CASE),IEEE,"Department of Electrical and Electronics Engineering, Middle East Technical University, Ankara, Turkey; Milvus Robotics, Ankara, Turkey; Department of Electrical and Electronics Engineering, Middle East Technical University, Ankara, Turkey",0,0,,,Location awareness;Measurement;Automation;Accuracy;Service robots;Predictive models;Feature extraction;Prediction algorithms;Safety;Logistics,,https://github.com/ulassbin/localization_performance_analyser,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10711419,,,10.1109/CASE59546.2024.10711419,False,"Autonomous robots play a vital role in various applications, including industrial automation and logistics. Guaranteeing their safe and dependable operation is of utmost importance, particularly in human-robot collaborative scenarios. While existing safety standards address numerous aspects of autonomous systems, the persisting challenges linked to localization issues demand attention. Accurate localization is paramount for safety in autonomous robotic systems due to its direct impact on preventing accidents, mitigating risks, and ensuring human well-being. In scenarios where robots operate alongside humans, such as collaborative workspaces or shared environments, precise localization is crucial to avoid collisions and potential harm to individuals. Furthermore, robust localization enhances system dependability by minimizing errors and ensuring consistent performance, vital for industrial automation and logistics where disruptions can lead to costly downtime. To proactively manage these risks and ensure uninterrupted operations, this paper introduces a novel solution: the Localization Performance Analyzer (LPA). We present an architecture for estimating localization performance and a set of innovative features that serve as predictors of localization quality. Our findings indicate that implementing the LPA algorithm in a fleet of robots leads to more accurate estimations of localization performance. Our results from ablation studies demonstrate that integrating the LPA algorithm into a fleet of robots significantly enhances the accuracy of localization performance estimation. The predictor model and trainer is available at https://github.com/ulassbin/localization_performance_analyser.",979-8-3503-5851-3,2
702.0,,Automatic Robot Path Planning for Active Visual Inspection on Free-Form Surfaces,O. Tasneem; R. Pieters,2024,,,2024 IEEE 20th International Conference on Automation Science and Engineering (CASE),IEEE,"Cognitive Robotics Group, Unit of Automation Technology and Mechanical Engineering, Tampere University, Tampere, Finland; Cognitive Robotics Group, Unit of Automation Technology and Mechanical Engineering, Tampere University, Tampere, Finland",0,0,,,Point cloud compression;Training;Visualization;Shape;Robot vision systems;Training data;Inspection;Cameras;Path planning;Hardware,,https://github.com/CuriousLad1000/Auto-Path-Planner,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10711320,,,10.1109/CASE59546.2024.10711320,False,"Visual inspection is a crucial yet time-consuming task across various industries. Numerous established methods employ machine learning in inspection tasks, necessitating specific training data that includes predefined inspection poses and training images essential for the training of models. The acquisition of such data and their integration into an inspection framework is challenging due to the variety in objects and scenes involved and due to additional bottlenecks caused by the manual collection of training data by humans, thereby hindering the automation of visual inspection across diverse domains. This work proposes a solution for automatic path planning using a single depth camera mounted on a robot manipulator. Point clouds obtained from the depth images are processed and filtered to extract object profiles and transformed to inspection target paths for the robot end-effector. The approach relies on the geometry of the object and generates an inspection path that follows the shape normal to the surface. Depending on the object size and shape, inspection paths can be defined as single or multi-path plans. Results are demonstrated in both simulated and real-world environments, yielding promising inspection paths for objects with varying sizes and shapes. Code and video are open-source available at: https://github.com/CuriousLad1000/Auto-Path-Planner",979-8-3503-5851-3,2
703.0,,Calibration of Deep Learning Classification Models in fNIRS,Z. Cao; Z. Luo,2024,,,2024 IEEE 20th International Conference on Automation Science and Engineering (CASE),IEEE,"Department of Mathematics, ETH, Zurich, Switzerland; Department of Informatics, University of Zurich, Switzerland",0,0,,,Deep learning;Computer aided software engineering;Neural networks;Reliability engineering;Robustness;Mathematical models;Calibration;Reliability;Functional near-infrared spectroscopy;Monitoring,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10711714,,,10.1109/CASE59546.2024.10711714,False,"Functional near-infrared spectroscopy (fNIRS) is a valuable non-invasive tool for monitoring brain activity. The classification of fNIRS data in relation to conscious activity holds significance for advancing our understanding of the brain and facilitating the development of brain-computer interfaces (BCI). Many researchers have turned to deep learning to tackle the classification challenges inherent in fNIRS data due to its strong generalization and robustness. In the application of fNIRS, reliability is really important, and one mathematical formulation of the reliability of confidence is calibration. However, many researchers overlook the important issue of calibration. To address this gap, we propose integrating calibration into fNIRS field and assess the reliability of existing models. Surprisingly, our results indicate poor calibration performance in many proposed models. To advance calibration development in the fNIRS field, we summarize three practical tips. Through this letter, we hope to emphasize the critical role of calibration in fNIRS research and argue for enhancing the reliability of deep learning-based predictions in fNIRS classification tasks. All data from our experimental process are openly available on GitHub§.",979-8-3503-5851-3,2
704.0,,DAVIS-Ag: A Synthetic Plant Dataset for Prototyping Domain-Inspired Active Vision in Agricultural Robots,T. Choi; D. Guevara; Z. Cheng; G. Bandodkar; C. Wang; B. N. Bailey; M. Earles; X. Liu,2024,,,2024 IEEE 20th International Conference on Automation Science and Engineering (CASE),IEEE,"Department of Information Technology, Kennesaw State University, USA; Department of Viticulture and Enology, The University of California, Davis, USA; Department of Computer Science, The University of California, Davis, USA; Department of Computer Science, The University of California, Davis, USA; Department of Computer Science, The University of California, Davis, USA; Department of Plant Sciences, The University of California, Davis, USA; Department of Viticulture and Enology, The University of California, Davis, USA; Department of Computer Science, The University of California, Davis, USA",0,0,,,Instance segmentation;Visualization;Three-dimensional displays;Computer aided software engineering;Plants (biology);Pipelines;Benchmark testing;Robot sensing systems;Sensors;Planning,,https://github.com/ctyeong/DAVIS-Ag,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10711368,,,10.1109/CASE59546.2024.10711368,False,"In agricultural environments, viewpoint planning can be a critical functionality for a robot with visual sensors to obtain informative observations of objects of interest (e.g., fruits) from complex structures of plant with random occlusions. Although recent studies on active vision have shown some potential for agricultural tasks, each model has been designed and validated on a unique environment that would not easily be replicated for benchmarking novel methods being developed later. In this paper, we introduce a dataset, so-called DAVIS-Ag, for promoting more extensive research on Domain-inspired Active VISion in Agriculture. To be specific, we leveraged our open-source ""AgML"" framework and 3D plant simulator of ""Helios"" to produce 502K RGB images from 30K densely sampled spatial locations in 632 synthetic orchards. Moreover, plant environments of strawberries, tomatoes, and grapes are considered at two different scales (i.e., Single-Plant and Multi-Plant). Useful labels are also provided for each image, including (1) bounding boxes and (2) instance segmentation masks for all identifiable fruits, and also (3) pointers to other images of the viewpoints that are reachable by an execution of action so as to simulate active viewpoint selections at each time step. Using DAVIS-Ag, we visualize motivating examples where fruit visibility can dramatically change depending on the pose of the camera view primarily due to occlusions by other components, such as leaves. Furthermore, we present several baseline models with experiment results for benchmarking in the task of target visibility maximization. Transferability to real strawberry environments is also investigated to demonstrate the feasibility of using the dataset for prototyping real-world solutions. For future research, our dataset is made publicly available online: https://github.com/ctyeong/DAVIS-Ag.",979-8-3503-5851-3,2
705.0,,Towards Robust Perception for Assistive Robotics: An RGB-Event-LiDAR Dataset and Multi-Modal Detection Pipeline,A. Scicluna; C. Le Gentil; S. Sutjipto; G. Paul,2024,,,2024 IEEE 20th International Conference on Automation Science and Engineering (CASE),IEEE,"Robotics Institute, Faculty of Engineering and Information Technology, University of Technology Sydney (UTS), Australia; Robotics Institute, Faculty of Engineering and Information Technology, University of Technology Sydney (UTS), Australia; Robotics Institute, Faculty of Engineering and Information Technology, University of Technology Sydney (UTS), Australia; Robotics Institute, Faculty of Engineering and Information Technology, University of Technology Sydney (UTS), Australia",0,0,,,Three-dimensional displays;Laser radar;Accuracy;Tracking;Event detection;Pipelines;Object detection;Assistive robots;Vehicle dynamics;Detection algorithms,,https://github.com/nerfies/nerfies.github.io,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10711832,,,10.1109/CASE59546.2024.10711832,False,"The increasing adoption of human-robot interaction presents opportunities for technology to positively impact lives, particularly those with visual impairments, through applications such as guide-dog-like assistive robotics. We present a pipeline exploring the perception and ""intelligent disobedience"" required by such a system. A dataset of two people moving in and out of view has been prepared to compare RGB-based and event-based multi-modal dynamic object detection using LiDAR data for 3D position localisation. Our analysis highlights challenges in accurate 3D localisation using 2D image-LiDAR fusion, indicating the need for further refinement. Compared to the performance of the frame-based detection algorithm utilised (YOLOv4), current cutting-edge event-based detection models appear limited to contextual scenarios, such as for automotive platforms. This is highlighted by weak precision and recall over varying confidence and Intersection over Union (IoU) thresholds when using frame-based detections as a ground truth. Therefore, we have publicly released this dataset to the community, containing RGB, event, point cloud and Inertial Measurement Unit (IMU) data along with ground truth poses for the two people in the scene to fill a gap in the current landscape of publicly available datasets and provide a means to assist in the development of safer and more robust algorithms in the future: https://uts-ri.github.io/revel/.",979-8-3503-5851-3,2
706.0,,KeyMatchNet: Zero-Shot Pose Estimation in 3D Point Clouds by Generalized Keypoint Matching,F. Hagelskjær; R. L. Haugaard,2024,,,2024 IEEE 20th International Conference on Automation Science and Engineering (CASE),IEEE,"SDU Robotics, Mærsk Mc-Kinney Møller Institute, University of Southern Denmark, Odense M, Denmark; SDU Robotics, Mærsk Mc-Kinney Møller Institute, University of Southern Denmark, Odense M, Denmark",0,0,,,Training;Point cloud compression;Three-dimensional displays;Costs;Computer aided software engineering;Image color analysis;Computational modeling;Pose estimation;Training data;Data collection,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10711403,,,10.1109/CASE59546.2024.10711403,False,"In this paper, we present KeyMatchNet, a novel network for zero-shot pose estimation in 3D point clouds. Our method uses only depth information, making it more applicable for many industrial use cases, as color information is seldom available. The network is composed of two parallel components for computing object and scene features. The features are then combined to create matches used for pose estimation. The parallel structure allows for pre-processing of the individual parts, which decreases the run-time.Using a zero-shot network allows for a very short set-up time, as it is not necessary to train models for new objects. However, as the network is not trained for the specific object, zero-shot pose estimation methods generally have lower accuracy compared with conventional methods. To address this, we reduce the complexity of the task by including the scenario information during training. This is typically not feasible as collecting real data for new tasks drastically increases the cost. However, for zero-shot pose estimation, training for new objects is not necessary and the expensive data collection can thus be performed only once.Our method is trained on 1,500 objects and is only tested on unseen objects. We demonstrate that the trained network can not only accurately estimate poses for novel objects, but also demonstrate the ability of the network on objects outside of the trained class. Test results are also shown on real data. We believe that the presented method is valuable for many real-world scenarios. Project page available at keymatchnet.github.io",979-8-3503-5851-3,2
707.0,,Preference Elicitation and Incorporation for Human-Robot Task Scheduling,N. Dhanaraj; M. Jeon; J. H. Kang; S. Nikolaidis; S. K. Gupta,2024,,,2024 IEEE 20th International Conference on Automation Science and Engineering (CASE),IEEE,"University of Southern California, Los Angeles, CA, USA; University of Southern California, Los Angeles, CA, USA; University of Southern California, Los Angeles, CA, USA; University of Southern California, Los Angeles, CA, USA; University of Southern California, Los Angeles, CA, USA",0,0,,,Constraint handling;Schedules;Computer aided software engineering;Automation;Large language models;Iterative methods,,https://github.com/RROS-Lab/Human-Robot-Preference-Planning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10711695,,,10.1109/CASE59546.2024.10711695,False,"In this work, we address the challenge of incorporating human preferences into the task-scheduling process for human-robot teams. Humans have various individual preferences that can be influenced by context and situational information. Incorporating these preferences can lead to improved team performance. Our main contribution is a framework that helps elicit and incorporate preferences during task scheduling. We achieve this by proposing 1) a constraint programming method to generate a range of plans, 2) an intelligent approach for selecting and presenting task schedules based on task features, and 3) a preference incorporation method that uses large language models to convert preferences into soft constraints. Our results demonstrate that we can efficiently generate diverse plans for preference elicitation and incorporate them into the task-scheduling process. We evaluate our framework using an assembly-inspired case study and show how it can effectively incorporate complex and realistic preferences. Our implementation can be found at github.com/RROS-Lab/Human-Robot-Preference-Planning.",979-8-3503-5851-3,2
708.0,,Sparse Color-Code Net: Real-Time RGB-Based 6D Object Pose Estimation on Edge Devices,X. Yang; Z. Yu; A. G. Banerjee,2024,,,2024 IEEE 20th International Conference on Automation Science and Engineering (CASE),IEEE,"Department of Mechanical Engineering, University of Washington, Seattle, WA, USA; Department of Mechanical Engineering, University of Washington, Seattle, WA, USA; Department of Industrial & Systems Engineering and the Department of Mechanical Engineering, University of Washington, Seattle, WA, USA",0,0,,,Performance evaluation;Training;Point cloud compression;Visualization;Accuracy;Pose estimation;Predictive models;Probabilistic logic;Real-time systems;Robots,,https://github.com/smartslab/Sparse-Color-Code-Net,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10711289,,,10.1109/CASE59546.2024.10711289,False,"As robotics and augmented reality applications increasingly rely on precise and efficient 6D object pose estimation, real-time performance on edge devices is required for more interactive and responsive systems. Our proposed Sparse ColorCode Net (SCCN) embodies a clear and concise pipeline design to effectively address this requirement. SCCN performs pixellevel predictions on the target object in the RGB image, utilizing the sparsity of essential object geometry features to speed up the Perspective-n-Point (PnP) computation process. Additionally, it introduces a novel pixel-level geometry-based object symmetry representation that seamlessly integrates with the initial pose predictions, effectively addressing symmetric object ambiguities. SCCN notably achieves an estimation rate of 19 frames per second (FPS) and 6 FPS on the benchmark LINEMOD and LINEMOD Occlusion dataset, respectively, for an NVIDIA Jetson AGX Xavier, while consistently maintaining high estimation accuracy at these rates. The code is available at: https://github.com/smartslab/Sparse-Color-Code-Net.",979-8-3503-5851-3,2
709.0,,TDLE: 2-D LiDAR Exploration with Hierarchical Planning Using Regional Division,X. Zhao; C. Yu; E. Xu; Y. Liu,2023,,,2023 IEEE 19th International Conference on Automation Science and Engineering (CASE),IEEE,"School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China",0,0,,,Laser radar;Computer aided software engineering;Codes;Automation;Planning;Robots,,https://github.com/SeanZsya/tdle,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260441,,,10.1109/CASE56687.2023.10260441,False,"Exploration systems are critical for enhancing the autonomy of robots. Due to the unpredictability of the future planning space, existing methods either adopt an inefficient greedy strategy, or require a lot of resources to obtain a global solution. In this work, we address the challenge of obtaining global exploration routes with minimal computing resources. A hierarchical planning framework dynamically divides the planning space into subregions and arranges their orders to provide global guidance for exploration. Indicators that are compatible with the subregion order are used to choose specific exploration targets, thereby considering estimates of spatial structure and extending the planning space to unknown regions. Extensive simulations and field tests demonstrate the efficacy of our method in comparison to existing 2D LiDAR-based approaches. Our code has been made public for further investigation11Available at https://github.com/SeanZsya/tdle.",979-8-3503-2069-5,2
710.0,,Robot Raconteur®: Updates on an Open Source Interoperable Middleware for Robotics,J. D. Wason; J. T. Wen,2023,,,2023 IEEE 19th International Conference on Automation Science and Engineering (CASE),IEEE,"Wason Technology, LLC; Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute",0,0,,,Training;Automation;Ecosystems;Standardization;Production;Programming;Real-time systems,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260569,,,10.1109/CASE56687.2023.10260569,False,"Robot Raconteur® (RR) is a powerful communication framework for robotics, automation, building control, and the “Internet of Things”. RR provides unique plug-and-play, standardized, augmented object-oriented capabilities that greatly accelerate the development of systems and provide stable production performance. The project became open-source in 2018, and is available on GitHub under the Apache 2.0 license. This paper provides updates on the development of RR, the standardization of RR, the development of an ecosystem of standardized drivers, and example use cases for the framework. The RR Core library is approaching a 1.0 release with ROS Quality Level 2.",979-8-3503-2069-5,2
711.0,,Mobile MoCap: Retroreflector Localization On-The-Go,G. Lvov; M. Zolotas; N. Hanson; A. Allison; X. Hubbard; M. Carvajal; T. Padir,2023,,,2023 IEEE 19th International Conference on Automation Science and Engineering (CASE),IEEE,"Institute for Experiential Robotics, Northeastern University, Boston, MA, USA; Institute for Experiential Robotics, Northeastern University, Boston, MA, USA; Institute for Experiential Robotics, Northeastern University, Boston, MA, USA; Institute for Experiential Robotics, Northeastern University, Boston, MA, USA; Institute for Experiential Robotics, Northeastern University, Boston, MA, USA; Institute for Experiential Robotics, Northeastern University, Boston, MA, USA; Institute for Experiential Robotics, Northeastern University, Boston, MA, USA",0,0,,,Location awareness;Tracking;Robot vision systems;Pose estimation;Systems architecture;Cameras;Motion capture,,https://github.com/RIVeR-Lab/mobile_mocap,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260562,,,10.1109/CASE56687.2023.10260562,False,"Motion capture through tracking retroreflectors obtains highly accurate pose estimation, which is frequently used in robotics. Unlike commercial motion capture systems, fiducial marker-based tracking methods, such as AprilTags, can perform relative localization without requiring a static camera setup. However, popular pose estimation methods based on fiducial markers have lower localization accuracy than commercial motion capture systems. We propose Mobile Mo-Cap, a system that utilizes inexpensive near-infrared cameras for accurate relative localization even while in motion. We present a retroreflector feature detector that performs 6- DoF (six degrees-of-freedom) tracking and operates with minimal camera exposure times to reduce motion blur. To evaluate the proposed localization technique while in motion, we mount our Mobile MoCap system, as well as an RGB camera to benchmark against fiducial markers, onto a precision-controlled linear rail and servo. The fiducial marker approach employs AprilTags, which are pervasively used for localization in robotics. We evaluate the two systems at varying distances, marker viewing angles, and relative velocities. Across all experimental conditions, our stereo-based Mobile MoCap system obtains higher position and orientation accuracy than the fiducial approach. The code for Mobile MoCap is implemented in ROS 2 and made publicly available at https//github.com/RIVeR-Lab/mobile_mocap",979-8-3503-2069-5,2
712.0,,PolyPoD: An Algorithm for Polyculture Seed Placement,V. Kamat; S. Aeron; A. Gu; H. Jalan; S. Adebola; K. Goldberg,2023,,,2023 IEEE 19th International Conference on Automation Science and Engineering (CASE),IEEE,"The AUTOLab at UC Berkeley, automation.berkeley.edu; The AUTOLab at UC Berkeley, automation.berkeley.edu; The AUTOLab at UC Berkeley, automation.berkeley.edu; The AUTOLab at UC Berkeley, automation.berkeley.edu; The AUTOLab at UC Berkeley, automation.berkeley.edu; The AUTOLab at UC Berkeley, automation.berkeley.edu",0,0,,,Computer aided software engineering;Automation;Source coding;Plants (biology);Pesticides;Documentation;Agriculture,,https://github.com/BerkeleyAutomation/PolyPoD,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260454,,,10.1109/CASE56687.2023.10260454,False,"The spatial arrangement of plants in a garden or small farm is especially important for polyculture agriculture to reduce water and pesticide use. We present PolyPoD, a new algorithm for seed placement. Given a polygonal planting area boundary (convex or non-convex) and number and types of seeds, PolyPoD generates the variable radius poisson disk distribution, yielding viable seed placements. Results suggest that PolyPoD outperforms our previous seed placement algorithm with the goal of avoiding over-competition for resources, under-utilization of space, and plant segmentation difficulties. The PolyPoD platform will also offer free access on a website for polyculture farmers worldwide. Source code, documentation, and the website link are available at https://github.com/BerkeleyAutomation/PolyPoD.",979-8-3503-2069-5,2
713.0,,Challenges of Indoor SLAM: A Multi-Modal Multi-Floor Dataset for SLAM Evaluation,P. Kaveti; A. Gupta; D. Giaya; M. Karp; C. Keil; J. Nir; Z. Zhang; H. Singh,2023,,,2023 IEEE 19th International Conference on Automation Science and Engineering (CASE),IEEE,"Northeastern University, Boston, MA, USA; Northeastern University, Boston, MA, USA; Northeastern University, Boston, MA, USA; Northeastern University, Boston, MA, USA; Northeastern University, Boston, MA, USA; Northeastern University, Boston, MA, USA; Northeastern University, Boston, MA, USA; Northeastern University, Boston, MA, USA",0,0,,,Visualization;Simultaneous localization and mapping;Laser radar;Heuristic algorithms;Benchmark testing;Cameras;Trajectory,,https://github.com/neufieldrobotics/NUFR-M3F,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260618,,,10.1109/CASE56687.2023.10260618,False,"Robustness in Simultaneous Localization and Mapping (SLAM) remains one of the key challenges for the real-world deployment of autonomous systems. SLAM research has seen significant progress in the last two and a half decades, yet many state-of-the-art (SOTA) algorithms still struggle to perform reliably in real-world environments. There is a general consensus in the research community that we need challenging real-world scenarios which bring out different failure modes in sensing modalities. In this paper, we present a novel multi-modal indoor SLAM dataset covering challenging common scenarios that a robot will encounter and should be robust to. Our data was collected with a mobile robotics platform across multiple floors at Northeastern University's ISEC building. Such a multi-floor sequence is typical of commercial office spaces characterized by symmetry across floors and, thus, is prone to perceptual aliasing due to similar floor layouts. The sensor suite comprises seven global shutter cameras, a high-grade MEMS inertial measurement unit (IMU), a ZED stereo camera, and a 128-channel high-resolution lidar. Along with the dataset, we benchmark several SLAM algorithms and highlight the problems faced during the runs, such as perceptual aliasing, visual degradation, and trajectory drift. The benchmarking results indicate that parts of the dataset work well with some algorithms, while other data sections are challenging for even the best SOTA algorithms. The dataset is available at https://github.com/neufieldrobotics/NUFR-M3F.",979-8-3503-2069-5,2
714.0,,Autonomous Blimp Control via $H_{\infty}$ Robust Deep Residual Reinforcement Learning,Y. Zuo; Y. T. Liu; A. Ahmad,2023,,,2023 IEEE 19th International Conference on Automation Science and Engineering (CASE),IEEE,"Institute of Flight Mechanics and Controls, University of Stuttgart, Stuttgart, Germany; Max Planck Institute for Intelligent Systems, Tübingen, Germany; Max Planck Institute for Intelligent Systems, Tübingen, Germany",0,0,,,Computer aided software engineering;Codes;Reinforcement learning;Buoyancy;Stability analysis;Robustness;Energy efficiency,,https://github.com/robot-perception-group/robust_deep_residual_blimp,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260561,,,10.1109/CASE56687.2023.10260561,False,"Due to their superior energy efficiency, blimps may replace quadcopters for long-duration aerial tasks. However, designing a controller for blimps to handle complex dynamics, modeling errors, and disturbances remains an unsolved challenge. One recent work combines reinforcement learning (RL) and a PID controller to address this challenge and demonstrates its effectiveness in real-world experiments. In the current work, we build on that using an $H_{\infty}$ robust controller to expand the stability margin and improve the RL agent's performance. Empirical analysis of different mixing methods reveals that the resulting $\mathrm{H}_{\infty}$. RL controller outperforms the prior PID-RL combination and can handle more complex tasks involving intensive thrust vectoring. We provide our code as open-source at https://github.com/robot-perception-group/robust_deep_residual_blimp.",979-8-3503-2069-5,2
715.0,,HapNet: A Learning-Based Haptic-Kinematic Model for Surface Material Classification in Robot Perception,T. Yan; T. Xu; T. Zhang; Z. Sun,2023,,,2023 IEEE 19th International Conference on Automation Science and Engineering (CASE),IEEE,"Future Network of Intelligence Institute and School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen; Faculty of Natural Sciences, Imperial College London; Shenzhen Institute of Artificial Intelligence and Robotics for Society; Future Network of Intelligence Institute and School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen",0,0,,,Costs;Computer aided software engineering;Computational modeling;Source coding;Kinematics;Robot sensing systems;Robustness,,https://github.com/henryyantq/haptic-kinematics,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260360,,,10.1109/CASE56687.2023.10260360,False,"The advancement of modern robotic systems is inseparable from the development of artificial sensory modules and functions that provide robots with human-like perception. Among these, haptic sensation is of vital importance for robot interaction with the environment. Many research works have been conducted in the past decades to enable robots to sense the environment, especially in the area of surface material classification. This paper offers a learning-based low-cost but high computational efficient method adopting multiple kinematic modalities, namely the haptic-kinematic data. We introduce a haptic surface material dataset and propose a kinematics-motivated haptic surface material classification network (Hap-Net) trained and tested on our dataset. The results demonstrate the feasibility and robustness of a purely kinematic haptic scheme without the need for other perceptual modalities. The source codes, datasets, etc., have been made publicly available as an open-source package11https://github.com/henryyantq/haptic-kinematics.",979-8-3503-2069-5,2
716.0,,Learning Generalizable Tool Use with Non-Rigid Grasp-Pose Registration,M. Mosbach; S. Behnke,2023,,,2023 IEEE 19th International Conference on Automation Science and Engineering (CASE),IEEE,"The Autonomous Intelligent Systems group, University of Bonn, Germany; The Autonomous Intelligent Systems group, University of Bonn, Germany",0,0,,,Visualization;Computer aided software engineering;Automation;Human intelligence;Reinforcement learning;Grasping;Behavioral sciences,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260551,,,10.1109/CASE56687.2023.10260551,False,"Tool use, a hallmark feature of human intelligence, remains a challenging problem in robotics due the complex contacts and high-dimensional action space. In this work, we present a novel method to enable reinforcement learning of tool use behaviors. Our approach provides a scalable way to learn the operation of tools in a new category using only a single demonstration. To this end, we propose a new method for generalizing grasping configurations of multi-fingered robotic hands to novel objects. This is used to guide the policy search via favorable initializations and a shaped reward signal. The learned policies solve complex tool use tasks and generalize to unseen tools at test time. Visualizations and videos of the trained policies are available at https://maltemosbach.github.io/generalizable_tool_use.",979-8-3503-2069-5,2
717.0,,Correlated Time Series Self-Supervised Representation Learning via Spatiotemporal Bootstrapping,L. Wang; L. Bai; Z. Li; R. Zhao; F. Tsung,2023,,,2023 IEEE 19th International Conference on Automation Science and Engineering (CASE),IEEE,"Interdisciplinary Programs Office, The Hong Kong University of Science and Technology, Hong Kong SAR; The Shanghai AI Laboratory, Shanghai, China; Information System, University of Cologne, Cologne, NRW, Germany; SenseTime Reasearch and Qing Yuan Research Institute of Shanghai Jiao Tong University, Shanghai, China; The Hong Kong University of Science and Technology and The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China",0,0,,,Representation learning;Source coding;Time series analysis;Self-supervised learning;Predictive models;Data models;Spatiotemporal phenomena,,https://github.com/bonaldli/Spatiotemporal-TS-Representation-Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260640,,,10.1109/CASE56687.2023.10260640,False,"Correlated time series analysis plays an important role in many real-world industries. Learning an efficient representation of this large-scale data for further downstream tasks is necessary but challenging. In this paper, we propose a time-step-level representation learning framework for individual instances via bootstrapped spatiotemporal representation prediction. We evaluated the effectiveness and flexibility of our representation learning framework on correlated time series forecasting and cold-start transferring the forecasting model to new instances with limited data. A linear regression model trained on top of the learned representations demonstrates our model performs best in most cases. Especially compared to representation learning models, we reduce the RMSE, MAE, and MAPE by 37%, 49%, and 48% on the PeMS-BAY dataset, respectively. Furthermore, in real-world metro passenger flow data, our framework demonstrates the ability to transfer to infer future information of new cold-start instances, with gains of 15%, 19%, and 18%. The source code will be released under the GitHub https://github.com/bonaldli/Spatiotemporal-TS-Representation-Learning.",979-8-3503-2069-5,2
718.0,,FRobs_RL: A Flexible Robotics Reinforcement Learning Library,J. M. Fajardo; F. G. Roldan; S. Realpe; J. D. Hernández; Z. Ji; P. -F. Cardenas,2022,,,2022 IEEE 18th International Conference on Automation Science and Engineering (CASE),IEEE,"Mechanical and Mechatronics Deparment, National University of Colombia, Bogota, Colombia; Mechanical and Mechatronics Deparment, National University of Colombia, Bogota, Colombia; Mechanical and Mechatronics Deparment, National University of Colombia, Bogota, Colombia; School of Computer Science and Informatics, Cardiff University, UK; School of Engineering, Cardiff University, UK; Mechanical and Mechatronics Deparment, National University of Colombia, Bogota, Colombia",0,0,,,Automation;Navigation;Operating systems;Reinforcement learning;Libraries;Task analysis;Robots,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9926586,,,10.1109/CASE49997.2022.9926586,False,"Reinforcement learning (RL) has become an interesting topic in robotics applications as it can solve complex problems in specific scenarios. The small amount of RL-tools focused on robotics, plus the lack of features such as easy transfer of simulated environments to real hardware, are obstacles to the widespread use of RL in robotic applications. FRobs_RL is a Python library that aims to facilitate the implementation, testing, and deployment of RL algorithms in intelligent robotic applications using robot operating system (ROS), Gazebo, and OpenAI Gym. FRobs_RL provides an Application Programming Interface (API) to simplify the creation of RL environments, where users can import a wide variety of robot models as well as different simulated environments. With the FRobs_RL library, users do not need to be experts in ROS, Gym, or Gazebo to create a realistic RL application. Using the library, we created and tested two environments containing common robotic tasks; one is a reacher task using a robotic manipulator, and the other is a mapless navigation task using a mobile robot. The library is available in GitHub 1.",978-1-6654-9042-9,2
719.0,,CIPCaD-Bench: Continuous Industrial Process datasets for benchmarking Causal Discovery methods,G. Menegozzo; D. Dall’Alba; P. Fiorini,2022,,,2022 IEEE 18th International Conference on Automation Science and Engineering (CASE),IEEE,"Department of Computer Science, University of Verona, Verona, Italy; Department of Computer Science, University of Verona, Verona, Italy; Department of Computer Science, University of Verona, Verona, Italy",0,0,,,Measurement;Manufacturing processes;Food manufacturing;Fault detection;Time series analysis;Process control;Benchmark testing,,https://github.com/giovanniMen,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9926420,,,10.1109/CASE49997.2022.9926420,False,"Causal relationships are commonly examined in manufacturing processes to support faults investigations, perform interventions, and make strategic decisions. Industry 4.0 has made available an increasing amount of data that enable data-driven Causal Discovery (CD). Considering the growing number of recently proposed CD methods, it is necessary to introduce strict benchmarking procedures on publicly available datasets since they represent the foundation for a fair comparison and validation of different methods. This work introduces two novel public datasets for CD in continuous manufacturing processes. The first dataset employs the well-known Tennessee Eastman simulator for fault detection and process control. The second dataset is extracted from an ultra-processed food manufacturing plant, and it includes a description of the plant, as well as multiple ground truths. These datasets are used to propose a benchmarking procedure based on different metrics and evaluated on a wide selection of CD algorithms. This work allows testing CD methods in realistic conditions enabling the selection of the most suitable method for specific target applications. The datasets are available at the following link: [https://github.com/giovanniMen]",978-1-6654-9042-9,2
720.0,,SingleDemoGrasp: Learning to Grasp From a Single Image Demonstration,A. M. Sefat; A. Angleraud; E. Rahtu; R. Pieters,2022,,,2022 IEEE 18th International Conference on Automation Science and Engineering (CASE),IEEE,"Unit of Automation Technology and Mechanical Engineering, Tampere University, Tampere, Finland; Unit of Automation Technology and Mechanical Engineering, Tampere University, Tampere, Finland; Unit of Computing Sciences, Tampere University, Tampere, Finland; Unit of Automation Technology and Mechanical Engineering, Tampere University, Tampere, Finland",0,0,,,Training;Solid modeling;Visualization;Three-dimensional displays;Annotations;Service robots;Diesel engines,,https://github.com/opendr-eu/opendr,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9926463,,,10.1109/CASE49997.2022.9926463,False,"Learning-based grasping models typically require a large amount of training data and training time to generate an effective grasping model. Alternatively, small non-generic grasp models have been proposed that are tailored to specific objects by, for example, directly predicting the object’s location in 2/3D space, and determining suitable grasp poses by post processing. In both cases, data generation is a bottleneck, as this needs to be separately collected and annotated for each individual object and image. In this work, we tackle these issues and propose a grasping model that is developed in four main steps: 1. Visual object grasp demonstration, 2. Data augmentation, 3. Grasp detection model training and 4. Robot grasping action. Four different vision-based grasp models are evaluated with industrial and 3D printed objects, robot and standard gripper, in both simulation and real environments. The grasping model is implemented in the OpenDR toolkit at: https://github.com/opendr-eu/opendr/tree/master/projects/control/single_demo_grasp.",978-1-6654-9042-9,2
721.0,,"Complete, Decomposition-Free Coverage Path Planning",T. Kusnur; M. Likhachev,2022,,,2022 IEEE 18th International Conference on Automation Science and Engineering (CASE),IEEE,"Robotics Institute in the School of Computer Science at Carnegie Mellon University, Pittsburgh, USA; Robotics Institute in the School of Computer Science at Carnegie Mellon University, Pittsburgh, USA",0,0,,,Computer aided software engineering;Scalability;Traveling salesman problems;Search problems;Probabilistic logic;Path planning;Libraries,,https://github.com/ktushar14/cdf_cpp,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9926483,,,10.1109/CASE49997.2022.9926483,False,"Coverage Path Planning (CPP) requires planning collision-free paths for a robot that observes all reachable points of interest in an environment. Most popular CPP approaches are hierarchical and decomposition-based, involving three steps: (1) decomposing the environment into sub-regions (rectangles or polygons) that simplify the generation of space-filling paths, (2) determining a visitation order over these sub-regions via graph search or a Traveling Salesman Problem (TSP) solver, and (3) generation of space-filling paths in each sub-region. This approach requires significant processing of the environment and the availability of suitable TSP solvers. Furthermore, step (1) can sometimes fail in non-convex environments or lead to ""over-decomposition"" in cluttered environments. To the best of our knowledge, existing decomposition-free approaches are heuristic or random, and therefore typically inefficient and probabilistically complete. We present a resolution-complete decomposition-free coverage path planner that effectively folds steps (1) and (2) above into a single online search routine, making it significantly easier to integrate into existing robot architectures and applicable to a larger set of environments. Our approach leverages a precomputed library of space-filling coverage patterns and automatically determines where to apply them. We evaluate our approach on a variety of environments to demonstrate its benefits and provide an open-source implementation at https://github.com/ktushar14/cdf_cpp.",978-1-6654-9042-9,2
722.0,,A Game Benchmark for Real-Time Human-Swarm Control,J. Meyer; A. Pinosky; T. Trzpit; E. Colgate; T. D. Murphey,2022,,,2022 IEEE 18th International Conference on Automation Science and Engineering (CASE),IEEE,Center for Robotics and Biosystems at Northwestern University; Center for Robotics and Biosystems at Northwestern University; Center for Robotics and Biosystems at Northwestern University; Center for Robotics and Biosystems at Northwestern University; Center for Robotics and Biosystems at Northwestern University,0,0,,,Computer aided software engineering;Codes;Games;Transforms;Benchmark testing;Touch sensitive screens;Control systems,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9926423,,,10.1109/CASE49997.2022.9926423,False,"We present a game benchmark for testing human-swarm control algorithms and interfaces in a real-time, high-cadence scenario. Our benchmark consists of a swarm vs. swarm game in a virtual ROS environment in which the goal of the game is to ""capture"" all agents from the opposing swarm; the game’s high-cadence is a result of the capture rules, which cause agent team sizes to fluctuate rapidly. These rules require players to consider both the number of agents currently at their disposal and the behavior of their opponent’s swarm when they plan actions. We demonstrate our game benchmark with a default human-swarm control system that enables a player to interact with their swarm through a high-level touchscreen interface. The touchscreen interface transforms player gestures into swarm control commands via a low-level decentralized ergodic control framework. We compare our default human-swarm control system to a flocking-based control system, and discuss traits that are crucial for swarm control algorithms and interfaces operating in real-time, high-cadence scenarios like our game benchmark. Our game benchmark code is available on Github; more information can be found at https://sites.google.com/view/swarm-game-benchmark",978-1-6654-9042-9,2
723.0,,FogROS: An Adaptive Framework for Automating Fog Robotics Deployment,K. E. Chen; Y. Liang; N. Jha; J. Ichnowski; M. Danielczuk; J. Gonzalez; J. Kubiatowicz; K. Goldberg,2021,,,2021 IEEE 17th International Conference on Automation Science and Engineering (CASE),IEEE,"Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA, USA",0,0,,,Computers;Cloud computing;Automation;Codes;Adaptive systems;Software;Hardware,,https://github.com/BerkeleyAutomation/FogROS,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9551628,,,10.1109/CASE49439.2021.9551628,False,"As many robot automation applications increasingly rely on multi-core processing or deep-learning models, cloud computing is becoming an attractive and economically viable resource for systems that do not contain high computing power onboard. Despite its immense computing capacity, it is often underused by the robotics and automation community due to lack of expertise in cloud computing and cloud-based infrastructure. Fog Robotics balances computing and data between cloud edge devices. We propose a software framework, FogROS, as an extension of the Robot Operating System (ROS), the defacto standard for creating robot automation applications and components. It allows researchers to deploy components of their software to the cloud with minimal effort, and correspondingly gain access to additional computing cores, GPUs, FPGAs, and TPUs, as well as predeployed software made available by other researchers. FogROS allows a researcher to specify which components of their software will be deployed to the cloud and to what type of computing hardware. We evaluate FogROS on 3 examples: (1) simultaneous localization and mapping (ORB-SLAM2), (2) Dexterity Network (Dex-Net) GPU-based grasp planning, and (3) multi-core motion planning using a 96-core cloud-based server. In all three examples, a component is deployed to the cloud and accelerated with a small change in system launch configuration, while incurring additional latency of 1.2 s, 0.6 s, and 0.5 s due to network communication, the computation speed is improved by 2.6×, 6.0× and 34.2×, respectively. Code, videos, and supplementary material can be found at https://github.com/BerkeleyAutomation/FogROS.",978-1-6654-1873-7,2
724.0,,Time-Efficient Mars Exploration of Simultaneous Coverage and Charging with Multiple Drones,Y. Chang; C. Yan; X. Liu; X. Wang; H. Zhou; X. Xiang; D. Tang,2021,,,2021 IEEE 17th International Conference on Automation Science and Engineering (CASE),IEEE,"National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China",0,0,,,Mars;Scheduling algorithms;Navigation;Heuristic algorithms;Conferences;Reinforcement learning;Surface charging,,https://github.com/changmsdn,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9551658,,,10.1109/CASE49439.2021.9551658,False,"This paper presents a time-efficient scheme for Mars exploration by the cooperation of multiple drones and a rover. To maximize effective coverage of the Mars surface in the long run, a comprehensive framework has been developed with joint consideration for limited energy, sensor model, communication radius and safety radius, which we call TIME-SC2 (TIme-efficient Mars Exploration of Simultaneous Coverage and Charging). First, we propose a multi-drone coverage control algorithm by leveraging emerging deep reinforcement learning and design a novel information map to represent dynamic system states. Second, we propose a near-optimal charging scheduling algorithm to navigate each drone to an individual charging slot, and we have proven that there always exists feasible solutions. The attractiveness of this framework not only resides on its ability to maximize exploration efficiency, but also on its high autonomy that has greatly reduced the non-exploring time. Extensive simulations have been conducted to demonstrate the remarkable performance of TIME-SC2 in terms of time-efficiency, adaptivity and flexibility. Video is available at: https://github.com/changmsdn\Coverage-control",978-1-6654-1873-7,2
725.0,,Chance-Constrained Motion Planning using Modeled Distance- to-Collision Functions,J. J. Johnson; M. C. Yip,2021,,,2021 IEEE 17th International Conference on Automation Science and Engineering (CASE),IEEE,"Department of Electrical and Computer Engineering, University of California, San Diego; Department of Electrical and Computer Engineering, University of California, San Diego",0,0,,,Histograms;Uncertainty;Simulation;Measurement uncertainty;Gaussian processes;Planning;Trajectory,,https://github.com/jacobjj/gp_prob_planning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9551655,,,10.1109/CASE49439.2021.9551655,False,"This paper introduces Chance Constrained Gaussian Process-Motion Planning (CCGP-MP), a motion planning algorithm for robotic systems under motion and state estimate uncertainties. The paper's key idea is to capture the variations in the distance-to-collision measurements caused by the uncertainty in state estimation techniques using a Gaussian Process (GP) model. We formulate the planning problem as a chance constraint problem and propose a deterministic constraint that uses the modeled distance function to verify the chance-constraints. We apply Simplicial Homology Global Optimization (SHGO) approach to find the global minimum of the deterministic constraint function along the trajectory and use the minimum value to verify the chance-constraints. Under this formulation, we can show that the optimization function is smooth under certain conditions and that SHGO converges to the global minimum. Therefore, CCGP-MP will always guarantee that all points on a planned trajectory satisfy the given chance-constraints. The experiments in this paper show that CCGP-MP can generate paths that reduce collisions and meet optimality criteria under motion and state uncertainties. The implementation of our robot models and path planning algorithm can be found on GitHub11https://github.com/jacobjj/gp_prob_planning.",978-1-6654-1873-7,2
726.0,,A Deep Reinforcement Learning Approach for Long-term Short-term Planning on Frenet Frame,M. Moghadam; A. Alizadeh; E. Tekin; G. H. Elkaim,2021,,,2021 IEEE 17th International Conference on Automation Science and Engineering (CASE),IEEE,"Faculty of Electrical and computer Engineering, University of California Santa Cruz (UCSC), California, U.S; Department of Mechatronics Engineering, Istanbul Technical University (ITU); Faculty of Electrical and computer Engineering, University of California Santa Cruz (UCSC), California, U.S; Faculty of Electrical and computer Engineering, University of California Santa Cruz (UCSC), California, U.S",0,0,,,Performance evaluation;Roads;Heuristic algorithms;Decision making;Reinforcement learning;Feature extraction;Trajectory,,https://github.com/MajidMoghadam2006/RL-frenet-trajectory-planning-in-CARLA,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9551598,,,10.1109/CASE49439.2021.9551598,False,"Tactical decision-making and strategic motion planning for autonomous highway driving are challenging due to predicting other road users' behaviors, diversity of environments, and complexity of the traffic interactions. This paper presents a novel end-to-end continuous deep reinforcement learning approach towards autonomous cars' decision-making and motion planning. For the first time, we define both states and action spaces on the Frenet space to make the driving behavior less variant to the road curvatures than the surrounding actors' dynamics and traffic interactions. The agent receives time-series data of past trajectories of the surrounding vehicles and applies convolutional neural networks along the time channels to extract features in the backbone. The algorithm generates continuous spatiotemporal trajectories on the Frenet frame for the feedback controller to track. Extensive high-fidelity highway simulations on CARLA show the superiority of the presented approach compared with commonly used baselines and discrete reinforcement learning on various traffic scenarios. Furthermore, the proposed method's advantage is confirmed with a more comprehensive performance evaluation against 1000 randomly generated test scenarios. Code: https://github.com/MajidMoghadam2006/RL-frenet-trajectory-planning-in-CARLA",978-1-6654-1873-7,2
727.0,,OpenUAV Cloud Testbed: a Collaborative Design Studio for Field Robotics,H. Anand; S. A. Rees; Z. Chen; A. J. Poruthukaran; S. Bearman; L. G. Prasad Antervedi; J. Das,2021,,,2021 IEEE 17th International Conference on Automation Science and Engineering (CASE),IEEE,"School of Earth and Space Exploration, Tempe, Arizona; Vanderbilt University, Nashville, Tennessee; School of Earth and Space Exploration, Tempe, Arizona; School of Earth and Space Exploration, Tempe, Arizona; School of Earth and Space Exploration, Tempe, Arizona; School of Earth and Space Exploration, Tempe, Arizona; School of Earth and Space Exploration, Tempe, Arizona",0,0,,,Cloud computing;Automation;Robot vision systems;Collaboration;Software systems;Rendering (computer graphics);Reproducibility of results,,https://github.com/Open-UAV/openuav-turbovnc,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9551638,,,10.1109/CASE49439.2021.9551638,False,"Simulations play a crucial role in robotics research and education. This paper presents the OpenUAV testbed, an open-source, easy-to-use, web-based, and reproducible software system that enables students and researchers to run robotic simulations on the cloud. We have built upon our previous work and have addressed some of the educational and research challenges associated with the prior work. The critical contributions of the paper to the robotics and automation community are threefold: First, OpenUAV saves students and researchers from tedious and complicated software setups by providing web-browser-based Linux desktop sessions with standard robotics software like Gazebo, ROS, and flight autonomy stack. Second, a method for saving an individual's research work with its dependencies for the work's future reproducibility. Third, the platform provides a mechanism to support photorealistic robotics simulations by combining Unity game engine-based camera rendering and Gazebo physics. The paper addresses a research need for photorealistic simulations and describes a methodology for creating a photorealistic aquatic simulation. We also present the various academic and research use-cases of this platform to improve robotics education and research, especially during times like the COVID-19 pandemic, when virtual collaboration is necessary. GitHub https://github.com/Open-UAV/openuav-turbovnc Webpage: https://openuav.us",978-1-6654-1873-7,2
728.0,,Kit-Net: Self-Supervised Learning to Kit Novel 3D Objects into Novel 3D Cavities,S. Devgon; J. Ichnowski; M. Danielczuk; D. S. Brown; A. Balakrishna; S. Joshi; E. M. C. Rocha; E. Solowjow; K. Goldberg,2021,,,2021 IEEE 17th International Conference on Automation Science and Engineering (CASE),IEEE,"The AUTOLAB at the University of California, Berkeley; The AUTOLAB at the University of California, Berkeley; The AUTOLAB at the University of California, Berkeley; The AUTOLAB at the University of California, Berkeley; The AUTOLAB at the University of California, Berkeley; Siemens Research Lab, Berkeley, CA; Siemens Research Lab, Berkeley, CA; Siemens Research Lab, Berkeley, CA; The AUTOLAB at the University of California, Berkeley",0,0,,,Training;Deep learning;Solid modeling;Three-dimensional displays;Costs;Computer aided software engineering;Conferences,,https://github.com/BerkeleyAutomation/Kit-Net,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9551395,,,10.1109/CASE49439.2021.9551395,False,"In industrial part kitting, 3D objects are inserted into cavities for transportation or subsequent assembly. Kitting is a critical step as it can decrease downstream processing and handling times and enable lower storage and shipping costs. We present Kit-Net, a framework for kitting previously unseen 3D objects into cavities given depth images of both the target cavity and an object held by a gripper in an unknown initial orientation. Kit-Net uses self-supervised deep learning and data-augmentation to train a convolutional neural network (CNN) to robustly estimate 3D rotations between objects and matching concave or convex cavities using a large training dataset of simulated depth images pairs. Kit-Net then uses the trained CNN to implement a controller to orient and position novel objects for insertion into novel prismatic and conformal 3D cavities. Experiments in simulation suggest that Kit-Net can orient objects to have a 98.9% average intersection volume between the object mesh and that of the target cavity. Physical experiments with industrial objects succeed in 18 % of trials using a baseline method and in 63% of trials with Kit-Net. Video, code, and data are available at https://github.com/BerkeleyAutomation/Kit-Net.",978-1-6654-1873-7,2
729.0,,Simulating Polyculture Farming to Tune Automation Policies for Plant Diversity and Precision Irrigation,Y. Avigal; J. Gao; W. Wong; K. Li; G. Pierroz; F. S. Deng; M. Theis; M. Presten; K. Goldberg,2020,,,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),IEEE,"The AUTOLAB at UC Berkeley; The AUTOLAB at UC Berkeley; The AUTOLAB at UC Berkeley; The AUTOLAB at UC Berkeley; Department of Plant and Microbial Biology, University of California, Berkeley, CA; The AUTOLAB at UC Berkeley; The AUTOLAB at UC Berkeley; The AUTOLAB at UC Berkeley; The AUTOLAB at UC Berkeley",0,0,,,Water resources;Irrigation;Automation;Soil;Biological system modeling;Plants (biology),,https://github.com/BerkeleyAutomation/AlphaGarden,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9216984,,,10.1109/CASE48305.2020.9216984,False,"Polyculture farming, where multiple crop species are grown simultaneously, has potential to reduce pesticide and water usage, while improving the utilization of soil nutrients. However, it is much harder to automate than monoculture. As a first step toward developing automation control policies for polyculture farming, we present AlphaGardenSim, a fast, first order, open-access simulator that integrates single plant growth models with inter-plant dynamics, including light and water competition between plants in close proximity. The simulator approximates growth in a real greenhouse garden at 9, 000X the speed of natural growth, allowing for policy parameter tuning. We present an analytic automation policy that in simulation reduced water use and achieved high coverage and plant diversity compared with other policies, even in the presence of invasive species. Code and supplementary material can be found at https://github.com/BerkeleyAutomation/AlphaGarden.",978-1-7281-6904-0,2
730.0,,Learning to Play Cup-and-Ball with Noisy Camera Observations,M. Bujarbaruah; T. Zheng; A. Shetty; M. Sehr; F. Borrelli,2020,,,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),IEEE,"UC Berkeley, USA; UC Berkeley, USA; UC Berkeley, USA; Siemens Corporate Technology, USA; UC Berkeley, USA",0,0,,,Cameras;Manipulators;Noise measurement;Uncertainty;Trajectory;Task analysis,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9216806,,,10.1109/CASE48305.2020.9216806,False,"Playing the cup-and-ball game is an intriguing task for robotics research since it abstracts important problem characteristics including system nonlinearity, contact forces and precise positioning as terminal goal. In this paper, we present a learning model based control strategy for the cup-and-ball game, where a Universal Robots UR5e manipulator arm learns to catch a ball in one of the cups on a Kendama. Our control problem is divided into two sub-tasks, namely (i) swinging the ball up in a constrained motion, and (ii) catching the free-falling ball. The swing-up trajectory is computed offline, and applied in open-loop to the arm. Subsequently, a convex optimization problem is solved online during the ball's free-fall to control the manipulator and catch the ball. The controller utilizes noisy position feedback of the ball from an Intel RealSense D435 depth camera. We propose a novel iterative framework, where data is used to learn the support of the camera noise distribution iteratively in order to update the control policy. The probability of a catch with a fixed policy is computed empirically with a user specified number of roll-outs. Our design guarantees that probability of the catch increases in the limit, as the learned support nears the true support of the camera noise distribution. High-fidelity Mujoco simulations and preliminary experimental results support our theoretical analysis (video link- GitHub link).",978-1-7281-6904-0,2
731.0,,"RAPID-MOLT: A Meso-scale, Open-source, Low-cost Testbed for Robot Assisted Precision Irrigation and Delivery",M. Wiggert; L. Amladi; R. Berenstein; S. Carpin; J. Viers; S. Vougioukas; K. Goldberg,2019,,,2019 IEEE 15th International Conference on Automation Science and Engineering (CASE),IEEE,"The AUTOLab at UC Berkeley; The AUTOLab at UC Berkeley; The AUTOLab at UC Berkeley; School of Engineering, UC Merced; Center for Watershed Sciences, UC Merced; Biological and Agricultural Engineering, UC Davis; The AUTOLab at UC Berkeley",0,0,,,Irrigation;Temperature measurement;Robot sensing systems;Temperature sensors;Indexes,,https://github.com/BerkeleyAutomation/RAPID-MOLT,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8842877,,,10.1109/COASE.2019.8842877,False,"To study the automation of plant-level precision irrigation, specifically learning-based irrigation controllers, we present a modular, open-source testbed that enables real-time, fine-grained data collection and irrigation actuation. RAPID-MOLT costs USD $600 and has floor space of 0.37m2. The functionality of the platform is evaluated by measuring the correlation between plant growth (Leaf Area Index) and water stress (Crop Water Stress Index) with irrigation volume. In line with biological studies, the observed plant growth is positively correlated with irrigation volume while water stress is negatively correlated. Construction directions, experimental data, CAD models, and related software are available at github.com/BerkeleyAutomation/RAPID-MOLT.",978-1-7281-0356-3,2
732.0,,"Blue Gripper: A Robust, Low-Cost, and Force-Controlled Robot Hand",M. Guo; P. Wu; B. Yi; D. Gealy; S. McKinley; P. Abbeel,2019,,,2019 IEEE 15th International Conference on Automation Science and Engineering (CASE),IEEE,"Mechanical Engineering, University of California, Berkeleym; Computer Science, University of California, Berkeley; Computer Science, University of California, Berkeley; Mechanical Engineering, University of California, Berkeleym; Industrial Engineering & Operations Research, University of California, Berkeley; Computer Science, University of California, Berkeley",0,0,,,Grippers;Force;Robot sensing systems;Force control;Fasteners;Brushless motors,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8843134,,,10.1109/COASE.2019.8843134,False,"Recent trends in robotic manipulation have highlighted the need for force-controlled grippers that are not only robust to repeated contacts with the environment, but also low in cost. This paper presents the Blue Gripper, a simple parallel-jaw gripper focused on cost and reliability. The proposed hand weighs 660 grams, has a throw of 120 mm, and can apply up to 150 N of force. It is designed to be passively backdrivable, enabling accurate feedforward force control, reducing cost and complexity. In addition to detailing the gripper’s design and comparing it to other available options, this paper also quantifies the gripper’s backdrivability, robustness, and force-control characteristics. All design files have been open-sourced and can be found at https://berkeleyopengrippers.github.io.",978-1-7281-0356-3,2
733.0,,Automating Planar Object Singulation by Linear Pushing with Single-point and Multi-point Contacts,Z. Dong; S. Krishnan; S. Dolasia; A. Balakrishna; M. Danielczuk; K. Goldberg,2019,,,2019 IEEE 15th International Conference on Automation Science and Engineering (CASE),IEEE,"Department of Electrical Engineering and Computer Science, The AUTOLAB at UC Berkeley, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science, The AUTOLAB at UC Berkeley, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science, The AUTOLAB at UC Berkeley, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science, The AUTOLAB at UC Berkeley, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science, The AUTOLAB at UC Berkeley, Berkeley, CA, USA; Department of Industrial Engineering and Operations Research, The AUTOLAB at UC Berkeley, Berkeley, CA, USA",0,0,,,Friction;Two dimensional displays;Robots;Computational modeling;Grippers;Trajectory;Manufacturing,,https://github.com/Jekyll1021/MultiPointPushing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8843111,,,10.1109/COASE.2019.8843111,False,"Singulation is useful for manufacturing, logistics, and service applications; we consider the problem in a planar setting. We propose a novel O(n(n + v)) linear push policy (n denotes the number of objects, v denotes the maximum number of vertices per object), ClusterPush, that can be efficiently computed using clustering. To evaluate the policy, we define singulation distance as the average pairwise distance of polygon centroids given random arrangements of 2D polygonal objects on a surface, and seek pushing policies that can maximize singulation distance. When compared with a brute force evaluation of all candidate pushes in Box2D simulator using 50,000 pushing scenarios, ClusterPush achieves 70% of the singulation distance achieved using brute force and is 2000x faster. ClusterPush also improves on previous pushing policies and can be used for multi-point pushes with two-point and edge (infinite-point) contacts. Compared with pushes with single-point contacts using ClusterPush, pushes with two-point and edge contacts improve singulation by 7% and 13% respectively. In physical experiments conducted with an ABB YuMi robot on 40 sets of 3-7 blocks, ClusterPush increases singulation distance by 15-30%, outperforming the next best policy by 24% on average. Data and code are available at https://github.com/Jekyll1021/MultiPointPushing.",978-1-7281-0356-3,2
734.0,,A Labor-Efficient GAN-based Model Generation Scheme for Deep-Learning Defect Inspection among Dense Beans in Coffee Industry,C. -J. Kuo; C. -C. Chen; T. -T. Chen; Z. Tsai; M. -H. Hung; Y. -C. Lin; Y. -C. Chen; D. -C. Wang; G. -J. Homg; W. -T. Su,2019,,,2019 IEEE 15th International Conference on Automation Science and Engineering (CASE),IEEE,"Department of Computer Science and Information Engineering, Institute of Manufacturing Information and Systems, National Cheng Kung University, Taiwan; Department of Computer Science and Information Engineering, Institute of Manufacturing Information and Systems, National Cheng Kung University, Taiwan; Department of Computer Science and Information Engineering, Institute of Manufacturing Information and Systems, National Cheng Kung University, Taiwan; Dept. of Mgmt. Info. Sys, Southern Taiwan University of Science and Technology (STUST), Taiwan; Department of Computer Science and Information Engineering, Chinese Culture University, Taiwan; Department of Computer Science and Information Engineering, Institute of Manufacturing Information and Systems, National Cheng Kung University, Taiwan; Department of Industrial Engineering and Management, National Yunlin University of Science and Technology, Taiwan; Dept. of Mgmt. Info. Sys, Southern Taiwan University of Science and Technology (STUST), Taiwan; Dept. of Comp. Sci. & Info. Engr, STUST; Department of Computer Science and Information Engineering, Aletheia University, Taiwan",0,0,,,Labeling;Data models;Inspection;Training;Industries;Testing;Robots,,https://github.com/Louis8582/LEGAN,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8843259,,,10.1109/COASE.2019.8843259,False,"Coffee beans are one of most valuable agricultural products in the world, and defective bean removal plays a critical role to produce high-quality coffee products. In this work, we propose a novel labor-efficient deep learning-based model generation scheme, aiming at providing an effective model with less human labeling effort. The key idea is to iteratively generate new training images containing defective beans in various locations by using a generative-adversarial network framework, and these images incur low successful detection rate so that they are useful for improving model quality. Our proposed scheme brings two main impacts to the intelligent agriculture. First, our proposed scheme is the first work to reduce human labeling effort among solutions of vision-based defective bean removal. Second, our scheme can inspect all classes of defective beans categorized by the SCAA (Specialty Coffee Association of America) at the same time. The above two advantages increase the degree of automation to the coffee industry. We implement the prototype of the proposed scheme for conducting integrated tests. Testing results of a case study reveal that the proposed scheme ca] efficiently and effectively generating models for identifying defect beans.Our implementation of the proposed scheme is available a https://github.com/Louis8582/LEGAN.",978-1-7281-0356-3,2
735.0,,Towards Automating Precision Irrigation: Deep Learning to Infer Local Soil Moisture Conditions from Synthetic Aerial Agricultural Images,D. Tseng; D. Wang; C. Chen; L. Miller; W. Song; J. Viers; S. Vougioukas; S. Carpin; J. A. Ojea; K. Goldberg,2018,,,2018 IEEE 14th International Conference on Automation Science and Engineering (CASE),IEEE,"EECS, Berkeley, UC; EECS, Berkeley, UC; EECS, Berkeley, UC; EECS, Berkeley, UC; EECS, Berkeley, UC; Center for Watershed Sciences, Merced, UC; Biological and Agricultural Engineering, Davis, UC; School of Engineering, Merced, UC; Siemens Corporation, Corporate Technology; IEOR, Berkeley, UC",0,0,,,Support vector machines;Training;Irrigation;Soil moisture;Moisture;Unmanned aerial vehicles;Timing,,https://github.com/BerkeleyAutomation/RAPID,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8560431,,,10.1109/COASE.2018.8560431,False,"Recent advances in unmanned aerial vehicles suggest that collecting aerial agricultural images can be cost-efficient, which can subsequently support automated precision irrigation. To study the potential for machine learning to learn local soil moisture conditions directly from such images, we developed a very fast, linear discrete-time simulation of plant growth based on the Richards equation. We use the simulator to generate large datasets of synthetic aerial images of a vineyard with known moisture conditions and then compare seven methods for inferring moisture conditions from images, in which the “uncorrelated plant” methods look at individual plants and the “correlated field” methods look at the entire vineyard: 1) constant prediction baseline, 2) linear Support Vector Machines (SVM), 3) Random Forests Uncorrelated Plant (RFUP), 4) Random Forests Correlated Field (RFCF), 5) two-layer Neural Networks (NN), 6) Deep Convolutional Neural Networks Uncorrelated Plant (CNNUP), and 7) Deep Convolutional Neural Networks Correlated Field (CNNCF). Experiments on held-out test images show that a globally-connected CNN performs best with normalized mean absolute error of 3.4%. Sensitivity experiments suggest that learned global CNNs are robust to injected noise in both the simulator and generated images as well as in the size of the training sets. In simulation, we compare the agricultural standard of flood irrigation to a proportional precision irrigation controller using the output of the global CNN and find that the latter can reduce water consumption by up to 52% and is also robust to errors in irrigation level, location, and timing. The first-order plant simulator and datasets are available at https://github.com/BerkeleyAutomation/RAPID.",978-1-5386-3593-3,2
736.0,,FLUIDS: A First-Order Lightweight Urban Intersection Driving Simulator,H. Zhao; A. Cui; S. A. Cullen; B. Paden; M. Laskey; K. Goldberg,2018,,,2018 IEEE 14th International Conference on Automation Science and Engineering (CASE),IEEE,"Department of Electrical Engineering and Computer Science; Department of Electrical Engineering and Computer Science; Samsunz Stratezy and Innovation Center; Samsunz Stratezy and Innovation Center; Department of Electrical Engineering and Computer Science; The AUTOLAB at UC Berkeley, Berkeley, CA, USA",0,0,,,Training;Analytical models;Fluids;Automation;Computer aided software engineering;Sensitivity analysis;Conferences,,https://github.com/BerkeleyAutomation/Urban_Driving_Simulator,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8560386,,,10.1109/COASE.2018.8560386,False,"To facilitate automation of urban driving, we present an efficient, lightweight, open-source, first-order simulator with associated graphical display and algorithmic supervisors. FLUIDS can efficiently simulate traffic intersections with varying state configurations for the training and evaluation of learning algorithms. FLUIDS supports an image-based birds-eye state space and a lower dimensional quasi-LIDAR representation. FLUIDS additionally provides algorithmic supervisors for simulating realistic behavior of pedestrians and cars in the environment. FLUIDS generates data in parallel at 4000 state-action pairs per minute and evaluates in parallel an imitation learned policy at 20K evaluations per minute. A velocity controller for avoiding collisions and obeying traffic laws using imitation learning was learned from demonstration. We additionally demonstrate the flexibility of FLUIDS by reporting an extensive sensitivity analysis of the learned model to simulation parameters. FLUIDS 1.0 is available at https://berkeleyautomation.github.io/Urban_Driving_Simulator/.",978-1-5386-3593-3,2
737.0,,Learning Traffic Behaviors by Extracting Vehicle Trajectories from Online Video Streams,X. Ren; D. Wang; M. Laskey; K. Goldberg,2018,,,2018 IEEE 14th International Conference on Automation Science and Engineering (CASE),IEEE,"Department of Electrical Engineering and Computer Science, The AUTOLab at UC Berkeley; Department of Electrical Engineering and Computer Science, The AUTOLab at UC Berkeley; Department of Electrical Engineering and Computer Science, The AUTOLab at UC Berkeley; Department of Industrial Engineering and Operations Research, The AUTOLab at UC Berkeley",0,0,,,Deep learning;Computer aided software engineering;Conferences;Pipelines;Object detection;Detectors;Streaming media,,https://github.com/BerkeleyAutomation/Traffic_Camera_Pipeline,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8560597,,,10.1109/COASE.2018.8560597,False,"To collect extensive data on realistic driving behavior, we propose a framework using online public traffic cam video streams. We implement the Traffic Camera Pipeline (TCP), a system that leverages recent advances in deep learning for object detection to extract trajectories from the video stream to corresponding locations in a bird's eye view traffic simulator. We benchmarked several deep learning detectors for the task of vehicle detection: SSD-VGGNet, SSD-InceptionNet, and SSD-MobileNet, and Faster-RCNN; we found that SSD-VGGNet had the highest precision and quality of bounding boxes. We captured four hours of video streams and used it to train generative models describing both the starting and ending positions of vehicles as well as the trajectories of points traversed. We find that the negative log-likelihood of held-out real data is more likely to occur under the learned models compared to baseline models used in a traffic intersection simulator. The extracted dataset of 234 annotated minute-long videos containing 2618 labeled vehicle trajectories and 8980 additional unannotated minute-long videos is available at https://berkeleyautomation.github.io/Traffic_Camera_Pipeline/.",978-1-5386-3593-3,2
738.0,,An interchangeable surgical instrument system with application to supervised automation of multilateral tumor resection,S. McKinley; A. Garg; S. Sen; D. V. Gealy; J. P. McKinley; Y. Jen; Menglong Guo; D. Boyd; K. Goldberg,2016,,,2016 IEEE International Conference on Automation Science and Engineering (CASE),IEEE,"Mechanical Engineering, University of California, Berkeley, CA, USA; IEOR&EECS; EECS, University of California, Davis, CA, USA; Mechanical Engineering, University of California, Berkeley, CA, USA; Mechanical Engineering, University of California, Berkeley, CA, USA; EECS, University of California, Davis, CA, USA; Mechanical Engineering, University of California, Berkeley, CA, USA; UC Davis Medical Center; IEOR&EECS",0,0,,,Instruments;Tumors;Robots;Cavity resonators;Surgery;Grippers;Standards,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7743487,,,10.1109/COASE.2016.7743487,False,"Many surgical procedures require a sequence of different end-effectors but switching tools for robot-assisted minimally-invasive surgery (RMIS) requires time-consuming removal and replacement through the trocar port. We present an interchangeable instrument system that can be contained within the body cavity. It is based on a novel mounting mechanism compatible with a standard RMIS gripper and a tool-guide and sleeve to facilitate automated instrument switching. Experiments suggest that an Intuitive Surgical system using these interchangeable instruments can perform a multi-step tumor resection procedure that uses a novel haptic probe to localize the tumor, standard scalpel to expose the tumor, standard grippers to extract the subcutaneous tumor, and a fluid injection tool to seal the wound. Design details and video are available at: http://berkeleyautomation.github.io/surgicaltools.",978-1-5090-2409-4,2
