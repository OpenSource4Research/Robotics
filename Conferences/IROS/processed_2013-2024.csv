index,Document Title,Authors,Author Affiliations,Publication Title,Date Added To Xplore,Publication Year,Volume,Issue,Start Page,End Page,Abstract,ISSN,ISBNs,DOI,Funding Information,PDF Link,Author Keywords,IEEE Terms,Mesh_Terms,Article Citation Count,Patent Citation Count,Reference Count,License,Online Date,Issue Date,Meeting Date,Publisher,Document Identifier,github_url,github_code
0,Volumetric Mapping with Panoptic Refinement using Kernel Density Estimation for Mobile Robots,K. Nguyen; T. Dang; M. Huber,"Department of Computer Science and Engineering, Learning and Adaptive Robotics Laboratory, University of Texas at Arlington, Arlington, TX, USA; Department of Computer Science and Engineering, Learning and Adaptive Robotics Laboratory, University of Texas at Arlington, Arlington, TX, USA; Department of Computer Science and Engineering, Learning and Adaptive Robotics Laboratory, University of Texas at Arlington, Arlington, TX, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,9824,9830,"Reconstructing three-dimensional (3D) scenes with semantic understanding is vital in many robotic applications. Robots need to identify which objects, along with their positions and shapes, to manipulate them precisely with given tasks. Mobile robots, especially, usually use lightweight networks to segment objects on RGB images and then localize them via depth maps; however, they often encounter out-of-distribution scenarios where masks over-cover the objects. In this paper, we address the problem of panoptic segmentation quality in 3D scene reconstruction by refining segmentation errors using non-parametric statistical methods. To enhance mask precision, we map the predicted masks into a depth frame to estimate their distribution via kernel densities. The outliers in depth perception are then rejected without the need for additional parameters in an adaptive manner to out-of-distribution scenarios, followed by 3D reconstruction using projective signed distance functions (SDFs). We validate our method on a synthetic dataset, which shows improvements in both quantitative and qualitative results for panoptic mapping. Through real-world testing, the results furthermore show our method’s capability to be deployed on a real-robot system. Our source code is available at: https://github.com/mkhangg/refined_panoptic_mapping.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802224,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802224,,Image segmentation;Three-dimensional displays;Statistical analysis;Source coding;Semantics;Mobile robots;Kernel;Image reconstruction;Testing;Synthetic data,,,,37,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/mkhangg/refined_panoptic_mapping,https://github.com/mkhangg/refined_panoptic_mapping
1,Transcrib3D: 3D Referring Expression Resolution through Large Language Models,J. Fang; X. Tan; S. Lin; I. Vasiljevic; V. Guizilini; H. Mei; R. Ambrus; G. Shakhnarovich; M. R. Walter,Toyota Technological Institute at Chicago; Toyota Technological Institute at Chicago; Toyota Technological Institute at Chicago; Toyota Research Institute; Toyota Research Institute; Toyota Technological Institute at Chicago; Toyota Research Institute; Toyota Technological Institute at Chicago; Toyota Technological Institute at Chicago,2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,9737,9744,"If robots are to work effectively alongside people, they must be able to interpret natural language references to objects in their 3D environment. Understanding 3D referring expressions is challenging—it requires the ability to both parse the 3D structure of the scene and correctly ground free-form language in the presence of distraction and clutter. We introduce Transcrib3D, an approach that brings together 3D detection methods and the emergent reasoning capabilities of large language models (LLMs). Transcrib3D uses text as the unifying medium, which allows us to sidestep the need to learn shared representations connecting multi-modal inputs, which would require massive amounts of annotated 3D data. As a demonstration of its effectiveness, Transcrib3D achieves state-of-the-art results on 3D reference resolution benchmarks, with a great leap in performance from previous multi-modality baselines. To improve upon zero-shot performance and facilitate local deployment on edge computers and robots, we propose self-correction for fine-tuning that trains smaller models, resulting in performance close to that of large models. We show that our method enables a real robot to perform pick-and-place tasks given queries that contain challenging referring expressions. Code will be available at https://ripl.github.io/Transcrib3D.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802793,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802793,,Training;Computers;Three-dimensional displays;Grounding;Large language models;Computational modeling;Natural languages;Benchmark testing;Cognition;Intelligent robots,,,,43,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://ripl.github.io/Transcrib3D,https://github.com/ripl/Transcrib3D
2,V3D-SLAM: Robust RGB-D SLAM in Dynamic Environments with 3D Semantic Geometry Voting,T. Dang; K. Nguyen; M. Huber,"Department of Computer Science and Engineering, Learning and Adaptive Robotics Laboratory, University of Texas at Arlington, Arlington, TX, USA; Department of Computer Science and Engineering, Learning and Adaptive Robotics Laboratory, University of Texas at Arlington, Arlington, TX, USA; Department of Computer Science and Engineering, Learning and Adaptive Robotics Laboratory, University of Texas at Arlington, Arlington, TX, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,7847,7853,"Simultaneous localization and mapping (SLAM) in highly dynamic environments is challenging due to the correlation complexity between moving objects and the camera pose. Many methods have been proposed to deal with this problem; however, the moving properties of dynamic objects with a moving camera remain unclear. Therefore, to improve SLAM’s performance, minimizing disruptive events of moving objects with a physical understanding of 3D shapes and dynamics of objects is needed. In this paper, we propose a robust method, V3D-SLAM, to remove moving objects via two lightweight reevaluation stages, including identifying potentially moving and static objects using a spatial-reasoned Hough voting mechanism and refining static objects by detecting dynamic noise caused by intra-object motions using Chamfer distances as similarity measurements. Through our experiment on the TUM RGB-D benchmark on dynamic sequences with ground-truth camera trajectories, the results show that our methods outperform most other recent state-of-the-art SLAM methods. Our source code is available at https://github.com/tuantdang/v3d-slam.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801333,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801333,,Simultaneous localization and mapping;Three-dimensional displays;Source coding;Dynamics;Noise;Benchmark testing;Cameras;Feature extraction;Trajectory;Object recognition,,,,37,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/tuantdang/v3d-slam,https://github.com/tuantdang/v3d-slam
3,Robust Two-View Geometry Estimation with Implicit Differentiation,V. Pyatov; I. Koshelev; S. Lefkimmiatis,"Skolkovo Institute of Science and Technology (Skoltech), Center for AI Technology, Russia; AI Foundation and Algorithm Lab, Russia; MTS AI Group, Russia",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,11924,11931,"We present a novel two-view geometry estimation framework which is based on a differentiable robust loss function fitting. We propose to treat the robust fundamental matrix estimation as an implicit layer, which allows us to avoid backpropagation through time and significantly improves the numerical stability. To take full advantage of the information from the feature matching stage we incorporate learnable weights that depend on the matching confidences. In this way our solution brings together feature extraction, matching and two-view geometry estimation in a unified end-to-end trainable pipeline. We evaluate our approach on the camera pose estimation task in both outdoor and indoor scenarios. The experiments on several datasets show that the proposed method outperforms both classic and learning-based state-of- the-art methods by a large margin. The project webpage is available at: https://github.com/VladPyatov/ihls",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801617,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801617,,Geometry;Backpropagation;Pose estimation;Robot vision systems;Pipelines;Fitting;Cameras;Feature extraction;Numerical stability;Intelligent robots,,,,55,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/VladPyatov/ihls,https://github.com/VladPyatov/ihls
4,Realistic Rainy Weather Simulation for LiDARs in CARLA Simulator,D. Yang; X. Cai; Z. Liu; W. Jiang; B. Zhang; G. Yan; X. Gao; S. Liu; B. Shi,Beihang University; Shanghai Artificial Intelligence Laboratory; Nankai University; Beihang University; Shanghai Artificial Intelligence Laboratory; Shanghai Artificial Intelligence Laboratory; Shanghai Artificial Intelligence Laboratory; Beihang University; Shanghai Artificial Intelligence Laboratory,2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,951,957,"Data augmentation methods to enhance perception performance in adverse weather have recently attracted considerable attention. Most of the LiDAR data augmentation methods post-process the existing dataset by physics-based models or machine-learning methods. However, due to the limited environmental annotations and the fixed vehicle trajectories in existing datasets, it is challenging to edit the scene and expand the diversity of traffic flow and scenario. To this end, we propose a simulator-based physical modeling approach to augment LiDAR data in rainy weather, enhancing the performance of the perception model. We complete the modeling task of the rainy weather effect in the CARLA simulator and establish a data collection pipeline for LiDAR. Furthermore, we pay special attention to the spray generated by vehicles in rainy weather and simulate this phenomenon through the Spray Emitter method we developed. In addition, considering the influence of different weather conditions on point cloud intensity, we develop a prediction network to forecast the intensity of the LiDAR echo. This enables us to complete the rainy weather simulation of 4D point cloud data. In the experiment, we observe that the model augmented by our synthetic dataset improves the performance for 3D object detection in rainy weather. Both code and dataset are available at https://github.com/PJLab-ADG/PCSim#rainypcsim.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802036,National Science and Technology Major Project; National Natural Science Foundation of China; Science and Technology Commission of Shanghai Municipality; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802036,,Point cloud compression;Laser radar;Pipelines;Weather forecasting;Data collection;Data augmentation;Data models;Trajectory;Meteorology;Synthetic data,,,,36,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/PJLab-ADG/PCSim,https://github.com/PJLab-ADG/PCSim
5,Learning Generalizable Manipulation Policy with Adapter-Based Parameter Fine-Tuning,K. Lu; K. T. Ly; W. Hebberd; K. Zhou; I. Havoutis; A. Markham,"Department of Computer Science, University of Oxford; Oxford Robotics Institute, University of Oxford, Oxford, UK; Oxford Robotics Institute, University of Oxford, Oxford, UK; Department of Computer Science, University of Oxford; Oxford Robotics Institute, University of Oxford, Oxford, UK; Department of Computer Science, University of Oxford",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,13510,13517,"This study investigates the use of adapters in reinforcement learning for robotic skill generalization across multiple robots and tasks. Traditional methods are typically reliant on robot-specific retraining and face challenges such as efficiency and adaptability, particularly when scaling to robots with varying kinematics. We propose an alternative approach where a disembodied (virtual) hand manipulator learns a task (i.e., an abstract skill) and then transfers it to various robots with different kinematic constraints without retraining the entire model (i.e., the concrete, physical implementation of the skill). Whilst adapters are commonly used in other domains with strong supervision available, we show how weaker feedback from robotic control can be used to optimize task execution by preserving the abstract skill dynamics whilst adapting to new robotic domains. We demonstrate the effectiveness of our method with experiments conducted in the SAPIEN ManiSkill environment, showing improvements in generalization and task success rates. All code, data, and additional videos are at this GitHub link: https://kl-research.github.io/genrob.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801544,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801544,,Training;Hands;Adaptation models;Kinematics;Reinforcement learning;Manipulators;Robots;Intelligent robots;Videos;Software development management,,,,49,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://kl-research.github.io/genrob,
6,DiPPeST: Diffusion-based Path Planner for Synthesizing Trajectories Applied on Quadruped Robots,M. Stamatopoulou; J. Liu; D. Kanoulas,"Department of Computer Science, University College London, London, UK; Department of Computer Science, University College London, London, UK; Department of Computer Science, University College London, London, UK",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,7787,7793,"We present DiPPeST, a novel image and goal conditioned diffusion-based trajectory generator for quadrupedal robot path planning. DiPPeST is a zero-shot adaptation of our previously introduced diffusion-based 2D global trajectory generator (DiPPeR). The introduced system incorporates a novel strategy for local real-time path refinements, that is reactive to camera input, without requiring any further training, image processing, or environment interpretation techniques. DiPPeST achieves 92% success rate in obstacle avoidance for nominal environments and an average of 88% success rate when tested in environments that are up to 3.5 times more complex in pixel variation than DiPPeR. A visual-servoing framework is developed to allow for real-world execution, tested on the quadruped robot, achieving 80% success rate in different environments and showcasing improved behavior than complex state-of-the-art local planners, in narrow environments. Website: https://rpl-cs-ucl.github.io/DiPPeSTweb/",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802677,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802677,,Training;Navigation;Robot vision systems;Noise reduction;Generators;Real-time systems;Trajectory;Planning;Quadrupedal robots;Visual odometry,,,,24,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://rpl-cs-ucl.github.io/DiPPeSTweb,
7,Depth Completion using Galerkin Attention,Y. Xu; X. Zhang,"School of Computer Science (National Pilot Software Engineering School), Beijing University of Posts and Telecommunications, Beijing, China; School of Computer Science (National Pilot Software Engineering School), Beijing University of Posts and Telecommunications, Beijing, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,13269,13273,"Current depth completion methods usually employ a pair of calibrated RGB and depth sensors to reconstruct a dense depth map. Although RGB (dense) and depth (sparse) measurements are collected from the same underlying scene, they reflect different physical characteristics and thus it remains rather intricate how the devised RGB guidance scheme can effectively leads to a faithful depth recovery. Different from existing 3D geometry representations, such as point cloud, voxels or meshes, we propose to define 3D scenes as vector-valued functions, f : Ω ∋ (u, v) ↦ (r, g, b, d) ∈ ℝ4 , mapping from the image plane Ω to RGBD vectors. This scene function representation brings two benefits: 1) allowing for the adaptation of the Galerkin method to explore the nodal basis of the scene function space, and 2) transforming the irregularly scattered (X, Y, Z) points in the Euclidean space into the depth function defined over the regular grid in the image plane. We further leverage these two benefits within a deep neural network, characterized by an efficient Galerkin attention-based RGBD function embedding to effectively explore the interaction of color and depth information, and by the utilization of equivariant convolution operation on the RGBD feature map as efficient basic blocks. Experiments show that the proposed method achieves significant performance improvement over state-of-the-arts. Code at https://github.com/ZXS-Labs/DCGA.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802287,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802287,,Point cloud compression;Geometry;Three-dimensional displays;Image color analysis;Sensor phenomena and characterization;Vectors;Encoding;Method of moments;Intelligent robots;Image reconstruction,,,,32,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/ZXS-Labs/DCGA,https://github.com/ZXS-Labs/DCGA
8,Perception-aware Full Body Trajectory Planning for Autonomous Systems using Motion Primitives,M. Kuhne; R. Giubilato; M. J. Schuster; M. A. Roa,"Institute of Robotics and Mechatronics, German Aerospace Center (DLR), Wessling, Germany; Institute of Robotics and Mechatronics, German Aerospace Center (DLR), Wessling, Germany; Institute of Robotics and Mechatronics, German Aerospace Center (DLR), Wessling, Germany; Institute of Robotics and Mechatronics, German Aerospace Center (DLR), Wessling, Germany",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,7765,7772,"Many robotic systems rely on visual sensing to accomplish simultaneously the tasks of state estimation, mapping, and path planning. One one hand, the usage of camera sensors represents a power-efficient and lightweight option for solving this problem. On the other hand, these tasks pose requirements on the quality of the visual input (e.g. number of tracked features for Visual Odometry) that are often in contrast to the optimal viewpoint planning for local mapping and obstacle avoidance. Dealing with this constraint is actively researched in the field of perception-aware planning. The approaches delivered by this field mostly concern Micro air vehicles (MAVs), but could be applied to a larger group of robotic systems. We propose a perception-aware trajectory planner for a class of robotic systems that can orient their cameras independently from their direction of travel. By using motion primitives, our planner does not require differentiable models for motion and perception objectives. We evaluate our method in simulation, showing increased capabilities in localization-aware motions around obstacles, and demonstrate its run-time capability on a real planetary rover. The code is released publicly under github.com/DLR-RM/palp.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801381,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801381,,Space vehicles;Visualization;Costs;Tracking;Robot vision systems;Cameras;Trajectory;Planning;State estimation;Robots,,,,24,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/DLR-RM/palp,https://github.com/DLR-RM/palp
9,LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic Object Rearrangement,H. Chang; K. Gao; K. Boyalakuntla; A. Lee; B. Huang; J. Yu; A. Boularias,"Department of Computer Science, Rutgers University, New Brunswick, USA; Department of Computer Science, Rutgers University, New Brunswick, USA; Department of Computer Science, Rutgers University, New Brunswick, USA; Department of Computer Science, Rutgers University, New Brunswick, USA; Department of Computer Science, Rutgers University, New Brunswick, USA; Department of Computer Science, Rutgers University, New Brunswick, USA; Department of Computer Science, Rutgers University, New Brunswick, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,13607,13612,"We present LGMCTS, a framework that uniquely combines language guidance with geometrically informed sampling distributions to effectively rearrange objects according to geometric patterns dictated by natural language descriptions. LGMCTS uses Monte Carlo Tree Search (MCTS) to create feasible action plans that ensure executable semantic object rearrangement. We present a comprehensive comparison with leading approaches that use language to generate goal rearrangements independently of actionable planning, including Structformer, StructDiffusion, and Code as policies. We also present a new benchmark, the Executable Language Guided Rearrangement (ELGR) Bench, containing tasks involving intricate geometry. With the ELGR bench, we show limitations of task and motion planning (TAMP) solutions that are purely based on Large Language Models (LLM) such as Code as Policies and Progprompt on such tasks. Our findings advocate for using LLMs to generate intermediary representations rather than direct action planning in geometrically complex rearrangement scenarios, aligning with perspectives from recent literature. Our code and supplementary materials are accessible at https://lgmcts.github.io/.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802562,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802562,,Geometry;Codes;Monte Carlo methods;Large language models;Semantics;Natural languages;Benchmark testing;Search problems;Planning;Intelligent robots,,,,34,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
10,JointLoc: A Real-time Visual Localization Framework for Planetary UAVs Based on Joint Relative and Absolute Pose Estimation,X. Luo; X. Wan; Y. Gao; Y. Tian; W. Zhang; L. Shu,"School of Aeronautics and Astronautics, University of Chinese Academy of Sciences, Beijing, China; Chinese Academy of Sciences, Technology and Engineering Center for Space Utilization, Beijing, China; School of Artificial Intelligence, Jilin University, Changchun, China; School of Aeronautics and Astronautics, University of Chinese Academy of Sciences, Beijing, China; Chinese Academy of Sciences, Technology and Engineering Center for Space Utilization, Beijing, China; Chinese Academy of Sciences, Technology and Engineering Center for Space Utilization, Beijing, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,3348,3355,"Unmanned aerial vehicles (UAVs) visual localization in planetary aims to estimate the absolute pose of the UAV in the world coordinate system through satellite maps and images captured by on-board cameras. However, since planetary scenes often lack significant landmarks and there are modal differences between satellite maps and UAV images, the accuracy and real-time performance of UAV positioning will be reduced. In order to accurately determine the position of the UAV in a planetary scene in the absence of the global navigation satellite system (GNSS), this paper proposes JointLoc, which estimates the real-time UAV position in the world coordinate system by adaptively fusing the absolute 2-degree-of-freedom (2-DoF) pose and the relative 6-degree-of-freedom (6-DoF) pose. Extensive comparative experiments were conducted on a proposed planetary UAV image cross-modal localization dataset, which contains three types of typical Martian topography generated via a simulation engine as well as real Martian UAV images from the Ingenuity helicopter. JointLoc achieved a root-mean-square error of 0.237m in the trajectories of up to 1,000m, compared to 0.594m and 0.557m for ORB-SLAM2 and ORB-SLAM3 respectively. The source code will be available at https://github.com/LuoXubo/JointLoc.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802040,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802040,,Location awareness;Global navigation satellite system;Visualization;Mars;Satellites;Autonomous aerial vehicles;Real-time systems;6-DOF;Trajectory;Surface topography,,,,35,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/LuoXubo/JointLoc,https://github.com/LuoXubo/JointLoc
11,ActNeRF: Uncertainty-aware Active Learning of NeRF-based Object Models for Robot Manipulators using Visual and Re-orientation Actions,S. Dasgupta; A. Gupta; S. Tuli; R. Paul,"Indian Institute of Technology Delhi, India; Indian Institute of Technology Delhi, India; Indian Institute of Technology Delhi, India; Indian Institute of Technology Delhi, India",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,13062,13069,"Manipulating unseen objects is challenging without a 3D representation, as objects generally have occluded surfaces. This requires physical interaction with objects to build their internal representations. This paper presents an approach that enables a robot to rapidly learn the complete 3D model of a given object for manipulation in unfamiliar orientations. We use an ensemble of partially constructed NeRF models to quantify model uncertainty to determine the next action (a visual or re-orientation action) by optimizing informativeness and feasibility. Further, our approach determines when and how to grasp and re-orient an object given its partial NeRF model and re-estimates the object pose to rectify misalignments introduced during the interaction. Experiments with a simulated Franka Emika Robot Manipulator operating in a tabletop environment with benchmark objects demonstrate an improvement of (i) 14% in visual reconstruction quality (PSNR), (ii) 20% in the geometric/depth reconstruction of the object surface (F-score) and (iii) 71% in the task success rate of manipulating objects a-priori unseen orientations/stable configurations in the scene; over current methods. The project page can be found at https://actnerf.github.io/",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801767,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801767,,Surface reconstruction;Solid modeling;Uncertainty;Three-dimensional displays;Active learning;Benchmark testing;Neural radiance field;Manipulators;Timing;Intelligent robots,,,,32,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
12,Deep Visual Odometry with Events and Frames,R. Pellerito; M. Cannici; D. Gehrig; J. Belhadj; O. Dubois-Matra; M. Casasco; D. Scaramuzza,"Robotics and Perception Group, University of Zurich, Switzerland; Robotics and Perception Group, University of Zurich, Switzerland; Robotics and Perception Group, University of Zurich, Switzerland; European Space Agency; European Space Agency; European Space Agency; Robotics and Perception Group, University of Zurich, Switzerland",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,8966,8973,"Visual Odometry (VO) is crucial for autonomous robotic navigation, especially in GPS-denied environments like planetary terrains. To improve robustness, recent model-based VO systems have begun combining standard and event-based cameras. While event cameras excel in low-light and high-speed motion, standard cameras provide dense and easier-to-track features. However, the field of image- and event-based VO still predominantly relies on model-based methods and is yet to fully integrate recent image-only advancements leveraging end-to-end learning-based architectures. Seamlessly integrating the two modalities remains challenging due to their different nature, one asynchronous, the other not, limiting the potential for a more effective image- and event-based VO. We introduce RAMP-VO, the first end-to-end learned image- and event-based VO system. It leverages novel Recurrent, Asynchronous, and Massively Parallel (RAMP) encoders capable of fusing asynchronous events with image data, providing 8× faster inference and 33% more accurate predictions than existing solutions. Despite being trained only in simulation, RAMP-VO outperforms previous methods on the newly introduced Apollo and Malapert datasets, and on existing benchmarks, where it improves image- and event-based methods by 58.8% and 30.6%, paving the way for robust and asynchronous VO in space.Multimedial Material: For code and datasets visit https://github.com/uzh-rpg/rampvo.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801307,Horizon Europe; European Research Council; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801307,,Accuracy;Event detection;Navigation;Robot vision systems;Cameras;Hardware;Trajectory;Sparks;Standards;Visual odometry,,,,55,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/uzh-rpg/rampvo,https://github.com/uzh-rpg/rampvo
13,Self Supervised Detection of Incorrect Human Demonstrations: A Path Toward Safe Imitation Learning by Robots in the Wild,N. Sojib; M. Begum,"Department of Computer Science, University of New Hampshire, USA; Department of Computer Science, University of New Hampshire, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,2862,2869,"A major appeal of learning from demonstrations or imitation learning (IL) in robotics is that it learns a policy directly from lay users. However, Lay users may inadvertently provide erroneous demonstrations that lead to learning of policies that are inaccurate and hence, unsafe for humans and/or robot. This paper makes two contributions in the endeavour of recognizing human errors in demonstrations and thereby helping to learn a safe IL policy. First, we created a dataset – Layman V1.0 – with 15 lay users who provided a total of 1200 demonstrations for three simulated tasks – Lift, Can and Square in the simulated Robosuite environment – and two real robot tasks with a Sawyer robot, using a custom designed Android app for tele-operation. Second, we propose a framework named Behavior Cloning for Error Detection (BED) to autonomously detect and discard erroneous demonstrations from a demonstration pool. Our method uses a Behavior Cloning method as self-supervised technique and assigns binary weight to each demonstration based on its inconsistencies with the rest of the demonstrations. We show the effectiveness of this framework in detecting incorrect demonstrations in the Layman V1.0 dataset. We further show that state-of-the-art (SOTA) policy learners learns a better policy when bad demonstrations, identified through the proposed framework, are removed from the training pool. Dataset and Codes are available in https://github.com/AssistiveRoboticsUNH/bed",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802106,National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802106,,Training;Codes;Imitation learning;Cloning;Labeling;Robots;Intelligent robots,,,,27,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/AssistiveRoboticsUNH/bed,https://github.com/AssistiveRoboticsUNH/bed
14,MCGMapper: Light-Weight Incremental Structure from Motion and Visual Localization with Planar Markers and Camera Groups,Y. Xie; Z. Huang; K. Chen; L. Zhu; J. Ma,"Robotics and Autonomous Systems Thrust, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; Robotics and Autonomous Systems Thrust, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; Robotics and Autonomous Systems Thrust, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; Robotics and Autonomous Systems Thrust, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; Robotics and Autonomous Systems Thrust, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,7885,7892,"Structure from Motion (SfM) and visual localization in indoor texture-less scenes and industrial scenarios present prevalent yet challenging research topics. Existing SfM methods designed for natural scenes typically yield low accuracy or map-building failures due to insufficient robust feature extraction in such settings. Visual markers, with their artificially designed features, can effectively address these issues. Nonetheless, existing marker-assisted SfM methods encounter problems like slow running speed and difficulties in convergence; and also, they are governed by the strong assumption of unique marker size. In this paper, we propose a novel SfM framework that utilizes planar markers and multiple cameras with known extrinsics to capture the surrounding environment and reconstruct the marker map. In our algorithm, the initial poses of markers and cameras are calculated with Perspective-n-Points (PnP) in the front-end, while bundle adjustment methods customized for markers and camera groups are designed in the back-end to optimize the 6-DOF pose directly. Our algorithm facilitates the reconstruction of large scenes with different marker sizes, and its accuracy and speed of map building are shown to surpass existing methods. Our approach is suitable for a wide range of scenarios, including laboratories, basements, warehouses, and other industrial settings. Furthermore, we incorporate representative scenarios into simulations and also supply our datasets with pose labels to address the scarcity of quantitative ground-truth datasets in this research field. The datasets and source code are available on GitHub1.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802612,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802612,,Location awareness;Bundle adjustment;Visualization;Accuracy;Structure from motion;Source coding;Laboratories;Cameras;Feature extraction;Intelligent robots,,,,30,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
15,CAIS: Culvert Autonomous Inspection Robotic System,C. P. Le; P. Walunj; A. D. Nguyen; Y. Zhou; B. Nguyen; T. Nguyen; A. Netchaev; H. M. La,"Department of Computer Science and Engineering, Advanced Robotics and Automation (ARA) Lab, University of Nevada, Reno, NV, USA; Department of Computer Science and Engineering, Advanced Robotics and Automation (ARA) Lab, University of Nevada, Reno, NV, USA; Department of Computer Science and Engineering, Advanced Robotics and Automation (ARA) Lab, University of Nevada, Reno, NV, USA; Department of Computer Science and Engineering, Advanced Robotics and Automation (ARA) Lab, University of Nevada, Reno, NV, USA; Department of Engineering, Texas A&M University–Corpus Christi, Corpus Christi, TX, USA; Department of Engineering, Texas A&M University–Corpus Christi, Corpus Christi, TX, USA; USACE Engineer Research and Development Center (ERDC), Information Technology Lab, Vicksburg, MS, USA; Department of Computer Science and Engineering, Advanced Robotics and Automation (ARA) Lab, University of Nevada, Reno, NV, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,11744,11749,"Culverts, essential components of drainage systems, require regular inspection to ensure optimal functionality. However, culvert inspections pose numerous challenges, including accessibility, manpower, defect localization, and reliance on superficial assessments. To address these challenges, we propose a novel Culvert Autonomous Inspection Robotic System (CAIS) equipped with advanced sensing and evaluation capabilities. Our solution integrates an RGBD camera, deep learning, lighting systems, and non-destructive evaluation (NDE) techniques to enable accurate and comprehensive condition assessments. We present a pioneering Partially Observable Markov Decision Process (POMDP) framework to resolve uncertainty in autonomous inspections, especially in confined and unstructured environments like culverts or tunnels. The framework outputs detailed 3D maps highlighting visual defects and NDE condition assessments, demonstrating consistent and reliable performance in both indoor and outdoor scenarios. Additionally, we provide an open-source implementation of our framework on GitHub, contributing to the advancement of autonomous inspection technology and fostering collaboration within the research community. Source codes are available *.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802162,National Science Foundation; Engineer Research and Development Center; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802162,,Visualization;Uncertainty;Three-dimensional displays;Source coding;Markov decision processes;Robot vision systems;Inspection;Sensors;Reliability;Software development management,,,,32,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
16,DSLO: Deep Sequence LiDAR Odometry Based on Inconsistent Spatio-temporal Propagation,H. Zhang; G. Wang; X. Wu; C. Xu; M. Ding; M. Tomizuka; W. Zhan; H. Wang,"Department of Automation, Key Laboratory of System Control and Information Processing of Ministry of Education, Key Laboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai Engineering Research Center of Intelligent Control and Management, Shanghai Jiao Tong University, Shanghai, China; Department of Engineering, University of Cambridge, Cambridge, U.K.; Department of Automation, Key Laboratory of System Control and Information Processing of Ministry of Education, Key Laboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai Engineering Research Center of Intelligent Control and Management, Shanghai Jiao Tong University, Shanghai, China; UC Berkeley, Berkeley, CA, USA; UC Berkeley, Berkeley, CA, USA; UC Berkeley, Berkeley, CA, USA; UC Berkeley, Berkeley, CA, USA; Department of Automation, Key Laboratory of System Control and Information Processing of Ministry of Education, Key Laboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai Engineering Research Center of Intelligent Control and Management, Shanghai Jiao Tong University, Shanghai, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,10672,10677,"This paper introduces a 3D point cloud sequence learning model based on inconsistent spatio-temporal propagation for LiDAR odometry, termed DSLO. It consists of a pyramid structure with a spatial information reuse strategy, a sequential pose initialization module, a gated hierarchical pose refinement module, and a temporal feature propagation module. First, spatial features are encoded using a point feature pyramid, with features reused in successive pose estimations to reduce computational overhead. Second, a sequential pose initialization method is introduced, leveraging the high-frequency sampling characteristic of LiDAR to initialize the LiDAR pose. Then, a gated hierarchical pose refinement mechanism refines poses from coarse to fine by selectively retaining or discarding motion information from different layers based on gate estimations. Finally, temporal feature propagation is proposed to incorporate the historical motion information from point cloud sequences, and address the spatial inconsistency issue when transmitting motion information embedded in point clouds between frames. Experimental results on the KITTI odometry dataset and Argoverse dataset demonstrate that DSLO outperforms state-of-the-art methods, achieving at least a 15.67% improvement on RTE and a 12.64% improvement on RRE, while also achieving a 34.69% reduction in runtime compared to baseline methods. Our implementation will be available at https://github.com/IRMVLab/DSLO.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802130,Science and Technology Commission of Shanghai Municipality; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802130,,Point cloud compression;Solid modeling;Laser radar;Accuracy;Three-dimensional displays;Runtime;Logic gates;Robot sensing systems;Real-time systems;Odometry,,,,33,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/IRMVLab/DSLO,https://github.com/IRMVLab/DSLO
17,SPVSoAP3D: A Second-order Average Pooling Approach to enhance 3D Place Recognition in Horticultural Environments,T. Barros; C. Premebida; S. Aravecchia; C. Pradalier; U. J. Nunes,"Department of Electrical and Computer Engineering, Institute of Systems and Robotics, University of Coimbra, Portugal; Department of Electrical and Computer Engineering, Institute of Systems and Robotics, University of Coimbra, Portugal; IRL2958 GeorgiaTech-CNRS, Metz, France; IRL2958 GeorgiaTech-CNRS, Metz, France; Department of Electrical and Computer Engineering, Institute of Systems and Robotics, University of Coimbra, Portugal",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,9,15,"3D LiDAR-based place recognition has been extensively researched in urban environments, yet it remains underexplored in agricultural settings. Unlike urban contexts, horticultural environments, characterized by their permeability to laser beams, result in sparse and overlapping LiDAR scans with suboptimal geometries. This phenomenon leads to intra-and inter-row descriptor ambiguity. In this work, we address this challenge by introducing SPVSoAP3D, a novel modeling approach that combines a voxel-based feature extraction network with an aggregation technique based on a second-order average pooling operator, complemented by a descriptor enhancement stage. Furthermore, we augment the existing HORTO-3DLM dataset by introducing two new sequences derived from horticultural environments. We evaluate the performance of SPVSoAP3D against state-of-the-art (SOTA) models, including OverlapTransformer, PointNetVLAD, and LOGG3D-Net, utilizing a cross-validation protocol on both the newly introduced sequences and the existing HORTO-3DLM dataset. The findings indicate that the average operator is more suitable for horticultural environments compared to the max operator and other first-order pooling techniques. Additionally, the results highlight the improvements brought by the descriptor enhancement stage. The code is publicly available at https://github.com/Cybonic/SPVSoAP3D.git",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802603,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802603,,Solid modeling;Three-dimensional displays;Protocols;Laser radar;Urban areas;Green products;Laser modes;Permeability;Laser beams;Intelligent robots,,,,33,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/Cybonic/SPVSoAP3D,https://github.com/Cybonic/SPVSoAP3D
18,SiCP: Simultaneous Individual and Cooperative Perception for 3D Object Detection in Connected and Automated Vehicles,D. Qu; Q. Chen; T. Bai; H. Lu; H. Fan; H. Zhang; S. Fu; Q. Yang,"University of North Texas, Denton, TX, USA; Toyota InfoTech Labs, Mountain View, CA, USA; University of North Texas, Denton, TX, USA; Toyota InfoTech Labs, Mountain View, CA, USA; University of North Texas, Denton, TX, USA; University of Massachusetts Amherst, Amherst, MA, USA; University of North Texas, Denton, TX, USA; University of North Texas, Denton, TX, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,8905,8912,"Cooperative perception for connected and automated vehicles is traditionally achieved through the fusion of feature maps from two or more vehicles. However, the absence of feature maps shared from other vehicles can lead to a significant decline in 3D object detection performance for cooperative perception models compared to standalone 3D detection models. This drawback impedes the adoption of cooperative perception as vehicle resources are often insufficient to concurrently employ two perception models. To tackle this issue, we present Simultaneous Individual and Cooperative Perception (SiCP), a generic framework that supports a wide range of the state-of-the-art standalone perception backbones and enhances them with a novel Dual-Perception Network (DP-Net) designed to facilitate both individual and cooperative perception. In addition to its lightweight nature with only 0.13M parameters, DP-Net is robust and retains crucial gradient information during feature map fusion. As demonstrated in a comprehensive evaluation on the V2V4Real and OPV2V datasets, thanks to DP-Net, SiCP surpasses state-of-the-art cooperative perception solutions while preserving the performance of standalone perception solutions. The source code can be found at https://github.com/DarrenQu/SiCP.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801398,National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801398,,Solid modeling;Three-dimensional displays;Costs;Source coding;Pipelines;Object detection;Interference;Feature extraction;Automobiles;Intelligent robots,,,,32,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/DarrenQu/SiCP,https://github.com/DarrenQu/SiCP
19,A Multi-model Fusion of LiDAR-inertial Odometry via Localization and Mapping,A. D. Nguyen; C. Phuoc Le; P. Walunj; A. Netchaev; T. N. Do; H. Manh La,"Department of Computer Science and Engineering, Advanced Robotics and Automation (ARA) Lab, University of Nevada, Reno, NV, USA; Department of Computer Science and Engineering, Advanced Robotics and Automation (ARA) Lab, University of Nevada, Reno, NV, USA; Department of Computer Science and Engineering, Advanced Robotics and Automation (ARA) Lab, University of Nevada, Reno, NV, USA; USACE Engineer Research and Development Center (ERDC), Information Technology Lab, Vicksburg, MS, USA; Graduate School of Biomedical Engineering, UNSW Sydney, Kensington Campus, NSW, Australia; Department of Computer Science and Engineering, Advanced Robotics and Automation (ARA) Lab, University of Nevada, Reno, NV, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,3334,3339,"This work presents a comprehensive LiDAR-inertial odometry framework featuring robust smoothing and mapping capabilities, effectively correcting LiDAR feature point skewness using an inertial measurement unit (IMU). While the Extended Kalman Filter (EKF) is a common choice for nonlinear motion estimation, its complexity grows when handling maneuvering targets. To overcome this challenge, a new framework that incorporates the Iterated Interactive Multiple Models of Kalman Filter (IMMKF) is given, providing a solution for reliable navigation in dynamic motion and noisy conditions. To ensure map consistency, an ikd-tree that facilitates continuous updates and adaptive rebalance is employed, preserving the map’s integrity. To guarantee the robustness of our approach, it undergoes extensive testing across diverse scales of indoor and outdoor environments. This testing scenario simulates absolute GPS denial. In terms of estimated motion, the new algorithm demonstrates superior accuracy compared to existing approaches. The implementation is openly accessible on GitHub4 for further exploration.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802004,National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802004,,Location awareness;Technological innovation;Laser radar;Accuracy;Smoothing methods;Computational modeling;Feature extraction;Odometry;Kalman filters;Testing,,,,27,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
20,Event-Free Moving Object Segmentation from Moving Ego Vehicle,Z. Zhou; Z. Wu; D. P. Paudel; R. Boutteau; F. Yang; L. Van Gool; R. Timofte; D. Ginhac,"University of Burgundy, Dijon, France; University of Wurzburg, Wurzburg, Germany; INSAIT, Sofia, Bulgaria; Université Rouen Normandie, INSA Rouen Normandie, Université Le Havre Normandie, Normandie Université, Rouen, France; University of Burgundy, Dijon, France; INSAIT, Sofia, Bulgaria; University of Wurzburg, Wurzburg, Germany; University of Burgundy, Dijon, France",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,8960,8965,"Moving object segmentation (MOS) in dynamic scenes is an important, challenging, but under-explored research topic for autonomous driving, especially for sequences obtained from moving ego vehicles. Most segmentation methods leverage motion cues obtained from optical flow maps. However, since these methods are often based on optical flows that are pre-computed from successive RGB frames, this neglects the temporal consideration of events occurring within the inter-frame, consequently constraining its ability to discern objects exhibiting relative staticity but genuinely in motion. To address these limitations, we propose to exploit event cameras for better video understanding, which provide rich motion cues without relying on optical flow. To foster research in this area, we first introduce a novel large-scale dataset called DSEC-MOS for moving object segmentation from moving ego vehicles, which is the first of its kind. For benchmarking, we select various mainstream methods and rigorously evaluate them on our dataset. Subsequently, we devise EmoFormer, a novel network able to exploit the event data. For this purpose, we fuse the event temporal prior with spatial semantic maps to distinguish genuinely moving objects from the static background, adding another level of dense supervision around our object of interest. Our proposed network relies only on event data for training but does not require event input during inference, making it directly comparable to frame-only methods in terms of efficiency and more widely usable in many application cases. The exhaustive comparison highlights a significant performance improvement of our method over all other methods. The source code and dataset are publicly available at: https://github.com/ZZYZhou/DSEC-MOS.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801383,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801383,,Training;Source coding;Semantics;Dynamics;Object segmentation;Cameras;Vehicle dynamics;Optical flow;Autonomous vehicles;Standards,,,,51,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/ZZYZhou/DSEC-MOS,https://github.com/ZZYZhou/DSEC-MOS
21,ASY-VRNet: Waterway Panoptic Driving Perception Model based on Asymmetric Fair Fusion of Vision and 4D mmWave Radar,R. Guan; S. Yao; K. L. Man; X. Zhu; Y. Yue; J. Smith; E. G. Lim; Y. Yue,"Department of EEE, University of Liverpool, Liverpool, UK; Department of EEE, University of Liverpool, Liverpool, UK; SAT, Xi’an Jiaotong-Liverpool University, Suzhou, China; SAT, Xi’an Jiaotong-Liverpool University, Suzhou, China; SAT, Xi’an Jiaotong-Liverpool University, Suzhou, China; Department of EEE, University of Liverpool, Liverpool, UK; SAT, Xi’an Jiaotong-Liverpool University, Suzhou, China; HKUST (GZ), Thrust of Artificial Intelligence and Thrust of Intelligent Transportation, Guangzhou, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,12479,12486,"Panoptic Driving Perception (PDP) is critical for the autonomous navigation of Unmanned Surface Vehicles (USVs). A PDP model typically integrates multiple tasks, necessitating the simultaneous and robust execution of various perception tasks to facilitate downstream path planning. The fusion of visual and radar sensors is currently acknowledged as a robust and cost-effective approach. However, most existing research has primarily focused on fusing visual and radar features dedicated to object detection or utilizing a shared feature space for multiple tasks, neglecting the individual representation differences between various tasks. To address this gap, we propose a pair of Asymmetric Fair Fusion (AFF) modules with favorable explainability designed to efficiently interact with independent features from both visual and radar modalities, tailored to the specific requirements of object detection and semantic segmentation tasks. The AFF modules treat image and radar maps as irregular point sets and transform these features into a crossed-shared feature space for multitasking, ensuring equitable treatment of vision and radar point cloud features. Leveraging AFF modules, we propose a novel and efficient PDP model, ASY-VRNet, which processes image and radar features based on irregular super-pixel point sets. Additionally, we propose an effective multi-task learning method specifically designed for PDP models. Compared to other lightweight models, ASY-VRNet achieves state-of-the-art performance in object detection, semantic segmentation, and drivable-area segmentation on the WaterScenes benchmark. Our project is publicly available at https://github.com/GuanRunwei/ASY-VRNet.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802447,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802447,,Point cloud compression;Visualization;Spaceborne radar;Semantic segmentation;Radar detection;Object detection;Transforms;Radar imaging;Multitasking;Feature extraction,,,,29,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/GuanRunwei/ASY-VRNet,https://github.com/GuanRunwei/ASY-VRNet
22,DDS-SLAM: Dense Semantic Neural SLAM for Deformable Endoscopic Scenes,J. Shan; Y. Li; L. Yang; Q. Feng; L. Han; H. Wang,"Department of Automation, Key Laboratory of System Control and Information Processing of Ministry of Education, Key Laboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai Engineering Research Center of Intelligent Control and Management, Shanghai Jiao Tong University, Shanghai, China; School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Key Laboratory of System Control and Information Processing of Ministry of Education, Key Laboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai Engineering Research Center of Intelligent Control and Management, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Key Laboratory of System Control and Information Processing of Ministry of Education, Key Laboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai Engineering Research Center of Intelligent Control and Management, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Key Laboratory of System Control and Information Processing of Ministry of Education, Key Laboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai Engineering Research Center of Intelligent Control and Management, Shanghai Jiao Tong University, Shanghai, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,10837,10842,"Estimating camera motion and continuously reconstructing dense scenes in deformable environments presents a complex and open challenge. Many existing approaches tend to rely on assumptions about the scene’s topology or the nature of deformable motion. However, these assumptions do not hold true in medical endoscopy applications. To address these challenges, we introduce DDS-SLAM, a novel dense deformable semantic neural SLAM that achieves accurate camera tracking, continuous dense scene reconstruction, and high-quality image rendering in deformable scenes. First, we propose a novel hybrid neural scene representation method capable of capturing both natural and artificial deformations. Additionally, by leveraging the 2D semantic information of the scene, we introduce a semantic loss function based on semantic distance fields. This approach guides network optimization at a higher level, thereby enhancing system performance. Furthermore, we validate our method through a series of experiments conducted on several representative medical datasets, demonstrating its superiority over other state-of-the-art approaches. The code is available at: https://github.com/IRMVLab/DDS-SLAM.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802442,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802442,,Simultaneous localization and mapping;Tracking;System performance;Semantics;Cameras;Rendering (computer graphics);Topology;Image reconstruction;Optimization;Biomedical imaging,,,,28,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/IRMVLab/DDS-SLAM,https://github.com/IRMVLab/DDS-SLAM
23,Nerve Block Target Localization and Needle Guidance for Autonomous Robotic Ultrasound Guided Regional Anesthesia,A. Tyagi; A. Tyagi; M. Kaur; R. Aggarwal; K. D. Soni; J. Sivaswamy; A. Trikha,"International Institute of Information Technology, Hyderabad, India; Brigham & Women’s Hospital, Harvard Medical School, Boston, MA, USA; Dept. of Anesthesiology, All India Institute of Medical Sciences, New Delhi, India; Dept. of Anesthesiology, All India Institute of Medical Sciences, New Delhi, India; Dept. of Anesthesiology, All India Institute of Medical Sciences, New Delhi, India; International Institute of Information Technology, Hyderabad, India; Dept. of Anesthesiology, All India Institute of Medical Sciences, New Delhi, India",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,5867,5872,"Visual servoing for the development of autonomous robotic systems capable of administering UltraSound (US) guided regional anesthesia requires real-time segmentation of nerves, needle tip localization and needle trajectory extrapolation. First, we recruited 227 patients to build a large dataset of 41,000 anesthesiologist annotated images from US videos of brachial plexus nerves and developed models to localize nerves in the US images. Generalizability of the best suited model was tested on the datasets constructed from separate US scanners. Using these nerve segmentation predictions, we define automated anesthesia needle targets by fitting an ellipse to the nerve contours. Next, we developed an image analysis tool to guide the needle toward their targets. For the segmentation of the needle, a natural RGB pre-trained neural network was first fine-tuned on a large US dataset for domain transfer and then adapted for the needle using a small dataset. The segmented needle’s trajectory angle is calculated using Radon transformation and the trajectory is extrapolated from the needle tip. The intersection of the extrapolated trajectory with the needle target guides the needle navigation for drug delivery. The needle trajectory’s average error was within acceptable range of 5 mm as per experienced anesthesiologists. The entire dataset has been released publicly for further study by the research community at https://github.com/Regional-US/",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801467,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801467,,Location awareness;Image segmentation;Ultrasonic imaging;Radon;Needles;Anesthesia;Visual servoing;Real-time systems;Trajectory;Videos,,,,24,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/Regional-US,https://github.com/Regional-US
24,Temporal- and Viewpoint-Invariant Registration for Under-Canopy Footage using Deep-Learning-based Bird’s-Eye View Prediction,J. Zhou; R. Mascaro; C. Cadena; M. Chli; L. Teixeira,"Vision for Robotics Lab, ETH Zurich and University of Cyprus; Vision for Robotics Lab, ETH Zurich and University of Cyprus; Robotic Systems Lab, ETH Zurich; Vision for Robotics Lab, ETH Zurich and University of Cyprus; Vision for Robotics Lab, ETH Zurich and University of Cyprus",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,61,68,"Conducting visual assessments under the canopy using mobile robots is an emerging task in smart farming and forestry. However, it is challenging to register images across different data-collection days, especially across seasons, due to the self-occluding geometry and temporal dynamics in forests and orchards. This paper proposes a new approach for registering under-canopy image sequences in general and in these situations. Our methodology leverages standard GPS data and deep-learning-based perspective to bird’s-eye view conversion to provide an initial estimation of the positions of the trees in images and their association across datasets. Furthermore, it introduces an innovative strategy for extracting tree trunks and clean ground surfaces from noisy and sparse 3D reconstructions created from the image sequences, utilizing these features to achieve precise alignment. Our robust alignment method effectively mitigates position and scale drift, which may arise from GPS inaccuracies and Sparse Structure from Motion (SfM) limitations. We evaluate our approach on three challenging real-world datasets, demonstrating that our method outperforms ICP-based methods on average by 50%, and surpasses FGR and TEASER++ by over 90% in alignment accuracy. These results highlight our method’s cost efficiency and robustness, even in the presence of severe outliers and sparsity. https://github.com/VIS4ROB-lab/bev_undercanopy_registration",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802707,European Research Council; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802707,,Surface cleaning;Smart agriculture;Surface reconstruction;Accuracy;Structure from motion;Feature extraction;Robustness;Image sequences;Standards;Global Positioning System,,,,33,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/VIS4ROB-lab/bev_undercanopy_registration,https://github.com/VIS4ROB-lab/bev_undercanopy_registration
25,InverseMatrixVT3D: An Efficient Projection Matrix-Based Approach for 3D Occupancy Prediction,Z. Ming; J. S. Berrio; M. Shan; S. Worrall,"Australian Centre for Robotics (ACFR), University of Sydney, NSW, Australia; Australian Centre for Robotics (ACFR), University of Sydney, NSW, Australia; Australian Centre for Robotics (ACFR), University of Sydney, NSW, Australia; Australian Centre for Robotics (ACFR), University of Sydney, NSW, Australia",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,9565,9572,"This paper introduces InverseMatrixVT3D, an efficient method for transforming multi-view image features into 3D feature volumes for 3D semantic occupancy prediction. Existing methods for constructing 3D volumes often rely on depth estimation, device-specific operators, or transformer queries, which hinders the widespread adoption of 3D occupancy models. In contrast, our approach leverages two projection matrices to store the static mapping relationships and matrix multiplications to efficiently generate global Bird’s Eye View (BEV) features and local 3D feature volumes. Specifically, we achieve this by performing matrix multiplications between multi-view image feature maps and two sparse projection matrices. We introduce a sparse matrix handling technique for the projection matrices to optimize GPU memory usage. Moreover, a global-local attention fusion module is proposed to integrate the global BEV features with the local 3D feature volumes to obtain the final 3D volume. We also employ a multi-scale supervision mechanism to enhance performance further. Extensive experiments performed on the nuScenes and SemanticKITTI datasets reveal that our approach not only stands out for its simplicity and effectiveness but also achieves the top performance in detecting vulnerable road users (VRU), crucial for autonomous driving and road safety. The code has been made available at: https://github.com/DanielMing123/InverseMatrixVT3D",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802434,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802434,,Performance evaluation;Solid modeling;Three-dimensional displays;Depth measurement;Semantics;Prediction methods;Transformers;Road safety;Sparse matrices;Autonomous vehicles,,,,40,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/DanielMing123/InverseMatrixVT3D,https://github.com/DanielMing123/InverseMatrixVT3D
26,WidthFormer: Toward Efficient Transformer-based BEV View Transformation,C. Yang; T. Lin; L. Huang; E. J. Crowley,"School of Engineering, University of Edinburgh; Horizon Robotics; Horizon Robotics; School of Engineering, University of Edinburgh",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,8457,8464,"We present WidthFormer, a novel transformer-based module to compute Bird’s-Eye-View (BEV) representations from multi-view cameras for real-time autonomous-driving applications. WidthFormer is computationally efficient, robust and does not require any special engineering effort to deploy. We first introduce a novel 3D positional encoding mechanism capable of accurately encapsulating 3D geometric information, which enables our model to compute high-quality BEV representations with only a single transformer decoder layer. This mechanism is also beneficial for existing sparse 3D object detectors. Inspired by the recently proposed works, we further improve our model’s efficiency by vertically compressing the image features when serving as attention keys and values, and then we develop two modules to compensate for potential information loss due to feature compression. Experimental evaluation on the widely-used nuScenes 3D object detection benchmark demonstrates that our method outperforms previous approaches across different 3D detection architectures. More importantly, our model is highly efficient. For example, when using 256 × 704 input images, it achieves 1.5 ms and 2.8 ms latency on NVIDIA 3090 GPU and Horizon Journey-5 computation solutions. Furthermore, WidthFormer also exhibits strong robustness to different degrees of camera perturbations. Our study offers valuable insights into the deployment of BEV transformation methods in real-world, complex road environments. Code is available at https://github.com/ChenhongyiYang/WidthFormer.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801452,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801452,,Visualization;Three-dimensional displays;Image coding;Computational modeling;Perturbation methods;Transformers;Cameras;Robustness;Encoding;Decoding,,,,38,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/ChenhongyiYang/WidthFormer,https://github.com/ChenhongyiYang/WidthFormer
27,Fast Explicit-Input Assistance for Teleoperation in Clutter,N. Walker; X. Yang; A. Garg; M. Cakmak; D. Fox; C. Pérez-D’Arpino,University of Washington; NVIDIA; NVIDIA; University of Washington; University of Washington; NVIDIA,2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,9270,9276,"The performance of prediction-based assistance for robot teleoperation degrades in unseen or goal-rich environments due to incorrect or quickly-changing intent inferences. Poor predictions can confuse operators or cause them to change their control input to implicitly signal their goal. We present a new assistance interface for robotic manipulation where an operator can explicitly communicate a manipulation goal by pointing the end-effector. The pointing target specifies a region for local pose generation and optimization, providing interactive control over grasp and placement pose candidates. We evaluate this explicit pointing interface against an implicit inference-based assistance scheme and an unassisted control condition in a within-subjects user study (N=20), where participants teleoperate a simulated robot to complete a multi-step singulation and stacking task in cluttered environments. We find that operators prefer the explicit interface, experience fewer pick failures and report lower cognitive workload. Our code is available at: github.com/NVlabs/fast-explicit-teleop.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802138,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802138,,Codes;Stacking;Grasping;End effectors;Clutter;Optimization;Intelligent robots,,,,33,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/NVlabs/fast-explicit-teleop,https://github.com/NVlabs/fast-explicit-teleop
28,Enhanced Language-guided Robot Navigation with Panoramic Semantic Depth Perception and Cross-modal Fusion,L. Wang; J. Tang; Z. He; R. Dang; C. Liu; Q. Chen,NA; NA; NA; NA; NA; NA,2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,7726,7733,"Integrating visual observation with linguistic instruction holds significant promise for enhancing robot navigation across unstructured environments and enriches the human-robot interaction experience. However, while panoramic RGB views furnish robots with extensive environmental visuals, current methods significantly overlook crucial semantic and depth cues. This incomplete representation may lead to misinterpretation or inadequate execution of language instructions, thereby impeding navigation performance and adaptability. In this paper, we introduce SEAT, a semantic-depth aware cross-modal transformer model. Our approach incorporates an efficient panoramic multi-type visual encoder to capture comprehensive environmental details. To mitigate the rigidity of feature mapping stemming from the freezing of pre-training encoders, we propose a novel region query pre-training task. Additionally, we leverage an improved dual-scale cross-modal transformer to facilitate the integration of instructions, topological memory, and action prediction. Extensive experiments on three language-guided robot navigation datasets demonstrate the efficacy of our model, achieving competitive navigation success rates with fewer parameters and computational load. Furthermore, we validate SEAT’s effectiveness in real-world scenarios by deploying it on a mobile robot across various environments. The code is available at https://github.com/CrystalSixone/SEAT.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801563,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801563,,Visualization;Navigation;Semantics;Human-robot interaction;Linguistics;Transformers;Rigidity;Mobile robots;Intelligent robots;Load modeling,,,,46,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/CrystalSixone/SEAT,https://github.com/CrystalSixone/SEAT
29,Dynamic SpectraFormer for Ultra-High-Definition Underwater Image Enhancement,Z. Hu; T. Yu; S. Huang; M. Ishikawa,"Research Institute for Science & Technology, Tokyo University of Science; Tokyo Institute of Technology; Research Institute for Science & Technology, Tokyo University of Science; Research Institute for Science & Technology, Tokyo University of Science",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,8634,8641,"Underwater images suffer from color distortion, haze, and poor visibility due to light refraction and absorption in water. These challenges significantly impact the utilization of Autonomous Underwater Vehicles (AUVs) or marine robots. Typically, color and brightness distortions manifest at lower frequencies, while edge and texture distortions are prevalent at higher frequencies. Traditional methods struggle to concurrently rectify these mixed distortions as they primarily concentrate on the spatial domain. To address these issues, we introduce the Dynamic SpectraFormer, which enhances under-water images through a frequency domain transformer. The Dynamic SpectraFormer introduces an ultra-high-resolution sparse spectrum attention module, which could capture the long-term dependency without losing the universal approximating power. Additionally, we have developed a dynamic spectrum weight generation layer that serves as an adaptive spectrum band selector, accentuating critical frequency bands and suppressing less relevant ones. Consequently, this method significantly improves underwater image quality by addressing both high-and low-frequency distortions. Our extensive ablation studies and comparative evaluations consolidate the Dynamic SpectraFormer’s efficacy across multiple underwater image enhancement benchmarks. The source code is available at https://github.com/arifence2024/DynamicSpectraFormer.git.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802529,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802529,,Image color analysis;Frequency-domain analysis;Source coding;Distortion;Transformers;Image restoration;Vehicle dynamics;Image enhancement;Intelligent robots;Marine robots,,,,41,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/arifence2024/DynamicSpectraFormer,https://github.com/arifence2024/DynamicSpectraFormer
30,Enhancing Exploratory Capability of Visual Navigation Using Uncertainty of Implicit Scene Representation,Y. Wang; Q. Liu; Z. Liu; H. Wang,"Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,13824,13829,"In the context of visual navigation in unknown scenes, both “exploration” and “exploitation” are equally crucial. Robots must first establish environmental cognition through exploration and then utilize the cognitive information to accomplish target searches. However, most existing methods for image-goal navigation prioritize target search over the generation of exploratory behavior. To address this, we propose the Navigation with Uncertainty-driven Exploration (NUE) pipeline, which uses an implicit and compact scene representation, NeRF, as a cognitive structure. We estimate the uncertainty of NeRF and augment the exploratory ability by the uncertainty to in turn facilitate the construction of implicit representation. Simultaneously, we extract memory information from NeRF to enhance the robot’s reasoning ability for determining the location of the target. Ultimately, we seamlessly combine the two generated abilities to produce navigational actions. Our pipeline is end-to-end, with the environmental cognitive structure being constructed online. Extensive experimental results on image-goal navigation demonstrate the capability of our pipeline to enhance exploratory behaviors, while also enabling a natural transition from the exploration to exploitation phase. This enables our model to outperform existing memory-based cognitive navigation structures in terms of navigation performance. Project page: https://github.com/IRMVLab/NUE-NeRF-nav",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801778,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801778,,Visualization;Uncertainty;Navigation;Pipelines;Estimation;Neural radiance field;Cognition;Data mining;Intelligent robots,,,,26,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/IRMVLab/NUE-NeRF-nav,https://github.com/IRMVLab/NUE-NeRF-nav
31,Skeleton-Based Human Action Recognition with Noisy Labels,Y. Xu; K. Peng; D. Wen; R. Liu; J. Zheng; Y. Chen; J. Zhang; A. Roitberg; K. Yang; R. Stiefelhagen,"Anthropomatics and Robotics, Karlsruhe Institute of Technology, Germany; Anthropomatics and Robotics, Karlsruhe Institute of Technology, Germany; Anthropomatics and Robotics, Karlsruhe Institute of Technology, Germany; Anthropomatics and Robotics, Karlsruhe Institute of Technology, Germany; Anthropomatics and Robotics, Karlsruhe Institute of Technology, Germany; Anthropomatics and Robotics, Karlsruhe Institute of Technology, Germany; Anthropomatics and Robotics, Karlsruhe Institute of Technology, Germany; Institute for Artificial Intelligence, University of Stuttgart, Germany; School of Robotics, Hunan University, China; Anthropomatics and Robotics, Karlsruhe Institute of Technology, Germany",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,4716,4723,"Understanding human actions from body poses is critical for assistive robots sharing space with humans in order to make informed and safe decisions about the next interaction. However, precise temporal localization and annotation of activity sequences is time-consuming and the resulting labels are often noisy. If not effectively addressed, label noise negatively affects the model’s training, resulting in lower recognition quality. Despite its importance, addressing label noise for skeleton-based action recognition has been overlooked so far. In this study, we bridge this gap by implementing a framework that augments well-established skeleton-based human action recognition methods with label-denoising strategies from various research areas to serve as the initial benchmark. Observations reveal that these baselines yield only marginal performance when dealing with sparse skeleton data. Consequently, we introduce a novel methodology, NoiseEraSAR, which integrates global sample selection, co-teaching, and Cross-Modal Mixture-of-Experts (CM-MOE) strategies, aimed at mitigating the adverse impacts of label noise. Our proposed approach demonstrates better performance on the established benchmark, setting new state-of-the-art standards. The source code for this study will be made accessible at https://github.com/xuyizdby/NoiseEraSAR.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801681,Deutsche Forschungsgemeinschaft; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801681,,Training;Source coding;Noise;Training data;Benchmark testing;Skeleton;Noise measurement;Human activity recognition;Streams;Standards,,,,50,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/xuyizdby/NoiseEraSAR,https://github.com/xuyizdby/NoiseEraSAR
32,Current-Based Impedance Control for Interacting with Mobile Manipulators,J. de Wolde; L. Knoedler; G. Garofalo; J. Alonso-Mora,"Cognitive Robotics (CoR) Department, Delft University of Technology, Delft, The Netherlands; Cognitive Robotics (CoR) Department, Delft University of Technology, Delft, The Netherlands; ABB Corporate Research, Västerås, Sweden; Cognitive Robotics (CoR) Department, Delft University of Technology, Delft, The Netherlands",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,753,760,"As robots shift from industrial to human-centered spaces, adopting mobile manipulators, which expand workspace capabilities, becomes crucial. In these settings, seamless interaction with humans necessitates compliant control. Two common methods for safe interaction, admittance, and impedance control, require force or torque sensors, often absent in lower-cost or lightweight robots. This paper presents an adaption of impedance control that can be used on current-controlled robots without the use of force or torque sensors and shows its application for compliant control of a mobile manipulator. A calibration method is designed that enables estimation of the actuators’ current/torque ratios and frictions, used by the adapted impedance controller, and that can handle model errors. The calibration method and the performance of the designed controller are experimentally validated using the Kinova GEN3 Lite arm. Results show that the calibration method is consistent and that the designed controller for the arm is compliant while also being able to track targets with five-millimeter precision when no interaction is present. Additionally, this paper presents two operational modes for interacting with the mobile manipulator: one for guiding the robot around the workspace through interacting with the arm and another for executing a tracking task, both maintaining compliance to external forces. These operational modes were tested in real-world experiments, affirming their practical applicability and effectiveness.Code: https://github.com/tud-amr/mobile-manipulator-compliance",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802856,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802856,,Target tracking;Torque;Service robots;Friction;Force;Manipulators;Robot sensing systems;Calibration;Impedance;Intelligent sensors,,,,19,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/tud-amr/mobile-manipulator-compliance,https://github.com/tud-amr/mobile-manipulator-compliance
33,RISE: 3D Perception Makes Real-World Robot Imitation Simple and Effective,C. Wang; H. Fang; H. -S. Fang; C. Lu,Shanghai Noematrix Intelligence Technology Ltd; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University,2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,2870,2877,"Precise robot manipulations require rich spatial information in imitation learning. Image-based policies model object positions from fixed cameras, which are sensitive to camera view changes. Policies utilizing 3D point clouds usually predict keyframes rather than continuous actions, posing difficulty in dynamic and contact-rich scenarios. To utilize 3D perception efficiently, we present RISE, an end-to-end baseline for real-world imitation learning, which predicts continuous actions directly from single-view point clouds. It compresses the point cloud to tokens with a sparse 3D encoder. After adding sparse positional encoding, the tokens are featurized using a transformer. Finally, the features are decoded into robot actions by a diffusion head. Trained with 50 demonstrations for each real-world task, RISE surpasses currently representative 2D and 3D policies by a large margin, showcasing significant advantages in both accuracy and efficiency. Experiments also demonstrate that RISE is more general and robust to environmental change compared with previous baselines. Project website: rise-policy.github.io.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801678,Research and Development; Research and Development; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801678,,Point cloud compression;Three-dimensional displays;Head;Accuracy;Imitation learning;Robot vision systems;Transformers;Cameras;Encoding;Intelligent robots,,,,62,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
34,NRDF - Neural Region Descriptor Fields as Implicit ROI Representation for Robotic 3D Surface Processing,A. Pratheepkumar; M. Ikeda; M. Hofmann; F. Widmoser; A. Pichler; M. Vincze,"Profactor Gmbh, Steyr-Gleink, Austria; Profactor Gmbh, Steyr-Gleink, Austria; Profactor Gmbh, Steyr-Gleink, Austria; Profactor Gmbh, Steyr-Gleink, Austria; Profactor Gmbh, Steyr-Gleink, Austria; Vision for Robotics Laboratory, Automation and Control Institute, TU Wien, Vienna, Austria",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,12955,12962,"To automate 3D surface processing across diverse category-level objects it is imperative to represent process-related region of interest (P-ROI), which is not obtained with conventional keypoint or semantic part correspondences. To resolve this issue, we propose Neural Region Descriptor Fields (NRDF) for achieving unsupervised dense 3D surface region correspondence such that arbitrary ROI is retrieved for a new instance of a known category of object. We utilize the NRDF representation as a medium to facilitate one-shot P-ROI level process knowledge transfer. Recent developments in implicit 3D object representations have focused on keypoint or part correspondences, which have resulted in applications like robotic grasping and manipulation. However, explicit one-shot P-ROI correspondence, and its application for 3D surface process knowledge transfer, is treated for the first time in this work, to the best of our knowledge. The evaluation results show that the proposed approach outperforms the dense correspondence baselines in implicit shape representation and the capacity to retrieve matching arbitrary ROIs. In addition, we validate the practicality of our proposed system in a real-world robotic surface processing application. Our code is available at https://github.com/Profactor/Neural-Region-Descriptor-Fields.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802862,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802862,,Training;Three-dimensional displays;Codes;Shape;Semantics;Grasping;Surface treatment;Knowledge transfer;Intelligent robots,,,,43,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/Profactor/Neural-Region-Descriptor-Fields,https://github.com/Profactor/Neural-Region-Descriptor-Fields
35,All-day Depth Completion,V. Ezhov; H. Park; Z. Zhang; R. Upadhyay; H. Zhang; C. C. Chandrappa; A. Kadambi; Y. Ba; J. Dorsey; A. Wong,"Department of Computer Science, Yale University, CT, USA; Department of Computer Science, Yale University, CT, USA; Department of Computer Science, Yale University, CT, USA; UCLA, CA, USA; UCLA, CA, USA; UCLA, CA, USA; UCLA, CA, USA; UCLA, CA, USA; Department of Computer Science, Yale University, CT, USA; Department of Computer Science, Yale University, CT, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,9425,9431,"We propose a method for depth estimation under different illumination conditions, i.e., day and night time. As photometry is uninformative in regions under low-illumination, we tackle the problem through a multi-sensor fusion approach, where we take as input an additional synchronized sparse point cloud (i.e., from a LiDAR) projected onto the image plane as a sparse depth map, along with a camera image. The crux of our method lies in the use of the abundantly available synthetic data to first approximate the 3D scene structure by learning a mapping from sparse to (coarse) dense depth maps along with their predictive uncertainty – we term this, SpaDe. In poorly illuminated regions where photometric intensities do not afford the inference of local shape, the coarse approximation of scene depth serves as a prior; the uncertainty map is then used with the image to guide refinement through an uncertainty-driven residual learning (URL) scheme. The resulting depth completion network leverages complementary strengths from both modalities – depth is sparse but insensitive to illumination and in metric scale, and image is dense but sensitive with scale ambiguity. SpaDe can be used in a plug-and-play fashion, which allows for 24% improvement when augmented onto existing methods to preprocess sparse depth. We demonstrate URL on the nuScenes dataset where we improve over all baselines by an average 12.39% in all-day scenarios, 12.02% when tested specifically for daytime, and 14.95% for nighttime scenes. Code available at : https://github.com/ezhovv/all-day-depth",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802376,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802376,,Uniform resource locators;Point cloud compression;Uncertainty;Three-dimensional displays;Laser radar;Shape;Lighting;Synchronization;Photometry;Synthetic data,,,,50,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/ezhovv/all-day-depth,https://github.com/ezhovv/all-day-depth
36,PathFinder: Attention-Driven Dynamic Non-Line-of-Sight Tracking with a Mobile Robot,S. Kannapiran; S. Chandran; S. Jayasuriya; S. Berman,"School for Engineering of Matter, Transport and Energy, Arizona State University, Tempe, AZ, USA; School of Electrical, Computer and Energy Engineering, Arizona State University, Tempe, AZ, USA; School of Electrical, Computer and Energy Engineering, Arizona State University, Tempe, AZ, USA; School for Engineering of Matter, Transport and Energy, Arizona State University, Tempe, AZ, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,12880,12887,"The study of non-line-of-sight (NLOS) imaging is growing due to its many potential applications, including rescue operations and pedestrian detection by self-driving cars. However, implementing NLOS imaging on a moving camera remains an open area of research. Existing NLOS imaging methods rely on time-resolved detectors and laser configurations that require precise optical alignment, making it difficult to deploy them in dynamic environments. This work proposes a data-driven approach to NLOS imaging, PathFinder, that can be used with a standard RGB camera mounted on a small, power-constrained mobile robot, such as an aerial drone. Our experimental pipeline is designed to accurately estimate the 2D trajectory of a person who moves in a Manhattan-world environment while remaining hidden from the camera’s field-of-view. We introduce a novel approach to process a sequence of dynamic successive frames in a line-of-sight (LOS) video using an attention-based neural network that performs inference in real-time. The method also includes a preprocessing selection metric that analyzes images from a moving camera which contain multiple vertical planar surfaces, such as walls and building facades, and extracts planes that return maximum NLOS information. We validate the approach on in-the-wild scenes using a drone for video capture, thus demonstrating low-cost NLOS imaging in dynamic capture environments. The real-world dataset that we collected and used to train the network can be found at https://srchandr.github.io/DynamicNLOS/.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801990,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801990,,Robot vision systems;Pipelines;Neural networks;Streaming media;Cameras;Trajectory;Mobile robots;Nonlinear optics;Standards;Drones,,,,39,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://srchandr.github.io/DynamicNLOS,https://github.com/nerfies/nerfies.github.io
37,JUICER: Data-Efficient Imitation Learning for Robotic Assembly,L. Ankile; A. Simeonov; I. Shenfeld; P. Agrawal,Harvard University; Massachusetts Institute of Technology; Massachusetts Institute of Technology; Massachusetts Institute of Technology,2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,5096,5103,"While learning from demonstrations is powerful for acquiring visuomotor policies, high-performance imitation without large demonstration datasets remains challenging for tasks requiring precise, long-horizon manipulation. This paper proposes a pipeline for improving imitation learning performance with a small human demonstration budget. We apply our approach to assembly tasks that require precisely grasping, reorienting, and inserting multiple parts over long horizons and multiple task phases. Our pipeline combines expressive policy architectures and various techniques for dataset expansion and simulation-based data augmentation. These help expand dataset support and supervise the model with locally corrective actions near bottleneck regions requiring high precision. We demonstrate our pipeline on four furniture assembly tasks in simulation, enabling a manipulator to assemble up to five parts over nearly 2500 time steps directly from RGB images, outperforming imitation and data augmentation baselines. Project website: https://imitation-juicer.github.io/.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802498,Sony; Harvard University; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802498,,Robotic assembly;Imitation learning;Pipelines;Grasping;Data augmentation;Manipulators;Assembly;Intelligent robots,,,,69,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
38,DVT: Decoupled Dual-Branch View Transformation for Monocular Bird’s Eye View Semantic Segmentation,J. Du; X. Pan; M. Shen; S. Su; J. Yang; C. Liu; Q. Chen,"Robotics & Artificial Intelligence Laboratory (RAIL), Tongji University, Shanghai, China; Robotics & Artificial Intelligence Laboratory (RAIL), Tongji University, Shanghai, China; Robotics & Artificial Intelligence Laboratory (RAIL), Tongji University, Shanghai, China; Robotics & Artificial Intelligence Laboratory (RAIL), Tongji University, Shanghai, China; Robotics & Artificial Intelligence Laboratory (RAIL), Tongji University, Shanghai, China; Robotics & Artificial Intelligence Laboratory (RAIL), Tongji University, Shanghai, China; Robotics & Artificial Intelligence Laboratory (RAIL), Tongji University, Shanghai, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,9769,9776,"Monocular Bird’s Eye View (BEV) semantic segmentation is critical for autonomous driving for its inherent advantages in spatial representation and downstream tasks. However, it is challenging to simultaneously learn view transformation and pixel-wise classification. Previous works suffer from non-flat region distortion, distant depth ambiguity, and visual occlusion. To address these aforementioned concerns, we propose dual-branch view transformation (DVT), a novel framework for monocular BEV semantic segmentation. Our method consists of: (i) A dual-branch view transformation to decouple features into flat region and non-flat region and process them independently. (ii) A depth-aware weighting method to make the model pay more attention to the distant depth. (iii) An auxiliary task to introduce more inductive biases to alleviate the inaccuracy caused by visual occlusion. Furthermore, we design a class-aware weighting method to address the class and size imbalance of datasets. Experimental results on nuScenes and KITTI-360 datasets demonstrate that DVT outperforms previous state-of-the-art (SOTA). Our codes are available at https://github.com/MrPicklesGG/DVT.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802126,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802126,,Visualization;Incremental learning;Codes;Semantic segmentation;Semantics;Predictive models;Benchmark testing;Distortion;Labeling;Intelligent robots,,,,40,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/MrPicklesGG/DVT,https://github.com/MrPicklesGG/DVT
39,HabiCrowd: A High Performance Simulator for Crowd-Aware Visual Navigation,A. Vuong; T. Nguyen; M. N. Vu; B. Huang; H. T. T. Binh; T. Vo; A. Nguyen,"FPT Software AI Center, Vietnam; FPT Software AI Center, Vietnam; Automation & Control Institute, TU Wien, Austria; Imperial College London, UK; Hanoi University of Science and Technology, Vietnam; Faculty of Mathematics and Statistics, TDTU, Vietnam; Department of Computer Science, University of Liverpool, UK",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,5821,5827,"Visual navigation, a foundational aspect of Embodied AI (E-AI) and robotics has been extensively studied in the past few years. While many 3D simulators have been introduced for the visual navigation tasks, scarcely works have combined human dynamics, creating the gap between simulation and real-world applications. Furthermore, current 3D simulators incorporating human dynamics have several limitations, particularly in terms of computational efficiency, which is a promise of modern simulators. To overcome these issues, we introduce HabiCrowd, the new standard benchmark for crowd-aware visual navigation that includes a crowd dynamics model with diverse human settings into photorealistic environments. Empirical evaluations demonstrate that our proposed human dynamics model achieves state-of-the-art performance in collision avoidance while exhibiting superior computational efficiency compared to its counterparts. We leverage HabiCrowd to conduct several comprehensive studies on crowd-aware visual navigation tasks and human-robot interactions. The source code and data can be found at https://habicrowd.github.io/.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801823,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801823,,Visualization;Three-dimensional displays;Navigation;Computational modeling;Source coding;Human-robot interaction;Computational efficiency;Collision avoidance;Standards;Intelligent robots,,,,48,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
40,OVGNet: A Unified Visual-Linguistic Framework for Open-Vocabulary Robotic Grasping,M. Li; Q. Zhao; S. Lyu; C. Wang; Y. Ma; G. Cheng; C. Yang,"Department of Electronic and Information Engineering, Beihang University; Department of Electronic and Information Engineering, Beihang University; Department of Electronic and Information Engineering, Beihang University; Department of Electronic and Information Engineering, Beihang University; SenseTime, Beijing, China; Department of Computer Science, University of Liverpool; Department of Computer Science, University of Liverpool",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,7507,7513,"Recognizing and grasping novel-category objects remains a crucial yet challenging problem in real-world robotic applications. Despite its significance, limited research has been conducted in this specific domain. To address this, we seamlessly propose a novel framework that integrates open-vocabulary learning into the domain of robotic grasping, empowering robots with the capability to adeptly handle novel objects. Our contributions are threefold. Firstly, we present a large-scale benchmark dataset specifically tailored for evaluating the performance of open-vocabulary grasping tasks. Secondly, we propose a unified visual-linguistic framework that serves as a guide for robots in successfully grasping both base and novel objects. Thirdly, we introduce two alignment modules designed to enhance visual-linguistic perception in the robotic grasping process. Extensive experiments validate the efficacy and utility of our approach. Notably, our framework achieves an average accuracy of 71.2% and 64.4% on base and novel categories in our new dataset, respectively. Our code and dataset are available at https://github.com/cv516Buaa/OVGNet.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802654,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802654,,Codes;Accuracy;Grasping;Benchmark testing;Robots;Intelligent robots,,,,40,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/cv516Buaa/OVGNet,https://github.com/cv516Buaa/OVGNet
41,Exploiting Local Features and Range Images for Small Data Real-Time Point Cloud Semantic Segmentation,D. Fusaro; S. Mosco; E. Menegatti; A. Pretto,"Department of Information Engineering, University of Padova, Italy; Department of Information Engineering, University of Padova, Italy; Department of Information Engineering, University of Padova, Italy; Department of Information Engineering, University of Padova, Italy",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,4980,4987,"Semantic segmentation of point clouds is an essential task for understanding the environment in autonomous driving and robotics. Recent range-based works achieve real-time efficiency, while point- and voxel-based methods produce better results but are affected by high computational complexity. Moreover, highly complex deep learning models are often not suited to efficiently learn from small datasets. Their generalization capabilities can easily be driven by the abundance of data rather than the architecture design. In this paper, we harness the information from the three-dimensional representation to proficiently capture local features, while introducing the range image representation to incorporate additional information and facilitate fast computation. A GPU-based KDTree allows for rapid building, querying, and enhancing projection with straightforward operations. Extensive experiments on SemanticKITTI and nuScenes datasets demonstrate the benefits of our modification in a ""small data"" setup, in which only one sequence of the dataset is used to train the models, but also in the conventional setup, where all sequences except one are used for training. We show that a reduced version of our model not only demonstrates strong competitiveness against full-scale state-of-the-art models but also operates in real-time, making it a viable choice for real-world case applications. The code of our method is available at https://github.com/Bender97/WaffleAndRange.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801329,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801329,,Point cloud compression;Training;Codes;Accuracy;Semantic segmentation;Computational modeling;Architecture;Buildings;Computer architecture;Real-time systems,,,,50,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/Bender97/WaffleAndRange,https://github.com/Bender97/WaffleAndRange
42,DarkGS: Learning Neural Illumination and 3D Gaussians Relighting for Robotic Exploration in the Dark,T. Zhang; K. Huang; W. Zhi; M. Johnson-Roberson,"Robotics Institute, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,12864,12871,"Humans have the remarkable ability to construct consistent mental models of an environment, even under limited or varying levels of illumination. We wish to endow robots with this same capability. In this paper, we tackle the challenge of constructing a photorealistic scene representation under poorly illuminated conditions and with a moving light source. We approach the task of modeling illumination as a learning problem, and utilize the developed illumination model to aid in scene reconstruction. We introduce an innovative framework that uses a data-driven approach, Neural Light Simulators (NeLiS), to model and calibrate the camera-light system. Furthermore, we present DarkGS, a method that applies NeLiS to create a relightable 3D Gaussian scene model capable of real-time, photorealistic rendering from novel viewpoints. We show the applicability and robustness of our proposed simulator and system in a variety of real-world environments. Code released at https://tyz1030.github.io/proj/darkgs.html",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802684,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802684,,Solid modeling;Three-dimensional displays;Codes;Lighting;Rendering (computer graphics);Robustness;Real-time systems;Cognitive science;Intelligent robots;Light sources,,,,40,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://tyz1030.github.io/proj/darkgs,https://github.com/tyz1030/darkgs
43,Co-RaL: Complementary Radar-Leg Odometry with 4-DoF Optimization and Rolling Contact,S. Jung; W. Yang; A. Kim,"Department of Mechanical Engineering, SNU, Seoul, S. Korea; Department of Mechanical Engineering, SNU, Seoul, S. Korea; Department of Mechanical Engineering, SNU, Seoul, S. Korea",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,13289,13296,"Robust and accurate localization in challenging environments is becoming crucial for SLAM. In this paper, we propose a unique sensor configuration for precise and robust odometry by integrating chip radar and a legged robot. Specifically, we introduce a tightly coupled radar-leg odometry algorithm for complementary drift correction. Adopting the 4-DoF optimization and decoupled RANSAC to mmWave chip radar significantly enhances radar odometry beyond the existing method, especially z-directional even when using a single radar. For the leg odometry, we employ rolling contact modeling-aided forward kinematics, accommodating scenarios with the potential possibility of contact drift and radar failure. We evaluate our method by comparing it with other chip radar odometry algorithms using real-world datasets with diverse environments while the datasets will be released for the robotics community. https://github.com/SangwooJung98/Co-RaL-Dataset",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801960,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801960,,Legged locomotion;Location awareness;Simultaneous localization and mapping;Noise;Radar;Stairs;Robustness;Odometry;Millimeter wave communication;Optimization,,,,24,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/SangwooJung98/Co-RaL-Dataset,https://github.com/SangwooJung98/Co-RaL-Dataset
44,BEVLoc: Cross-View Localization and Matching via Birds-Eye-View Synthesis,C. Klammer; M. Kaess,Carnegie Mellon University; Carnegie Mellon University,2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,5656,5663,"Ground to aerial matching is a crucial and challenging task in outdoor robotics, particularly when GPS is absent or unreliable. Structures like buildings or large dense forests create interference, requiring GNSS replacements for global positioning estimates. The true difficulty lies in reconciling the perspective difference between the ground and air images for acceptable localization.Taking inspiration from the autonomous driving community, we propose a novel framework for synthesizing a birds-eye-view (BEV) scene representation to match and localize against an aerial map in off-road environments. We leverage contrastive learning with domain specific hard negative mining to train a network to learn similar representations between the synthesized BEV and the aerial map.During inference, BEVLoc guides the identification of the most probable locations within the aerial map through a coarse-to-fine matching strategy. Our results demonstrate promising initial outcomes in extremely difficult forest environments with limited semantic diversity. We analyze our model’s performance for coarse and fine matching, assessing both the raw matching capability of our model and its performance as a GNSS replacement.Our work delves into off-road map localization while establishing a foundational baseline for future developments in localization. Our code is available at: https://github.com/rpl-cmu/bevloc",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801643,Arm; Arm; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801643,,Location awareness;Global navigation satellite system;Analytical models;Forests;Codes;Semantics;Interference;Contrastive learning;Intelligent robots;Global Positioning System,,,,27,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/rpl-cmu/bevloc,https://github.com/rpl-cmu/bevloc
45,Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery,C. Cornelio; M. Diab,"Samsung AI, Cambridge, UK; University of Plymouth, UK",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,12435,12442,"Recognizing failures during task execution and implementing recovery procedures is challenging in robotics. Traditional approaches rely on the availability of extensive data or a tight set of constraints, while more recent approaches leverage large language models (LLMs) to verify task steps and replan accordingly. However, these methods often operate offline, necessitating scene resets and incurring in high costs. This paper introduces Recover, a neuro-symbolic framework for online failure identification and recovery. By integrating ontologies, logical rules, and LLM-based planners, Recover exploits symbolic information to enhance the ability of LLMs to generate recovery plans and also to decrease the associated costs. In order to demonstrate the capabilities of our method in a simulated kitchen environment, we introduce OntoThor, an ontology describing the AI2Thor simulator setting. Empirical evaluation shows that OntoThor’s logical rules accurately detect all failures in the analyzed tasks, and that Recover considerably outperforms, for both failure detection and recovery, a baseline method reliant solely on LLMs. Supplementary material, including the OntoThor ontology, is available at: https://recover-ontothor.github.io.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801853,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801853,,Costs;Large language models;Ontologies;Intelligent robots,,,,29,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
46,Efficient-PIP: Large-scale Pixel-level Aligned Image Pair Generation for Cross-time Infrared-RGB Translation,J. Li; K. Fei; Y. Sun; J. Wang; B. Liu; Z. Zhou; Y. Zheng; Z. Sun,"College of Intelligence Science and Technology, National University of Defense Technology, China; College of Intelligence Science and Technology, National University of Defense Technology, China; College of Intelligence Science and Technology, National University of Defense Technology, China; College of Intelligence Science and Technology, National University of Defense Technology, China; College of Intelligence Science and Technology, National University of Defense Technology, China; College of Intelligence Science and Technology, National University of Defense Technology, China; College of Intelligence Science and Technology, National University of Defense Technology, China; College of Intelligence Science and Technology, National University of Defense Technology, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,8974,8981,"Generative models are gaining momentum in both academic and industrial applications driven by the availability of large-scale datasets, especially in tasks involving Image-to-Image Translation. Meanwhile, poor human perception of nighttime environment has led to a demand for translation from night-vision infrared to day-vision RGB images. However, collecting such cross-modal training data at the same time is impossible due to the thermal imaging properties of infrared cameras, the challenge lies in constructing image pairs during the day and at night respectively, where the requirement for data alignment poses significant difficulties. In this paper, we propose a Pixel-level aligned Image Pair generation framework PIP to explore efficient colorization of high-resolution infrared images. Specifically, we first construct a 3D high-precision point cloud map for the purpose of establishing the correlation between day and night scenes. Corresponding point clouds of modal images are collected simultaneously during data acquisition to obtain image sensor poses by Global Matching with the map, which allows us to calculate the transformation relationship from infrared to RGB image coordinate systems based on the sensor parameters and depth information of the map. Leveraging the relationship, the pixel values of RGB image is projected onto the infrared image followed by optimization as the colored image. Accordingly, we present a dataset NUDT-PIP, the first of its kind containing large-scale pixel-level aligned cross-time infrared-RGB image pairs of complicated real road scenes. Experimental results demonstrate the reliability and strong applicability of our dataset in Image-to-Image Translation. Our code will be released at https://github.com/wjjjjyourFA/NUDT-PIP.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802486,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802486,,Point cloud compression;Image sensors;Translation;Three-dimensional displays;Roads;Training data;Robot sensing systems;Reliability;Optimization;Intelligent robots,,,,30,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/wjjjjyourFA/NUDT-PIP,https://github.com/wjjjjyourFA/NUDT-PIP
47,Radiance Fields for Robotic Teleoperation,M. Wilder-Smith; V. Patil; M. Hutter,"Robotic Systems Lab, ETH Zurich; Robotic Systems Lab, ETH Zurich; Robotic Systems Lab, ETH Zurich",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,13861,13868,"Radiance field methods such as Neural Radiance Fields (NeRFs) or 3D Gaussian Splatting (3DGS), have revo-lutionized graphics and novel view synthesis. Their ability to synthesize new viewpoints with photo-realistic quality, as well as capture complex volumetric and specular scenes, makes them an ideal visualization for robotic teleoperation setups. Direct camera teleoperation provides high-fidelity operation at the cost of maneuverability, while reconstruction-based approaches offer controllable scenes with lower fidelity. With this in mind, we propose replacing the traditional reconstruction-visualization components of the robotic teleoperation pipeline with online Radiance Fields, offering highly maneuverable scenes with photorealistic quality. As such, there are three main contributions to state of the art: (1) online training of Radiance Fields using live data from multiple cameras, (2) support for a variety of radiance methods including NeRF and 3DGS, (3) visualization suite for these methods including a virtual reality scene. To enable seamless integration with existing setups, these components were tested with multiple robots in multiple configurations and were displayed using traditional tools as well as the VR headset. The results across methods and robots were compared quantitatively to a baseline of mesh reconstruction, and a user study was conducted to compare the different visualization methods. The code and additional samples are available at https://leggedrobotics.github.io/rffr.github.io/.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801345,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801345,,Training;Three-dimensional displays;Robot vision systems;Pipelines;Virtual reality;Cameras;Neural radiance field;User experience;User preference;Robots,,,,18,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://leggedrobotics.github.io/rffr,
48,DiffPrompter: Differentiable Implicit Visual Prompts for Semantic-Segmentation in Adverse Conditions,S. Kalwar; M. Ungarala; S. Jain; A. Monis; K. R. Konda; S. Garg; K. M. Krishna,"RRC, IIIT, Hyderabad, India; RRC, IIIT, Hyderabad, India; RRC, IIIT, Hyderabad, India; RRC, IIIT, Hyderabad, India; ZF TCI, Hyderabad, India; Australian Institute for Machine Learning, University of Adelaide, Australia; RRC, IIIT, Hyderabad, India",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,9791,9796,"Semantic segmentation in adverse weather scenarios is a critical task for autonomous driving systems. While foundation models have shown promise, the need for specialized adaptors becomes evident for handling more challenging scenarios. We introduce DiffPrompter, a novel differentiable visual and latent prompting mechanism aimed at expanding the learning capabilities of existing adaptors in foundation models. Our proposed ∇HFC (High Frequency Components) based image processing block excels particularly in adverse weather conditions, where conventional methods often fall short. Furthermore, we investigate the advantages of jointly training visual and latent prompts, demonstrating that this combined approach significantly enhances performance in out-of-distribution scenarios. Our differentiable visual prompts leverage parallel and series architectures to generate prompts, effectively improving object segmentation tasks in adverse conditions. Through a comprehensive series of experiments and evaluations, we provide empirical evidence to support the efficacy of our approach. Project page: diffprompter.github.io",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802718,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802718,,Training;Visualization;Foundation models;Semantic segmentation;Object segmentation;High frequency;Intelligent robots;Autonomous vehicles;Meteorology,,,,25,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
49,Dual-Branch Graph Transformer Network for 3D Human Mesh Reconstruction from Video,T. Tang; H. Liu; Y. You; T. Wang; W. Li,"State Key Laboratory of General Artificial Intelligence, Peking University, Shenzhen Graduate School, Shenzhen, China; State Key Laboratory of General Artificial Intelligence, Peking University, Shenzhen Graduate School, Shenzhen, China; State Key Laboratory of General Artificial Intelligence, Peking University, Shenzhen Graduate School, Shenzhen, China; State Key Laboratory of General Artificial Intelligence, Peking University, Shenzhen Graduate School, Shenzhen, China; State Key Laboratory of General Artificial Intelligence, Peking University, Shenzhen Graduate School, Shenzhen, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,11493,11499,"Human Mesh Reconstruction (HMR) from monocular video plays an important role in human-robot interaction and collaboration. However, existing video-based human mesh reconstruction methods face a trade-off between accurate reconstruction and smooth motion. These methods design networks based on either RNNs or attention mechanisms to extract local temporal correlations or global temporal dependencies, but the lack of complementary long-term information and local details limits their performance. To address this problem, we propose a Dual-branch Graph Transformer network for 3D human mesh Reconstruction from video, named DGTR. DGTR employs a dual-branch network including a Global Motion Attention (GMA) branch and a Local Details Refine (LDR) branch to par-allelly extract long-term dependencies and local crucial information, helping model global human motion and local human details (e.g., local motion, tiny movement). Specifically, GMA utilizes a global transformer to model long-term human motion. LDR combines modulated graph convolutional networks and the transformer framework to aggregate local information in adjacent frames and extract crucial information of human details. Experiments demonstrate that our DGTR outperforms state-of-the-art video-based methods in reconstruction accuracy and maintains competitive motion smoothness. Moreover, DGTR utilizes fewer parameters and FLOPs, which validate the effectiveness and efficiency of the proposed DGTR. Code is publicly available at https://github.com/TangTao-PKU/DGTR.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801569,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801569,,Three-dimensional displays;Accuracy;Graph convolutional networks;Design methodology;Human-robot interaction;Reconstruction algorithms;Transformers;Data mining;Intelligent robots;Faces,,,,39,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/TangTao-PKU/DGTR,https://github.com/TangTao-PKU/DGTR
50,I-ASM: Iterative Acoustic Scene Mapping for Enhanced Robot Auditory Perception in Complex Indoor Environments,L. Fu; Y. He; J. Wang; X. Qiao; H. Kong,"Shenzhen Key Laboratory of Control Theory and Intelligent Systems, Southern University of Science and Technology, Shenzhen, China; Shenzhen Key Laboratory of Control Theory and Intelligent Systems, Southern University of Science and Technology, Shenzhen, China; Shenzhen Key Laboratory of Control Theory and Intelligent Systems, Southern University of Science and Technology, Shenzhen, China; Shenzhen Key Laboratory of Control Theory and Intelligent Systems, Southern University of Science and Technology, Shenzhen, China; Shenzhen Key Laboratory of Control Theory and Intelligent Systems, Southern University of Science and Technology, Shenzhen, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,12318,12323,"This paper addresses the challenge of acoustic scene mapping (ASM) in complex indoor environments with multiple sound sources. Unlike existing methods that rely on prior data association or SLAM frameworks, we propose a novel particle filter-based iterative framework, termed I-ASM, for ASM using a mobile robot equipped with a microphone array and LiDAR. I-ASM harnesses an innovative ""implicit association"" to align sound sources with Direction of Arrival (DoA) observations without requiring explicit pairing, thereby streamlining the mapping process. Given inputs including an occupancy map, DoA estimates from various robot positions, and corresponding robot pose data, I-ASM performs multi-source mapping through an iterative cycle of ""Filtering-Clustering-Implicit Associating"". The proposed framework has been tested in real-world scenarios with up to 10 concurrent sound sources, demonstrating its robustness against missing and false DoA estimates while achieving high-quality ASM results. To benefit the community, we open-source all the codes and data at https://github.com/AISLAB-sustech/Acoustic-Scene-Mapping",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802630,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802630,,Direction-of-arrival estimation;Simultaneous localization and mapping;Refining;Acoustics;Robustness;Microphone arrays;Indoor environment;Iterative methods;Mobile robots;Resilience,,1.0,,25,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/AISLAB-sustech/Acoustic-Scene-Mapping,https://github.com/AISLAB-sustech/Acoustic-Scene-Mapping
51,Loss Distillation via Gradient Matching for Point Cloud Completion with Weighted Chamfer Distance,F. Lin; H. Liu; H. Zhou; S. Hou; K. D. Yamada; G. S. Fischer; Y. Li; H. K. Zhang; Z. Zhang,"Department of Robotics Engineering, Worcester Polytechnic Institute, Worcester, MA, USA; Department of Robotics Engineering, Worcester Polytechnic Institute, Worcester, MA, USA; Department of Robotics Engineering, Worcester Polytechnic Institute, Worcester, MA, USA; Dell Technologies, Hopkinton, MA, USA; Graduate School of Information Sciences, Tohoku University, Sendai, Japan; Department of Robotics Engineering, Worcester Polytechnic Institute, Worcester, MA, USA; Department of Computer Science, Worcester Polytechnic Institute, Worcester, MA, USA; Department of Robotics Engineering, Worcester Polytechnic Institute, Worcester, MA, USA; Department of Electrical & Computer Engineering, Worcester Polytechnic Institute, Worcester, MA, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,511,518,"3D point clouds enhanced the robot’s ability to perceive the geometrical information of the environments, making it possible for many downstream tasks such as grasp pose detection and scene understanding. The performance of these tasks, though, heavily relies on the quality of data input, as incomplete can lead to poor results and failure cases. Recent training loss functions designed for deep learning-based point cloud completion, such as Chamfer distance (CD) and its variants (e.g. HyperCD [1]), imply a good gradient weighting scheme can significantly boost performance. However, these CD-based loss functions usually require data-related parameter tuning, which can be time-consuming for data-extensive tasks. To address this issue, we aim to find a family of weighted training losses (weighted CD) that requires no parameter tuning. To this end, we propose a search scheme, Loss Distillation via Gradient Matching, to find good candidate loss functions by mimicking the learning behavior in backpropagation between HyperCD and weighted CD. Once this is done, we propose a novel bilevel optimization formula to train the backbone network based on the weighted CD loss. We observe that: (1) with proper weighted functions, the weighted CD can always achieve similar performance to HyperCD, and (2) the Landau weighted CD, namely Landau CD, can outperform HyperCD for point cloud completion and lead to new state-of-the-art results on several benchmark datasets. Our demo code is available at https://github.com/Zhang-VISLab/IROS2024-LossDistillationWeightedCD.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801828,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801828,,Point cloud compression;Training;Three-dimensional displays;Laser radar;Benchmark testing;Iterative algorithms;Tuning;Optimization;Intelligent robots;Convergence,,,,43,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/Zhang-VISLab/IROS2024-LossDistillationWeightedCD,https://github.com/Zhang-VISLab/IROS2024-LossDistillationWeightedCD
52,AutoInst: Automatic Instance-Based Segmentation of LiDAR 3D Scans,C. Perauer; L. A. Heidrich; H. Zhang; M. Nießner; A. Kornilova; A. Artemov,"Technical University of Munich, Garching, Germany; Technical University of Munich, Garching, Germany; Technical University of Munich, Garching, Germany; Technical University of Munich, Garching, Germany; Skolkovo Institute of Science and Technology, Moscow, Russia; Technical University of Munich, Garching, Germany",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,12340,12347,"Recently, progress in acquisition equipment such as LiDAR sensors has enabled sensing increasingly spacious outdoor 3D environments. Making sense of such 3D acquisitions requires fine-grained scene understanding, such as constructing instance-based 3D scene segmentations. Commonly, a neural network is trained for this task; however, this requires access to a large, densely annotated dataset, which is widely known to be challenging to obtain. To address this issue, in this work we propose to predict instance segmentations for 3D scenes in an unsupervised way, without relying on ground-truth annotations. To this end, we construct a learning framework consisting of two components: (1) a pseudo-annotation scheme for generating initial unsupervised pseudo-labels; and (2) a self-training algorithm for instance segmentation to fit robust, accurate instances from initial noisy proposals. To enable generating 3D instance mask proposals, we construct a weighted proxy-graph by connecting 3D points with edges integrating multi-modal image-and point-based self-supervised features, and perform graphcuts to isolate individual pseudo-instances. We then build on a state-of-the-art point-based architecture and train a 3D instance segmentation model, resulting in significant refinement of initial proposals. To scale to arbitrary complexity 3D scenes, we design our algorithm to operate on local 3D point chunks and construct a merging step to generate scene-level instance segmentations. Experiments on the challenging SemanticKITTI benchmark demonstrate the potential of our approach, where it attains 13.3% higher Average Precision and 9.1% higher F1 score compared to the best-performing baseline. The code is publicly available at https://github.com/artonson/autoinst.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10803059,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10803059,3D mapping;normalized cuts;instance segmentation;unsupervised learning,Instance segmentation;Point cloud compression;Solid modeling;Three-dimensional displays;Laser radar;Annotations;Refining;Prediction algorithms;Sensors;Proposals,,,,46,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/artonson/autoinst,https://github.com/artonson/autoinst
53,ToolEENet: Tool Affordance 6D Pose Estimation,Y. Wang; L. Zhang; Y. Tu; H. Zhang; K. Bai; Z. Chen; J. Zhang,"Department of Informatics, Universität Hamburg, Hamburg, Germany; Department of Informatics, Universität Hamburg, Hamburg, Germany; Department of Informatics, Universität Hamburg, Hamburg, Germany; Department of Informatics, Universität Hamburg, Hamburg, Germany; Department of Informatics, Universität Hamburg, Hamburg, Germany; Agile Robots AG; Department of Informatics, Universität Hamburg, Hamburg, Germany",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,10519,10526,"The exploration of robotic dexterous hands utilizing tools has recently attracted considerable attention. A significant challenge in this field is the precise awareness of a tool’s pose when grasped, as occlusion by the hand often degrades the quality of the estimation. Additionally, the tool’s overall pose often fails to accurately represent the contact interaction, thereby limiting the effectiveness of vision-guided, contact-dependent activities. To overcome this limitation, we present the innovative TOOLEE dataset, which, to the best of our knowledge, is the first to feature affordance segmentation of a tool’s end-effector (EE) along with its defined 6D pose based on its usage. Furthermore, we propose the ToolEENet framework for accurate 6D pose estimation of the tool’s EE. This framework begins by segmenting the tool’s EE from raw RGB-D data, then uses a diffusion model-based pose estimator for 6D pose estimation at a category-specific level. Addressing the issue of symmetry in pose estimation, we introduce a symmetry-aware pose representation that enhances the consistency of pose estimation. Our approach excels in this field, demonstrating high levels of precision and generalization. Furthermore, it shows great promise for application in contact-based manipulation scenarios. All data and codes are available on the project website: https://tooleenet-iros2024.github.io/",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801680,National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801680,,Hands;Limiting;Codes;Accuracy;Affordances;Pose estimation;End effectors;Data models;Intelligent robots,,,,32,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
54,MQE: Unleashing the Power of Interaction with Multi-agent Quadruped Environment,Z. Xiong; B. Chen; S. Huang; W. -W. Tu; Z. He; Y. Gao,"Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China; AI Institute, Beijing University of Posts and Telecommunications, Beijing, China; Zhipu AI, Beijing, China; 4Paradigm Inc., Beijing, China; AI Institute, Beijing University of Posts and Telecommunications, Beijing, China; Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,5918,5924,"The advent of deep reinforcement learning (DRL) has significantly advanced the field of robotics, particularly in the control and coordination of quadruped robots. However, the complexity of real-world tasks often necessitates the deployment of multi-robot systems capable of sophisticated interaction and collaboration. To address this need, we introduce the Multi-agent Quadruped Environment (MQE), a novel platform designed to facilitate the development and evaluation of multi-agent reinforcement learning (MARL) algorithms in realistic and dynamic scenarios. MQE emphasizes complex interactions between robots and objects, hierarchical policy structures, and challenging evaluation scenarios that reflect real-world applications. We present a series of collaborative and competitive tasks within MQE, ranging from simple coordination to complex adversarial interactions, and benchmark state-of-the-art MARL algorithms. Our findings indicate that hierarchical reinforcement learning can simplify task learning, but also highlight the need for advanced algorithms capable of handling the intricate dynamics of multi-agent interactions. MQE serves as a stepping stone towards bridging the gap between simulation and practical deployment, offering a rich environment for future research in multi-agent systems and robot learning. For open-sourced code and more details of MQE, please refer to https://ziyanx02.github.io/multiagent-quadruped-environment/.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801682,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801682,quadrupedal locomotion;multi-agent reinforcement learning;hierarchical reinforcement learning,Heuristic algorithms;Robot kinematics;Collaboration;Deep reinforcement learning;Robot learning;Distance measurement;Quadrupedal robots;Multi-robot systems;Intelligent robots;Multi-agent systems,,,,44,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://ziyanx02.github.io/multiagent-quadruped-environment,https://github.com/ziyanx02/multiagent-quadruped-environment
55,DiffusionNOCS: Managing Symmetry and Uncertainty in Sim2Real Multi-Modal Category-level Pose Estimation,T. Ikeda; S. Zakharov; T. Ko; M. Z. Irshad; R. Lee; K. Liu; R. Ambrus; K. Nishiwaki,"Woven by Toyota, Tokyo, Japan; Toyota Research Institute, Los Altos, CA, USA; Woven by Toyota, Tokyo, Japan; Toyota Research Institute, Los Altos, CA, USA; Woven by Toyota, Tokyo, Japan; Toyota Research Institute, Los Altos, CA, USA; Toyota Research Institute, Los Altos, CA, USA; Woven by Toyota, Tokyo, Japan",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,7406,7413,"This paper addresses the challenging problem of category-level pose estimation. Current state-of-the-art methods for this task face challenges when dealing with symmetric objects and when attempting to generalize to new environments solely through synthetic data training. In this work, we address these challenges by proposing a probabilistic model that relies on diffusion to estimate dense canonical maps crucial for recovering partial object shapes as well as establishing correspondences essential for pose estimation. Furthermore, we introduce critical components to enhance performance by leveraging the strength of the diffusion models with multi-modal input representations. We demonstrate the effectiveness of our method by testing it on a range of real datasets. Despite being trained solely on our generated synthetic data, our approach achieves state-of-the-art performance and unprecedented generalization qualities, outperforming baselines, even those specifically trained on the target domain. Our code and data for the generalization benchmark can be found at https://woven-planet.github.io/DiffusionNOCS/.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802487,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802487,,Training;Uncertainty;Shape;Pose estimation;Pipelines;Benchmark testing;Probabilistic logic;Diffusion models;Intelligent robots;Synthetic data,,,,40,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://woven-planet.github.io/DiffusionNOCS,https://github.com/woven-planet/DiffusionNOCS
56,Real-Time Semantic Segmentation in Natural Environments with SAM-assisted Sim-to-Real Domain Transfer,H. Wang; R. Mascaro; M. Chli; L. Teixeira,"Vision For Robotics Lab, ETH Zürich and University of Cyprus; Vision For Robotics Lab, ETH Zürich and University of Cyprus; Vision For Robotics Lab, ETH Zürich and University of Cyprus; Vision For Robotics Lab, ETH Zürich and University of Cyprus",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,53,60,"Semantic segmentation plays a pivotal role in many robotic applications requiring high-level scene understanding, such as smart farming, where the precise identification of trees or plants can aid navigation and crop monitoring tasks. While deep-learning-based semantic segmentation approaches have reached outstanding performance in recent years, they demand large amounts of labeled data for training. Inspired by modern Unsupervised Domain Adaptation (UDA) techniques, in this paper, we introduce a two-step training pipeline specifically tailored to challenging natural scenes, where the availability of annotated data is often quite limited. Our strategy involves the initial training of a powerful domain adaptive architecture, followed by a refinement stage, where segmentation masks predicted by the Segment Anything Model (SAM) are used to improve the accuracy of the predictions on the target dataset. These refined predictions serve as pseudo-labels to supervise the training of a final distilled architecture for real-time deployment. Extensive experiments conducted in two real-world scenes demonstrate the effectiveness of the proposed method. Specifically, we show that our pipeline enables the training of a MobileNetV3 that achieves significant mIoU gains of 3.60% and 11.40% on our two datasets compared to the DAFormer while only demanding 1/15 of the latter’s inference time. Code and datasets are available at https://github.com/VIS4ROB-lab/nature_uda_rt_segmentation.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801798,European Research Council; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801798,,Training;Smart agriculture;Adaptation models;Navigation;Semantic segmentation;Computational modeling;Pipelines;Computer architecture;Predictive models;Real-time systems,,,,42,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/VIS4ROB-lab/nature_uda_rt_segmentation,https://github.com/VIS4ROB-lab/nature_uda_rt_segmentation
57,UWB-Based Localization System Considering Antenna Anisotropy and NLOS/Multipath Conditions,T. Kim; B. Yoon; D. Lee,"Department of Mechanical Engineering, IAMD and IOER, Seoul National University, Seoul, Republic of Korea; Department of Mechanical Engineering, IAMD and IOER, Seoul National University, Seoul, Republic of Korea; Department of Mechanical Engineering, IAMD and IOER, Seoul National University, Seoul, Republic of Korea",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,8810,8816,"Ultra-wideband (UWB) communication technology has gained attention in robotics due to its ability to provide range measurements possibly with centimeter-level accuracy. Nevertheless, practical UWB range measurements are susceptible to disturbances from multiple sources, including the anisotropic characteristics of antennas, non-line-of-sight (NLOS) conditions, and multipath propagation. In this paper, we introduce a UWB range measurement model that addresses these sources of error. To accommodate the effects of antenna anisotropy, we adopt real spherical harmonics to represent directional bias in the UWB range measurement model. To handle delayed measurements induced by NLOS conditions and multipath propagation, an asymmetric heavy-tailed distribution is utilized to model the measurement noise. We calibrate this measurement model based on the maximum likelihood estimation method and propose a UWB-based localization system based on that. Our localization system provides: 1) anchor self-calibration, which identifies anchor placement by fusing visual-inertial-ranging measurements based on continuous-time state representation; and 2) filtering-based state estimation, which applies our measurement model into Kalman filtering framework via an iterative update algorithm. Experimental validation is conducted to demonstrate the effectiveness of the measurement model for our localization system. We open source our implementation of the proposed UWB-based localization system at https://github.com/INRoL/inrol_uwb_localization.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802170,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802170,,Antenna measurements;Location awareness;Analytical models;Anisotropic magnetoresistance;Accuracy;Computational modeling;Noise;Ultra wideband communication;Ultra wideband antennas;Noise measurement,,1.0,,25,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/INRoL/inrol_uwb_localization,https://github.com/INRoL/inrol_uwb_localization
58,Towards Cross-View-Consistent Self-Supervised Surround Depth Estimation,L. Ding; H. Jiang; J. Li; Y. Chen; R. Huang,"School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China; Insta360 Research, Shenzhen, China; School of Artificial Intelligence, Shenzhen Polytechnic University, Shenzhen, China; Shenzhen Institute of Artificial Intelligence and Robotics for Society, The Chinese University of Hong Kong, Shenzhen, China; School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,10043,10050,"Depth estimation is a cornerstone for autonomous driving, yet acquiring per-pixel depth ground truth for supervised learning is challenging. Self-Supervised Surround Depth Estimation (SSSDE) from consecutive images offers an economical alternative. While previous SSSDE methods have proposed different mechanisms to fuse information across images, few of them explicitly consider the cross-view constraints, leading to inferior performance, particularly in overlapping regions. This paper proposes an efficient and consistent pose estimation design and two loss functions to enhance cross-view consistency for SSSDE. For pose estimation, we propose to use only front-view images to reduce training memory and sustain pose estimation consistency. The first loss function is the dense depth consistency loss, which penalizes the difference between predicted depths in overlapping regions. The second one is the multi-view reconstruction consistency loss, which aims to maintain consistency between reconstruction from spatial and spatial-temporal contexts. Additionally, we introduce a novel flipping augmentation to improve the performance further. Our techniques enable a simple neural model to achieve state-of-the-art performance on the DDAD and nuScenes datasets. Last but not least, our proposed techniques can be easily applied to other methods. The code is available at https://github.com/denyingmxd/CVCDepth.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802436,Shenzhen Polytechnic; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802436,,Training;Codes;Fuses;Depth measurement;Pose estimation;Supervised learning;Image reconstruction;Intelligent robots;Autonomous vehicles,,,,37,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/denyingmxd/CVCDepth,https://github.com/denyingmxd/CVCDepth
59,SuFIA: Language-Guided Augmented Dexterity for Robotic Surgical Assistants,M. Moghani; L. Doorenbos; W. C. -H. Panitch; S. Huver; M. Azizian; K. Goldberg; A. Garg,"University of Toronto; University of Bern; University of California, Berkeley; NVIDIA; NVIDIA; University of California, Berkeley; University of Toronto",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,6969,6976,"In this work, we present SuFIA, the first framework for natural language-guided augmented dexterity for robotic surgical assistants. SuFIA incorporates the strong reasoning capabilities of large language models (LLMs) with perception modules to implement high-level planning and low-level control of a robot for surgical sub-task execution. This enables a learning-free approach to surgical augmented dexterity without any in-context examples or motion primitives. SuFIA uses a human-in-the-loop paradigm by restoring control to the surgeon in the case of insufficient information, mitigating unexpected errors for mission-critical tasks. We evaluate SuFIA on four surgical sub-tasks in a simulation environment and two sub-tasks on a physical surgical robotic platform in the lab, demonstrating its ability to perform common surgical sub-tasks through supervised autonomous operation under challenging physical and workspace conditions.Project website: orbit-surgical.github.io/sufia",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802053,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802053,,Privacy;Medical robotics;Large language models;Mission critical systems;Surgery;Human in the loop;Orbits;Safety;Planning;Servers,,1.0,,47,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://orbit-surgical.github.io/sufia,https://github.com/nerfies/nerfies.github.io
60,LDIP: Real-time on-road object detection with depth estimation from a single image,C. Xu; X. Sun; Y. Xu; R. Wang,"Institute of Advanced Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, Hefei University of Technology, Hefei, China; Institute of Advanced Technology, University of Science and Technology of China, Hefei, China; NA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,9455,9460,"Detecting on-road objects with absolute depth information is one of the most crucial tasks in autonomous driving to ensure safety. Traditional 2D object detection aims to classify and locate objects in image space, but it cannot acquire in-depth information. While 3D object detection and pixel-level depth detection tasks can provide accurate depth information for objects, they are challenging to deploy in real-world scenarios due to their significant inference overhead. This paper proposes a novel deep learning-based model named the Location and Depth Information Perceptron (LDIP), designed to provide positional, categorical, and absolute depth information for given objects in the images.We first conducted model training and validation on the vehicle-side autonomous driving dataset—KITTI. The experimental results show that we achieved a 68.6% mAP in object recognition tasks and an RMSE of 0.101 and AbsRel of 2.327 in depth estimation tasks, all of which represent state-of-the-art performance in comparable tasks. Subsequently, we fine-tuned the trained model on DAIR, where the validated mAP, AbsRel, and RMSE reached 65.4%, 0.092, and 2.461 respectively. This demonstrates the robustness and generalization of our model across different types of road datasets.Moreover, in comparison to other models, our model is more compact while maintaining accuracy, achieving an inference speed of 70 frames per second on an NVIDIA 4060 GPU, thus making it deployable in practical scenarios. Relevant code is available at https://github.com/xcp-ustc/LDIP.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801572,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801572,,Training;Accuracy;Three-dimensional displays;Depth measurement;Roads;Object detection;Robustness;Real-time systems;Safety;Autonomous vehicles,,,,26,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/xcp-ustc/LDIP,https://github.com/xcp-ustc/LDIP
61,Switching Sampling Space of Model Predictive Path-Integral Controller to Balance Efficiency and Safety in 4WIDS Vehicle Navigation,M. Aoki; K. Honda; H. Okuda; T. Suzuki,"The Department of Mechanical Systems Engineering, Graduate School of Engineering, Nagoya University, Nagoya, Aichi, Japan; The Department of Mechanical Systems Engineering, Graduate School of Engineering, Nagoya University, Nagoya, Aichi, Japan; The Department of Mechanical Systems Engineering, Graduate School of Engineering, Nagoya University, Nagoya, Aichi, Japan; The Department of Mechanical Systems Engineering, Graduate School of Engineering, Nagoya University, Nagoya, Aichi, Japan",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,3196,3203,"Four-wheel independent drive and steering vehicle (4WIDS Vehicle, Swerve Drive Robot) has the ability to move in any direction by its eight degrees of freedom (DoF) control inputs. Although the high maneuverability enables efficient navigation in narrow spaces, obtaining the optimal command is challenging due to the high dimension of the solution space. This paper presents a navigation architecture using the Model Predictive Path Integral (MPPI) control algorithm to avoid collisions with obstacles of any shape and reach a goal point. The key idea to make the problem easier is to explore the optimal control input in a reasonably reduced dimension that is adequate for navigation. Through evaluation in simulation, we found that the selecting sampling space of MPPI greatly affects navigation performance. In addition, our proposed controller which switches multiple sampling spaces according to the real-time situation can achieve balanced behavior between efficiency and safety.Source code is available at https://github.com/MizuhoAOKI/mppi_swerve_drive_ros.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802359,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802359,,Space vehicles;Systematics;Navigation;Shape;Optimal control;Switches;Aerospace electronics;Predictive models;Real-time systems;Stability analysis,,,,26,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/MizuhoAOKI/mppi_swerve_drive_ros,https://github.com/MizuhoAOKI/mppi_swerve_drive_ros
62,Preventing Catastrophic Forgetting in Continuous Online Learning for Autonomous Driving,R. Yang; T. Yang; Z. Yan; T. Krajnik; Y. Ruichek,"UTBM, Belfort, France; Unmanned System Research Institute, Northwestern Polytechnical University, China; UTBM, Belfort, France; Artificial Intelligence Center, Czech Technical University in Prague, Czechia; UTBM, Belfort, France",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,5505,5512,"Autonomous vehicles require online learning capabilities to enable long-term, unattended operation. However, long-term online learning is accompanied by the problem of forgetting previously learned knowledge. This paper introduces an online learning framework that includes a catastrophic forgetting prevention mechanism, named Long-Short-Term Online Learning (LSTOL). The framework consists of a set of shortterm learners and a long-term controller, where the former is based on the concept of ensemble learning and aims to achieve rapid learning iterations, while the latter contains a simple yet efficient probabilistic decision-making mechanism combined with four control primitives to achieve effective knowledge maintenance. A novel feature of the proposed LSTOL is that it avoids forgetting while learning autonomously. In addition, LSTOL makes no assumptions about the model type of short-term learners and the continuity of the data. The effectiveness of the proposed framework is demonstrated through experiments across well-known datasets in autonomous driving, including KITTI and Waymo. The source code for the method implementation is publicly available at https://github.com/epan-utbm/lstol.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801619,China Scholarship Council; National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801619,,Sensitivity;Navigation;Source coding;Roads;Prevention and mitigation;Robot sensing systems;Probabilistic logic;Vehicle dynamics;Autonomous vehicles;Meteorology,,,,36,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/epan-utbm/lstol,https://github.com/epan-utbm/lstol
63,Accelerating Model Predictive Control for Legged Robots through Distributed Optimization,L. Amatucci; G. Turrisi; A. Bratta; V. Barasuol; C. Semini,"Dynamic Legged Systems Laboratory, Istituto Italiano di Tecnologia (IIT), Genova, Italy; Dynamic Legged Systems Laboratory, Istituto Italiano di Tecnologia (IIT), Genova, Italy; Dynamic Legged Systems Laboratory, Istituto Italiano di Tecnologia (IIT), Genova, Italy; Dynamic Legged Systems Laboratory, Istituto Italiano di Tecnologia (IIT), Genova, Italy; Dynamic Legged Systems Laboratory, Istituto Italiano di Tecnologia (IIT), Genova, Italy",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,12734,12741,"This paper presents a novel approach to enhance Model Predictive Control (MPC) for legged robots through Distributed Optimization. Our method focuses on decomposing the robot dynamics into smaller, parallelizable subsystems, and utilizing the Alternating Direction Method of Multipliers (ADMM) to ensure consensus among them. Each subsystem is managed by its own Optimal Control Problem, with ADMM facilitating consistency between their optimizations. This approach not only decreases the computational time but also allows for effective scaling with more complex robot configurations, facilitating the integration of additional subsystems such as articulated arms on a quadruped robot. We demonstrate, through numerical evaluations, the convergence of our approach on two systems with increasing complexity. In addition, we showcase that our approach converges towards the same solution when compared to a state-of-the-art centralized whole-body MPC implementation. Moreover, we quantitatively compare the computational efficiency of our method to the centralized approach, revealing up to a 75% reduction in computational time. Overall, our approach offers a promising avenue for accelerating MPC solutions for legged robots, paving the way for more effective utilization of the computational performance of modern hardware. Accompanying video at https://youtu.be/Yar4W-Vlh2A. The related code can be found at https://github.com/iit-DLSLab/DWMPC",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801676,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801676,,Legged locomotion;Manipulators;Hardware;Convex functions;Computational efficiency;Numerical models;Quadrupedal robots;Robots;Optimization;Predictive control,,1.0,,27,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/iit-DLSLab/DWMPC,https://github.com/iit-DLSLab/DWMPC
64,GenCHiP: Generating Robot Policy Code for High-Precision and Contact-Rich Manipulation Tasks,K. Burns; A. Jain; K. Go; F. Xia; M. Stark; S. Schaal; K. Hausman,Google Intrinsic; Google Intrinsic; Google Intrinsic; Google DeepMind; Google Intrinsic; Google Intrinsic; Stanford University,2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,9596,9603,"Large Language Models (LLMs) have been successful at generating robot policy code, but so far these results have been limited to high-level tasks that do not require precise movement. It is an open question how well such approaches work for tasks that require reasoning over contact forces and working within tight success tolerances. We find that, with the right action space, LLMs are capable of successfully generating policies for a variety of contact-rich and high-precision manipulation tasks, even under noisy conditions, such as perceptual errors or grasping inaccuracies. Specifically, we reparameterize the action space to include compliance with constraints on the interaction forces and stiffnesses involved in reaching a target pose. We validate this approach on subtasks derived from the Functional Manipulation Benchmark (FMB) and NIST Task Board Benchmarks. Exposing this action space alongside methods for estimating object poses improves policy generation with an LLM by greater than 3x and 4x when compared to non-compliant action spaces. More material is available on our project webpage: https://dex-code-gen.github.io/dex-code-gen/",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801525,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801525,,Codes;Large language models;Grasping;Benchmark testing;NIST;Cognition;Noise measurement;Intelligent robots,,,,43,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://dex-code-gen.github.io/dex-code-gen,
65,DexDribbler: Learning Dexterous Soccer Manipulation via Dynamic Supervision,Y. Hu; K. Wen; F. Yu,"VIS Group, Computer Vision Lab, ETH Zurich, Zurich, Switzerland; VIS Group, Computer Vision Lab, ETH Zurich, Zurich, Switzerland; VIS Group, Computer Vision Lab, ETH Zurich, Zurich, Switzerland",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,12910,12917,"Learning dexterous locomotion policy for legged robots is becoming increasingly popular due to its ability to handle diverse terrains and resemble intelligent behaviors. However, joint manipulation of moving objects and locomotion with legs, such as playing soccer, receive scant attention in the learning community, although it is natural for humans and smart animals. A key challenge to solve this multitask problem is to infer the objectives of locomotion from the states and targets of the manipulated objects. The implicit relation between the object states and robot locomotion can be hard to capture directly from the training experience. We propose adding a feedback control block to compute the necessary body- level movement accurately and using the outputs as dynamic joint-level locomotion supervision explicitly. We further utilize an improved ball dynamic model, an extended context-aided estimator, and a comprehensive ball observer to facilitate transferring policy learned in simulation to the real world. We observe that our learning scheme can not only make the policy network converge faster but also enable soccer robots to perform sophisticated maneuvers like sharp cuts and turns on flat surfaces, a capability that was lacking in previous methods. Video and code are available at github.com/SysCV/soccer-player.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802022,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802022,,Legged locomotion;Training;Codes;Computational modeling;Observers;Feedback control;Manipulator dynamics;Intelligent robots;Sports;Context modeling,,,,36,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/SysCV/soccer-player,https://github.com/SysCV/soccer-player
66,SoftMAC: Differentiable Soft Body Simulation with Forecast-based Contact Model and Two-way Coupling with Articulated Rigid Bodies and Clothes,M. Liu; G. Yang; S. Luo; L. Shao,"Machine Learning Department, Carnegie Mellon University, USA; Department of Computer Science, National University of Singapore, Singapore; Department of Computer Science, National University of Singapore, Singapore; Department of Computer Science, National University of Singapore, Singapore",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,12008,12015,"Differentiable physics simulation provides an avenue to tackle previously intractable challenges through gradient-based optimization, thereby greatly improving the efficiency of solving robotics-related problems. To apply differentiable simulation in diverse robotic manipulation scenarios, a key challenge is to integrate various materials in a unified framework. We present SoftMAC, a differentiable simulation framework that couples soft bodies with articulated rigid bodies and clothes. SoftMAC simulates soft bodies with the continuum-mechanics-based Material Point Method (MPM). We provide a novel forecast-based contact model for MPM, which effectively reduces penetration without introducing other artifacts like unnatural rebound. To couple MPM particles with deformable and non-volumetric clothes meshes, we also propose a penetration tracing algorithm that reconstructs the signed distance field in local area. Diverging from previous works, SoftMAC simulates the complete dynamics of each modality and incorporates them into a cohesive system with an explicit and differentiable coupling mechanism. The feature empowers SoftMAC to handle a broader spectrum of interactions, such as soft bodies serving as manipulators and engaging with underactuated systems. We conducted comprehensive experiments to validate the effectiveness and accuracy of the proposed differentiable pipeline in downstream robotic manipulation applications. Supplementary materials are available on our project website at https://damianliumin.github.io/SoftMAC.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801308,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801308,,Couplings;Accuracy;Heuristic algorithms;Pipelines;Predictive models;Physics;Optimization;Manipulator dynamics;Intelligent robots,,,,38,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://damianliumin.github.io/SoftMAC,https://github.com/damianliumin/SoftMAC
67,GMMCalib: Extrinsic Calibration of LiDAR Sensors using GMM-based Joint Registration,I. Tahiraj; F. Fent; P. Hafemann; E. Ye; M. Lienkamp,"TUM School of Engineering and Design, Chair of Automotive Technology, Technical University of Munich; TUM School of Engineering and Design, Chair of Automotive Technology, Technical University of Munich; TUM School of Engineering and Design, Chair of Automotive Technology, Technical University of Munich; TUM School of Computation, Information and Technology, Professorship of Cyber Physical Systems, Technical University of Munich; TUM School of Engineering and Design, Chair of Automotive Technology, Technical University of Munich",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,8555,8562,"State-of-the-art LiDAR calibration frameworks mainly use non-probabilistic registration methods such as Iterative Closest Point (ICP) and its variants. These methods suffer from biased results due to their pair-wise registration procedure as well as their sensitivity to initialization and parameterization. This often leads to misalignments in the calibration process. Probabilistic registration methods compensate for these drawbacks by specifically modeling the probabilistic nature of the observations. This paper presents GMMCalib, an automatic target-based extrinsic calibration approach for multi-LiDAR systems. Using an implementation of a Gaussian Mixture Model (GMM)-based registration method that allows joint registration of multiple point clouds, this data-driven approach is compared to ICP algorithms. We perform simulation experiments using the digital twin of the EDGAR research vehicle and validate the results in a real-world environment. We also address the local minima problem of local registration methods for extrinsic sensor calibration and use a distance-based metric to evaluate the calibration results. Our results show that an increase in robustness against sensor miscalibrations can be achieved by using GMM-based registration algorithms. The code is open source and available on GitHub3.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801262,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801262,,Point cloud compression;Laser radar;Sensitivity;Iterative closest point algorithm;Robot sensing systems;Probabilistic logic;Robustness;Calibration;Iterative methods;Intelligent robots,,,,27,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
68,"NeuFlow: Real-time, High-accuracy Optical Flow Estimation on Robots Using Edge Devices",Z. Zhang; H. Jiang; H. Singh,"Department of Electrical and Computer Engineering, Northeastern University, Boston, MA; Khoury College of Computer Sciences, Northeastern University, Boston, MA; Department of Electrical and Computer Engineering, Northeastern University, Boston, MA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,5048,5055,"Real-time high-accuracy optical flow estimation is a crucial component in various applications, including localization and mapping in robotics, object tracking, and activity recognition in computer vision. While recent learning-based optical flow methods have achieved high accuracy, they often come with heavy computation costs. In this paper, we propose a highly efficient optical flow architecture, called NeuFlow, that addresses both high accuracy and computational cost concerns. The architecture follows a global-to-local scheme. Given the features of the input images extracted at different spatial resolutions, global matching is employed to estimate an initial optical flow on the 1/16 resolution, capturing large displacement, which is then refined on the 1/8 resolution with lightweight CNN layers for better accuracy. We evaluate our approach on Jetson Orin Nano and RTX 2080 to demonstrate efficiency improvements across different computing platforms. We achieve a notable 10-80 speedup compared to several state-of-the-art methods, while maintaining comparable accuracy. Our approach achieves around 30 FPS on edge computing platforms, which represents a significant breakthrough in deploying complex computer vision tasks such as SLAM on small robots like drones. The full training and evaluation code is available at https://github.com/neufieldrobotics/NeuFlow.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802353,National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802353,,Training;Computer vision;Accuracy;Simultaneous localization and mapping;Estimation;Computer architecture;Feature extraction;Real-time systems;Spatial resolution;Optical flow,,,,53,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/neufieldrobotics/NeuFlow,https://github.com/neufieldrobotics/NeuFlow
69,Prompt-Driven Temporal Domain Adaptation for Nighttime UAV Tracking,C. Fu; Y. Wang; L. Yao; G. Zheng; H. Zuo; J. Pan,"School of Mechanical Engineering, Tongji University, Shanghai, China; School of Mechanical Engineering, Tongji University, Shanghai, China; School of Mechanical Engineering, Tongji University, Shanghai, China; Department of Computer Science, The University of Hong Kong, Hong Kong, China; Department of Computer Science, The University of Hong Kong, Hong Kong, China; Department of Computer Science, The University of Hong Kong, Hong Kong, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,9706,9713,"Nighttime UAV tracking under low-illuminated scenarios has achieved great progress by domain adaptation (DA). However, previous DA training-based works are deficient in narrowing the discrepancy of temporal contexts for UAV trackers. To address the issue, this work proposes a prompt-driven temporal domain adaptation training framework to fully utilize temporal contexts for challenging nighttime UAV tracking, i.e., TDA. Specifically, the proposed framework aligns the distribution of temporal contexts from daytime and nighttime domains by training the temporal feature generator against the discriminator. The temporal-consistent discriminator progressively extracts shared domain-specific features to generate coherent domain discrimination results in the time series. Additionally, to obtain high-quality training samples, a prompt-driven object miner is employed to precisely locate objects in unannotated nighttime videos. Moreover, a new benchmark for long-term nighttime UAV tracking is constructed. Exhaustive evaluations on both public and self-constructed nighttime benchmarks demonstrate the remarkable performance of the tracker trained in TDA framework, i.e., TDA-Track. Real-world tests at nighttime also show its practicality. The code and demo videos are available at https://github.com/vision4robotics/TDA-Track.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801335,National Natural Science Foundation of China; Natural Science Foundation of Shanghai; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801335,,Training;Codes;Time series analysis;Benchmark testing;Autonomous aerial vehicles;Feature extraction;Generators;Trajectory;Intelligent robots;Videos,,,,37,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/vision4robotics/TDA-Track,https://github.com/vision4robotics/TDA-Track
70,Agile and Safe Trajectory Planning for Quadruped Navigation with Motion Anisotropy Awareness,W. Zhang; S. Xu; P. Cai; L. Zhu,"School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,8839,8846,"Quadruped robots demonstrate robust and agile movements in various terrains; however, their navigation autonomy is still insufficient. One of the challenges is that the motion capabilities of the quadruped robot are anisotropic along different directions, which significantly affects the safety of quadruped robot navigation. This paper proposes a navigation framework that takes into account the motion anisotropy of quadruped robots including kinodynamic trajectory generation, nonlinear trajectory optimization, and nonlinear model predictive control. In simulation and real robot tests, we demonstrate that our motion-anisotropy-aware navigation framework could: (1) generate more efficient trajectories and realize more agile quadruped navigation; (2) significantly improve the navigation safety in challenging scenarios. The implementation is realized as an open-source package at https://github.com/ZWT006/agile_navigation.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10803105,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10803105,,Anisotropic magnetoresistance;Three-dimensional displays;Navigation;Trajectory planning;Robot kinematics;Planning;Safety;Quadrupedal robots;Robots;Trajectory optimization,,,,40,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/ZWT006/agile_navigation,https://github.com/ZWT006/agile_navigation
71,Multimodal Evolutionary Encoder for Continuous Vision-Language Navigation,Z. He; L. Wang; L. Chen; S. Li; Q. Yan; C. Liu; Q. Chen,"Department of Control Science and Engineering, Tongji University, Shanghai, China; Department of Control Science and Engineering, Tongji University, Shanghai, China; Department of Control Science and Engineering, Tongji University, Shanghai, China; Department of Control Science and Engineering, Tongji University, Shanghai, China; Department of Control Science and Engineering, Tongji University, Shanghai, China; Department of Control Science and Engineering, Tongji University, Shanghai, China; Department of Control Science and Engineering, Tongji University, Shanghai, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,1443,1450,"Can multimodal encoder evolve when facing increasingly tough circumstances? Our work investigates this possibility in the context of continuous vision-language navigation (continuous VLN), which aims to navigate robots under linguistic supervision and visual feedback. We propose a multimodal evolutionary encoder (MEE) comprising a unified multimodal encoder architecture and an evolutionary pre-training strategy. The unified multimodal encoder unifies rich modalities, including depth and sub-instruction, to enhance the solid understanding of environments and tasks. It also effectively utilizes monocular observation, reducing the reliance on panoramic vision. The evolutionary pre-training strategy exposes the encoder to increasingly unfamiliar data domains and difficult objectives. The multi-stage adaption helps the encoder establish robust intra- and inter-modality connections and improve its generalization to unfamiliar environments. To achieve such evolution, we collect a large-scale multi-stage dataset with specialized objectives, addressing the absence of suitable continuous VLN pre-training. Evaluation on VLN-CE demonstrates the superiority of MEE over other direct action-predicting methods. Furthermore, we deploy MEE in real scenes using self-developed service robots, showcasing its effectiveness and potential for real-world applications. Our code and dataset are available at https://github.com/RavenKiller/MEE.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802484,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802484,,Visualization;Costs;Codes;Navigation;Service robots;Linguistics;Feature extraction;Solids;Decoding;Intelligent robots,,,,52,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/RavenKiller/MEE,https://github.com/RavenKiller/MEE
72,Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion Descriptors,N. Tsagkas; J. Rome; S. Ramamoorthy; O. M. Aodha; C. X. Lu,"School of Informatics, University of Edinburgh, UK; School of Informatics, University of Edinburgh, UK; School of Informatics, University of Edinburgh, UK; School of Informatics, University of Edinburgh, UK; Department of Computer Science, University College London, UK",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,11610,11617,"Precise manipulation that is generalizable across scenes and objects remains a persistent challenge in robotics. Current approaches for this task heavily depend on having a significant number of training instances to handle objects with pronounced visual and/or geometric part ambiguities. Our work explores the grounding of fine-grained part descriptors for precise manipulation in a zero-shot setting by utilizing web-trained text-to-image diffusion-based generative models. We tackle the problem by framing it as a dense semantic part correspondence task. Our model returns a gripper pose for manipulating a specific part, using as reference a user-defined click from a source image of a visually different instance of the same object. We require no manual grasping demonstrations as we leverage the intrinsic object geometry and features. Practical experiments in a real-world tabletop scenario validate the efficacy of our approach, demonstrating its potential for advancing semantic-aware robotics manipulation.Web page: https://tsagkas.github.io/click2grasp",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801488,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801488,,Training;Visualization;Foundation models;Annotations;Target recognition;Semantics;Text to image;Grasping;Manuals;Grippers,,,,47,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://tsagkas.github.io/click2grasp,https://github.com/tsagkas/click2grasp
73,IR2: Implicit Rendezvous for Robotic Exploration Teams under Sparse Intermittent Connectivity,D. M. Siang Tan; Y. Ma; J. Liang; Y. C. Chng; Y. Cao; G. Sartoretti,"Department of Mechanical Engineering, College of Design and Engineering, National University of Singapore; Department of Mechanical Engineering, College of Design and Engineering, National University of Singapore; Department of Mechanical Engineering, College of Design and Engineering, National University of Singapore; Singapore Technologies Engineering Ltd; Department of Mechanical Engineering, College of Design and Engineering, National University of Singapore; Department of Mechanical Engineering, College of Design and Engineering, National University of Singapore",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,13245,13252,"Information sharing is critical in time-sensitive and realistic multi-robot exploration, especially for smaller robotic teams in large-scale environments where connectivity may be sparse and intermittent. Existing methods often overlook such communication constraints by assuming unrealistic global connectivity. Other works account for communication constraints (by maintaining close proximity or line of sight during information exchange), but are often inefficient. For instance, preplanned rendezvous approaches typically involve unnecessary detours resulting from poorly timed rendezvous, while pursuit-based approaches often result in short-sighted decisions due to their greedy nature. We present IR2, a deep reinforcement learning approach to information sharing for multi-robot exploration. Leveraging attention-based neural networks trained via reinforcement and curriculum learning, IR2 allows robots to effectively reason about the longer-term trade-offs between disconnecting for solo exploration and reconnecting for information sharing. In addition, we propose a hierarchical graph formulation to maintain a sparse yet informative graph, enabling our approach to scale to large-scale environments. We present simulation results in three large-scale Gazebo environments, which show that our approach yields 6.6−34.1% shorter exploration paths and significantly improved mapped area consistency among robots when compared to state-of-the-art baselines. Our simulation training and testing code is available at https://github.com/marmotlab/IR2.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801761,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801761,,Training;Three-dimensional displays;Simulation;Scalability;Neural networks;Information sharing;Packet loss;Deep reinforcement learning;Robots;Standards,,,,29,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/marmotlab/IR2,https://github.com/marmotlab/IR2
74,"ParkingE2E: Camera-based End-to-end Parking Network, from Images to Planning",C. Li; Z. Ji; Z. Chen; T. Qin; M. Yang,"Global Institute of Future Technology, Shanghai Jiao Tong University, Shanghai, China; Global Institute of Future Technology, Shanghai Jiao Tong University, Shanghai, China; Global Institute of Future Technology, Shanghai Jiao Tong University, Shanghai, China; Global Institute of Future Technology, Shanghai Jiao Tong University, Shanghai, China; Global Institute of Future Technology, Shanghai Jiao Tong University, Shanghai, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,13206,13212,"Autonomous parking is a crucial task in the intelligent driving field. Traditional parking algorithms are usually implemented using rule-based schemes. However, these methods are less effective in complex parking scenarios due to the intricate design of the algorithms. In contrast, neural-network-based methods tend to be more intuitive and versatile than the rule-based methods. By collecting a large number of expert parking trajectory data and emulating human strategy via learning-based methods, the parking task can be effectively addressed. In this paper, we employ imitation learning to perform end-to-end planning from RGB images to path planning by imitating human driving trajectories. The proposed end-to-end approach utilizes a target query encoder to fuse images and target features, and a transformer-based decoder to autoregressively predict future waypoints. We conduct extensive experiments in real-world scenarios, and the results demonstrate that the proposed method achieved an average parking success rate of 87.8% across four different real-world garages. Real-vehicle experiments further validate the feasibility and effectiveness of the method proposed in this paper. The code can be found at: https://github.com/qintonguav/ParkingE2E.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801763,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801763,,Learning systems;Trajectory planning;Imitation learning;Predictive models;Transformers;Trajectory;Planning;Decoding;Reliability;Intelligent robots,,,,32,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/qintonguav/ParkingE2E,https://github.com/qintonguav/ParkingE2E
75,Asynchronous Microphone Array Calibration using Hybrid TDOA Information,C. Zhang; J. Wang; H. Kong,"Shenzhen Key Laboratory of Control Theory and Intelligent Systems, Southern University of Science and Technology, Shenzhen, China; Shenzhen Key Laboratory of Control Theory and Intelligent Systems, Southern University of Science and Technology, Shenzhen, China; Shenzhen Key Laboratory of Control Theory and Intelligent Systems, Southern University of Science and Technology, Shenzhen, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,913,918,"Asynchronous microphone array calibration is a prerequisite for many audition robot applications. A popular solution to the above calibration problem is the batch form of Simultaneous Localisation and Mapping (SLAM), using the time difference of arrival measurements between two microphones (TDOA-M), and the robot (which serves as a moving sound source during calibration) odometry information. In this paper, we introduce a new form of measurement for microphone array calibration, i.e. the time difference of arrival between adjacent sound events (TDOA-S) with respect to the microphone channels. We propose to use TDOA-S and TDOA-M, called hybrid TDOA, together with odometry measurements for bath SLAM-based calibration of asynchronous microphone arrays. Extensive simulation and real-world experiments show that our method is more independent of microphone number, less sensitive to initial values (when using off-the-shelf algorithms such as Gauss-Newton iterations), and has better calibration accuracy and robustness under various TDOA noises. Simulation results also demonstrate that our method has a lower Cramér-Rao lower bound (CRLB) for microphone parameters. To benefit the community, we open-source our code and data at https://github.com/AISLAB-sustech/Hybrid-TDOA-Calib.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801363,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801363,,Simultaneous localization and mapping;Accuracy;Time difference of arrival;Noise;Microphone arrays;Time measurement;Robustness;Calibration;Odometry;Arrays,,,,23,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/AISLAB-sustech/Hybrid-TDOA-Calib,https://github.com/AISLAB-sustech/Hybrid-TDOA-Calib
76,DragTraffic: Interactive and Controllable Traffic Scene Generation for Autonomous Driving,S. Wang; G. Sun; F. Ma; T. Hu; Q. Qin; Y. Song; L. Zhu; J. Liang,"Robotics and Autonomous Systems, Division of Emerging Interdisciplinary Areas (EMIA) Under Interdisciplinary Programs Office (IPO), The Hong Kong University of Science and Technology, Hong Kong SAR, China; Robotics and Autonomous Systems, Division of Emerging Interdisciplinary Areas (EMIA) Under Interdisciplinary Programs Office (IPO), The Hong Kong University of Science and Technology, Hong Kong SAR, China; Artificial Intelligence Thrust, Robotics and Autonomous Systems Thrust, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; Robotics and Autonomous Systems, Division of Emerging Interdisciplinary Areas (EMIA) Under Interdisciplinary Programs Office (IPO), The Hong Kong University of Science and Technology, Hong Kong SAR, China; Department of Production Engineering, KTH Royal Institute of Technology, Sweden; Lotus Technology Ltd, China; Artificial Intelligence Thrust, Robotics and Autonomous Systems Thrust, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; Artificial Intelligence Thrust, Robotics and Autonomous Systems Thrust, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,14241,14247,"Evaluating and training autonomous driving systems require diverse and scalable corner cases. However, most existing scene generation methods lack controllability, accuracy, and versatility, resulting in unsatisfactory generation results. Inspired by DragGAN in image generation, we propose DragTraffic, a generalized, interactive, and controllable traffic scene generation framework based on conditional diffusion. DragTraffic enables non-experts to generate a variety of realistic driving scenarios for different types of traffic agents through an adaptive mixture expert architecture. We employ a regression model to provide a general initial solution and a refinement process based on the conditional diffusion model to ensure diversity. User-customized context is introduced through cross-attention to ensure high controllability. Experiments on a real-world driving dataset show that DragTraffic outperforms existing methods in terms of authenticity, diversity, and freedom. Demo videos and code are available at https://chantsss.github.io/Dragtraffic/.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801623,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801623,,Training;Adaptation models;Accuracy;Noise reduction;Process control;Controllability;Diffusion models;Autonomous vehicles;Intelligent robots;Videos,,,,31,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://chantsss.github.io/Dragtraffic,https://github.com/chantsss/Dragtraffic
77,Mastering Scene Rearrangement with Expert-Assisted Curriculum Learning and Adaptive Trade-Off Tree-Search,Z. Wang; H. Wang; W. Liang,"Beijing Institute of Technology, Beijing, China; Beijing Institute of Technology, Beijing, China; Beijing Institute of Technology, Beijing, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,8039,8046,"Scene Rearrangement Planning (SRP) has recently emerged as a crucial interior scene task; however, current approaches still face two primary issues. First, prior works define the action space of SRP using handcrafted coarse-grained actions, which are inflexible for scene arrangement transition and impractical for real-world deployment. Secondly, the scarcity of realistic indoor scene rearrangement data hinders popular data-hungry learning approaches and quantitative evaluation. To tackle these issues, we propose a fine-grained action space definition and curate a large-scale scene rearrangement dataset to facilitate the training of learning approaches and comprehensive benchmarking. Building upon this dataset, we introduce a novel framework, PLATO, designed for efficient agent training and inference. Our approach features an exPert-assisted curriculum Learning (PL) paradigm that possesses a Behavior Cloning (BC) and an offline Reinforcement Learning (RL) curriculum for agent training, along with an advanced tree-search-based planner enhanced by an Adaptive Trade-Off (ATO) strategy to improve expert agent performance further. We demonstrate the superior performance of our method over baseline agents through extensive experiments and provide a detailed analysis to elucidate its rationale. Our project website can be accessed at plato.github.io.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802526,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802526,,Training;Adaptive systems;Buildings;Cloning;Reinforcement learning;Benchmark testing;Planning;Intelligent robots;Faces,,,,43,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
78,Constrained 6-DoF Grasp Generation on Complex Shapes for Improved Dual-Arm Manipulation,G. Singh; S. Kalwar; M. F. Karim; B. Sen; N. Govindan; S. Sridhar; K. M. Krishna,"Robotics Research Center, IIIT, Hyderabad; Robotics Research Center, IIIT, Hyderabad; Robotics Research Center, IIIT, Hyderabad; Massachusetts Institute of Technology; Robotics Research Center, IIIT, Hyderabad; Brown University; Robotics Research Center, IIIT, Hyderabad",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,7344,7350,"Efficiently generating grasp poses tailored to specific regions of an object is vital for various robotic manipulation tasks, especially in a dual-arm setup. This scenario presents a significant challenge due to the complex geometries involved, requiring a deep understanding of the local geometry to generate grasps efficiently on the specified constrained regions. Existing methods only explore settings involving table-top/small objects and require augmented datasets to train, limiting their performance on complex objects. We propose CGDF: Constrained Grasp Diffusion Fields, a diffusion-based grasp generative model that generalizes to objects with arbitrary geometries, as well as generates dense grasps on the target regions. CGDF uses a part-guided diffusion approach that enables it to get high sample efficiency in constrained grasping without explicitly training on massive constraint-augmented datasets. We provide qualitative and quantitative comparisons using analytical metrics and in simulation, in both unconstrained and constrained settings to show that our method can generalize to generate stable grasps on complex objects, especially useful for dual-arm manipulation settings, while existing methods struggle to do so. More results, code and an extended version of the paper can be found on the project page: https://constrained-grasp-diffusion.github.io/",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802268,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802268,,Geometry;Training;Measurement;Limiting;Codes;Shape;Grasping;Manipulators;6-DOF;Intelligent robots,,,,37,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
79,RaceMOP: Mapless Online Path Planning for Multi-Agent Autonomous Racing using Residual Policy Learning,R. Trumpp; E. Javanmardi; J. Nakazato; M. Tsukada; M. Caccamo,"TUM School of Engineering and Design, Technical University of Munich, Germany; Graduate School of Information Science and Technology, The University of Tokyo, Japan; Graduate School of Information Science and Technology, The University of Tokyo, Japan; Graduate School of Information Science and Technology, The University of Tokyo, Japan; TUM School of Engineering and Design, Technical University of Munich, Germany",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,8449,8456,"The interactive decision-making in multi-agent autonomous racing offers insights valuable beyond the domain of self-driving cars. Mapless online path planning is particularly of practical appeal but poses a challenge for safely overtaking opponents due to the limited planning horizon. To address this, we introduce RaceMOP, a novel method for mapless online path planning designed for multi-agent racing of F1TENTH cars. Unlike classical planners that rely on predefined racing lines, RaceMOP operates without a map, utilizing only local observations to execute high-speed overtaking maneuvers. Our approach combines an artificial potential field method as a base policy with residual policy learning to enable long-horizon planning. We advance the field by introducing a novel approach for policy fusion with the residual policy directly in probability space. Extensive experiments on twelve simulated racetracks validate that RaceMOP is capable of long-horizon decision-making with robust collision avoidance during overtaking maneuvers. RaceMOP demonstrates superior handling over existing mapless planners and generalizes to unknown racetracks, affirming its potential for broader applications in robotics. Our code is available at http://github.com/raphajaner/racemop.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801657,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801657,,Training;Decision making;Learning (artificial intelligence);Gaussian distribution;Path planning;Planning;Automobiles;Collision avoidance;Intelligent robots;Testing,,,,39,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,http://github.com/raphajaner/racemop,http://github.com/raphajaner/racemop
80,Diff-Control: A Stateful Diffusion-based Policy for Imitation Learning,X. Liu; Y. Zhou; F. Weigend; S. Sonawani; S. Ikemoto; H. B. Amor,"Computing and Augmented Intelligence, Arizona State University, USA; Computing and Augmented Intelligence, Arizona State University, USA; Computing and Augmented Intelligence, Arizona State University, USA; Computing and Augmented Intelligence, Arizona State University, USA; Department of Human Intelligence Systems, Graduate School of Life Science and Systems Engineering, Kyushu Institute of Technology; Computing and Augmented Intelligence, Arizona State University, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,7453,7460,"While imitation learning provides a simple and effective framework for policy learning, acquiring consistent action during robot execution remains a challenging task. Existing approaches primarily focus on either modifying the action representation at data curation stage or altering the model itself, both of which do not fully address the scalability of consistent action generation. To overcome this limitation, we introduce the Diff-Control policy, which utilizes a diffusion-based model to learn action representation from a state-space modeling viewpoint. We demonstrate that diffusion-based policies can acquire statefulness through a Bayesian formulation facilitated by ControlNet, leading to improved robustness and success rates. Our experimental results demonstrate the significance of incorporating action statefulness in policy learning, where Diff-Control shows improved performance across various tasks. Specifically, Diff-Control achieves an average success rate of 72% and 84% on stateful and dynamic tasks, respectively. Notably, Diff-Control also shows consistent performance in the presence of perturbations, outperforming other state-of-the-art methods that falter under similar conditions. Project page: https://diff-control.github.io/",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801557,National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801557,,Training;Torque;Imitation learning;Scalability;Perturbation methods;Process control;Sensor systems and applications;Robustness;Intelligent sensors;Intelligent robots,,,,41,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
81,Task-Oriented Dexterous Hand Pose Synthesis Using Differentiable Grasp Wrench Boundary Estimator,J. Chen; Y. Chen; J. Zhang; H. Wang,"CFCS, School of Computer Science, Peking University; CFCS, School of Computer Science, Peking University; CFCS, School of Computer Science, Peking University; CFCS, School of Computer Science, Peking University",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,5281,5288,"This work tackles the problem of task-oriented dexterous hand pose synthesis, which involves generating a static hand pose capable of applying a task-specific set of wrenches to manipulate objects. Unlike previous approaches that focus solely on force-closure grasps, which are unsuitable for non-prehensile manipulation tasks (e.g., turning a knob or pressing a button), we introduce a unified framework covering force-closure grasps, non-force-closure grasps, and a variety of non-prehensile poses. Our key idea is a novel optimization objective quantifying the disparity between the Task Wrench Space (TWS, the desired wrenches predefined as a task prior) and the Grasp Wrench Space (GWS, the achievable wrenches computed from the current hand pose). By minimizing this objective, gradient-based optimization algorithms can synthe-size task-oriented hand poses without additional human demonstrations. Our specific contributions include 1) a fast, accurate, and differentiable technique for estimating the GWS boundary; 2) a task-oriented objective function based on the disparity between the estimated GWS boundary and the provided TWS boundary; and 3) an efficient implementation of the synthesis pipeline that leverages CUDA accelerations and supports large-scale parallelization. Experimental results on 10 diverse tasks demonstrate a 72.6% success rate in simulation. Furthermore, real-world validation for 4 tasks confirms the effectiveness of synthesized poses for manipulation. Notably, despite being primarily tailored for task-oriented hand pose synthesis, our pipeline can generate force-closure grasps 50 times faster than DexGraspNet while maintaining comparable grasp quality. Project page: https://pku-epic.github.io/TaskDexGrasp/.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802652,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802652,,Hands;Accuracy;Pipelines;Graphics processing units;Pressing;Turning;Linear programming;Optimization;Intelligent robots,,,,43,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://pku-epic.github.io/TaskDexGrasp,
82,Safe Offline-to-Online Multi-Agent Decision Transformer: A Safety Conscious Sequence Modeling Approach,A. B. Shah; Y. Wen; J. Chen; X. Wu; X. Fu,"Department of Electrical and Computer Engineering, University of Houston, USA; Department of Electrical and Computer Engineering, University of Houston, USA; Department of Electrical and Computer Engineering, University of Houston, USA; Department of Information Science Technology, University of Houston, USA; Department of Electrical and Computer Engineering, University of Houston, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,12400,12407,"We introduce the Safe Offline-to-Online Multi-Agent Decision Transformer (SO2-MADT), an innovative framework that revolutionizes safety considerations in Multi-agent Reinforcement Learning (MARL) through a novel sequence modeling approach. Leveraging the dynamic capabilities inherent in Decision Transformers, our methodology seamlessly incorporates safety protocols as a cornerstone element, ensuring secure operations throughout both the offline pre-training phase and the adaptive online fine-tuning phase. At the core of our framework lie two pivotal innovations: the Safety-To-Go (STG) token, embedding safety at a macro level, and the Agent Prioritization Module (APM), facilitating explicit credit assignment at a micro level. Through extensive testing against the challenging environments of the StarCraft Multi-Agent Challenge (SMAC) and Multi-agent MuJoCo, our SO2-MADT not only excels in offline pre-training but also demonstrates superior performance during online fine-tuning, without any degradation in performance. The implications of our work provide a pathway for deployment in critical real-world applications where safety is paramount and non-negotiable. The code is available at https://github.com/shahaamirbader/SO2-MADT.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801292,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801292,,Technological innovation;Protocols;Navigation;Reinforcement learning;Transformer cores;Transformers;Safety;Vehicle dynamics;Standards;Testing,,,,42,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/shahaamirbader/SO2-MADT,https://github.com/shahaamirbader/SO2-MADT
83,DHP-Mapping: A Dense Panoptic Mapping System with Hierarchical World Representation and Label Optimization Techniques,T. Hu; J. Jiao; Y. Xu; H. Liu; S. Wang; M. Liu,"The Hong Kong University of Science and Technology, China; University College London, UK; University of Edinburgh, UK; The Hong Kong University of Science and Technology, China; The Hong Kong University of Science and Technology, China; The Hong Kong University of Science and Tehcnology (Guangzhou), China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,1101,1107,"Maps provide robots with crucial environmental knowledge, thereby enabling them to perform interactive tasks effectively. Easily accessing accurate abstract-to-detailed geometric and semantic concepts from maps is crucial for robots to make informed and efficient decisions. To comprehensively model the environment and effectively manage the map data structure, we propose DHP-Mapping, a dense mapping system that utilizes multiple Truncated Signed Distance Field (TSDF) submaps and panoptic labels to hierarchically model the environment. The output map is able to maintain both voxel- and submap-level metric and semantic information. Two modules are presented to enhance the mapping efficiency and label consistency: (1) an inter-submaps label fusion strategy to eliminate duplicate points across submaps and (2) a conditional random field (CRF) based approach to enhance panoptic labels. We conducted experiments with two public datasets including indoor and outdoor scenarios. Our system performs comparably to state-of-the-art (SOTA) methods across geometry and label accuracy evaluation metrics. The experiment results highlight the effectiveness and scalability of our system, as it is capable of constructing precise geometry and maintaining consistent panoptic labels. Our code is publicly available at https://github.com/hutslib/DHP-Mapping.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802278,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802278,,Measurement;Geometry;Accuracy;Scalability;Semantics;Data structures;Data models;Conditional random fields;Optimization;Intelligent robots,,,,31,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/hutslib/DHP-Mapping,https://github.com/hutslib/DHP-Mapping
84,LF-3PM: a LiDAR-based Framework for Perception-aware Planning with Perturbation-induced Metric,K. Chai; L. Xu; Q. Wang; C. Xu; P. Yin; F. Gao,"City University of Hong Kong, Hong Kong, China; State Key Laboratory of Industrial Control Technology, Zhejiang University, Hangzhou, China; State Key Laboratory of Industrial Control Technology, Zhejiang University, Hangzhou, China; State Key Laboratory of Industrial Control Technology, Zhejiang University, Hangzhou, China; City University of Hong Kong, Hong Kong, China; State Key Laboratory of Industrial Control Technology, Zhejiang University, Hangzhou, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,5372,5379,"Just as humans can become disoriented in featureless deserts or thick fogs, not all environments are conducive to the Localization Accuracy and Stability (LAS) of autonomous robots. This paper introduces an efficient framework designed to enhance LiDAR-based LAS through strategic trajectory generation, known as Perception-aware Planning. Unlike vision-based frameworks, the LiDAR-based requires different considerations due to unique sensor attributes. Our approach focuses on two main aspects: firstly, assessing the impact of LiDAR observations on LAS. We introduce a perturbation-induced metric to provide a comprehensive and reliable evaluation of LiDAR observations. Secondly, we aim to improve motion planning efficiency. By creating a Static Observation Loss Map (SOLM) as an intermediary, we logically separate the time-intensive evaluation and motion planning phases, significantly boosting the planning process. In the experimental section, we demonstrate the effectiveness of the proposed metrics across various scenes and the feature of trajectories guided by different metrics. Ultimately, our framework is tested in a real-world scenario, enabling the robot to actively choose topologies and orientations preferable for localization. The source code is accessible at https://github.com/ZJU-FAST-Lab/LF-3PM.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802556,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802556,,Measurement;Location awareness;Learning systems;Laser radar;Source coding;Robot sensing systems;Planning;Trajectory;Topology;Reliability,,,,30,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/ZJU-FAST-Lab/LF-3PM,https://github.com/ZJU-FAST-Lab/LF-3PM
85,Thermal-NeRF: Neural Radiance Fields from an Infrared Camera,T. Ye; Q. Wu; J. Deng; G. Liu; L. Liu; S. Xia; L. Pang; W. Yu; L. Pei,"Shanghai Key Laboratory of Navigation and Location Based Services, Shanghai Jiao Tong University; Shanghai Key Laboratory of Navigation and Location Based Services, Shanghai Jiao Tong University; Shanghai Key Laboratory of Navigation and Location Based Services, Shanghai Jiao Tong University; Shanghai Key Laboratory of Navigation and Location Based Services, Shanghai Jiao Tong University; Hefei University of Technology; Shanghai Key Laboratory of Navigation and Location Based Services, Shanghai Jiao Tong University; Shanghai Slamtec Research; Shanghai Key Laboratory of Navigation and Location Based Services, Shanghai Jiao Tong University; Shanghai Key Laboratory of Navigation and Location Based Services, Shanghai Jiao Tong University",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,1046,1053,"In recent years, Neural Radiance Fields (NeRFs) have demonstrated significant potential in encoding highly-detailed 3D geometry and environmental appearance, positioning themselves as a promising alternative to traditional explicit representation for 3D scene reconstruction. However, the predominant reliance on RGB imaging presupposes ideal lighting conditions—a premise frequently unmet in robotic applications plagued by poor lighting or visual obstructions. This limitation overlooks the capabilities of infrared (IR) cameras, which excel in low-light detection and present a robust alternative under such adverse scenarios. To tackle these issues, we introduce Thermal-NeRF, the first method that estimates a volumetric scene representation in the form of a NeRF solely from IR imaging. By leveraging a thermal mapping and structural thermal constraint derived from the thermal characteristics of IR imaging, our method showcases unparalleled proficiency in recovering NeRFs in visually degraded scenes where RGB-based methods fall short. We conduct extensive experiments to demonstrate that Thermal-NeRF can achieve superior quality compared to existing methods. Furthermore, we contribute a dataset for IR-based NeRF applications, paving the way for future research in IR NeRF reconstruction, see https://github.com/Cerf-Volant425/Thermal-NeRF.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802480,Nature; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802480,,Visualization;Three-dimensional displays;Robot vision systems;Lighting;Neural radiance field;Rendering (computer graphics);Cameras;Thermal conductivity;Image reconstruction;Thermal degradation,,,,42,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/Cerf-Volant425/Thermal-NeRF,https://github.com/Cerf-Volant425/Thermal-NeRF
86,Self-Selecting Semi-Supervised Transformer-Attention Convolutional Network for Four Class EEG-Based Motor Imagery Decoding,H. W. Ng; C. Guan,"Faculty of Computer Science and Engineering, Nanyang Technological University; Faculty of Computer Science and Engineering, Nanyang Technological University",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,4636,4642,"Brain-computer interfaces (BCI) serve as an important tool in areas such as neurorehabilitation and constructing prostheses. Electroencephalogram (EEG) motor imagery (MI) signal is a common method used to communicate between the human brain and the computer interface. However, differentiating between multiple motor imagery signals may be challenging due to the presence of high noise-to-signal ratio and small dataset sizes. In this study, we propose a variational autoencoder and transformer-attention based convolutional neural network (SSTACNet) for multi-class EEG-based motor imagery classification. The SSTACNet model leverages upon variational autoencoders’ ability to measure the contrastive distance between two sets of inputs to perform data self-selection. The model further utilizes multi-head self-attention as well as spatial and temporal convolutional filters to achieve superior extraction of signal features. The model additionally utilizes the variational autoencoder’s ability to augment the dataset with feature-informed pseudo-data, achieving stronger classification results. The proposed model outperforms the current state-of-the-art techniques in the BCI Competition IV-2a dataset with an accuracy of 85.52% and 70.56% for the subject-dependent and subject-independent modes, respectively. Codes may be found at: https://github.com/NgHanWei/SSTACNet",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801654,National Research Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801654,,Convolutional codes;Filters;Convolution;Autoencoders;Training data;Transformers;Motors;Feature extraction;Brain modeling;Convolutional neural networks,,,,27,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/NgHanWei/SSTACNet,https://github.com/NgHanWei/SSTACNet
87,Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding,H. Tang; F. Berto; J. Park,"Department of Industrial and Systems Engineering, KAIST, South Korea; Department of Industrial and Systems Engineering, KAIST, South Korea; Department of Industrial and Systems Engineering, KAIST, South Korea",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,8047,8054,"Multi-Agent Reinforcement Learning (MARL) based Multi-Agent Path Finding (MAPF) has recently gained attention due to its efficiency and scalability. Several MARL-MAPF methods choose to use communication to enrich the information one agent can perceive. However, existing works still struggle in structured environments with high obstacle density and a high number of agents. To further improve the performance of the communication-based MARL-MAPF solvers, we propose a new method, Ensembling Prioritized Hybrid Policies (EPH). We first propose a selective communication block to gather richer information for better agent coordination within multi-agent environments and train the model with a Q-learning-based algorithm. We further introduce three advanced inference strategies aimed at bolstering performance during the execution phase. First, we hybridize the neural policy with single-agent expert guidance for navigating conflict-free zones. Secondly, we propose Q value-based methods for prioritized resolution of conflicts as well as deadlock situations. Finally, we introduce a robust ensemble method that can efficiently collect the best out of multiple possible solutions. We empirically evaluate EPH in complex multi-agent environments and demonstrate competitive performance against state-of-the-art neural methods for MAPF. We open-source our code at https://github.com/ai4co/eph-mapf.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801914,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801914,,Training;Codes;Navigation;Scalability;Reinforcement learning;System recovery;Inference algorithms;Ensemble learning;Intelligent robots,,,,48,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/ai4co/eph-mapf,https://github.com/ai4co/eph-mapf
88,Belief-Aided Navigation using Bayesian Reinforcement Learning for Avoiding Humans in Blind Spots,J. Kim; D. Kwak; H. Rim; D. Kim,"Department of Artificial Intelligence, College of Software, Kyung Hee University, Yongin, Republic of Korea; Department of Artificial Intelligence, College of Software, Kyung Hee University, Yongin, Republic of Korea; Department of Electronic Engineering (AgeTech-Service Convergence Major), College of Electronics & Information, Kyung Hee University, Yongin, Republic of Korea; Department of Electronic Engineering (AgeTech-Service Convergence Major), College of Electronics & Information, Kyung Hee University, Yongin, Republic of Korea",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,2528,2535,"Recent research on mobile robot navigation has focused on socially aware navigation in crowded environments. However, existing methods do not adequately account for human–robot interactions and demand accurate location information from omnidirectional sensors, rendering them unsuitable for practical applications. In response to this need, this study introduces a novel algorithm, BNBRL+, predicated on the partially observable Markov decision process framework to assess risks in unobservable areas and formulate movement strategies under uncertainty. BNBRL+ consolidates belief algorithms with Bayesian neural networks to probabilistically infer beliefs based on the positional data of humans. It further integrates the interactions between the robot, humans, and inferred beliefs to determine the navigation paths, thereby facilitating socially aware navigation. Through experiments in various risk-laden scenarios, this study validates the effectiveness of BNBRL+ in navigating crowded environments with blind spots. The model’s ability to navigate effectively in spaces with limited visibility and avoid obstacles dynamically can significantly improve the safety and reliability of autonomous vehicles. The complement source code can be accessed here: https://github.com/JinnnK/BNBRLplus.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802765,Ministry of Education; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802765,,Space vehicles;Accuracy;Uncertainty;Navigation;Source coding;Prediction algorithms;Bayes methods;Trajectory;Sensors;Safety,,,,27,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/JinnnK/BNBRLplus,https://github.com/JinnnK/BNBRLplus
89,Multimodal Coherent Explanation Generation of Robot Failures,P. Pramanick; S. Rossi,"Interdepartmental Center for Advances in Robotic Surgery - ICAROS, University of Naples Federico II, Naples, Italy; Interdepartmental Center for Advances in Robotic Surgery - ICAROS, University of Naples Federico II, Naples, Italy",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,2487,2493,"The explainability of a robot’s actions is crucial to its acceptance in social spaces. Explaining why a robot fails to complete a given task is particularly important for non-expert users to be aware of the robot’s capabilities and limitations. So far, research on explaining robot failures has only considered generating textual explanations, even though several studies have shown the benefits of multimodal ones. However, a simple combination of multiple modalities may lead to semantic incoherence between the information across different modalities - a problem that is not well-studied. An incoherent multimodal explanation can be difficult to understand, and it may even become inconsistent with what the robot and the human observe and how they perform reasoning with the observations. Such inconsistencies may lead to wrong conclusions about the robot’s capabilities. In this paper, we introduce an approach to generate coherent multimodal explanations by checking the logical coherence of explanations from different modalities, followed by refinements as required. We propose a classification approach for coherence assessment, where we evaluate if an explanation logically follows another. Our experiments suggest that fine-tuning a neural network that was pre-trained to recognize textual entailment, performs well for coherence assessment of multimodal explanations. Code & data: https://pradippramanick.github.io/coherent-explain/.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802671,Horizon Europe; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802671,,Training;Codes;Accuracy;Semantics;Neural networks;Natural languages;Coherence;Cognition;Robots;Intelligent robots,,,,40,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://pradippramanick.github.io/coherent-explain,https://github.com/pradippramanick/coexp-iros24
90,Efficient Incremental Penetration Depth Estimation between Convex Geometries,W. Gao,Mech-Mind Robotics,2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,5444,5451,"Penetration depth (PD) is essential for robotics due to its extensive applications in dynamic simulation, motion planning, haptic rendering, etc. The Expanding Polytope Algorithm (EPA) is the de facto standard for this problem, which estimates PD by expanding an inner polyhedral approximation of an implicit set. In this paper, we propose a novel optimization-based algorithm that incrementally estimates minimum penetration depth and its direction. One major advantage of our method is the capability to be warm-started by leveraging the spatial and temporal coherence. This coherence emerges naturally in many robotic applications (e.g., the temporal coherence between adjacent simulation time knots). As a result, our algorithm achieves substantial speedup — we demonstrate it is 5-30x faster than EPA on several benchmarks. Moreover, our approach is built upon the same implicit geometry representation as EPA, which enables easy integration into existing software stacks. The code and supplemental document are available on: https://github.com/weigao95/mind-fcl.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801965,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801965,,Geometry;Shape;Heuristic algorithms;Software algorithms;Estimation;Coherence;Approximation algorithms;Rendering (computer graphics);Planning;Standards,,,,25,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/weigao95/mind-fcl,https://github.com/weigao95/mind-fcl
91,Language-Embedded Gaussian Splats (LEGS): Incrementally Building Room-Scale Representations with a Mobile Robot,J. Yu; K. Hari; K. Srinivas; K. El-Refai; A. Rashid; C. M. Kim; J. Kerr; R. Cheng; M. Z. Irshad; A. Balakrishna; T. Kollar; K. Goldberg,"The AUTOLab at UC Berkeley; The AUTOLab at UC Berkeley; The AUTOLab at UC Berkeley; The AUTOLab at UC Berkeley; The AUTOLab at UC Berkeley; The AUTOLab at UC Berkeley; The AUTOLab at UC Berkeley; Toyota Research Institute, Los Altos, CA; Toyota Research Institute, Los Altos, CA; Toyota Research Institute, Los Altos, CA; Toyota Research Institute, Los Altos, CA; The AUTOLab at UC Berkeley",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,13326,13332,"Building semantic 3D maps is valuable for searching for objects of interest in offices, warehouses, stores, and homes. We present a mapping system that incrementally builds a Language-Embedded Gaussian Splat (LEGS): a detailed 3D scene representation that encodes both appearance and semantics in a unified representation. LEGS is trained online as a robot traverses its environment to enable localization of open-vocabulary object queries. We evaluate LEGS on 4 room-scale scenes where we query for objects in the scene to assess how LEGS can capture semantic meaning. We compare LEGS to LERF [1] and find that while both systems have comparable object query success rates, LEGS trains over 3.5x faster than LERF. Results suggest that a multi-camera setup and incremental bundle adjustment can boost visual reconstruction quality in constrained robot trajectories, and suggest LEGS can localize open-vocabulary and long-tail object queries with up to 66% accuracy. See project website at: berkeleyautomation.github.io/LEGS",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802196,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802196,,Legged locomotion;Bundle adjustment;Training;Location awareness;Visualization;Semantics;Buildings;Search problems;Trajectory;Intelligent robots,,,,57,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://berkeleyautomation.github.io/LEGS,https://github.com/BerkeleyAutomation/L3GS
92,AirShot: Efficient Few-Shot Detection for Autonomous Exploration,Z. Wang; B. Li; C. Wang; S. Scherer,"Institute for Imaging, Data and Communications, School of Engineering, The University of Edinburgh, UK; AirLab, Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Computer Science and Engineering, Spatial AI & Robotics (SAIR) Lab, Institute for Artificial Intelligence and Data Science, University at Buffalo, NY, USA; AirLab, Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,11654,11661,"Few-shot object detection has drawn increasing attention in the field of robotic exploration, where robots are required to find unseen objects with a few online provided examples. Despite recent efforts have been made to yield online processing capabilities, slow inference speeds of low-powered robots fail to meet the demands of real-time detection-making them impractical for autonomous exploration. Existing methods still face performance and efficiency challenges, mainly due to unreliable features and exhaustive class loops. In this work, we propose a new paradigm AirShot, and discover that, by fully exploiting the valuable correlation map, AirShot can result in a more robust and faster few-shot object detection system, which is more applicable to robotics community. The core module Top Prediction Filter (TPF) can operate on multi-scale correlation maps in both the training and inference stages. During training, TPF supervises the generation of a more representative correlation map, while during inference, it reduces looping iterations by selecting top-ranked classes, thus cutting down on computational costs with better performance. Surprisingly, this dual functionality exhibits general effectiveness and efficiency on various off-the-shelf models. Exhaustive experiments on COCO2017, VOC2014, and SubT datasets demonstrate that TPF can significantly boost the efficacy and efficiency of most off-the-shelf models, achieving up to 36.4% precision improvements along with 56.3% faster inference speed. Code and Data are at: https://github.com/ImNotPrepared/AirShot.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801738,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801738,,Training;Correlation;Codes;Computational modeling;Atmospheric modeling;Object detection;Real-time systems;Computational efficiency;Intelligent robots;Faces,,,,37,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/ImNotPrepared/AirShot,https://github.com/ImNotPrepared/AirShot
93,Robot Traversability Prediction: Towards Third-Person-View Extension of Walk2Map with Photometric and Physical Constraints,J. T. Y. Liang; K. Tanaka,"Fundamental Engineering for Knowledge-Based Society, Graduate School of Engineering, University of Fukui, Japan; Fundamental Engineering for Knowledge-Based Society, Graduate School of Engineering, University of Fukui, Japan",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,11602,11609,"Walk2Map has emerged as a promising data-driven method to generate indoor traversability maps based solely on pedestrian trajectories, offering great potential for indoor robot navigation. In this study, we investigate a novel approach called Walk2Map++, which involves replacing Walk2Map’s first-person sensor (i.e., IMU) with a human observing third-person view from the robot’s onboard camera. However, human observation from a third-person camera is significantly ill-posed due to visual uncertainties resulting from occlusion, nonlinear perspective, depth ambiguity, and human-to-human interaction. To regularize the ill-posedness, we propose integrating two types of constraints: photometric (i.e., occlusion ordering) and physical (i.e., collision avoidance). We demonstrate that these constraints can be effectively inferred from the interaction between past and present observations, human trackers, and object reconstructions. We depict the seamless integration of asynchronous map optimization events, like loop closure, into the real-time traversability map, facilitating incremental and efficient map refinement. We validate the efficacy of our enhanced methodology through rigorous fusion and comparison with established techniques, demonstrating its capability to advance traversability prediction in complex indoor environments. The code and datasets associated with this study are available for further research and adoption in the field at https://github.com/jonathantyl97/HO3-SLAM.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802356,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802356,,Legged locomotion;Visualization;Pedestrians;Uncertainty;Robot vision systems;Cameras;Prediction algorithms;Real-time systems;Trajectory;Collision avoidance,,,,41,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/jonathantyl97/HO3-SLAM,https://github.com/jonathantyl97/HO3-SLAM
94,On Learning Scene-aware Generative State Abstractions for Task-level Mobile Manipulation Planning,J. Förster; J. J. Chung; L. Ott; R. Siegwart,"Autonomous Systems Lab, ETH Zurich, Switzerland; The University of Queensland, St. Lucia, Australia; Autonomous Systems Lab, ETH Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Switzerland",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,13599,13606,"Task and motion planning (TAMP) is a promising approach for efficient long-horizon manipulation planning, which is a prerequisite for being able to deploy manipulation systems in human-centered environments at scale. TAMP systems often rely on so-called predicates to abstractly describe the world. Today, predicates and their groundings are often hand-engineered. Furthermore, robot action parameterizations required to fulfill desired predicates are typically discovered by sampling naively or using oracles (again hand-engineered). We aim to automate predicate discovery and grounding with a system that learns to classify the state of predicates in a set of scenes while concurrently learning to generate scene configurations that fulfill the desired predicates. Our results show that high classification accuracies and generation success rates can be achieved with architectures based on multi-layer perceptrons (MLPs) and graph neural networks (GNNs) that are trained on bounding box as well as point cloud-based features in a Generative Adversarial Network (GAN)-inspired fashion, decisively outperforming both decision tree and uniform sampler baselines. The integration of our framework into a TAMP system demonstrates its positive impact on solving mobile manipulation tasks. A reference implementation of our method and data are available at https://github.com/ethzasl/predicate_learning.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802285,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802285,,Accuracy;Grounding;Generative adversarial networks;Graph neural networks;Planning;Decision trees;Intelligent robots,,,,27,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/ethzasl/predicate_learning,https://github.com/ethzasl/predicate_learning
95,MERSYS: A Collaborative Estimation and Dense Mapping System for Multi-Agent Generic SLAM,Q. Lai; E. Zhao; S. Fan; J. Zou,"University of Electronic Science and Technology, China; University of Electronic Science and Technology, China; University of Electronic Science and Technology, China; University of Electronic Science and Technology, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,5688,5695,"Multi-agent collaborative Simultaneous Localization and Mapping (SLAM) is an effective way for large-scale mapping. However, this approach, which relies on Visual-Inertial Odometry(VIO) as input, suffers from limitations such as susceptibility to environmental influences and the difficulty in accurately constructing dense 3D maps. To address these challenges, this paper presents Multi-Estimation Robust SLAM System (MERSYS), a novel framework for three-dimensional dense mapping based on the fusion of Lidar-Inertial Odometry(LIO) and VIO. Benefiting from lower communication’s costs and dense information acquisition capability, the proposed framework aims to achieve compatibility in processing both LIO and VIO inputs, establish joint loop closure detection to enable multi-map fusion, and then create a comprehensive global 3D dense point cloud map. Furthermore, an efficient communication strategy has been proposed to enable bidirectional transmission of dense and voluminous data. Experimental evaluations conducted on the publicly available HILTI SLAM 2021 dataset [10] as well as a real world dataset. Experimental results show that MERSYS achieves better results than state-of-the-art methods. The source code is available on the GitHub1.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801603,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801603,,Point cloud compression;Simultaneous localization and mapping;Three-dimensional displays;Costs;Source coding;Soft sensors;Pipelines;Collaboration;Estimation;Intelligent robots,,,,31,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
96,Proprioception Is All You Need: Terrain Classification for Boreal Forests,D. LaRocque; W. Guimont-Martin; D. -A. Duclos; P. Giguère; F. Pomerleau,"Northern Robotics Laboratory, Université Laval, Quebec City, Canada; Northern Robotics Laboratory, Université Laval, Quebec City, Canada; Northern Robotics Laboratory, Université Laval, Quebec City, Canada; Northern Robotics Laboratory, Université Laval, Quebec City, Canada; Northern Robotics Laboratory, Université Laval, Quebec City, Canada",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,11686,11693,"Recent works in field robotics highlighted the importance of resiliency against different types of terrains. Boreal forests, in particular, are home to many mobility-impeding terrains that should be considered for off-road autonomous navigation. Also, being one of the largest land biomes on Earth, boreal forests are an area where autonomous vehicles are expected to become increasingly common. In this paper, we address the issue of classifying boreal terrains by introducing BorealTC, a publicly available dataset for proprioceptive-based terrain classification (TC). Recorded with a Husky A200, our dataset contains 116 min of Inertial Measurement Unit (IMU), motor current, and wheel odometry data, focusing on typical boreal forest terrains, notably snow, ice, and silty loam. Combining our dataset with another dataset from the literature, we evaluate both a Convolutional Neural Network (CNN) and the novel state space model (SSM)-based Mamba architecture on a TC task. We show that while CNN outperforms Mamba on each separate dataset, Mamba achieves greater accuracy when trained on a combination of both. In addition, we demonstrate that Mamba’s learning capacity is greater than a CNN for increasing amounts of data. We show that the combination of two TC datasets yields a latent space that can be interpreted with the properties of the terrains. We also discuss the implications of merging datasets on classification. Our source code and dataset are publicly available online: https://github.com/norlab-ulaval/BorealTC.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801407,Natural Sciences and Engineering Research Council of Canada; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801407,,Forests;Snow;Source coding;Merging;Wheels;Biomes;Motors;Sensors;Convolutional neural networks;Odometry,,,,29,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/norlab-ulaval/BorealTC,https://github.com/norlab-ulaval/BorealTC
97,MIXED-SENSE: A Mixed Reality Sensor Emulation Framework for Test and Evaluation of UAVs Against False Data Injection Attacks,K. A. Pant; L. -Y. Lin; J. Kim; W. Sribunma; J. M. Goppert; I. Hwang,"School of Aeronautics and Astronautics, Purdue University, West Lafayette, IN; School of Aeronautics and Astronautics, Purdue University, West Lafayette, IN; School of Aeronautics and Astronautics, Purdue University, West Lafayette, IN; School of Aeronautics and Astronautics, Purdue University, West Lafayette, IN; School of Aeronautics and Astronautics, Purdue University, West Lafayette, IN; School of Aeronautics and Astronautics, Purdue University, West Lafayette, IN",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,12414,12419,"We present a high-fidelity Mixed Reality sensor emulation framework for testing and evaluating the resilience of Unmanned Aerial Vehicles (UAVs) against false data injection (FDI) attacks. The proposed approach can be utilized to assess the impact of FDI attacks, benchmark attack detector performance, and validate the effectiveness of mitigation/reconfiguration strategies in single-UAV and UAV swarm operations. Our Mixed Reality framework leverages high-fidelity simulations of Gazebo and a Motion Capture system to emulate proprioceptive (e.g., GNSS) and exteroceptive (e.g., camera) sensor measurements in real-time. We propose an empirical approach to faithfully recreate signal characteristics such as latency and noise in these measurements. Finally, we illustrate the efficacy of our proposed framework through a Mixed Reality experiment consisting of an emulated GNSS attack on an actual UAV, which (i) demonstrates the impact of false data injection attacks on GNSS measurements and (ii) validates a mitigation strategy utilizing a distributed camera network developed in our previous work. Our open-source implementation is available at https://github.com/CogniPilot/mixed_sense",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802327,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802327,,Global navigation satellite system;Prevention and mitigation;Emulation;Mixed reality;Propioception;Autonomous aerial vehicles;Cameras;Real-time systems;Physics;Resilience,,1.0,,29,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/CogniPilot/mixed_sense,https://github.com/CogniPilot/mixed_sense
98,TeFF: Tracking-enhanced Forgetting-free Few-shot 3D LiDAR Semantic Segmentation,J. Zhou; J. Mei; P. Wu; L. Chen; F. Zhao; X. Zhao; Y. Hu,"Research Center for Intelligent Computing Systems, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Research Center for Intelligent Computing Systems, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Research Center for Intelligent Computing Systems, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Research Center for Intelligent Computing Systems, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Research Center for Intelligent Computing Systems, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; China North Artificial Intelligence & Innovation Research Institute; Research Center for Intelligent Computing Systems, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,14103,14110,"In autonomous driving, 3D LiDAR plays a crucial role in understanding the vehicle’s surroundings. However, the newly emerged, unannotated objects presents few-shot learning problem for semantic segmentation. This paper addresses the limitations of current few-shot semantic segmentation by exploiting the temporal continuity of LiDAR data. Employing a tracking model to generate pseudo-ground-truths from a sequence of LiDAR frames, our method significantly augments the dataset, enhancing the model’s ability to learn on novel classes. However, this approach introduces a data imbalance biased to novel data that presents a new challenge of catastrophic forgetting. To mitigate this, we incorporate LoRA, a technique that reduces the number of trainable parameters, thereby preserving the model’s performance on base classes while improving its adaptability to novel classes. This work represents a significant step forward in few-shot 3D LiDAR semantic segmentation for autonomous driving. Our code is available at https://github.com/BowmanChow/Track-no-forgetting.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801543,National Natural Science Foundation of China; Chinese Academy of Sciences; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801543,,Adaptation models;Laser radar;Three-dimensional displays;Codes;Semantic segmentation;Few shot learning;Autonomous vehicles;Intelligent robots,,,,56,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/BowmanChow/Track-no-forgetting,https://github.com/BowmanChow/Track-no-forgetting
99,Communication-Constrained Multi-Robot Exploration with Intermittent Rendezvous,A. R. Da Silva; L. Chaimowicz; T. C. Silva; M. A. Hsieh,"VeR-Lab, Universidade Federal de Minas Gerais, Brazil; VeR-Lab, Universidade Federal de Minas Gerais, Brazil; GRASP Laboratory, University of Pennsylvania, USA; GRASP Laboratory, University of Pennsylvania, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,3490,3497,"Communication constraints can significantly impact robots’ ability to share information, coordinate their movements, and synchronize their actions, thus limiting coordination in Multi-Robot Exploration (MRE) applications. In this work, we address these challenges by modeling the MRE application as a DEC-POMDP and designing a joint policy that follows a rendezvous plan. This policy allows robots to explore unknown environments while intermittently sharing maps opportunistically or at rendezvous locations without being constrained by joint path optimizations. To generate the rendezvous plan, robots represent the MRE task as an instance of the Job Shop Scheduling Problem (JSSP) and minimize JSSP metrics. They aim to reduce waiting times and increase connectivity, which correlates to the DEC-POMDP rewards and time to complete the task. Our simulation results suggest that our method is more efficient than using relays or maintaining intermittent communication with a base station, being a suitable approach for Multi-Robot Exploration. We developed a proof-of-concept using the Robot Operating System (ROS) that is available at: https://github.com/multirobotplayground/Noetic-Multi-Robot-Sandbox.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802343,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802343,,Measurement;Base stations;Uncertainty;Robot kinematics;Simulation;Relay networks;Trajectory;Synchronization;Robots;Optimization,,,,18,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/multirobotplayground/Noetic-Multi-Robot-Sandbox,https://github.com/multirobotplayground/Noetic-Multi-Robot-Sandbox
100,A Framework for Reproducible Benchmarking and Performance Diagnosis of SLAM Systems,N. Radulov; Y. Zhang; M. Bujanca; R. Ye; M. Luján,"Department of Computer Science, University of Manchester, UK; Department of Computer Science, University of Manchester, UK; Qualcomm Technologies XR Labs, Austria; Department of Computer Science, University of Manchester, UK; Department of Computer Science, University of Manchester, UK",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,14225,14232,"We propose SLAMFuse, an open-source SLAM benchmarking framework that provides consistent cross-platform environments for evaluating multi-modal SLAM algorithms, along with tools for data fuzzing, failure detection, and diagnosis across different datasets. Our framework introduces a fuzzing mechanism to test the resilience of SLAM algorithms against dataset perturbations. This enables the assessment of pose estimation accuracy under varying conditions and identifies critical perturbation thresholds. SLAMFuse improves diagnostics with failure detection and analysis tools, examining algorithm behaviour against dataset characteristics. SLAMFuse uses Docker to ensure reproducible testing conditions across diverse datasets and systems by streamlining dependency management. Emphasizing the importance of reproducibility and introducing advanced tools for algorithm evaluation and performance diagnosis, our work sets a new precedent for reliable benchmarking of SLAM systems. We provide ready-to-use docker compatible versions of the algorithms and datasets used in the experiments, together with guidelines for integrating and benchmarking new algorithms. Code is available at https://github.com/nikolaradulov/slamfuse",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801690,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801690,,Software maintenance;Simultaneous localization and mapping;Perturbation methods;Software algorithms;Benchmark testing;Fuzzing;Reproducibility of results;Reliability;Stress;Resilience,,,,23,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/nikolaradulov/slamfuse,https://github.com/nikolaradulov/slamfuse
101,"M3-GMN: A Multi-environment, Multi-LiDAR, Multi-task dataset for Grid Map based Navigation",G. Xie; H. Fu; H. Xue; B. Liu; X. Xu; X. Li; Z. Sun,"College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,13213,13220,"In this paper, we propose a multi-environment, multi-LiDAR, multi-task dataset to promote the grid map-based navigation capability for autonomous vehicles. The dataset comprises structured and unstructured environmental data captured by different types of LiDAR and contains various challenging scenarios, including moving objects, negative obstacles, steep slopes, cliffs, overhangs, etc. Further, we have devised an innovative method for generating ground truth, facilitating the creation of dense, accurate, and stable grid maps with a minimal requirement for human annotation efforts. A new baseline method and two existing approaches are evaluated on this dataset. Results indicate that existing approaches perform much worse than the proposed baseline. The dataset will be made publicly available at https://github.com/guanglei96/M3-GMN.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802553,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802553,,Laser radar;Accuracy;Navigation;Annotations;Multitasking;Labeling;Autonomous vehicles;Intelligent robots,,,,31,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/guanglei96/M3-GMN,https://github.com/guanglei96/M3-GMN
102,Leveraging Neural Radiance Field in Descriptor Synthesis for Keypoints Scene Coordinate Regression,H. -H. Bui; B. -T. Bui; D. -T. Tran; J. -H. Lee,"Graduate School of Information Science and Engineering, Ritsumeikan University, Japan; Graduate School of Information Science and Engineering, Ritsumeikan University, Japan; College of Information Science, Ritsumeikan University, Japan; College of Information Science, Ritsumeikan University, Japan",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,5581,5588,"Classical structural-based visual localization methods offer high accuracy but face trade-offs in terms of storage, speed, and privacy. A recent innovation, keypoint scene coordinate regression (KSCR) named D2S addresses these issues by leveraging graph attention networks to enhance keypoint relationships and predict their 3D coordinates using a simple multilayer perceptron (MLP). Camera pose is then determined via PnP+RANSAC, using established 2D-3D correspondences. While KSCR achieves competitive results, rivaling state-of-the-art image-retrieval methods like HLoc across multiple benchmarks, its performance is hindered when data samples are limited due to the deep learning model’s reliance on extensive data. This paper proposes a solution to this challenge by introducing a pipeline for keypoint descriptor synthesis using Neural Radiance Field (NeRF). By generating novel poses and feeding them into a trained NeRF model to create new views, our approach enhances the KSCR’s generalization capabilities in data-scarce environments. The proposed system could significantly improve localization accuracy by up to 50% and cost only a fraction of time for data synthesis. Furthermore, its modular design allows for the integration of multiple NeRFs, offering a versatile and efficient solution for visual localization. The implementation is publicly available at: https://github.com/ais-lab/DescriptorSynthesis4Feat2Map.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801953,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801953,,Location awareness;Visualization;Technological innovation;Privacy;Accuracy;Three-dimensional displays;Pipelines;Neural radiance field;Feature extraction;Rendering (computer graphics),,,,40,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/ais-lab/DescriptorSynthesis4Feat2Map,https://github.com/ais-lab/DescriptorSynthesis4Feat2Map
103,DuCAS: a knowledge-enhanced dual-hand compositional action segmentation method for human-robot collaborative assembly,H. Zheng; R. Lee; H. Liang; Y. Lu; X. Xu,"Department of Mechanical and Mechatronics Engineering, The University of Auckland, New Zealand; Department of Mechanical and Mechatronics Engineering, The University of Auckland, New Zealand; Department of Mechanical and Mechatronics Engineering, The University of Auckland, New Zealand; Department of Mechanical and Mechatronics Engineering, The University of Auckland, New Zealand; Department of Mechanical and Mechatronics Engineering, The University of Auckland, New Zealand",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,7175,7180,"Recognising and tracking human actions from videos is crucial for human-robot collaborative assembly (HRCA). However, traditional action segmentation methods suffer from limited scene adaptability, partly because they conceptualise actions as unified verb-object entities with complete semantics. To overcome this, we propose a compositional action segmentation method. Following the human-robot shared assembly taxonomy, we deconstruct an assembly action into four elements: action verb, manipulated object, target object and tool. Our approach employs individual segmentation models for each action element, and then integrates general knowledge from large language models and domain-specific knowledge from predefined rules to form semantic-complete actions. Our method’s emphasis on general action elements and a modular design endows it with greater flexibility and adaptability than traditional approaches. Another attribute of our method is its capability to segment actions of each hand concurrently, facilitating more nuanced HRCA. Comparative experiments validate the superiority of our method over traditional action segmentation methods. More details can be found at https://github.com/LISMS-AKL-NZ/DuCAS.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802758,China Scholarship Council; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802758,,Knowledge engineering;Hands;Large language models;Taxonomy;Semantics;Collaboration;Real-time systems;Assembly;Intelligent robots;Videos,,,,29,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/LISMS-AKL-NZ/DuCAS,https://github.com/LISMS-AKL-NZ/DuCAS
104,A Hybrid Model and Learning-Based Force Estimation Framework for Surgical Robots,H. Yang; H. Zhou; G. S. Fischer; J. Y. Wu,"Department of Computer Science, Vanderbilt University, TN, USA; Department of Robotics Engineering, Worcester Polytechnic Institute, MA, USA; Department of Robotics Engineering, Worcester Polytechnic Institute, MA, USA; Department of Computer Science, Vanderbilt University, TN, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,906,912,"Haptic feedback to the surgeon during robotic surgery would enable safer and more immersive surgeries but estimating tissue interaction forces at the tips of robotically controlled surgical instruments has proven challenging. Few existing surgical robots can measure interaction forces directly and the additional sensor may limit the life of instruments. We present a hybrid model and learning-based framework for force estimation for the Patient Side Manipulators (PSM) of a da Vinci Research Kit (dVRK). The model-based component identifies the dynamic parameters of the robot and estimates free-space joint torque, while the learning-based component compensates for environmental factors, such as the additional torque caused by trocar interaction between the PSM instrument and the patient’s body wall. We evaluate our method in an abdominal phantom and achieve an error in force estimation of under 10% normalized root-mean-squared error. We show that by using a model-based method to perform dynamics identification, we reduce reliance on the training data covering the entire workspace. Although originally developed for the dVRK, the proposed method is a generalizable framework for other compliant surgical robots. The code is available at https://github.com/vu-maple-lab/dvrk_force_estimation.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802648,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802648,,Learning systems;Torque;Medical robotics;Instruments;Force;Dynamics;Estimation;Surgery;Robot sensing systems;Trajectory optimization,,,,28,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/vu-maple-lab/dvrk_force_estimation,https://github.com/vu-maple-lab/dvrk_force_estimation
105,ShapeGrasp: Zero-Shot Task-Oriented Grasping with Large Language Models through Geometric Decomposition,S. Li; S. Bhagat; J. Campbell; Y. Xie; W. Kim; K. Sycara; S. Stepputtis,"Robotics Institute, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,10527,10534,"Task-oriented grasping of unfamiliar objects is a necessary skill for robots in dynamic in-home environments. Inspired by the human capability to grasp such objects through intuition about their shape and structure, we present a novel zero-shot task-oriented grasping method leveraging a geometric decomposition of the target object into simple, convex shapes that we represent in a graph structure, including geometric attributes and spatial relationships. Our approach employs minimal essential information – the object’s name and the intended task – to facilitate zero-shot task-oriented grasping. We utilize the commonsense reasoning capabilities of large language models to dynamically assign semantic meaning to each decomposed part and subsequently reason over the utility of each part for the intended task. Through extensive experiments on a real-world robotics platform, we demonstrate that our grasping approach’s decomposition and reasoning pipeline is capable of selecting the correct part in 92% of the cases and successfully grasping the object in 82% of the tasks we evaluate. Additional videos, experiments, code, and data are available on our project website: https://shapegrasp.github.io/.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801661,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801661,,Codes;Shape;Large language models;Semantics;Pipelines;Grasping;Intelligent robots;Commonsense reasoning;Videos,,,,40,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
106,GRID: Scene-Graph-based Instruction-driven Robotic Task Planning,Z. Ni; X. Deng; C. Tai; X. Zhu; Q. Xie; W. Huang; X. Wu; L. Zeng,"Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Shenzhen Pudu Technology Inc., Shenzhen, China; Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,13765,13772,"Recent works have shown that Large Language Models (LLMs) can facilitate the grounding of instructions for robotic task planning. Despite this progress, most existing works have primarily focused on utilizing raw images to aid LLMs in understanding environmental information. However, this approach not only limits the scope of observation but also typically necessitates extensive multimodal data collection and large-scale models. In this paper, we propose a novel approach called Graph-based Robotic Instruction Decomposer (GRID), which leverages scene graphs instead of images to perceive global scene information and iteratively plan subtasks for a given instruction. Our method encodes object attributes and relationships in graphs through an LLM and Graph Attention Networks, integrating instruction features to predict subtasks consisting of pre-defined robot actions and target objects in the scene graph. This strategy enables robots to acquire semantic knowledge widely observed in the environment from the scene graph. To train and evaluate GRID, we establish a dataset construction pipeline to generate synthetic datasets for graph-based robotic task planning. Experiments have shown that our method outperforms GPT-4 by over 25.4% in subtask accuracy and 43.6% in task accuracy. Moreover, our method achieves a real-time speed of 0.11s per inference. Experiments conducted on datasets of unseen scenes and scenes with varying numbers of objects demonstrate that the task accuracy of GRID declined by at most 3.8%, showcasing its robust cross-scene generalization ability. We validate our method in both physical simulation and the real world. More details can be found on the project page https://jackyzengl.github.io/GRID.github.io/.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801291,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801291,,Accuracy;Grounding;Large language models;Semantics;Pipelines;Real-time systems;Planning;Robots;Intelligent robots;Synthetic data,,,,39,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://jackyzengl.github.io/GRID,
107,ModaLink: Unifying Modalities for Efficient Image-to-PointCloud Place Recognition,W. Xie; L. Luo; N. Ye; Y. Ren; S. Du; M. Wang; J. Xu; R. Ai; W. Gu; X. Chen,"National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center for Visual Information and Applications, and Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China; Haomo.AI Technology Co., Ltd.; Haomo.AI Technology Co., Ltd.; Carnegie Mellon University; National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center for Visual Information and Applications, and Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China; Haomo.AI Technology Co., Ltd.; Haomo.AI Technology Co., Ltd.; Haomo.AI Technology Co., Ltd.; Haomo.AI Technology Co., Ltd.; National University of Defense Technology",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,3326,3333,"Place recognition is an important task for robots and autonomous cars to localize themselves and close loops in pre-built maps. While single-modal sensor-based methods have shown satisfactory performance, cross-modal place recognition that retrieving images from a point-cloud database remains a challenging problem. Current cross-modal methods transform images into 3D points using depth estimation for modality conversion, which are usually computationally intensive and need expensive labeled data for depth supervision. In this work, we introduce a fast and lightweight framework to encode images and point clouds into place-distinctive descriptors. We propose an effective Field of View (FoV) transformation module to convert point clouds into an analogous modality as images. This module eliminates the necessity for depth estimation and helps subsequent modules achieve real-time performance. We further design a non-negative factorization-based encoder to extract mutually consistent semantic features between point clouds and images. This encoder yields more distinctive global descriptors for retrieval. Experimental results on the KITTI dataset show that our proposed methods achieve state-of-the-art performance while running in real time. Additional evaluation on the HAOMO dataset covering a 17 km trajectory further shows the practical generalization capabilities. We have released the implementation of our methods as open source at: https://github.com/haomo-ai/ModaLink.git.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801556,National Science and Technology Major Project; National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801556,,Point cloud compression;Image recognition;Three-dimensional displays;Depth measurement;Transforms;Robot sensing systems;Feature extraction;Real-time systems;Trajectory;Autonomous vehicles,,,,40,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/haomo-ai/ModaLink,https://github.com/haomo-ai/ModaLink
108,One-Shot Transfer of Long-Horizon Extrinsic Manipulation Through Contact Retargeting,A. Wu; R. Wang; S. Chen; C. Eppner; C. K. Liu,"Computer Science Department, Stanford University, Stanford, CA, USA; Computer Science Department, Stanford University, Stanford, CA, USA; Computer Science Department, Stanford University, Stanford, CA, USA; NVIDIA, Seattle, WA, USA; Computer Science Department, Stanford University, Stanford, CA, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,13891,13898,"Extrinsic manipulation, the use of environment contacts to achieve manipulation objectives, enables strategies that are otherwise impossible with a parallel jaw gripper. However, orchestrating a long-horizon sequence of contact interactions between the robot, object, and environment is notoriously challenging due to the scene diversity, large action space, and difficult contact dynamics. We observe that most extrinsic manipulation are combinations of short-horizon primitives, each of which depend strongly on initializing from a desirable contact configuration to succeed. Therefore, we propose to generalize one extrinsic manipulation trajectory to diverse objects and environments by retargeting contact requirements. We prepare a single library of robust short-horizon, goal-conditioned primitive policies, and design a framework to compose state constraints stemming from contacts specifications of each primitive. Given a test scene and a single demo prescribing the primitive sequence, our method enforces the state constraints on the test scene and find intermediate goal states using inverse kinematics. The goals are then tracked by the primitive policies. Using a 7+1 DoF robotic arm-gripper system, we achieved an overall success rate of 80.5% on hardware over 4 long-horizon extrinsic manipulation tasks, each with up to 4 primitives. Our experiments cover 10 objects and 6 environment configurations. We further show empirically that our method admits a wide range of demonstrations, and that contact retargeting is indeed the key to successfully combining primitives for long-horizon extrinsic manipulation. Code and additional details are available at stanford-tml.github. io/extrinsic-manipulation.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801356,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801356,,Codes;Diversity reception;Kinematics;Libraries;Hardware;Trajectory;Grippers;Intelligent robots,,,,40,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
109,"TřiVis: Versatile, Reliable, and High-Performance Tool for Computing Visibility in Polygonal Environments",J. Mikula; M. Kulich; L. Přeučil,"Czech Institute of Informatics, Robotics and Cybernetics, Czech Technical University in Prague, Czech Republic; Czech Institute of Informatics, Robotics and Cybernetics, Czech Technical University in Prague, Czech Republic; Czech Institute of Informatics, Robotics and Cybernetics, Czech Technical University in Prague, Czech Republic",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,10503,10510,"Visibility is a fundamental concept in computational geometry, with numerous applications in surveillance, robotics, and games. This software paper presents TřiVis, a C++ library developed by the authors for computing numerous visibility-related queries in highly complex polygonal environments. Adapting the triangular expansion algorithm, TřiVis stands out as a versatile, high-performance, more reliable and easy-to-use alternative to current solutions that is also free of heavy dependencies. Through evaluation on a challenging dataset, TřiVis has been benchmarked against existing visibility libraries. The results demonstrate that TřiVis outperforms the competing solutions by at least an order of magnitude in query times, while exhibiting more reliable runtime behavior. TřiVis is freely available for private, research, and institutional use at https://github.com/janmikulacz/trivis.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801476,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801476,,Computational geometry;Runtime;Surveillance;Software algorithms;Games;C++ languages;Libraries;Software;Software reliability;Intelligent robots,,,,24,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/janmikulacz/trivis,https://github.com/janmikulacz/trivis
110,Blending Distributed NeRFs with Tri-stage Robust Pose Optimization,B. Ye; C. Liu; X. Ye; Y. Chen; Y. Wang; Z. Yan; Y. Shi; H. Zhao; G. Zhou,"IIIS, Tsinghua University; Peking University; Institute for AI Industry Research (AIR), Tsinghua University; Institute for AI Industry Research (AIR), Tsinghua University; University of Southern California; Institute for AI Industry Research (AIR), Tsinghua University; Institute for AI Industry Research (AIR), Tsinghua University; Institute for AI Industry Research (AIR), Tsinghua University; Institute for AI Industry Research (AIR), Tsinghua University",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,7975,7981,"Due to the limited model capacity, leveraging distributed Neural Radiance Fields (NeRFs) for modeling extensive urban environments has become a necessity. However, current distributed NeRF registration approaches encounter aliasing artifacts, arising from discrepancies in rendering resolutions and suboptimal pose precision. These factors collectively deteriorate the fidelity of pose estimation within NeRF frameworks, resulting in occlusion artifacts during the NeRF blending stage. In this paper, we present a distributed NeRF system with tri-stage pose optimization. In the first stage, precise poses of images are achieved by bundle adjusting Mip-NeRF 360 with a coarse-to-fine strategy. In the second stage, we incorporate the inverting Mip-NeRF 360, coupled with the truncated dynamic low-pass filter, to enable the achievement of robust and precise poses, termed Frame2Model optimization. On top of this, we obtain a coarse transformation between NeRFs in different coordinate systems. In the third stage, we fine-tune the transformation between NeRFs by Model2Model pose optimization. After obtaining precise transformation parameters, we proceed to implement NeRF blending, showcasing superior performance metrics in both real-world and simulation scenarios. Codes and data will be publicly available at https://github.com/boilcy/Distributed-NeRF.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802232,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802232,,Measurement;Image resolution;Pose estimation;Urban areas;Low-pass filters;Neural radiance field;Rendering (computer graphics);Optimization;Intelligent robots;Image reconstruction,,,,28,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/boilcy/Distributed-NeRF,https://github.com/boilcy/Distributed-NeRF
111,Transformer-Based Relationship Inference Model for Household Object Organization by Integrating Graph Topology and Ontology,X. Li; G. Tian; Y. Cui; Y. Gu,"School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,4831,4837,"In domestic environments, the conventional organization of objects by service robots often relies on the inherent properties of each object, such as placing fragile bowls in enclosed cupboards. However, this approach tends to overlook the importance of the orderly arrangement of objects, neglecting the specific placement order of bowls within the cabinet. In practice, effective object organization necessitates consideration of both individual properties and the relationships defined by these properties. In this paper, we have constructed a specialized dataset encompassing the ontological properties of household objects along with their relationships. Furthermore, we have introduced a graph-based model to explicitly represent these relationships and proposed a novel feature extraction technique that integrates the Graph Attention Network (GAT) with the BERT model to predict the relationships among objects. Subsequently, we utilized the Transformer framework to train a model, enabling it to infer relationships between objects. Experimental validation demonstrates the effectiveness of our approach in accurately predicting relationships between household objects, thus facilitating their orderly organization. Our approach significantly augments the object organization capabilities for service robots by accurately predicting the relationships among household objects. Our code is available at: https://github.com/Li-XD-Pro/Household-Object-Organization",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802781,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802781,,Codes;Service robots;Organizations;Predictive models;Ontologies;Transformers;Feature extraction;Topology;Intelligent robots,,,,21,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/Li-XD-Pro/Household-Object-Organization,https://github.com/Li-XD-Pro/Household-Object-Organization
112,Knowledge-based Programming by Demonstration using semantic action models for industrial assembly,J. Ding; H. Zhang; W. Li; L. Zhou; A. Perzylo,"Fortiss, Research Institute of the Free State of Bavaria Associated With Technical University of Munich, München, Germany; Fortiss, Research Institute of the Free State of Bavaria Associated With Technical University of Munich, München, Germany; Fortiss, Research Institute of the Free State of Bavaria Associated With Technical University of Munich, München, Germany; Fortiss, Research Institute of the Free State of Bavaria Associated With Technical University of Munich, München, Germany; Fortiss, Research Institute of the Free State of Bavaria Associated With Technical University of Munich, München, Germany",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,5128,5135,"In this paper, we introduce a knowledge-based Programming by Demonstration (kb-PbD) paradigm to facilitate robot programming in small and medium-sized enterprises (SMEs). PbD in production scenarios requires the recognition of product-specific actions but faces challenges in the lack of suitable and comprehensive datasets, due to the large variety of involved hand actions across different production scenarios. To address this issue, we utilize standardized grasp types as the fundamental feature to recognize basic hand movements, where a Long Short-Term Memory (LSTM) network is employed to recognize grasp types from hand landmarks. The product-specific actions, aggregated from the basic hand movements, are formally modeled in a semantic description language based on the Web Ontology Language (OWL). Description Logic (DL) is used to define the actions with their characteristic properties, which enables the efficient classification of new action instances by an OWL reasoner.The semantic models of hand actions, robot tasks, and work-cell resources are interconnected and stored in a Knowledge Base (KB), which enables the efficient pair-wise translation between hand actions and robot tasks. For the reproduction of human assembly processes, actions are converted to robot tasks via skill descriptions, while reusing the action parameters of involved objects to ensure product integrity. We showcase and evaluate our method in an industrial production setting for control cabinet assembly. Demonstration video available at: https://kb-pbd.github.io/.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802511,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802511,,Hands;Translation;Service robots;Semantics;OWL;Knowledge based systems;Production;Usability;Assembly;Long short term memory,,,,27,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
113,V2I-Calib: A Novel Calibration Approach for Collaborative Vehicle and Infrastructure LiDAR Systems,Q. Qu; Y. Xiong; G. Zhang; X. Wu; X. Gao; X. Gao; H. Li; S. Guo; G. Zhang,"School of Artificial Intelligence, China University of Mining and Technology -Beijing, Beijing, China; School of Artificial Intelligence, China University of Mining and Technology -Beijing, Beijing, China; Computer Science and Technology in Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Automotive Safety and Energy, and School of Vehicle and Mobility, Tsinghua University, Beijing, China; State Key Laboratory of Automotive Safety and Energy, and School of Vehicle and Mobility, Tsinghua University, Beijing, China; State Key Laboratory of Automotive Safety and Energy, and School of Vehicle and Mobility, Tsinghua University, Beijing, China; State Key Laboratory of Automotive Safety and Energy, and School of Vehicle and Mobility, Tsinghua University, Beijing, China; State Key Laboratory of Automotive Safety and Energy, and School of Vehicle and Mobility, Tsinghua University, Beijing, China; School of Artificial Intelligence, China University of Mining and Technology -Beijing, Beijing, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,892,897,"Cooperative LiDAR systems integrating vehicles and road infrastructure, termed V2I calibration, exhibit substantial potential, yet their deployment encounters numerous challenges. A pivotal aspect of ensuring data accuracy and consistency across such systems involves the calibration of LiDAR units across heterogeneous vehicular and infrastructural endpoints. This necessitates the development of calibration methods that are both real-time and robust, particularly those that can ensure robust performance in urban canyon scenarios without relying on initial positioning values. Accordingly, this paper introduces a novel approach to V2I calibration, leveraging spatial association information among perceived objects. Central to this method is the innovative Overall Intersection over Union (oIoU) metric, which quantifies the correlation between targets identified by vehicle and infrastructure systems, thereby facilitating the real-time monitoring of calibration results. Our approach involves identifying common targets within the perception results of vehicle and infrastructure LiDAR systems through the construction of an affinity matrix. These common targets then form the basis for the calculation and optimization of extrinsic parameters. Comparative and ablation studies conducted using the DAIR-V2X dataset substantiate the superiority of our approach. For further insights and resources, our project repository is accessible at https://github.com/MassimoQu/v2i-calib.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802098,National Natural Science Foundation of China; Central University Basic Research Fund of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802098,,Point cloud compression;Laser radar;Accuracy;Roads;Stability criteria;Real-time systems;Calibration;Safety;Synchronization;Reliability,,,,25,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/MassimoQu/v2i-calib,https://github.com/MassimoQu/v2i-calib
114,OBHMR: Robust Partial-to-full Generalized Point Set Registration with Overlap-guided Bidirectional Hybrid Mixture Model,X. Du; Z. Zhang; A. Zhang; R. Song; Y. Li; M. Q. . -H. Meng; Z. Min,"School of Control Science and Engineering, Shandong University, China; Department of Aeronautical and Aviation Engineering, The Hong Kong Polytechnic University; Yuanhua Robotics, Perception and AI Technologies Ltd, Shenzhen, China; School of Control Science and Engineering, Shandong University, China; School of Control Science and Engineering, Shandong University, China; Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen, China; School of Control Science and Engineering, Shandong University, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,7235,7242,"In this paper, we introduce a novel overlap-based bidirectional point set registration approach, i.e., Overlap-guided Bidirectional Hybrid Mixture Registration (OBHMR), which incorporates geometric information (i.e., normal vectors) in both the correspondence and transformation stages and formulates the optimization objective of registration in a bidirectional manner. More importantly, to address the issue of partial-to-full registration, OBHMR utilises the predicted point-wise overlap score using networks to formulate the overlap-guided Hybrid Mixture Model consisting of the Gaussian Mixture Model (GMM) and Fisher Mixture Model (FMM). OBHMR contains four components: (1) the overlap-guided correspondence network that estimates the correspondence probabilities and calculates the point-wise overlap score; (2) the learning posterior module that estimates the overlap-guided HMM parameters; (3) the transformation module that computes the rigid transformation by formulating the optimisation objective in a bidirectional registration way, given correspondences and overlap-guided HMM parameters. Experiments using 1457 human femur and 1301 human hip models demonstrate significant improvements in partial-to-full registration performance (p < 0.01) under different overlapping ratios, compared to state-of-the-art registration approaches. Furthermore, individual contributions of three modules (i.e., additional normal vectors, overlap score estimation module and the bidirectional mechanism) in OBHMR have been validated in ablation studies. The results demonstrate OBHMR’s capability of tackling the challenging partial-to-full registration problems in computer-assisted orthopedic surgery. The codes are available at https://github.com/Dxinz/DeepOBHMR.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801428,National Natural Science Foundation of China; Young Scientists Fund; Jinan Science and Technology Bureau; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801428,,Accuracy;Orthopedic surgery;Noise;Hidden Markov models;Estimation;Mixture models;Probability;Vectors;Optimization;Intelligent robots,,,,21,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/Dxinz/DeepOBHMR,https://github.com/Dxinz/DeepOBHMR
115,GeRM: A Generalist Robotic Model with Mixture-of-experts for Quadruped Robot,W. Song; H. Zhao; P. Ding; C. Cui; S. Lyu; Y. Fan; D. Wang,"MiLAB, Westlake University, China; MiLAB, Westlake University, China; MiLAB, Westlake University, China; MiLAB, Westlake University, China; MiLAB, Westlake University, China; MiLAB, Westlake University, China; MiLAB, Westlake University, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,11879,11886,"Multi-task robot learning holds significant importance in tackling diverse and complex scenarios. However, current approaches are hindered by performance issues and difficulties in collecting training datasets. In this paper, we propose GeRM (Generalist Robotic Model). We utilize offline reinforcement learning to optimize data utilization strategies to learn from both demonstrations and sub-optimal data, thus surpassing the limitations of human demonstrations. Thereafter, we employ a transformer-based VLA network to process multi-modal inputs and output actions. By introducing the Mixture-of-Experts structure, GeRM allows faster inference speed with higher whole model capacity, and thus resolves the issue of limited RL parameters, enhancing model performance in multi-task learning while controlling computational costs. Through a series of experiments, we demonstrate that GeRM outperforms other methods across all tasks, while also validating its efficiency in both training and inference processes. Additionally, we uncover its potential to acquire emergent skills. Additionally, we contribute the QUARD-Auto dataset, collected automatically to support our training approach and foster advancements in multi-task quadruped robot learning. This work presents a new paradigm for reducing the cost of collecting robot data and driving progress in the multi-task learning community.You can reach our project and video through the link: https://songwxuan.github.io/GeRM/.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801816,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801816,,Training;Costs;Computational modeling;Reinforcement learning;Multitasking;Transformers;Robot learning;Computational efficiency;Quadrupedal robots;Intelligent robots,,,,54,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://songwxuan.github.io/GeRM,https://github.com/Songwxuan/GeRM
116,Visual Preference Inference: An Image Sequence-Based Preference Reasoning in Tabletop Object Manipulation,J. Lee; S. Park; Y. Kwon; J. Lee; M. Ahn; S. Choi,"Department of Artificial Intelligence, Korea University, Seoul, Korea; Department of Artificial Intelligence, Korea University, Seoul, Korea; Electronics and Telecommunications Research Institute, Daejeon, Korea; Electronics and Telecommunications Research Institute, Daejeon, Korea; Neubla, Seoul, Korea; Department of Artificial Intelligence, Korea University, Seoul, Korea",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,9745,9752,"In robotic object manipulation, human preferences can often be influenced by the visual attributes of objects, such as color and shape. These properties play a crucial role in operating a robot to interact with objects and align with human intention. In this paper, we focus on the problem of inferring underlying human preferences from a sequence of raw visual observations in tabletop manipulation environments with a variety of object types, named Visual Preference Inference (VPI). To facilitate visual reasoning in the context of manipulation, we introduce the Chain-of-Visual-Residuals (CoVR) method. CoVR employs a prompting mechanism that describes the difference between the consecutive images (i.e., visual residuals) and incorporates such texts with a sequence of images to infer the user’s preference. This approach significantly enhances the ability to understand and adapt to dynamic changes in its visual environment during manipulation tasks. Furthermore, we incorporate such texts along with a sequence of images to infer the user’s preferences. Our method outperforms baseline methods in terms of extracting human preferences from visual sequences in both simulation and real-world environments. Code and videos are available at: https://joonhyung-lee.github.io/vpi/",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801806,Korea University; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801806,,Visualization;Image color analysis;Shape;Robot kinematics;Semantics;User interfaces;Cognition;Image sequences;Robots;Videos,,,,30,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://joonhyung-lee.github.io/vpi,https://github.com/joonhyung-lee/vpi
117,Map-Aware Human Pose Prediction for Robot Follow-Ahead,Q. Jiang; B. Susam; J. -J. Chao; V. Isler,"Shepherd Laboratories, University of Minnesota; Shepherd Laboratories, University of Minnesota; Shepherd Laboratories, University of Minnesota; Shepherd Laboratories, University of Minnesota",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,13031,13038,"In the robot follow-ahead task, a mobile robot is tasked to maintain its relative position in front of a moving human actor while keeping the actor in sight. To accomplish this task, it is important that the robot understand the full 3D pose of the human (since the head orientation can be different than the torso) and predict future human poses so as to plan accordingly. This prediction task is especially tricky in a complex environment with junctions and multiple corridors. In this work, we address the problem of forecasting the full 3D trajectory of a human in such environments. Our main insight is to show that one can first predict the 2D trajectory and then estimate the full 3D trajectory by conditioning the estimator on the predicted 2D trajectory. With this approach, we achieve results comparable or better than the state-of-the-art methods three times faster. As part of our contribution, we present a new dataset where, in contrast to existing datasets, the human motion is in a much larger area than a single room. We also present a complete robot system that integrates our human pose forecasting network on the mobile robot to enable real-time robot follow-ahead and present results from real-world experiments in multiple buildings on campus. Our project page, including supplementary material and videos, can be found at: https://qingyuan-jiang.github.io/iros2024_poseForecasting/",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802110,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802110,,Torso;Three-dimensional displays;Tracking;Robot kinematics;Robot vision systems;Trajectory;Mobile robots;Forecasting;Robots;Videos,,,,44,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://qingyuan-jiang.github.io/iros2024_poseForecasting,https://github.com/Qingyuan-Jiang/iros2024_poseForecasting
118,ManipVQA: Injecting Robotic Affordance and Physically Grounded Information into Multi-Modal Large Language Models,S. Huang; I. Ponomarenko; Z. Jiang; X. Li; X. Hu; P. Gao; H. Li; H. Dong,Shanghai AI Laboratory; Peking University; UCAS; Peking University; TUM; Shanghai AI Laboratory; Shanghai AI Laboratory; Peking University,2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,7580,7587,"While the integration of Multi-modal Large Language Models (MLLMs) with robotic systems has significantly improved robots’ ability to understand and execute natural language instructions, their performance in manipulation tasks remains limited due to a lack of robotics-specific knowledge. Conventional MLLMs are typically trained on generic image-text pairs, leaving them deficient in understanding affordances and physical concepts crucial for manipulation. To address this gap, we propose ManipVQA, a novel framework that infuses MLLMs with manipulation-centric knowledge through a Visual Question-Answering (VQA) format. This approach encompasses tool detection, affordance recognition, and a broader understanding of physical concepts. We curated a diverse dataset of images depicting interactive objects, to challenge robotic understanding in tool detection, affordance prediction, and physical concept comprehension. To effectively integrate this robotics-specific knowledge with the inherent vision-reasoning capabilities of MLLMs, we leverage a unified VQA format and devise a fine-tuning strategy. This strategy preserves the original vision-reasoning abilities while incorporating the newly acquired robotic insights. Empirical evaluations conducted in robotic simulators and across various vision task benchmarks demonstrate the robust performance of ManipVQA. The code and dataset are publicly available at https://github.com/SiyuanHuang95/ManipVQA.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801993,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801993,,Visualization;Codes;Affordances;Large language models;Natural languages;Object detection;Benchmark testing;Robots;Intelligent robots;Resilience,,,,39,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/SiyuanHuang95/ManipVQA,https://github.com/SiyuanHuang95/ManipVQA
119,Learning Variable Compliance Control From a Few Demonstrations for Bimanual Robot with Haptic Feedback Teleoperation System,T. Kamijo; C. C. Beltran-Hernandez; M. Hamaya,"OMRON SINIC X Corporation, Tokyo, Japan; OMRON SINIC X Corporation, Tokyo, Japan; OMRON SINIC X Corporation, Tokyo, Japan",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,12663,12670,"Automating dexterous, contact-rich manipulation tasks using rigid robots is a significant challenge in robotics. Rigid robots, defined by their actuation through position commands, face issues of excessive contact forces due to their inability to adapt to contact with the environment, potentially causing damage. While compliance control schemes have been introduced to mitigate these issues by controlling forces via external sensors, they are hampered by the need for fine-tuning task-specific controller parameters. Learning from Demonstrations (LfD) offers an intuitive alternative, allowing robots to learn manipulations through observed actions. In this work, we introduce a novel system to enhance the teaching of dexterous, contact-rich manipulations to rigid robots. Our system is twofold: firstly, it incorporates a teleoperation interface utilizing Virtual Reality (VR) controllers, designed to provide an intuitive and cost-effective method for task demonstration with haptic feedback. Secondly, we present Comp-ACT (Compliance Control via Action Chunking with Transformers), a method that leverages the demonstrations to learn variable compliance control from a few demonstrations. Our methods have been validated across various complex contact-rich manipulation tasks using single-arm and bimanual robot setups in simulated and real-world environments, demonstrating the effectiveness of our system in teaching robots dexterous manipulations with enhanced adaptability and safety. Code available at https://github.com/omron-sinicx/CompACT.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801731,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801731,,Education;Virtual reality;Robot sensing systems;Transformers;Haptic interfaces;Trajectory;Sensors;Safety;Robots;Intelligent robots,,,,37,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/omron-sinicx/CompACT,https://github.com/omron-sinicx/CompACT
120,Raising Body Ownership in End-to-End Visuomotor Policy Learning via Robot-Centric Pooling,Z. Zhuang; V. Kyrki; D. Kragic,"Robotics, Perception and Learning Lab, EECS, KTH Royal Institute of Technology, Stockholm, Sweden; Department of Electrical Engineering and Automation (EEA), Intelligent Robotics Group, Aalto University, Espoo, Finland; Robotics, Perception and Learning Lab, EECS, KTH Royal Institute of Technology, Stockholm, Sweden",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,7514,7520,"We present Robot-centric Pooling (RcP), a novel pooling method designed to enhance end-to-end visuomo-tor policies by enabling differentiation between the robots and similar entities or their surroundings. Given an image-proprioception pair, RcP guides the aggregation of image features by highlighting image regions correlating with the robot’s proprioceptive states, thereby extracting robot-centric image representations for policy learning. Leveraging contrastive learning techniques, RcP integrates seamlessly with existing visuomotor policy learning frameworks and is trained jointly with the policy using the same dataset, requiring no extra data collection involving self-distractors. We evaluate the proposed method with reaching tasks in both simulated and real-world settings. The results demonstrate that RcP significantly enhances the policies’ robustness against various unseen distractors, including self-distractors, positioned at different locations. Additionally, the inherent robot-centric characteristic of RcP enables the learnt policy to be far more resilient to aggressive pixel shifts compared to the baselines. Code available at: https://github.com/Zheyu-Zhuang/RcP",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802462,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802462,,Propioception;Performance gain;Image representation;Data collection;Feature extraction;Robustness;History;Robots;Intelligent robots;Resilience,,,,34,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/Zheyu-Zhuang/RcP,https://github.com/Zheyu-Zhuang/RcP
121,Large Language Models Powered Context-aware Motion Prediction in Autonomous Driving,X. Zheng; L. Wu; Z. Yan; Y. Tang; H. Zhao; C. Zhong; B. Chen; J. Gong,"Institute for AI Industry Research, Tsinghua University, Beijing, China; Institute for AI Industry Research, Tsinghua University, Beijing, China; Institute for AI Industry Research, Tsinghua University, Beijing, China; Institute for AI Industry Research, Tsinghua University, Beijing, China; Institute for AI Industry Research, Tsinghua University, Beijing, China; Institute for AI Industry Research, Tsinghua University, Beijing, China; Institute for AI Industry Research, Tsinghua University, Beijing, China; Institute for AI Industry Research, Tsinghua University, Beijing, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,980,985,"Motion prediction is among the most fundamental tasks in autonomous driving. Traditional methods of motion forecasting primarily encode vector information of maps and historical trajectory data of traffic participants, lacking a comprehensive understanding of overall traffic semantics, which in turn affects the performance of prediction tasks. In this paper, we utilized Large Language Models (LLMs) to enhance the global traffic context understanding for motion prediction tasks. We first conducted systematic prompt engineering, visualizing complex traffic environments and historical trajectory information of traffic participants into image prompts— Transportation Context Map (TC-Map), accompanied by corresponding text prompts. Through this approach, we obtained rich traffic context information from the LLM. By integrating this information into the motion prediction model, we demonstrate that such context can enhance the accuracy of motion predictions. Furthermore, considering the cost associated with LLMs, we propose a cost-effective deployment strategy: enhancing the accuracy of motion prediction tasks at scale with 0.7% LLM-augmented datasets. Our research offers valuable insights into enhancing the understanding of traffic scenes of LLMs and the motion prediction performance of autonomous driving. The source code is available at https://github.com/AIR-DISCOVER/LLM-Augmented-MTR and https://aistudio.baidu.com/projectdetail/7809548.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802397,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802397,,Accuracy;Systematics;Large language models;Transportation;Predictive models;Vectors;Trajectory;Prompt engineering;Autonomous vehicles;Context modeling,,,,25,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/AIR-DISCOVER/LLM-Augmented-MTR,https://github.com/AIR-DISCOVER/LLM-Augmented-MTR
122,LCP-Fusion: A Neural Implicit SLAM with Enhanced Local Constraints and Computable Prior,J. Wang; Y. Deng; Y. Yang; Y. Yue,"School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,12263,12270,"Recently the dense Simultaneous Localization and Mapping (SLAM) based on neural implicit representation has shown impressive progress in hole filling and high-fidelity mapping. Nevertheless, existing methods either heavily rely on known scene bounds or suffer inconsistent reconstruction due to drift in potential loop-closure regions, or both, which can be attributed to the inflexible representation and lack of local constraints. In this paper, we present LCP-Fusion, a neural implicit SLAM system with enhanced local constraints and computable prior, which takes the sparse voxel octree structure containing feature grids and SDF priors as hybrid scene representation, enabling the scalability and robustness during mapping and tracking. To enhance the local constraints, we propose a novel sliding window selection strategy based on visual overlap to address the loop-closure, and a practical warping loss to constrain relative poses. Moreover, we estimate SDF priors as coarse initialization for implicit features, which brings additional explicit constraints and robustness, especially when a light but efficient adaptive early ending is adopted. Experiments demonstrate that our method achieve better localization accuracy and reconstruction consistency than existing RGB-D implicit SLAM, especially in challenging real scenes (ScanNet) as well as self-captured scenes with unknown scene bounds. The code is available at https://github.com/laliwang/LCP-Fusion.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802626,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802626,,Location awareness;Geometry;Visualization;Simultaneous localization and mapping;Accuracy;Scalability;Octrees;Noise;Robustness;Synthetic data,,,,36,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/laliwang/LCP-Fusion,https://github.com/laliwang/LCP-Fusion
123,Arm-Constrained Curriculum Learning for Loco-Manipulation of a Wheel-Legged Robot,Z. Wang; Y. Jia; L. Shi; H. Wang; H. Zhao; X. Li; J. Zhou; J. Ma; G. Zhou,"The Hong Kong University of Science and Technology (Guangzhou); Department of Electronic Engineering, Tsinghua University; Institute for AI Industry Research (AIR), Tsinghua University; Institute for AI Industry Research (AIR), Tsinghua University; Institute for AI Industry Research (AIR), Tsinghua University; DISCOVER Robotics; The Hong Kong University of Science and Technology (Guangzhou); The Hong Kong University of Science and Technology (Guangzhou); Institute for AI Industry Research (AIR), Tsinghua University",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,10770,10776,"Incorporating a robotic manipulator into a wheellegged robot enhances its agility and expands its potential for practical applications. However, the presence of potential instability and uncertainties presents additional challenges for control objectives. In this paper, we introduce an arm-constrained curriculum learning architecture to tackle the issues introduced by adding the manipulator. Firstly, we develop an arm-constrained reinforcement learning algorithm to ensure safety and reliability in control performance after equipping the manipulator. Additionally, to address discrepancies in reward settings between the arm and the base, we propose a reward-aware curriculum learning method. The policy is first trained in Isaac gym and transferred to the physical robot to complete grasping tasks, including the door-opening task, fan-twitching task and the relay-baton-picking and following task. The results demonstrate that our proposed approach effectively controls the arm-equipped wheel-legged robot to master grasping abilities including the dynamic grasping skills, allowing it to chase and catch a moving object while in motion. Please refer to our website (https://acodedog.github.io/wheel-legged-loco-manipulation/) for the code and supplemental videos.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802062,Department of Education of Guangdong Province; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802062,,Uncertainty;Dynamics;Grasping;Reinforcement learning;Reliability engineering;Safety;Mobile robots;Robots;Manipulator dynamics;Videos,,1.0,,32,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://acodedog.github.io/wheel-legged-loco-manipulation,https://github.com/aCodeDog/legged-robots-manipulation.git
124,Efficient Multimodal Semantic Segmentation via Dual-Prompt Learning,S. Dong; Y. Feng; Q. Yang; Y. Huang; D. Liu; H. Fan,"Dept. of Computer Science and Engineering, University of North Texas; Dept. of Computer Science and Engineering, University of North Texas; Dept. of Computer Science and Engineering, University of North Texas; Dept. of Computer Science and Engineering, University of North Texas; Dept. of Engineering, Rochester Institute of Technology; Dept. of Computer Science and Engineering, University of North Texas",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,14196,14203,"Multimodal (e.g., RGB-Depth/RGB-Thermal) fusion has shown great potential for improving semantic segmentation in complex scenes (e.g., indoor/low-light conditions). Existing approaches often fully fine-tune a dual-branch encoder-decoder framework with a complicated feature fusion strategy for achieving multimodal semantic segmentation, which is training-costly due to the massive parameter updates in feature extraction and fusion. To address this issue, we propose a surprisingly simple yet effective dual-prompt learning network (dubbed DPLNet) for training-efficient multimodal (e.g., RGBD/T) semantic segmentation. The core of DPLNet is to directly adapt a frozen pre-trained RGB model to multimodal semantic segmentation, reducing parameter updates. For this purpose, we present two prompt learning modules, comprising multimodal prompt generator (MPG) and multimodal feature adapter (MFA). MPG works to fuse the features from different modalities in a compact manner and is inserted from shallow to deep stages to generate the multi-level multimodal prompts that are injected into the frozen backbone, while MFA adapts prompted multimodal features in the frozen backbone for better multimodal semantic segmentation. Since both the MPG and MFA are lightweight, only a few trainable parameters (3.88M, 4.4% of the pre-trained backbone parameters) are introduced for multimodal feature fusion and learning. Using a simple decoder (3.27M parameters), DPLNet achieves new state-of-the-art performance or is on a par with other complex approaches on four RGB-D/T semantic segmentation datasets while satisfying parameter efficiency. Moreover, we show DPLNet is general and applicable to other multimodal segmentation tasks. Without special design, DPLNet outperforms many complicated models. The source code can be found at https://github.com/ShaohuaDong2021/DPLNet.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801872,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801872,,Adaptation models;Fuses;Semantic segmentation;Source coding;Object detection;Feature extraction;Generators;Decoding;Intelligent robots,,,,63,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/ShaohuaDong2021/DPLNet,https://github.com/ShaohuaDong2021/DPLNet
125,Greedy Perspectives: Multi-Drone View Planning for Collaborative Perception in Cluttered Environments,K. Suresh; A. Rauniyar; M. Corah; S. Scherer,"Olin College of Engineering, Needham, MA, USA; Robotics Institute, School of Computer Science at Carnegie Mellon University, Pittsburgh, PA, USA; Department of Computer Science, Colorado School of Mines, Golden, CO, USA; Robotics Institute, School of Computer Science at Carnegie Mellon University, Pittsburgh, PA, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,10990,10997,"Deployment of teams of aerial robots could enable large-scale filming of dynamic groups of people (actors) in complex environments for applications in areas such as team sports and cinematography. Toward this end, methods for submodular maximization via sequential greedy planning can enable scalable optimization of camera views across teams of robots but face challenges with efficient coordination in cluttered environments. Obstacles can produce occlusions and increase chances of inter-robot collision which can violate requirements for near-optimality guarantees. To coordinate teams of aerial robots in filming groups of people in dense environments, a more general view-planning approach is required. We explore how collision and occlusion impact performance in filming applications through the development of a multi-robot multi-actor view planner with an occlusion-aware objective for filming groups of people and compare with a formation planner and a greedy planner that ignores inter-robot collisions. We evaluate our approach based on five test environments and complex multi-actor behaviors. Compared with a formation planner, our sequential planner generates 14% greater view reward for filming the actors in three scenarios and comparable performance to formation planning on two others. We also observe near identical view rewards for sequential planning both with and without inter-robot collision constraints which indicates that robots are able to avoid collisions without impairing performance in the perception task. Overall, we demonstrate effective coordination of teams of aerial robots in environments cluttered with obstacles that may cause collisions or occlusions and for filming groups that may split, merge, or spread apart. Our implementation and the data used to produce results for this paper are available via the companion website: https://greedyperspectives.github.io/",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802601,National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802601,,Robot kinematics;Robot vision systems;Collaboration;Autonomous aerial vehicles;Planning;Collision avoidance;Optimization;Intelligent robots;Faces;Sports,,,,38,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
126,LiDAR-based 4D Occupancy Completion and Forecasting,X. Liu; M. Gong; Q. Fang; H. Xie; Y. Li; H. Zhao; C. Feng,New York University; New York University; University of Toronto; New York University; New York University; Tsinghua University; New York University,2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,11102,11109,"Scene completion and forecasting are two popular perception problems in research for mobile agents like autonomous vehicles. Existing approaches treat the two problems in isolation, resulting in a separate perception of the two aspects. In this paper, we introduce a novel LiDAR perception task of Occupancy Completion and Forecasting (OCF) in the context of autonomous driving to unify these aspects into a cohesive framework. This task requires new algorithms to address three challenges altogether: (1) sparse-to-dense reconstruction, (2) partial-to-complete hallucination, and (3) 3D-to-4D prediction. To enable supervision and evaluation, we curate a large-scale dataset termed OCFBench from public autonomous driving datasets. We analyze the performance of closely related existing baselines and variants on our dataset. We envision that this research will inspire and call for further investigation in this evolving and crucial area of 4D perception. Our code for data curation and baseline implementation is available at https://github.com/ai4ce/Occ4cast.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801302,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801302,,Training;Point cloud compression;Laser radar;Codes;Mobile agents;Robot sensing systems;Prediction algorithms;Forecasting;Autonomous vehicles;Intelligent robots,,,,62,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/ai4ce/Occ4cast,https://github.com/ai4ce/Occ4cast
127,SDGE: Stereo Guided Depth Estimation for 360°Camera Sets,J. Xu; W. Yin; D. Gong; J. Jiang; X. Liu,"School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; University of Adelaide, Australia; School of Computer Science and Engineering, The University of New South Wales (UNSW), Sydney, Australia; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,11179,11186,"Depth estimation is a critical technology in autonomous driving, and multi-camera systems are often used to achieve a 360° perception. These 360° camera sets often have limited or low-quality overlap regions, making multi-view stereo methods infeasible for the entire image. Alternatively, monocular methods may not produce consistent cross-view predictions. To address these issues, we propose the Stereo Guided Depth Estimation (SGDE) method, which enhances depth estimation of the full image by explicitly utilizing multi-view stereo results on the overlap. We suggest building virtual pinhole cameras to resolve the distortion problem of fisheye cameras and unify the processing for the two types of 360° cameras. For handling the varying noise on camera poses caused by unstable movement, the approach employs a self-calibration method to obtain highly accurate relative poses of the adjacent cameras with minor overlap. These enable the use of robust stereo methods to obtain a high-quality depth prior in the overlap region. This prior serves not only as an additional input but also as pseudo-labels that enhance the accuracy of depth estimation methods and improve cross-view prediction consistency. The effectiveness of SGDE is evaluated on one fisheye camera dataset, Synthetic Urban, and two pinhole camera datasets, DDAD and nuScenes. Our experiments demonstrate that SGDE is effective for both supervised and self-supervised depth estimation, and highlight the potential of our method for advancing autonomous driving technology. Our project page is at https://github.com/JialeiXu/SGDE.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802599,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802599,,Accuracy;Depth measurement;Robot vision systems;Pipelines;Noise;Cameras;Prediction algorithms;Autonomous vehicles;Optimization;Intelligent robots,,,,46,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/JialeiXu/SGDE,https://github.com/JialeiXu/SGDE
128,MANIP: A Modular Architecture for Integrating Interactive Perception for Robot Manipulation,J. Yu; T. Sadjadpour; A. O’Neill; M. Khfifi; L. Y. Chen; R. Cheng; M. Z. Irshad; A. Balakrishna; T. Kollar; K. Goldberg,"The AUTOLab at UC Berkeley; The AUTOLab at UC Berkeley; The AUTOLab at UC Berkeley; The AUTOLab at UC Berkeley; The AUTOLab at UC Berkeley; Toyota Research Institute, Los Altos, CA; Toyota Research Institute, Los Altos, CA; Toyota Research Institute, Los Altos, CA; Toyota Research Institute, Los Altos, CA; The AUTOLab at UC Berkeley",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,1283,1289,"We propose a modular systems architecture, MANIP, that can facilitate the design and development of robot manipulation systems by systematically combining learned subpolicies with well-established procedural algorithmic primitives such as Inverse Kinematics, Kalman Filters, RANSAC outlier rejection, PID modules, etc. (aka ""Good Old Fashioned Engineering (GOFE)""). The MANIP architecture grew from our lab’s experience developing robot systems for folding clothes, routing cables, and untangling knots. To address failure modes, MANIP can facilitate inclusion of ""interactive perception"" subpolicies that execute robot actions to modify system state to bring the system into alignment with the training distribution and / or to disambiguate system state when system state confidence is low. We demonstrate how MANIP can be applied with 3 case studies and then describe a detailed case study in cable tracing with experiments that suggest MANIP can improve performance by up to 88%. Code and details are available at: https://berkeleyautomation.github.io/MANIP/",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801746,Toyota Research Institute; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801746,,Training;Codes;Systems architecture;Kinematics;Routing;Kalman filters;Intelligent robots;Cables,,,,63,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://berkeleyautomation.github.io/MANIP,https://github.com/BerkeleyAutomation/multicable-decluttering
129,Det-Recon-Reg: An Intelligent Framework Towards Automated Large-Scale Infrastructure Inspection,G. Yang; J. Zhang; B. Zhao; C. Gao; Y. Huang; J. Wen; Q. Li; J. Tang; X. Chen; B. M. Chen,"Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong (CUHK), Hong Kong; Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong (CUHK), Hong Kong; Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong (CUHK), Hong Kong; Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong (CUHK), Hong Kong; Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong (CUHK), Hong Kong; Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong (CUHK), Hong Kong; Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong (CUHK), Hong Kong; Department of Mechanical Engineering, High Performance Robotics Lab, University of California Berkeley, USA; Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong (CUHK), Hong Kong; Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong (CUHK), Hong Kong",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,12742,12749,"Visual inspection plays a predominant role in inspecting infrastructure surface. However, the generalization of existing visual inspection systems to large-scale real-world scenes remains challenging. In this paper, we introduce Det-Recon-Reg, an intelligent framework separating the complex inspection procedure into three stages: Detect, Reconstruct, and Register. (1) For defect detection (Detect), we present the first high-resolution defect dataset tailored for large-scale defect detection. Based on the dataset, we evaluate the most effective real-time object detection algorithms and push the boundary by proposing CUBIT-Net for real-world defect inspection. (2) For infrastructure reconstruction (Reconstruct), we propose a learning-based multi-view stereo (MVS) network to adapt to large-scale scenes, taking as input the multi-view images and outputting the point cloud reconstruction, where its performance has been validated on the standard MVS datasets, including BlendedMVS, DTU, and Tanks and Temples datasets. (3) For defect localization (Register), we propose an effective registration method based on the geographic information system that registers the detected defects onto the reconstructed infrastructure model to establish a global reference for maintenance measures. The real-world experiments further verify the effectiveness and efficiency of our proposed framework. More details about our proposed dataset, code, and appendix are available on our project page: https://cuhk-usr-group.github.io/large-scale-inspect-framework/.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802463,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802463,,Location awareness;Point cloud compression;Surface reconstruction;Scalability;Inspection;Real-time systems;Registers;Image reconstruction;Standards;Defect detection,,,,50,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://cuhk-usr-group.github.io/large-scale-inspect-framework,https://github.com/academicpages/academicpages.github.io
130,PEERNet: An End-to-End Profiling Tool for Real-Time Networked Robotic Systems,A. Narayanan; P. Kasibhatla; M. Choi; P. -H. Li; R. Zhao; S. Chinchali,"The University of Texas, Austin; The University of Texas, Austin; The University of Texas, Austin; The University of Texas, Austin; The University of Texas, Austin; The University of Texas, Austin",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,10326,10333,"Networked robotic systems balance compute, power, and latency constraints in applications such as self-driving vehicles, drone swarms, and teleoperated surgery. A core problem in this domain is deciding when to offload a computationally expensive task to the cloud, a remote server, at the cost of communication latency. Task offloading algorithms often rely on precise knowledge of system-specific performance metrics, such as sensor data rates, network bandwidth, and machine learning model latency. While these metrics can be modeled during system design, uncertainties in connection quality, server load, and hardware conditions introduce real-time performance variations, hindering overall performance. We introduce PEERNet, an end-to-end and real-time profiling tool for cloud robotics. PEERNet enables performance monitoring on heterogeneous hardware through targeted yet adaptive profiling of system components such as sensors, networks, deep-learning pipelines, and devices. We showcase PEERNet’s capabilities through networked robotics tasks, such as image-based teleoperation of a Franka Emika Panda arm and querying vision language models using an Nvidia Jetson Orin. PEERNet reveals non-intuitive behavior in robotic systems, such as asymmetric network transmission and bimodal language model output. Our evaluation underscores the effectiveness and importance of benchmarking in networked robotics, demonstrating PEERNet’s adaptability. Our code is open-source and available at github.com/UTAustin-SwarmLab/PEERNet.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801747,Lockheed Martin; National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801747,,Performance evaluation;Adaptation models;Uncertainty;Computational modeling;Robot sensing systems;Real-time systems;Hardware;Servers;System analysis and design;Load modeling,,,,42,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/UTAustin-SwarmLab/PEERNet,https://github.com/UTAustin-SwarmLab/PEERNet
131,OmniRace: 6D Hand Pose Estimation for Intuitive Guidance of Racing Drone,V. Serpiva; A. Fedoseev; S. Karaf; A. A. Abdulkarim; D. Tsetserukou,"Intelligent Space Robotics Laboratory, Skolkovo Institute of Science and Technology Moscow, Moscow, Russia; Intelligent Space Robotics Laboratory, Skolkovo Institute of Science and Technology Moscow, Moscow, Russia; Intelligent Space Robotics Laboratory, Skolkovo Institute of Science and Technology Moscow, Moscow, Russia; Intelligent Space Robotics Laboratory, Skolkovo Institute of Science and Technology Moscow, Moscow, Russia; Intelligent Space Robotics Laboratory, Skolkovo Institute of Science and Technology Moscow, Moscow, Russia",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,2508,2513,"This paper presents the OmniRace approach to controlling a racing drone with 6-degree of freedom (DoF) hand pose estimation and gesture recognition. To our knowledge, this is the first technology enabling low-level control of high-speed drones through gestures. OmniRace employs a gesture interface based on computer vision and a deep neural network to estimate 6-DoF hand pose. The advanced machine learning algorithm robustly interprets human gestures, allowing users to control drone motion intuitively. Real-time control tests validate the system’s effectiveness and its potential to revolutionize drone racing and other applications. Experimental results conducted in simulation environment revealed that OmniRace allows the users to complite the UAV race track significantly (by 25.1%) faster and to decrease the length of the test drone path (from 102.9 to 83.7 m). Users preferred the gesture interface for attractiveness (1.57 UEQ score), hedonic quality (1.56 UEQ score), and lower perceived temporal demand (32.0 score in NASA-TLX), while noting the high efficiency (0.75 UEQ score) and low physical demand (19.0 score in NASA-TLX) of the baseline remote controller. The deep neural network attains an average accuracy of 99.75% when applied to both normalized datasets and raw datasets. OmniRace can potentially change the way humans interact with and navigate racing drones in dynamic and complex environments. The source code is available at https://github.com/SerValera/OmniRace.git.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801907,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801907,,Hands;Machine learning algorithms;Tracking;Navigation;Source coding;Pose estimation;Artificial neural networks;Real-time systems;Intelligent robots;Drones,,,,31,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/SerValera/OmniRace,https://github.com/SerValera/OmniRace
132,Volumetric Semantically Consistent 3D Panoptic Mapping,Y. Miao; I. Armeni; M. Pollefeys; D. Barath,ETH Zurich; ETH Zurich; ETH Zurich; ETH Zurich,2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,12924,12931,"We introduce an online 2D-to-3D semantic instance mapping algorithm aimed at generating comprehensive, accurate, and efficient semantic 3D maps suitable for autonomous agents in unstructured environments. The proposed approach is based on a Voxel-TSDF representation used in recent algorithms. It introduces novel ways of integrating semantic prediction confidence during mapping, producing semantic and instance-consistent 3D regions. Further improvements are achieved by graph optimization-based semantic labeling and instance refinement. The proposed method achieves accuracy superior to the state of the art on public large-scale datasets, improving on a number of widely used metrics. We also highlight a downfall in the evaluation of recent studies: using the ground truth trajectory as input instead of a SLAM-estimated one substantially affects the accuracy, creating a large gap between the reported results and the actual performance on real-world data. The code is available: https://github.com/y9miao/ConsistentPanopticSLAM.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802241,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802241,,Measurement;Accuracy;Three-dimensional displays;Semantics;Robot vision systems;Prediction algorithms;Real-time systems;Trajectory;Labeling;Intelligent robots,,,,30,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/y9miao/ConsistentPanopticSLAM,https://github.com/y9miao/ConsistentPanopticSLAM
133,Depth Helps: Improving Pre-trained RGB-based Policy with Depth Information Injection,X. Pang; W. Xia; Z. Wang; B. Zhao; D. Hu; D. Wang; X. Li,"Gaoling School of Artificial Intelligence, Renmin University of China; Gaoling School of Artificial Intelligence, Renmin University of China; Shanghai Artificial Intelligence Laboratory; Shanghai Artificial Intelligence Laboratory; Gaoling School of Artificial Intelligence, Renmin University of China; Shanghai Artificial Intelligence Laboratory; Shanghai Artificial Intelligence Laboratory",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,7251,7256,"3D perception ability is crucial for generalizable robotic manipulation. While recent foundation models have made significant strides in perception and decision-making with RGB-based input, their lack of 3D perception limits their effectiveness in fine-grained robotic manipulation tasks. To address these limitations, we propose a Depth Information Injection (DI2) framework that leverages the RGB-Depth modality for policy fine-tuning, while relying solely on RGB images for robust and efficient deployment. Concretely, we introduce the Depth Completion Module (DCM) to extract the spatial prior knowledge related to depth information and generate virtual depth information from RGB inputs to aid policy deployment. Further, we propose the Depth-Aware Codebook (DAC) to eliminate noise and reduce the cumulative error from the depth prediction. In the inference phase, this framework employs RGB inputs and accurately predicted depth data to generate the manipulation action. We conduct experiments on simulated LIBERO environments and real-world scenarios, and the experiment results prove that our method could effectively enhance the pre-trained RGB-based policy with 3D perception ability for robotic manipulation. The website is released at https://gewu-lab.github.io/DepthHelps-IROS2024.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802706,National Natural Science Foundation of China; National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802706,,Three-dimensional displays;Foundation models;Noise;Decision making;Data models;Trajectory;Reliability;Data mining;Intelligent robots,,,,27,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://gewu-lab.github.io/DepthHelps-IROS2024,https://github.com/GeWu-Lab/DepthHelps-IROS2024
134,Scheduling of Robotic Cellular Manufacturing Systems with Timed Petri Nets and Reinforcement Learning,Z. Yao; B. Huang; J. Lv; X. Lu; M. Cui; S. Yu,"School of Computer Science & Engineering, Nanjing University of Science & Technology, Nanjing, China; School of Computer Science & Engineering, Nanjing University of Science & Technology, Nanjing, China; School of Computer Science & Engineering, Nanjing University of Science & Technology, Nanjing, China; School of Computer Science & Engineering, Nanjing University of Science & Technology, Nanjing, China; School of Computer Science & Engineering, Nanjing University of Science & Technology, Nanjing, China; School of Computer Science & Engineering, Nanjing University of Science & Technology, Nanjing, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,2333,2338,"This paper proposes a new Petri-net-based Q-learning scheduling method to schedule robotic cellular manufacturing (RCM) systems efficiently. First, we use generalized and place-timed Petri nets to model RCM systems. Then, we design a reinforcement learning method with a sparse Q-table to evaluate state-transition pairs of the net’s reachability graph. It uses the negative transition firing time as a reward for an action selection and adopts a large penalty for any encountered deadlock. In addition, it balances the state space exploration and the experience exploitation by using a dynamic ϵ-greedy policy to update the state values with an accumulative reward. Three different dynamic ϵ-greedy policies are designed for different application scenarios. Some benchmark RCM systems are tested with the proposed method and several popular PN-based online dispatching rules, such as FIFO and SRPT. Simulation results demonstrate that our method schedules RCM systems as quickly as the online dispatching rules while outperforming them in terms of schedule makespan. For readers’ reference, our source code and test data are available at https://github.com/PNOptimizer/PNQL.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802059,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802059,,Schedules;Job shop scheduling;Q-learning;Source coding;Simulation;Petri nets;Cellular manufacturing;System recovery;Dispatching;Space exploration,,,,14,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/PNOptimizer/PNQL,https://github.com/PNOptimizer/PNQL
135,Hierarchical Search-Based Cooperative Motion Planning,Y. Wu; Y. Yang; G. Xu; J. Cao; Y. Chen; L. Wen; Y. Liu,"Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Shanghai AI Laboratory, Shanghai, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,8055,8062,"Cooperative path planning, a crucial aspect of multi-agent systems research, serves a variety of sectors, including military, agriculture, and industry. Many existing algorithms, however, come with certain limitations, such as simplified kinematic models and inadequate support for multiple group scenarios. Focusing on the planning problem associated with a nonholonomic Ackermann model for Unmanned Ground Vehicles (UGV), we propose a leaderless, hierarchical Search-Based Cooperative Motion Planning (SCMP) method. The high-level utilizes a binary conflict search tree to minimize runtime, while the low-level fabricates kinematically feasible, collision-free paths that are shape-constrained. Our algorithm can adapt to scenarios featuring multiple groups with different shapes, outlier agents, and elaborate obstacles. We conduct algorithm comparisons, performance testing, simulation, and real-world testing, verifying the effectiveness and applicability of our algorithm. The implementation of our method will be open-sourced at https://github.com/WYCUniverStar/SCMP.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801442,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801442,,Runtime;Shape;Kinematics;Search problems;Land vehicles;Planning;Trajectory;Spatiotemporal phenomena;Testing;Multi-agent systems,,,,26,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/WYCUniverStar/SCMP,https://github.com/WYCUniverStar/SCMP
136,Model Predictive Trees: Sample-Efficient Receding Horizon Planning with Reusable Tree Search,J. Lathrop; B. Rivière; J. Alindogan; S. -J. Chung,"California Institute of Technology, Pasadena, CA, USA; California Institute of Technology, Pasadena, CA, USA; California Institute of Technology, Pasadena, CA, USA; California Institute of Technology, Pasadena, CA, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,7671,7678,"We present Model Predictive Trees (MPT), a receding horizon tree search algorithm that improves its performance by reusing information efficiently. Whereas existing solvers reuse only the highest-quality trajectory from the previous iteration as a ""hotstart"", our method reuses the entire optimal subtree, enabling the search to be simultaneously guided away from the low-quality areas and towards the high-quality areas. We characterize the restrictions on tree reuse by analyzing the induced tracking error under time-varying dynamics, revealing a tradeoff between the search depth and the timescale of the changing dynamics. In numerical studies, our algorithm outperforms state-of-the-art sampling-based cross-entropy methods with hotstarting. We demonstrate our planner on an autonomous vehicle testbed performing a nonprehensile manipulation task: pushing a target object through an obstacle field. Code associated with this work will be made available at https://github.com/jplathrop/mpt.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802673,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802673,,Target tracking;Predictive models;Prediction algorithms;Search problems;Stability analysis;Robustness;Real-time systems;Planning;Trajectory;Vehicle dynamics,,,,25,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/jplathrop/mpt,https://github.com/jplathrop/mpt
137,Fine-tuning the Diffusion Model and Distilling Informative Priors for Sparse-view 3D Reconstruction,J. Tang; Y. Gao; T. Jiang; Y. Yang; M. Fu,"School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,7437,7444,"3D reconstruction methods such as Neural Radiance Fields (NeRFs) are capable of optimizing high-quality 3D representation from images. However, NeRF is limited by the requirement for a large number of multi-view images, making its application to real-world scenarios challenging. In this work, we propose a method that can reconstruct real-world scenes from a few input images and a simple text prompt. Specifically, we fine-tune a pretrained diffusion model to constrain its powerful priors to the visual inputs and generate 3D-aware images, leveraging the coarse renderings obtained from input images as the image condition, along with the text prompt as the text condition. Our fine-tuning method saves a significant amount of training time and GPU memory usage while also generating credible results. Moreover, to enable our method to have self-evaluation capabilities, we design a semantic switch to filter out generated images that do not match real scenes, ensuring that only informative priors from the fine-tuned diffusion model are distilled into the 3D model. The semantic switch we designed can be used as a plug-in and improve performance by 13%. We perform our approach on a real-world dataset and demonstrate competitive results compared to existing sparse-view 3D reconstruction methods. Please see our project page for more visualizations and code: https://bityia.github.io/FDfusion.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802155,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802155,,Training;Visualization;Solid modeling;Matched filters;Three-dimensional displays;Semantics;Switches;Diffusion models;Neural radiance field;Rendering (computer graphics),,,,34,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://bityia.github.io/FDfusion,
138,CLAT: Convolutional Local Attention Tracker for Real-time UAV Target Tracking System with Feedback Information,X. Sun; Z. Quan; W. Wang; W. Si; C. Wang; Y. Li; Y. Wu; M. Shen,"Purple Mountain Laboratories, Nanjing, China; School of Automation, Southeast University, Nanjing, China; School of Automation, Nanjing University of Information Science and Technology, Nanjing, China; Purple Mountain Laboratories, Nanjing, China; Purple Mountain Laboratories, Nanjing, China; Purple Mountain Laboratories, Nanjing, China; Purple Mountain Laboratories, Nanjing, China; Purple Mountain Laboratories, Nanjing, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,4912,4919,"Real-time UAV vision target tracking systems encounter the intricate challenges of striking a trade-off for tracking speed and performance, and the robustness of the following control. In existing tracking systems, the global attention mechanism enhances tracking performance, but it introduces higher computational complexity, impacting target tracking speed; the local attention mechanism can reduce computational complexity but often exhibits limitations in modeling the receptive field. In this paper, we propose a new framework named Convolutional Local Attention Tracker (CLAT) to address these challenges. Firstly, we design a hierarchical convolutional local attention structure as the feature extractor for CLAT. This leverages convolutional projection before local window partitioning, facilitating connections between non-overlapping windows and expanding the receptive field. Secondly, we introduce a streamlined feature fusion network comprising the unshared-weights convolutional layer and a global attention network. The whole design can balance speed and accuracy. Furthermore, to enhance servo control robustness, we have redesigned the upper-level controller by integrating all bounding box information. To capture feedback spatiotemporal information in CLAT, a dynamic template update is implemented by incorporating an IOU head into the predictor. Extensive experiments on visual tracking benchmarks and in the real world demonstrate that CLAT achieves competitive performance. Moreover, we have developed a comprehensive tracking system demonstration capable of precisely tracking targets across various categories. The tracker code will be released on https://github.com/xiaolousun/refine-pytracking.git.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801760,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801760,,Convolutional codes;Target tracking;Benchmark testing;Feature extraction;Autonomous aerial vehicles;Robustness;Real-time systems;Spatiotemporal phenomena;Servosystems;Computational complexity,,,,37,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/xiaolousun/refine-pytracking,https://github.com/xiaolousun/refine-pytracking
139,Driving Animatronic Robot Facial Expression From Speech,B. Li; H. Li; H. Liu,"State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI); State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI); State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI)",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,7012,7019,"Animatronic robots hold the promise of enabling natural human-robot interaction through lifelike facial expressions. However, generating realistic, speech-synchronized robot expressions poses significant challenges due to the complexities of facial biomechanics and the need for responsive motion synthesis. This paper introduces a novel, skinning-centric approach to drive animatronic robot facial expressions from speech input. At its core, the proposed approach employs linear blend skinning (LBS) as a unifying representation, guiding innovations in both embodiment design and motion synthesis. LBS informs the actuation topology, facilitates human expression retargeting, and enables efficient speech-driven facial motion generation. This approach demonstrates the capability to produce highly realistic facial expressions on an animatronic face in real-time at over 4000 fps on a single Nvidia RTX 4090, significantly advancing robots' ability to replicate nuanced human expressions for natural interaction. To foster further research and development in this field, the code has been made publicly available at: https://github.com/library87/OpenRoboExp.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801970,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801970,,Technological innovation;Codes;Animatronics;Human-robot interaction;Real-time systems;Topology;Complexity theory;Research and development;Intelligent robots;Faces,,,,30,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/library87/OpenRoboExp,https://github.com/library87/OpenRoboExp
140,Adaptive Stochastic Nonlinear Model Predictive Control with Look-ahead Deep Reinforcement Learning for Autonomous Vehicle Motion Control,B. Zarrouki; C. Wang; J. Betz,"Chair of Automotive Technology, Technical University of Munich; Professorship of Autonomous Vehicle Systems, TUM School of Engineering and Design, Technical University of Munich, Garching, Germany; Professorship of Autonomous Vehicle Systems, TUM School of Engineering and Design, Technical University of Munich, Garching, Germany",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,12726,12733,"Propagating uncertainties through nonlinear system dynamics in the context of Stochastic Nonlinear Model Predictive Control (SNMPC) is challenging, especially for high-dimensional systems requiring real-time control and operating under time-variant uncertainties such as autonomous vehicles. In this work, we propose an Adaptive SNMPC (aSNMPC) driven by Deep Reinforcement Learning (DRL) to optimize uncertainty handling, constraints robustification, feasibility, and closed-loop performance. To this end, our SNMPC uses Polynomial Chaos Expansion (PCE) for efficient uncertainty propagation, limits its propagation time through an Uncertainty Propagation Horizon (UPH), and transforms nonlinear chance constraints into robustified deterministic ones. We conceive a DRL agent to proactively anticipate upcoming control tasks and to dynamically reduce conservatism by determining the most suitable constraints robustification factor κ, and to enhance feasibility by choosing optimal UPH length Tu. We analyze the trained DRL agent’s decision-making process and highlight its ability to learn context-dependent optimal parameters. We showcase the enhanced robustness and feasibility of our DRL-driven aSNMPC through the real-time motion control task of an autonomous passenger vehicle when confronted with significant time-variant disturbances while achieving a minimum solution frequency of 110Hz. The code used in this research is publicly accessible as open-source software: https://github.com/bzarr/TUM-CONTROL",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801876,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801876,,Time-frequency analysis;Uncertainty;Stochastic processes;Transforms;Deep reinforcement learning;Real-time systems;Motion control;Vehicle dynamics;Autonomous vehicles;Predictive control,,,,34,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/bzarr/TUM-CONTROL,https://github.com/bzarr/TUM-CONTROL
141,TD-NeRF: Novel Truncated Depth Prior for Joint Camera Pose and Neural Radiance Field Optimization,Z. Tan; Z. Zhou; Y. Ge; Z. Wang; X. Chen; D. Hu,"College of Intelligence Science and Technology, National University of Defense Technology, China; College of Intelligence Science and Technology, National University of Defense Technology, China; College of Intelligence Science and Technology, National University of Defense Technology, China; College of Aerospace Science and Engineering, National University of Defense Technology, China; College of Intelligence Science and Technology, National University of Defense Technology, China; College of Intelligence Science and Technology, National University of Defense Technology, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,372,379,"The reliance on accurate camera poses is a significant barrier to the widespread deployment of Neural Radiance Fields (NeRF) models for 3D reconstruction and SLAM tasks. The existing method introduces monocular depth priors to jointly optimize the camera poses and NeRF, which fails to fully exploit the depth priors and neglects the impact of their inherent noise. In this paper, we propose Truncated Depth NeRF (TD-NeRF), a novel approach that enables training NeRF from unknown camera poses - by jointly optimizing learnable parameters of the radiance field and camera poses. Our approach explicitly utilizes monocular depth priors through three key advancements: 1) we propose a novel depth-based ray sampling strategy based on the truncated normal distribution, which improves the convergence speed and accuracy of pose estimation; 2) to circumvent local minima and refine depth geometry, we introduce a coarse-to-fine training strategy that progressively improves the depth precision; 3) we propose a more robust inter-frame point constraint that enhances robustness against depth noise during training. The experimental results on three datasets demonstrate that TD-NeRF achieves superior performance in the joint optimization of camera pose and NeRF, surpassing prior works, and generates more accurate depth geometry. The implementation of our method has been released at https://github.com/nubot-nudt/TD-NeRF.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802634,National Natural Science Foundation of China; Technology Development; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802634,,Training;Geometry;Accuracy;Three-dimensional displays;Pose estimation;Noise;Gaussian distribution;Neural radiance field;Cameras;Optimization,,,,36,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/nubot-nudt/TD-NeRF,https://github.com/nubot-nudt/TD-NeRF
142,RPMArt: Towards Robust Perception and Manipulation for Articulated Objects,J. Wang; W. Liu; Q. Yu; Y. You; L. Liu; W. Wang; C. Lu,"Shanghai Jiao Tong University, China; Shanghai Jiao Tong University, China; Shanghai Jiao Tong University, China; Stanford University, U.S.A.; Hefei University of Technology, China; Shanghai Jiao Tong University, China; Shanghai Jiao Tong University, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,7270,7277,"Articulated objects are commonly found in daily life. It is essential that robots can exhibit robust perception and manipulation skills for articulated objects in real-world robotic applications. However, existing methods for articulated objects insufficiently address noise in point clouds and struggle to bridge the gap between simulation and reality, thus limiting the practical deployment in real-world scenarios. To tackle these challenges, we propose a framework towards Robust Perception and Manipulation for Articulated Objects (RPMArt), which learns to estimate the articulation parameters and manipulate the articulation part from the noisy point cloud. Our primary contribution is a Robust Articulation Network (RoArtNet) that is able to predict both joint parameters and affordable points robustly by local feature learning and point tuple voting. Moreover, we introduce an articulation-aware classification scheme to enhance its ability for sim-to-real transfer. Finally, with the estimated affordable point and articulation joint constraint, the robot can generate robust actions to manipulate articulated objects. After learning only from synthetic data, RPMArt is able to transfer zero-shot to real-world articulated objects. Experimental results confirm our approach’s effectiveness, with our framework achieving state-of-the-art performance in both noise-added simulation and real-world environments. Code, data and more results can be found on the project website at https://r-pmart.github.io.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802368,Research and Development; National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802368,,Point cloud compression;Representation learning;Limiting;Codes;Noise;Estimation;Noise measurement;Joints;Intelligent robots;Synthetic data,,,,43,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
143,MonoPlane: Exploiting Monocular Geometric Cues for Generalizable 3D Plane Reconstruction,W. Zhao; J. Liu; S. Zhang; Y. Li; S. Chen; S. X. Huang; Y. -J. Liu; H. Guo,"Department of Computer Science and Technology, BNRist, MOEKey Laboratory of Pervasive Computing, Tsinghua University, Beijing, China; College of Information Sciences and Technology, Pennsylvania State University, University Park, PA, USA; Bytedance Inc., Beijing, China; Department of Computer Science and Technology, BNRist, MOEKey Laboratory of Pervasive Computing, Tsinghua University, Beijing, China; Bytedance Inc., Beijing, China; College of Information Sciences and Technology, Pennsylvania State University, University Park, PA, USA; Department of Computer Science and Technology, BNRist, MOEKey Laboratory of Pervasive Computing, Tsinghua University, Beijing, China; Bytedance Inc., Beijing, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,8481,8488,"This paper presents a generalizable 3D plane detection and reconstruction framework named MonoPlane. Unlike previous robust estimator-based works (which require multiple images or RGB-D input) and learning-based works (which suffer from domain shift), MonoPlane combines the best of two worlds and establishes a plane reconstruction pipeline based on monocular geometric cues, resulting in accurate, robust and scalable 3D plane detection and reconstruction in the wild. Specifically, we first leverage large-scale pre-trained neural networks to obtain the depth and surface normals from a single image. These monocular geometric cues are then incorporated into a proximity-guided RANSAC framework to sequentially fit each plane instance. We exploit effective 3D point proximity and model such proximity via a graph within RANSAC to guide the plane fitting from noisy monocular depths, followed by image-level multi-plane joint optimization to improve the consistency among all plane instances. We further design a simple but effective pipeline to extend this single-view solution to sparse-view 3D plane reconstruction. Extensive experiments on a list of datasets demonstrate our superior zero-shot generalizability over baselines, achieving state-of-the-art plane reconstruction performance in a transferring setting. Our code is available at https://github.com/thuzhaowang/MonoPlane.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802672,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802672,,Surface reconstruction;Solid modeling;Three-dimensional displays;Pipelines;Fitting;Neural networks;Surface fitting;Noise measurement;Image reconstruction;Optimization,,,,54,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/thuzhaowang/MonoPlane,https://github.com/thuzhaowang/MonoPlane
144,Progressive Representation Learning for Real-Time UAV Tracking,C. Fu; X. Lei; H. Zuo; L. Yao; G. Zheng; J. Pan,"School of Mechanical Engineering, Tongji University, Shanghai, China; School of Software Engineering, Tongji University, Shanghai, China; Department of Computer Science, University of Hong Kong, Hong Kong, China; School of Mechanical Engineering, Tongji University, Shanghai, China; Department of Computer Science, University of Hong Kong, Hong Kong, China; Department of Computer Science, University of Hong Kong, Hong Kong, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,5072,5079,"Visual object tracking has significantly promoted autonomous applications for unmanned aerial vehicles (UAVs). However, learning robust object representations for UAV tracking is especially challenging in complex dynamic environments, when confronted with aspect ratio change and occlusion. These challenges severely alter the original information of the object. To handle the above issues, this work proposes a novel progressive representation learning framework for UAV tracking, i.e., PRL-Track. Specifically, PRL-Track is divided into coarse representation learning and fine representation learning. For coarse representation learning, two innovative regulators, which rely on appearance and semantic information, are designed to mitigate appearance interference and capture semantic information. Furthermore, for fine representation learning, a new hierarchical modeling generator is developed to intertwine coarse object representations. Exhaustive experiments demonstrate that the proposed PRL-Track delivers exceptional performance on three authoritative UAV tracking benchmarks. Real-world tests indicate that the proposed PRL-Track realizes superior tracking performance with 42.6 frames per second on the typical UAV platform equipped with an edge smart camera. The code, model, and demo videos are available at https://github.com/vision4robotics/PRL-Track.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10803050,National Natural Science Foundation of China; Natural Science Foundation of Shanghai; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10803050,,Representation learning;Visualization;Regulators;Semantics;Smart cameras;Autonomous aerial vehicles;Generators;Real-time systems;Vehicle dynamics;Videos,,,,41,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/vision4robotics/PRL-Track,https://github.com/vision4robotics/PRL-Track
145,A Real-time Filter for Human Pose Estimation based on Denoising Diffusion Models for Edge Devices,C. Bozzini; M. Boldo; E. Martini; N. Bombieri,"Department of Engineering for Innovation Medicine, University of Verona, Italy; Department of Engineering for Innovation Medicine, University of Verona, Italy; Department of Engineering for Innovation Medicine, University of Verona, Italy; Department of Engineering for Innovation Medicine, University of Verona, Italy",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,10864,10869,"Human Pose Estimation (HPE) is increasingly utilized across various sectors, from healthcare to Industry 5.0. To address the inherent inaccuracies in CNN-based HPE systems, filtering models are commonly employed to refine and improve inference results. However, state-of-the-art filtering models often require substantial computational resources, limiting their applicability in resource-constrained environments. To overcome this limitation, we propose a real-time filtering approach based on denoising diffusion models (DM) specifically optimized for edge devices. Through a micro-benchmarking process, we analyze the DM adaptability to different types and levels of noise and determine the optimal setup for specific application scenarios. We present a real-time filter that takes advantage of the DM setup with two configurations to address different application scenarios. Using a widespread edge device, we evaluate the model’s effectiveness in handling both synthetic and real noise generated by state-of-the-art HPE systems. The results demonstrate a significant improvement in real-time filtering performance with minimal computational overhead. The code is available on github.com/PARCO-LAB/LUT-DM-filters.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802213,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802213,Denoising Diffusion Models;Human pose estimation;Filtering;Edge Devices,Performance evaluation;Adaptation models;Filtering;Computational modeling;Noise reduction;Noise;Pose estimation;Medical services;Diffusion models;Real-time systems,,,,28,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/PARCO-LAB/LUT-DM-filters,https://github.com/PARCO-LAB/LUT-DM-filters
146,NARRATE: Versatile Language Architecture for Optimal Control in Robotics,S. Ismail; A. Arbues; R. Cotterell; R. Zurbrügg; C. A. Alonso,ETH Zürich; ETH Zürich; ETH Zürich; ETH Zürich; ETH Zürich,2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,9628,9635,"The impressive capabilities of Large Language Models (LLMs) have led to various efforts in enabling robots to be controlled through natural language instructions, opening exciting possibilities for human-robot interaction. The goal is for the motor-control task to be performed accurately, efficiently and safely while also enjoying the flexibility imparted by LLMs to specify and adjust the task through natural language. In this work, we demonstrate how a careful layering of an LLM in combination with a Model Predictive Control (MPC) formulation allows for accurate and flexible robotic control via natural language while taking into consideration safety constraints. In particular, we rely on the LLM to effectively frame constraints and objective functions as mathematical expressions, which are later used in the motor-control module via MPC. The transparency of the optimization formulation allows for interpretability of the task and enables adjustments through human feedback. We demonstrate the validity of our method through extensive experiments on long-horizon reasoning, contact-rich, and multi-object interaction tasks. Our evaluations show that NARRATE outperforms current existing methods on these benchmarks and effectively transfers to the real world on two different embodiments.Videos, Code and Prompts at narrate-mpc.github.io",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801425,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801425,,Large language models;Natural languages;Optimal control;Human-robot interaction;Linear programming;Cognition;Safety;Optimization;Intelligent robots;Predictive control,,,,33,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
147,Hyp2Nav: Hyperbolic Planning and Curiosity for Crowd Navigation,G. M. D’Amely di Melendugno; A. Flaborea; P. Mettes; F. Galasso,"Sapienza University of Rome; Sapienza University of Rome; University of Amsterdam, Netherlands; Sapienza University of Rome",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,13023,13030,"Autonomous robots are increasingly becoming a strong fixture in social environments. Effective crowd navigation requires not only safe yet fast planning, but should also enable interpretability and computational efficiency for working in real-time on embedded devices. In this work, we advocate for hyperbolic learning to enable crowd navigation and we introduce Hyp2Nav. Different from conventional reinforcement learning-based crowd navigation methods, Hyp2Nav leverages the intrinsic properties of hyperbolic geometry to better encode the hierarchical nature of decision-making processes in navigation tasks. We propose a hyperbolic policy model and a hyperbolic curiosity module that results in effective social navigation, best success rates, and returns across multiple simulation settings, using up to 6 times fewer parameters than competitor state-of-the-art models. With our approach, it becomes even possible to obtain policies that work in 2-dimensional embedding spaces, opening up new possibilities for low-resource crowd navigation and model interpretability. Insightfully, the internal hyperbolic representation of Hyp2Nav correlates with how much attention the robot pays to the surrounding crowds, e.g. due to multiple people occluding its pathway or to a few of them showing colliding plans, rather than to its own planned route. The code is available at https://github.com/GDam90/hyp2nav.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801513,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801513,,Geometry;Navigation;Fixtures;Decision making;Real-time systems;Planning;Computational efficiency;Complexity theory;Monitoring;Intelligent robots,,,,33,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/GDam90/hyp2nav,https://github.com/GDam90/hyp2nav
148,NeuralLabeling: A versatile toolset for labeling vision datasets using Neural Radiance Fields,F. Erich; N. Chiba; A. Mustafa; Y. Yoshiyasu; N. Ando; R. Hanai; Y. Domae,"National Institute of Advanced Industrial Science and Technology, Tokyo, Japan; Tohoku University, Sendai, Japan; National Institute of Advanced Industrial Science and Technology, Tokyo, Japan; National Institute of Advanced Industrial Science and Technology, Tokyo, Japan; National Institute of Advanced Industrial Science and Technology, Tokyo, Japan; National Institute of Advanced Industrial Science and Technology, Tokyo, Japan; National Institute of Advanced Industrial Science and Technology, Tokyo, Japan",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,11086,11093,"We present NeuralLabeling, a labeling approach and toolset for annotating 3D scenes using either bounding boxes or meshes and generating segmentation masks, affordance maps, 2D bounding boxes, 3D bounding boxes, 6DOF object poses, depth maps, and object meshes. NeuralLabeling uses Neural Radiance Fields (NeRF) as a renderer, allowing labeling to be performed using 3D spatial tools while incorporating geometric clues such as occlusions, relying only on images captured from multiple viewpoints as input. To demonstrate the applicability of NeuralLabeling to a practical problem in robotics, we added ground truth depth maps to 30000 frames of transparent object RGB and noisy depth maps of glasses placed in a dishwasher captured using an RGBD sensor, yielding the Dishwasher30k dataset. We show that training a simple deep neural network with supervision using the annotated depth maps yields a higher reconstruction performance than training with the previously applied weakly supervised approach. We also show how instance segmentation and depth completion datasets generated using NeuralLabeling can be incorporated into a robot application for grasping transparent objects placed in a dishwasher with an accuracy of 83.3%, compared to 16.3% without depth completion. Supplementary URI: https://florise.github.io/neural_labeling_web/.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801449,New Energy and Industrial Technology Development Organization; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801449,,Training;Instance segmentation;Three-dimensional displays;Grasping;Neural radiance field;Robot sensing systems;Labeling;Noise measurement;Intelligent robots;Image reconstruction,,,,33,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://florise.github.io/neural_labeling_web,
149,SiSCo: Signal Synthesis for Effective Human-Robot Communication Via Large Language Models,S. Sonawani; F. Weigend; H. B. Amor,"School of Computing and Augmented Intelligence, Arizona State University; School of Computing and Augmented Intelligence, Arizona State University; School of Computing and Augmented Intelligence, Arizona State University",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,7107,7114,"Effective human-robot collaboration hinges on robust communication channels, with visual signaling playing a pivotal role due to its intuitive appeal. Yet, the creation of visually intuitive cues often demands extensive resources and specialized knowledge. The emergence of Large Language Models (LLMs) offers promising avenues for enhancing human-robot interactions and revolutionizing the way we generate context-aware visual cues. To this end, we introduce SiSCo–a novel framework that combines the computational power of LLMs with mixed-reality technologies to streamline the creation of visual cues for human-robot collaboration. Our results show that SiSCo improves the efficiency of communication in human-robot teaming tasks, reducing task completion time by approximately 73% and increasing task success rates by 18% compared to baseline natural language signals. Additionally, SiSCo reduces cognitive load for participants by 46%, as measured by the NASA-TLX subscale, and receives above-average user ratings for on-the-fly signals generated for unseen objects. To encourage further development and broader community engagement, we provide full access to SiSCo’s implementation and related materials on our GitHub repository.1",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802561,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802561,,Visualization;Large language models;Natural languages;Human-robot interaction;Virtual reality;Fasteners;Particle measurements;Signal synthesis;Intelligent robots;Software development management,,,,35,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
150,LiOn-XA: Unsupervised Domain Adaptation via LiDAR-Only Cross-Modal Adversarial Training,T. Kreutz; J. Lemke; M. Mühlhäuser; A. S. Guinea,"Telecooperation Lab at the Technical University, Darmstadt, Germany; Telecooperation Lab at the Technical University, Darmstadt, Germany; Telecooperation Lab at the Technical University, Darmstadt, Germany; Telecooperation Lab at the Technical University, Darmstadt, Germany",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,8875,8881,"In this paper, we propose LiOn-XA, an unsupervised domain adaptation (UDA) approach that combines LiDAR-Only Cross-Modal (X) learning with Adversarial training for 3D LiDAR point cloud semantic segmentation to bridge the domain gap arising from environmental and sensor setup changes. Unlike existing works that exploit multiple data modalities like point clouds and RGB image data, we address UDA in scenarios where RGB images might not be available and show that two distinct LiDAR data representations can learn from each other for UDA. More specifically, we leverage 3D voxelized point clouds to preserve important geometric structure in combination with 2D projection-based range images that provide information such as object orientations or surfaces. To further align the feature space between both domains, we apply adversarial training using both features and predictions of both 2D and 3D neural networks. Our experiments on 3 real-to-real adaptation scenarios demonstrate the effectiveness of our approach, achieving new state-of-the-art performance when compared to previous uni- and multi-model UDA methods. Our source code is publicly available at https://github.com/JensLe97/lion-xa.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801916,EWE; Ministry of Education; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801916,,Training;Point cloud compression;Laser radar;Three-dimensional displays;Semantic segmentation;Source coding;Neural networks;Data visualization;Robot sensing systems;Intelligent robots,,,,27,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/JensLe97/lion-xa,https://github.com/JensLe97/lion-xa
151,Roadmaps with Gaps over Controllers: Achieving Efficiency in Planning under Dynamics,A. Sivaramakrishnan; S. Tangirala; E. Granados; N. R. Carver; K. E. Bekris,"Dept. of Computer Science, Rutgers University, NJ, USA; Dept. of Computer Science, Rutgers University, NJ, USA; Dept. of Computer Science, Rutgers University, NJ, USA; Dept. of Computer Science, Rutgers University, NJ, USA; Dept. of Computer Science, Rutgers University, NJ, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,11064,11069,"This paper aims to improve the computational efficiency of motion planning for mobile robots with non-trivial dynamics through the use of learned controllers. Offline, a system-specific controller is first trained in an empty environment. Then, for the target environment, the approach constructs a data structure, a ""Roadmap with Gaps,"" to approximately learn how to solve planning queries using the learned controller. The roadmap nodes correspond to local regions. Edges correspond to applications of the learned controller that approximately connect these regions. Gaps arise as the controller does not perfectly connect pairs of individual states along edges. Online, given a query, a tree sampling-based motion planner uses the roadmap so that the tree’s expansion is informed towards the goal region. The tree expansion selects local subgoals given a wavefront on the roadmap that guides towards the goal. When the controller cannot reach a subgoal region, the planner resorts to random exploration to maintain probabilistic completeness and asymptotic optimality. The accompanying experimental evaluation shows that the approach significantly improves the computational efficiency of motion planning on various benchmarks, including physics-based vehicular models on uneven and varying friction terrains as well as a quadrotor under air pressure effects. Website: https://prx-kinodynamic.github.io/projects/rogue",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802619,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802619,,Tracking;Sparse approximation;Dynamics;Planning;Computational efficiency;Trajectory;System identification;Vehicle dynamics;State estimation;Quadrotors,,,,36,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://prx-kinodynamic.github.io/projects/rogue,
152,Robot Shape and Location Retention in Video Generation Using Diffusion Models,P. Wang; Z. Guo; A. L. Sait; M. H. Pham,"Department of Computing and Mathematics, Manchester Metropolitan University, Manchester, UK; Department of Computing and Mathematics, Manchester Metropolitan University, Manchester, UK; JD Sports Fashion PLC; Department of Computing and Mathematics, Manchester Metropolitan University, Manchester, UK",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,7375,7382,"Diffusion models have marked a significant mile-stone in the enhancement of image and video generation technologies. However, generating videos that precisely retain the shape and location of moving objects such as robots remains a challenge. This paper presents diffusion models specifically tailored to generate videos that accurately maintain the shape and location of mobile robots. The proposed models incorporate techniques such as embedding accessible robot pose information and applying semantic mask regulation within the scalable and efficient ConvNext backbone network. These techniques are designed to refine intermediate outputs, therefore improving the retention performance of shape and location. Through extensive experimentation, our models have demonstrated notable improvements in maintaining the shape and location of different robots, as well as enhancing overall video generation quality, compared to the benchmark diffusion model. Codes will be open-sourced at: https://github.com/PengPaulWang/diffusion-robots.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802156,Manchester Metropolitan University; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802156,,Training;Accuracy;Shape;Shape measurement;Semantics;Diffusion models;Regulation;Mobile robots;Robots;Intelligent robots,,,,19,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/PengPaulWang/diffusion-robots,https://github.com/PengPaulWang/diffusion-robots
153,Intelligent Fish Detection System with Similarity-Aware Transformer,S. Li; H. Zuo; C. Fu; Z. Wang; Z. Xu,"School of Mechanical Engineering, Tongji University, Shanghai, China; Department of Computer Science, University of Hong Kong, Hong Kong, China; School of Mechanical Engineering, Tongji University, Shanghai, China; Fishery Machinery and Instrument Research Institute, Shanghai, China; Fishery Machinery and Instrument Research Institute, Shanghai, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,11750,11757,"Fish detection in water-land transfer has significantly contributed to the fishery. However, manual fish detection in crowd-collaboration performs inefficiently and expensively, involving insufficient accuracy. To further enhance the water-land transfer efficiency, improve detection accuracy, and reduce labor costs, this work designs a new type of lightweight and plug-and-play edge intelligent vision system to automatically conduct fast fish detection with high-speed camera. Moreover, a novel similarity-aware vision Transformer for fast fish detection (FishViT) is proposed to onboard identify every single fish in a dense and similar group. Specifically, a novel similarity-aware multi-level encoder is developed to enhance multi-scale features in parallel, thereby yielding discriminative representations for varying-size fish. Additionally, a new soft-threshold attention mechanism is introduced, which not only effectively eliminates background noise from images but also accurately recognizes both the edge details and overall features of different similar fish. 85 challenging video sequences with high framerate and high-resolution are collected to establish a benchmark from real fish water-land transfer scenarios. Exhaustive evaluation conducted with this challenging benchmark has proved the robustness and effectiveness of FishViT with over 80 FPS. Real work scenario tests validate the practicality of the proposed method. The code and demo video are available at https://github.com/vision4robotics/FishViT.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802394,Chinese Academy of Fishery Sciences; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802394,,Costs;Accuracy;Machine vision;Image edge detection;Video sequences;Benchmark testing;Fish;Transformers;Robustness;Real-time systems,,,,35,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/vision4robotics/FishViT,https://github.com/vision4robotics/FishViT
154,"GELLO: A General, Low-Cost, and Intuitive Teleoperation Framework for Robot Manipulators",P. Wu; Y. Shentu; Z. Yi; X. Lin; P. Abbeel,"University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,12156,12163,"Humans can teleoperate robots to accomplish complex manipulation tasks. Imitation learning has emerged as a powerful framework that leverages human teleoperated demonstrations to teach robots new skills. However, the performance of the learned policies is bottlenecked by the quality, scale, and variety of the demonstration data. In this paper, we aim to lower the barrier to collecting large and high-quality human demonstration data by proposing a GEneraL framework for building LOw-cost and intuitive teleoperation systems for robotic manipulation (GELLO). Given a target robot arm, we build a GELLO controller device that has the same kinematic structure as the target arm, leveraging 3D-printed parts and economical off-the-shelf motors. GELLO is easy to build and intuitive to use. Through an extensive user study, we show that GELLO enables more reliable and efficient demonstration collection compared to other cost efficient teleoperation devices commonly used in the imitation learning literature such as virtual reality controllers and 3D spacemouses. We further demonstrate the capabilities of GELLO for performing complex bi-manual and contact-rich manipulation tasks. To make GELLO accessible to everyone, we have designed and built GELLO systems for 3 commonly used robotic arms: Franka, UR5, and xArm. All software and hardware are open-sourced and can be found on our website: https://wuphilipp.github.io/gello/.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801581,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801581,,Performance evaluation;Three-dimensional displays;Imitation learning;Virtual reality;Kinematics;Manipulators;Motors;Software;Reliability;Robots,,,,60,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://wuphilipp.github.io/gello,
155,Safe multi-agent reinforcement learning for bimanual dexterous manipulation,W. Zhan; P. Chin,"Thayer School of Engineering, Dartmouth College, Hanover; Thayer School of Engineering, Dartmouth College, Hanover",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,12420,12427,"Bimanual dexterous manipulation in robotics, essential for a wide range of applications, addresses the critical challenge of balancing intricate operational capabilities with assured safety and reliability. While Safe Reinforcement Learning is integral to the robustness of robotic systems, the area of safe multi-agent reinforcement learning (MARL), cooperative control of multiple robots has been scarcely studied. In this study, we explore MARL for safe cooperative control with multiple robot hands. Each robot must follow individual and collective safety guidelines to ensure safe team actions. However, the non-stationarity inherent in current algorithms hinders the precise updating of strategies to satisfy these safety constraints effectively. In this paper, we propose Multi-Agent Constrained Proximal Advantage Optimization (MAC-PAO), which considers the sequence of agent updates and integrates non-stationarity into sequential update schemes. This algorithm ensures consistent improvement in both rewards and adherence to safety constraints in each iteration. We tested MACPAO on various tasks with safety constraints and demonstrated that it outperforms other MARL algorithms in balancing reward enhancement and safety compliance. Supplementary materials and code are available at the provided link https://github.com/YONEX4090/MultiSafeHand.git.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801490,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801490,,Hands;Codes;Reinforcement learning;Robustness;Safety;Optimization;Intelligent robots;Guidelines,,,,49,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/YONEX4090/MultiSafeHand,https://github.com/YONEX4090/MultiSafeHand
156,PP-TIL: Personalized Planning for Autonomous Driving with Instance-based Transfer Imitation Learning,F. Lin; Y. He; F. Yu,"College of Computer Science and Software Engineering, Shenzhen University, P.R. China; College of Computer Science and Software Engineering, Shenzhen University, P.R. China; College of Computer Science and Software Engineering, Shenzhen University, P.R. China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,5136,5143,"Personalized motion planning holds significant importance within urban automated driving, catering to the unique requirements of individual users. Nevertheless, prior endeavors have frequently encountered difficulties in simultaneously addressing two crucial aspects: personalized planning within intricate urban settings and enhancing planning performance through data utilization. The challenge arises from the expensive and limited nature of user data, coupled with the scene state space tending towards infinity. These factors contribute to overfitting and poor generalization problems during model training. Henceforth, we propose an instance-based transfer imitation learning approach. This method facilitates knowledge transfer from extensive expert domain data to the user domain, presenting a resolution to these issues. We initially train a pre-trained model using large-scale expert data. Subsequently, during the fine-tuning phase, we feed the batch data, which comprises expert and user data. Employing the inverse reinforcement learning technique, we extract the style feature distribution from user demonstrations, constructing the regularization term for the approximation of user style. In our experiments, we conducted extensive evaluations of the proposed method. Compared to the baseline methods, our approach mitigates the overfitting issue caused by sparse user data. Furthermore, we discovered that integrating the driving model with a differentiable nonlinear optimizer as a safety protection layer for end-to-end personalized fine-tuning results in superior planning performance. The code will be available at https://github.com/LinFunster/PP-TIL.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802818,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802818,,Training;Imitation learning;Measurement uncertainty;Reinforcement learning;Feature extraction;Planning;Trajectory;Protection;Knowledge transfer;Overfitting,,,,34,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/LinFunster/PP-TIL,https://github.com/LinFunster/PP-TIL
157,MFCalib: Single-shot and Automatic Extrinsic Calibration for LiDAR and Camera in Targetless Environments Based on Multi-Feature Edge,T. Ye; W. Xu; C. Zheng; Y. Cui,"College of Mechatronics and Control Engineering, Shenzhen University, Shenzhen, China; Manifold Tech Limited, Hong Kong, China; Department of Mechanical Engineering, The University of Hong Kong, Hong Kong SAR, China; College of Mechatronics and Control Engineering, Shenzhen University, Shenzhen, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,864,871,"This paper presents MFCalib, an innovative extrinsic calibration technique for LiDAR and RGB camera that operates automatically in targetless environments with a single data capture. At the heart of this method is using a rich set of edge information, significantly enhancing calibration accuracy and robustness. Specifically, we extract both depth-continuous and depth-discontinuous edges, along with intensity-discontinuous edges on planes. This comprehensive edge extraction strategy ensures our ability to achieve accurate calibration with just one round of data collection, even in complex and varied settings. Addressing the uncertainty of depth-discontinuous edges, we delve into the physical measurement principles of LiDAR and develop a beam model, effectively mitigating the issue of edge inflation caused by the LiDAR beam. Extensive experiment results demonstrate that MFCalib outperforms the state-of-the-art targetless calibration methods across various scenes, achieving and often surpassing the precision of multi-scene calibrations in a single-shot collection. To support community development, we make our code available open-source on GitHub.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802395,National Natural Science Foundation of China; Department of Education of Guangdong Province; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802395,,Laser radar;Accuracy;Uncertainty;Measurement uncertainty;Data collection;Cameras;Robustness;Calibration;Intelligent robots;Software development management,,,,33,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
158,WasteGAN: Data Augmentation for Robotic Waste Sorting through Generative Adversarial Networks,A. Bacchin; L. Barcellona; M. Terreran; S. Ghidoni; E. Menegatti; T. Kiyokawa,"Department of Information Engineering, Intelligent Autonomous System Lab, University of Padova, Padua, Italy; Department of Information Engineering, Intelligent Autonomous System Lab, University of Padova, Padua, Italy; Department of Information Engineering, Intelligent Autonomous System Lab, University of Padova, Padua, Italy; Department of Information Engineering, Intelligent Autonomous System Lab, University of Padova, Padua, Italy; Department of Information Engineering, Intelligent Autonomous System Lab, University of Padova, Padua, Italy; Department of Systems Innovation, Graduate School of Engineering Science, Osaka University, Toyonaka, Osaka, Japan",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,5080,5087,"Robotic waste sorting poses significant challenges in both perception and manipulation, given the extreme variability of objects that should be recognized on a cluttered conveyor belt. While deep learning has proven effective in solving complex tasks, the necessity for extensive data collection and labeling limits its applicability in real-world scenarios like waste sorting. To tackle this issue, we introduce a data augmentation method based on a novel GAN architecture called wasteGAN. The proposed method allows to increase the performance of semantic segmentation models, starting from a very limited bunch of labeled examples, such as few as 100. The key innovations of wasteGAN include a novel loss function, a novel activation function, and a larger generator block. Overall, such innovations helps the network to learn from limited number of examples and synthesize data that better mirrors real-world distributions. We then leverage the higher-quality segmentation masks predicted from models trained on the wasteGAN synthetic data to compute semantic-aware grasp poses, enabling a robotic arm to effectively recognizing contaminants and separating waste in a real-world scenario. Through comprehensive evaluation encompassing dataset-based assessments and real-world experiments, our methodology demonstrated promising potential for robotic waste sorting, yielding performance gains of up to 5.8% in picking contaminants. The project page is available at https://github.com/bach05/wasteGAN.git.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802403,Technology Development; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802403,,Training;Technological innovation;Computational modeling;Semantic segmentation;Grasping;Computer architecture;Data augmentation;Generative adversarial networks;Robots;Sorting,,,,45,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/bach05/wasteGAN,https://github.com/bach05/wasteGAN
159,Multimodal Failure Prediction for Vision-based Manipulation Tasks with Camera Faults,Y. Ma; J. Liu; I. Mamaev; A. Morozov,"Institute of Industrial Automation and Software Engineering, University of Stuttgart, Stuttgart, Germany; Institute of Industrial Automation and Software Engineering, University of Stuttgart, Stuttgart, Germany; Karlsruhe University of Applied Sciences and with Proximity Robotics & Automation GmbH, Germany; Institute of Industrial Automation and Software Engineering, University of Stuttgart, Stuttgart, Germany",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,2951,2957,"Due to the increasing behavioral and structural complexity of robots, it is challenging to predict the execution outcome after error detection. Anomaly detection methods can help detect errors and prevent potential failures. However, not every fault leads to a failure due to the system’s fault tolerance or unintended error masking. In practical applications, a robotic system should have a potential failure evaluation module to estimate the probability of failures when receiving an error alert. Subsequently, a decision-making mechanism should help to take the next action, e.g., terminate, degrade performance, or continue the execution of the task. This paper proposes a multimodal method for failure prediction for vision-based manipulation systems that suffer from potential camera faults. We inject faults into images (e.g., noise and blur) and observe manipulation failure scenarios (e.g., pick failure, place failure, and collision) that can occur during the task. Through extensive fault injection experiments, we created a FAULT-to-FAILURE dataset containing 4000 real-world manipulation samples. The dataset is subsequently used to train the failure predictor. Our approach processes the combination of RGB images, masked images, and planned paths to effectively evaluate whether a certain faulty image could potentially lead to a manipulation failure. Results demonstrate that the proposed method outperforms state-of-the-art models in terms of overall performance, requires fewer sensors, and achieves faster inference speeds. The analytical software prototype and dataset are available at Github: MultimodalFailurePrediction.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802274,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802274,,Welding;Noise;Prototypes;Object detection;Cameras;Software;Sensors;Intelligent robots;Testing;Software development management,,,,21,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
160,TURTLMap: Real-time Localization and Dense Mapping of Low-texture Underwater Environments with a Low-cost Unmanned Underwater Vehicle,J. Song; O. Bagoren; R. Andigani; A. Sethuraman; K. A. Skinner,"Department of Robotics, University of Michigan, Ann Arbor, MI, USA; Department of Robotics, University of Michigan, Ann Arbor, MI, USA; Department of Robotics, University of Michigan, Ann Arbor, MI, USA; Department of Robotics, University of Michigan, Ann Arbor, MI, USA; Department of Robotics, University of Michigan, Ann Arbor, MI, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,1191,1198,"Significant work has been done on advancing localization and mapping in underwater environments. Still, state-of-the-art methods are challenged by low-texture environments, which is common for underwater settings. This makes it difficult to use existing methods in diverse, real-world scenes. In this paper, we present TURTLMap, a novel solution that focuses on textureless underwater environments through a real-time localization and mapping method. We show that this method is low-cost, and capable of tracking the robot accurately, while constructing a dense map of a low-textured environment in real-time. We evaluate the proposed method using real-world data collected in an indoor water tank with a motion capture system and ground truth reference map. Qualitative and quantitative results validate the proposed system achieves accurate and robust localization and precise dense mapping, even when subject to wave conditions. The project page for TURTLMap is https://umfieldrobotics.github.io/TURTLMap.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801692,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801692,,Location awareness;Water;Autonomous underwater vehicles;Accuracy;Tracking;Storage management;Real-time systems;Motion capture;Intelligent robots,,,,44,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://umfieldrobotics.github.io/TURTLMap,https://github.com/umfieldrobotics/TURTLMap
161,Weakly Scene Segmentation Using Efficient Transformer,H. Huang; S. Yuan; C. Wen; Y. Hao; Y. Fang,"New York University, Abu Dhabi, UAE; New York University, Abu Dhabi, UAE; New York University, Abu Dhabi, UAE; New York University, Abu Dhabi, UAE; New York University, Abu Dhabi, UAE",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,9784,9790,"Current methods for large-scale point cloud scene semantic segmentation rely on manually annotated dense point-wise labels, which are costly, labor-intensive, and prone to errors. Consequently, gathering point cloud scenes with billions of labeled points is impractical in real-world scenarios. In this paper, we introduce a novel weak supervision approach to semantically segment large-scale indoor scenes, requiring only 1‰ of the points to be labeled. Specifically, we develop an efficient point neighbor Transformer to capture the geometry of local point cloud patches. To address the quadratic complexity of self-attention computation in Transformers, particularly for large-scale point clouds, we propose approximating the self-attention matrix using low-rank and sparse decomposition. Building on the point neighbor Transformer as foundational blocks, we design a Low-rank Sparse Transformer Network (LST-Net) for weakly supervised large-scale point cloud scene semantic segmentation. Experimental results on two commonly used indoor point cloud scene segmentation benchmarks demonstrate that our model achieves performance comparable to those of both weakly supervised and fully supervised methods. Our code can be found in https://github.com/hhuang-code/LST-Net.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802479,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802479,,Point cloud compression;Geometry;Weak supervision;Semantic segmentation;Benchmark testing;Transformers;Sparse matrices;Matrix decomposition;Mobile computing;Intelligent robots,,,,45,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/hhuang-code/LST-Net,https://github.com/hhuang-code/LST-Net
162,MARVIS: Motion & Geometry Aware Real and Virtual Image Segmentation,J. Wu; X. Lin; S. Negahdaripour; C. Fermüller; Y. Aloimonos,"Maryland Robotics Center (MRC), University of Maryland, College Park, MD, USA; Maryland Robotics Center (MRC), University of Maryland, College Park, MD, USA; Maryland Robotics Center (MRC), University of Maryland, College Park, MD, USA; Maryland Robotics Center (MRC), University of Maryland, College Park, MD, USA; Maryland Robotics Center (MRC), University of Maryland, College Park, MD, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,2778,2785,"Tasks such as autonomous navigation, 3D reconstruction, and object recognition near the water surfaces are crucial in marine robotics applications. However, challenges arise due to dynamic disturbances, e.g., light reflections and refraction from the random air-water interface, irregular liquid flow, and similar factors, which can lead to potential failures in perception and navigation systems. Traditional computer vision algorithms struggle to differentiate between real and virtual image regions, significantly complicating tasks. A virtual image region is an apparent representation formed by the redirection of light rays, typically through reflection or refraction, creating the illusion of an object’s presence without its actual physical location. This work proposes a novel approach for segmentation on real and virtual image regions, exploiting synthetic images combined with domain-invariant information, a Motion Entropy Kernel, and Epipolar Geometric Consistency. Our segmentation network does not need to be re-trained if the domain changes. We show this by deploying the same segmentation network in two different domains: simulation and the real world. By creating realistic synthetic images that mimic the complexities of the water surface, we provide fine-grained training data for our network (MARVIS) to discern between real and virtual images effectively. By motion & geometry-aware design choices and through comprehensive experimental analysis, we achieve state-of-the-art real-virtual image segmentation performance in unseen real world domain, achieving an IoU over 78% and a F1-Score over 86% while ensuring a small computational footprint. MARVIS offers over 43 FPS (8 FPS) inference rates on a single GPU (CPU core). Our code and dataset are available here https://github.com/jiayi-wu-umd/MARVIS.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801473,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801473,,Image segmentation;Surface reconstruction;Three-dimensional displays;Navigation;Motion segmentation;Training data;Reflection;Object recognition;Kernel;Intelligent robots,,,,44,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/jiayi-wu-umd/MARVIS,https://github.com/jiayi-wu-umd/MARVIS
163,ASI-Seg: Audio-Driven Surgical Instrument Segmentation with Surgeon Intention Understanding,Z. Chen; Z. Zhang; W. Guo; X. Luo; L. Bai; J. Wu; H. Ren; H. Liu,"Centre for Artificial Intelligence and Robotics (CAIR), Hong Kong Institute of Science & Innovation, Chinese Academy of Sciences, China; Centre for Artificial Intelligence and Robotics (CAIR), Hong Kong Institute of Science & Innovation, Chinese Academy of Sciences, China; Centre for Artificial Intelligence and Robotics (CAIR), Hong Kong Institute of Science & Innovation, Chinese Academy of Sciences, China; Centre for Artificial Intelligence and Robotics (CAIR), Hong Kong Institute of Science & Innovation, Chinese Academy of Sciences, China; Department of Electronic Engineering, Chinese University of Hong Kong, China; Centre for Artificial Intelligence and Robotics (CAIR), Hong Kong Institute of Science & Innovation, Chinese Academy of Sciences, China; Department of Electronic Engineering, Chinese University of Hong Kong, China; Centre for Artificial Intelligence and Robotics (CAIR), Hong Kong Institute of Science & Innovation, Chinese Academy of Sciences, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,13773,13779,"Surgical instrument segmentation is crucial in surgical scene understanding, thereby facilitating surgical safety. Existing algorithms directly detected all instruments of predefined categories in the input image, lacking the capability to segment specific instruments according to the surgeon’s intention. During different stages of surgery, surgeons exhibit varying preferences and focus toward different surgical instruments. Therefore, an instrument segmentation algorithm that adheres to the surgeon’s intention can minimize distractions from irrelevant instruments and assist surgeons to a great extent. The recent Segment Anything Model (SAM) reveals the capability to segment objects following prompts, but the manual annotations for prompts are impractical during the surgery. To address these limitations in operating rooms, we propose an audio-driven surgical instrument segmentation framework, named ASI-Seg, to accurately segment the required surgical instruments by parsing the audio commands of surgeons. Specifically, we propose an intention-oriented multimodal fusion to interpret the segmentation intention from audio commands and retrieve relevant instrument details to facilitate segmentation. Moreover, to guide our ASI-Seg segment of the required surgical instruments, we devise a contrastive learning prompt encoder to effectively distinguish the required instruments from the irrelevant ones. Therefore, our ASI-Seg promotes the workflow in the operating rooms, thereby providing targeted support and reducing the cognitive load on surgeons. Extensive experiments are performed to validate the ASI-Seg framework, which reveals remarkable advantages over classical state-of-the-art and medical SAMs in both semantic segmentation and intention-oriented segmentation. The source code is available at https://github.com/Zonmgin-Zhang/ASI-Seg.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801703,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801703,,Instruments;Semantic segmentation;Source coding;Surgery;Contrastive learning;Manuals;Cognitive load;Safety;Intelligent robots;Biomedical imaging,,1.0,,38,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/Zonmgin-Zhang/ASI-Seg,https://github.com/Zonmgin-Zhang/ASI-Seg
164,CaFNet: A Confidence-Driven Framework for Radar Camera Depth Estimation,H. Sun; H. Feng; J. Ott; L. Servadei; R. Wille,"Infineon Technologies AG, Neubiberg, Germany; Technical University of Munich, Munich, Germany; Infineon Technologies AG, Neubiberg, Germany; Technical University of Munich, Munich, Germany; Technical University of Munich, Munich, Germany",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,2734,2740,"Depth estimation is critical in autonomous driving for interpreting 3D scenes accurately. Recently, radar-camera depth estimation has become of sufficient interest due to the robustness and low-cost properties of radar. Thus, this paper introduces a two-stage, end-to-end trainable Confidence-aware Fusion Net (CaFNet) for dense depth estimation, combining RGB imagery with sparse and noisy radar point cloud data. The first stage addresses radar-specific challenges, such as ambiguous elevation and noisy measurements, by predicting a radar confidence map and a preliminary coarse depth map. A novel approach is presented for generating the ground truth for the confidence map, which involves associating each radar point with its corresponding object to identify potential projection surfaces. These maps, together with the initial radar input, are processed by a second encoder. For the final depth estimation, we innovate a confidence-aware gated fusion mechanism to integrate radar and image features effectively, thereby enhancing the reliability of the depth map by filtering out radar noise. Our methodology, evaluated on the nuScenes dataset, demonstrates superior performance, improving upon the current leading model by 3.2% in Mean Absolute Error (MAE) and 2.7% in Root Mean Square Error (RMSE). Code: https://github.com/harborsarah/CaFNet",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801594,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801594,,Three-dimensional displays;Radar measurements;Depth measurement;Radar;Radar imaging;Cameras;Robustness;Noise measurement;Root mean square;Surface treatment,,,,32,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/harborsarah/CaFNet,https://github.com/harborsarah/CaFNet
165,Hierarchical Action Chunking Transformer: Learning Temporal Multimodality from Demonstrations with Fast Imitation Behavior,J. H. Park; W. Choi; S. Hong; H. Seo; J. Ahn; C. Ha; H. Han; J. Kwon,"Samsung Research, Samsung Electronics Co. Ltd, Seoul, Korea; Samsung Research, Samsung Electronics Co. Ltd, Seoul, Korea; Samsung Research, Samsung Electronics Co. Ltd, Seoul, Korea; Samsung Research, Samsung Electronics Co. Ltd, Seoul, Korea; Samsung Research, Samsung Electronics Co. Ltd, Seoul, Korea; Samsung Research, Samsung Electronics Co. Ltd, Seoul, Korea; Samsung Research, Samsung Electronics Co. Ltd, Seoul, Korea; Samsung Research, Samsung Electronics Co. Ltd, Seoul, Korea",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,12648,12654,"Behavioral cloning from human demonstrations has succeeded in programming a robot to generate fine-grained motion, but it is still challenging to learn multimodal trajectories such as with various speeds. This restricts the use of a robot dataset collected by multiusers because the different proficiency of robot operators makes the dataset have diverse distributions of speed. To tackle this issue, we develop Hierarchical Action Chunking Transformer with Vector-quantization (HACT-Vq) to efficiently learn temporal multimodality in addition to fine-grained motion. The proposed hierarchical model consists of a high-level policy to make planning for a latent subgoal and style, and a low-level policy to predict an action chunk conditioned with the latent subgoal and style. The latent subgoal and style are trained as discrete representations so that high-level policy can efficiently learn multimodal distributions of demonstrations and retrieve the mode of fast behavior. In experiments, we set up bimanual robots in both simulation and real-world environments, and collected demonstrations with various speeds. The proposed model with the quantized subgoal and style showed the highest success rates with fast imitation behavior. Our code is available at https://github.com/SamsungLabs/hierarchical-act.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802845,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802845,,Codes;Cloning;Programming;Predictive models;Transformers;Data models;Trajectory;Planning;Optimization;Intelligent robots,,,,16,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/SamsungLabs/hierarchical-act,https://github.com/SamsungLabs/hierarchical-act
166,DMFuser: Distilled Multi-Task Learning for End-to-end Transformer-Based Sensor Fusion in Autonomous Driving,P. Agand; M. Mahdavian; M. Savva; M. Chen,"School of Computing Science, Simon Fraser University (SFU), Burnaby, Canada; School of Computing Science, Simon Fraser University (SFU), Burnaby, Canada; School of Computing Science, Simon Fraser University (SFU), Burnaby, Canada; School of Computing Science, Simon Fraser University (SFU), Burnaby, Canada",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,14188,14195,"In end-to-end autonomous driving, current sensor fusion and navigational control techniques used by imitation learning algorithms are insufficient in challenging scenarios involving multiple dynamic agents and result in poor driving capabilities. To tackle this issue, we introduce DMFuser, a transformer-based algorithm that employs knowledge distillation between multi-task student and single-task teachers and combines attention and convolutions to fuse multiple RGB-D camera representations to produce vehicular navigational commands (throttle, steering and brake). Our model incorporates two modules. The first module, perception, encodes data from RGB-D cameras for tasks like semantic segmentation, semantic depth cloud (SDC) mapping, and traffic light state recognition. To enhance feature extraction and fusion from both RGB and depth sources, we harness local and global capabilities of convolution and transformer modules. We employ an attention-CNN fusion structure to effectively learn and fuse RGB and SDC map features. Subsequently, the control module decodes these features along with supplementary data, containing environment’s static and dynamic information, to predict waypoints and vehicular control actions. We evaluate the model and conduct a comparative analysis, in various scenarios, weather conditions, and traffic situations, spanning from normal to adversarial in the CARLA simulator. We achieve better or comparable results in term of driving score (DS) and other metrics with respect to our baselines. Also, our ablation studies demonstrate the effectiveness of our contributions to improve the driving skills. Our code is available at the following github page: https://github.com/pagand/e2etransfuser",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802740,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802740,,Convolutional codes;Navigation;Fuses;Heuristic algorithms;Sensor fusion;Transformers;Multitasking;Cameras;Vehicle dynamics;Autonomous vehicles,,,,38,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/pagand/e2etransfuser,https://github.com/pagand/e2etransfuser
167,Task-Driven Manipulation with Reconfigurable Parallel Robots,D. Morton; M. Cutkosky; M. Pavone,"Department of Mechanical Engineering, Stanford University, Stanford, CA; Department of Mechanical Engineering, Stanford University, Stanford, CA; Department of Aeronautics and Astronautics, Stanford University, Stanford, CA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,9924,9930,"ReachBot, a proposed robotic platform, employs extendable booms as limbs for mobility in challenging environments, such as martian caves. When attached to the environment, ReachBot acts as a parallel robot, with reconfiguration driven by the ability to detach and re-place the booms. This ability enables manipulation-focused scientific objectives: for instance, through operating tools, or handling and transporting samples. To achieve these capabilities, we develop a two-part solution, optimizing for robustness against task uncertainty and stochastic failure modes. First, we present a mixed-integer stance planner to determine the positioning of ReachBot’s booms to maximize the task wrench space about the nominal point(s). Second, we present a convex tension planner to determine boom tensions for the desired task wrenches, accounting for the probabilistic nature of microspine grasping. We demonstrate improvements in key robustness metrics from the field of dexterous manipulation, and show a large increase in the volume of the manipulation workspace. Finally, we employ Monte-Carlo simulation to validate the robustness of these methods, demonstrating good performance across a range of randomized tasks and environments, and generalization to cable-driven morphologies. We make our code available at our project webpage, https://stanfordasl.github.io/reachbot_manipulation/",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801313,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801313,,Measurement;Parallel robots;Uncertainty;Morphology;Grasping;Reliability engineering;Probabilistic logic;Robustness;Planning;Optimization,,,,20,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://stanfordasl.github.io/reachbot_manipulation,https://github.com/StanfordASL/reachbot_manipulation
168,High-Fidelity SLAM Using Gaussian Splatting with Rendering-Guided Densification and Regularized Optimization,S. Sun; M. Mielle; A. J. Lilienthal; M. Magnusson,"AASS Research Center, Örebro University, Sweden; Independent Researcher; AASS Research Center, Örebro University, Sweden; AASS Research Center, Örebro University, Sweden",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,10476,10482,"We propose a dense RGBD SLAM system based on 3D Gaussian Splatting that provides metrically accurate pose tracking and visually realistic reconstruction. To this end, we first propose a Gaussian densification strategy based on the rendering loss to map unobserved areas and refine reobserved areas. Second, we introduce extra regularization parameters to alleviate the ""forgetting"" problem during contiunous mapping, where parameters tend to overfit the latest frame and result in decreasing rendering quality for previous frames. Both mapping and tracking are performed with Gaussian parameters by minimizing re-rendering loss in a differentiable way. Compared to recent neural and concurrently developed Gaussian splatting RGBD SLAM baselines, our method achieves state-of-the-art results on the synthetic dataset Replica and competitive results on the real-world dataset TUM. The code is released on https://github.com/ljjTYJR/HF-SLAM.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802373,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802373,,Simultaneous localization and mapping;Three-dimensional displays;Codes;Accuracy;Rendering (computer graphics);Optimization;Intelligent robots;Synthetic data,,,,30,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/ljjTYJR/HF-SLAM,https://github.com/ljjTYJR/HF-SLAM
169,Conditional Generative Denoiser for Nighttime UAV Tracking,Y. Wang; C. Fu; K. Lu; L. Yao; H. Zuo,"School of Electronic and Information Engineering, Tongji University, Shanghai, China; School of Mechanical Engineering, Tongji University, Shanghai, China; School of Mechanical Engineering, Tongji University, Shanghai, China; School of Mechanical Engineering, Tongji University, Shanghai, China; Department of Computer Science, University of Hong Kong, Hong Kong, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,12971,12978,"State-of-the-art (SOTA) visual object tracking methods have significantly enhanced the autonomy of unmanned aerial vehicles (UAVs). However, in low-light conditions, the presence of irregular real noise from the environments severely degrades the performance of these SOTA methods. Moreover, existing SOTA denoising techniques often fail to meet the real-time processing requirements when deployed as plug-and-play denoisers for UAV tracking. To address this challenge, this work proposes a novel conditional generative denoiser (CG-Denoiser), which breaks free from the limitations of traditional deterministic paradigms and generates the noise conditioning on the input, subsequently removing it. To better align the input dimensions and accelerate inference, a novel nested residual Transformer conditionalizer is developed. Furthermore, an innovative multi-kernel conditional refiner is designed to pertinently refine the denoised output. Extensive experiments show that CGDenoiser promotes the tracking precision of the SOTA tracker by 18.18% on DarkTrack2021 whereas working 5.8 times faster than the second well-performed denoiser. Real-world tests with complex challenges also prove the effectiveness and practicality of CGDenoiser. Code, video demo and supplementary proof for CGDenoier are now available at: https://github.com/vision4robotics/CGDenoiser.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802714,National Natural Science Foundation of China; Natural Science Foundation of Shanghai; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802714,,Training;Visualization;Noise;Noise reduction;Process control;Autonomous aerial vehicles;Transformers;Real-time systems;Object tracking;Noise measurement,,,,33,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/vision4robotics/CGDenoiser,https://github.com/vision4robotics/CGDenoiser
170,Towards Long Term SLAM on Thermal Imagery,C. Keil; A. Gupta; P. Kaveti; H. Singh,"Institute of Experiential Robotics, Northeastern University, Boston, MA; Institute of Experiential Robotics, Northeastern University, Boston, MA; Institute of Experiential Robotics, Northeastern University, Boston, MA; Institute of Experiential Robotics, Northeastern University, Boston, MA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,10174,10181,"Visual SLAM with thermal imagery remains a difficult problem for many state of the art (SOTA) algorithms. Compared with visible spectrum imagery, thermal imagery generally has lower contrast, higher noise, and tends to have lower resolution, making for challenging front-end data association. Thermal imagery also presents a difficult problem for long term relocalization and map reuse, because the relative temperatures of objects in thermal imagery tend to change dramatically from day to night. Feature descriptors typically used for relocalization in SLAM are unable to maintain consistency over these diurnal changes. We show that learned feature descriptors can be used within existing bag of word based localization schemes to dramatically improve place recognition across large temporal gaps in thermal imagery. In order to demonstrate the effectiveness of our trained vocabulary, we have developed a baseline SLAM system, integrating learned features and matching into a classical SLAM algorithm. Our system demonstrates good local tracking on challenging thermal imagery, and relocalization that overcomes dramatic day to night thermal appearance changes. Our code and datasets are available here: https://github.com/neufieldrobotics/IRSLAM_Baseline",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802139,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802139,,Location awareness;Vocabulary;Visualization;Simultaneous localization and mapping;Image resolution;Image recognition;Codes;Noise;Thermal noise;Intelligent robots,,,,43,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/neufieldrobotics/IRSLAM_Baseline,https://github.com/neufieldrobotics/IRSLAM_Baseline
171,SSCBench: A Large-Scale 3D Semantic Scene Completion Benchmark for Autonomous Driving,Y. Li; S. Li; X. Liu; M. Gong; K. Li; N. Chen; Z. Wang; Z. Li; T. Jiang; F. Yu; Y. Wang; H. Zhao; Z. Yu; C. Feng,New York University; New York University; New York University; New York University; New York University; New York University; New York University; New York University; Tsinghua University; ETH Zurich; NVIDIA; Tsinghua University; NVIDIA; New York University,2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,13333,13340,"Monocular scene understanding is a foundational component of autonomous systems. Within the spectrum of monocular perception topics, one crucial and useful task for holistic 3D scene understanding is semantic scene completion (SSC), which jointly completes semantic information and geometric details from RGB input. However, progress in SSC, particularly in large-scale street views, is hindered by the scarcity of high-quality datasets. To address this issue, we introduce SSCBench, a comprehensive benchmark that integrates scenes from widely used automotive datasets (e.g., KITTI-360, nuScenes, and Waymo). SSCBench follows an established setup and format in the community, facilitating the easy exploration of SSC methods in various street views. We benchmark models using monocular, trinocular, and point cloud input to assess the performance gap resulting from sensor coverage and modality. Moreover, we have unified semantic labels across diverse datasets to simplify cross-domain generalization testing. We commit to including more datasets and SSC models to drive further advancements in this field. Our data and code are available at https://github.com/ai4ce/SSCBench.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802143,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802143,,Point cloud compression;Three-dimensional displays;Codes;Semantics;Benchmark testing;Robot sensing systems;Next generation networking;Intelligent robots;Autonomous vehicles;Automotive engineering,,1.0,,67,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/ai4ce/SSCBench,https://github.com/ai4ce/SSCBench
172,A Robot Kinematics Model Estimation Using Inertial Sensors for On-Site Building Robotics,H. Sato; T. Makabe; I. Yanokura; N. Yamaguchi; K. Okada; M. Inaba,"Department of Mechano-Informatics, Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan; Department of Mechano-Informatics, Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan; Department of Mechano-Informatics, Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan; Department of Mechano-Informatics, Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan; Department of Mechano-Informatics, Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan; Department of Mechano-Informatics, Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,8724,8731,"In order to make robots more useful in a variety of environments, they need to be highly portable so that they can be transported to wherever they are needed, and highly storable so that they can be stored when not in use. We propose ""on-site robotics"", which uses parts procured at the location where the robot will be active, and propose a new solution to the problem of portability and storability. In this paper, as a proof of concept for on-site robotics, we describe a method for estimating the kinematic model of a robot by using inertial measurement units (IMU) sensor module on rigid links, estimating the relative orientation between modules from angular velocity, and estimating the relative position from the measurement of centrifugal force.At the end of this paper, as an evaluation for this method, we present an experiment in which a robot made up of wooden sticks reaches a target position. In this experiment, even if the combination of the links is changed, the robot is able to reach the target position again immediately after estimation, showing that it can operate even after being reassembled. Our implementation is available on https://github.com/hiroya1224/urdf_estimation_with_imus.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802591,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802591,,Measurement units;Robot kinematics;Buildings;Estimation;Kinematics;Position measurement;Robot sensing systems;Manipulators;Velocity measurement;Robots,,,,26,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/hiroya1224/urdf_estimation_with_imus,https://github.com/hiroya1224/urdf_estimation_with_imus
173,"Look Gauss, No Pose: Novel View Synthesis using Gaussian Splatting without Accurate Pose Initialization",C. Schmidt; J. Piekenbrinck; B. Leibe,"Computer Vision Group, RWTH Aachen University; Computer Vision Group, RWTH Aachen University; Computer Vision Group, RWTH Aachen University",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,8732,8739,"3D Gaussian Splatting has recently emerged as a powerful tool for fast and accurate novel-view synthesis from a set of posed input images. However, like most novel-view synthesis approaches, it relies on accurate camera pose information, limiting its applicability in real-world scenarios where acquiring accurate camera poses can be challenging or even impossible. We propose an extension to the 3D Gaussian Splatting framework by optimizing the extrinsic camera parameters with respect to photometric residuals. We derive the analytical gradients and integrate their computation with the existing high-performance CUDA implementation. This enables downstream tasks such as 6-DoF camera pose estimation as well as joint reconstruction and camera refinement. In particular, we achieve rapid convergence and high accuracy for pose estimation on real-world scenes. Our method enables fast reconstruction of 3D scenes without requiring accurate pose information by jointly optimizing geometry and camera poses, while achieving state-of-the-art results in novel-view synthesis. Our approach is considerably faster to optimize than most competing methods, and several times faster in rendering. We show results on real-world scenes and complex trajectories through simulated environments, achieving state-of-the-art results on LLFF while reducing runtime by two to four times compared to the most efficient competing method. Source code will be available at https://github.com/Schmiddo/noposegs.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801639,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801639,,Accuracy;Three-dimensional displays;Runtime;Limiting;Source coding;Pose estimation;Cameras;Rendering (computer graphics);Trajectory;Image reconstruction,,,,33,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/Schmiddo/noposegs,https://github.com/Schmiddo/noposegs
174,Sim-to-Real Domain Shift in Online Action Detection,C. Patsch; W. Torjmene; M. Zakour; Y. Wu; D. Salihu; E. Steinbach,"Department of Computer Engineering, School of Computation, Information, and Technology, Munich Institute of Robotics and Machine Intelligence (MIRMI), Chair of Media Technology, Technical University of Munich, Muenchen, Germany; Department of Computer Engineering, School of Computation, Information, and Technology, Munich Institute of Robotics and Machine Intelligence (MIRMI), Chair of Media Technology, Technical University of Munich, Muenchen, Germany; Department of Computer Engineering, School of Computation, Information, and Technology, Munich Institute of Robotics and Machine Intelligence (MIRMI), Chair of Media Technology, Technical University of Munich, Muenchen, Germany; Department of Computer Engineering, School of Computation, Information, and Technology, Munich Institute of Robotics and Machine Intelligence (MIRMI), Chair of Media Technology, Technical University of Munich, Muenchen, Germany; Department of Computer Engineering, School of Computation, Information, and Technology, Munich Institute of Robotics and Machine Intelligence (MIRMI), Chair of Media Technology, Technical University of Munich, Muenchen, Germany; Department of Computer Engineering, School of Computation, Information, and Technology, Munich Institute of Robotics and Machine Intelligence (MIRMI), Chair of Media Technology, Technical University of Munich, Muenchen, Germany",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,388,394,"Human reasoning comprises the ability to understand and reason about the current action solely based on past information. To provide effective assistance in an eldercare or household environment an assistive robot or intelligent assistive system has to assess human actions correctly. Based on this presumption, the task of online action detection determines the current action solely based on the past without access to future information. During inference, the performance of the model is largely impacted by the attributes of the underlying training dataset. However, as high costs and ethical concerns are associated with the real-world data collection process, synthetically created data provides a way to mitigate these problems while providing additional data for the training process of the underlying action detection model to improve performanceDue to the inherent domain shift between the synthetic and real data, we introduce a new egocentric dataset called Human Kitchen Interactions (HKI) to investigate the sim-to-real gap. Our dataset contains in total 100 synthetic and real videos in which 21 different actions are executed in a kitchen environment. The synthetic data is acquired in an egocentric virtual reality (VR) setup while capturing the virtual environment in a game engine. We evaluate state-of-the-art online action detection models on our dataset and provide insights into sim-to-real domain shift. Upon acceptance, we will release our dataset and the corresponding features at https://c-patsch.github.io/HKI/.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802421,Bayer; Ministry of Education; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802421,Visual Learning;Datasets for Human Motion;Simulation and Animation,Training;Solid modeling;Visualization;Data acquisition;Video sequences;Virtual environments;Training data;Data models;Synthetic data;Videos,,,,36,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://c-patsch.github.io/HKI,https://github.com/c-patsch/HKI
175,AO-Grasp: Articulated Object Grasp Generation,C. P. Morlans; C. Chen; Y. Weng; M. Yi; Y. Huang; N. Heppert; L. Zhou; L. Guibas; J. Bohg,"Stanford University, CA, USA; Stanford University, CA, USA; Stanford University, CA, USA; Stanford University, CA, USA; Stanford University, CA, USA; University of Freiburg, Germany; Stanford University, CA, USA; Stanford University, CA, USA; Stanford University, CA, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,13096,13103,"We introduce AO-Grasp, a grasp proposal method that generates 6 DoF grasps that enable robots to interact with articulated objects, such as opening and closing cabinets and appliances. AO-Grasp consists of two main contributions: the AO-Grasp Model and the AO-Grasp Dataset. Given a segmented partial point cloud of a single articulated object, the AO-Grasp Model predicts the best grasp points on the object with an Actionable Grasp Point Predictor. Then, it finds corresponding grasp orientations for each of these points, resulting in stable and actionable grasp proposals. We train the AO-Grasp Model on our new AO-Grasp Dataset, which contains 78K actionable parallel-jaw grasps on synthetic articulated objects. In simulation, AO-Grasp achieves a 45.0% grasp success rate, whereas the highest performing baseline achieves a 35.0% success rate. Additionally, we evaluate AO-Grasp on 120 real-world scenes of objects with varied geometries, articulation axes, and joint states, where AO-Grasp produces successful grasps on 67.5% of scenes, while the baseline only produces successful grasps on 33.3% of scenes. To the best of our knowledge, AO-Grasp is the first method for generating 6 DoF grasps on articulated objects directly from partial point clouds without requiring part detection or hand-designed grasp heuristics. The AO-Grasp Dataset and a pre-trained AO-Grasp model are available at our project website: https://stanford-iprl-lab.github.io/ao-grasp/.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802558,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802558,,Point cloud compression;Geometry;Predictive models;6-DOF;Proposals;Joints;Intelligent robots,,,,41,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://stanford-iprl-lab.github.io/ao-grasp,https://github.com/stanford-iprl-lab/ao-grasp
176,Semantics from Space: Satellite-Guided Thermal Semantic Segmentation Annotation for Aerial Field Robots,C. Lee; S. Soedarmadji; M. Anderson; A. J. Clark; S. -J. Chung,California Institute of Technology; California Institute of Technology; California Institute of Technology; California Institute of Technology; California Institute of Technology,2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,10407,10414,"We present a new method to automatically generate semantic segmentation annotations for thermal imagery captured from an aerial vehicle by utilizing satellite-derived data products alongside onboard global positioning and attitude estimates. This new capability overcomes the challenge of developing thermal semantic perception algorithms for field robots due to the lack of annotated thermal field datasets and the time and costs of manual annotation, enabling precise and rapid annotation of thermal data from field collection efforts at a massively-parallelizable scale. By incorporating a thermal-conditioned refinement step with visual foundation models, our approach can produce highly-precise semantic segmentation labels using low-resolution satellite land cover data for little-tono cost. It achieves 98.5% of the performance from using costly high-resolution options and demonstrates between 70-160% improvement over popular zero-shot semantic segmentation methods based on large vision-language models currently used for generating annotations for RGB imagery. Code will be available at: https://github.com/connorlee77/aerial-auto-segment.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801479,Office of Naval Research; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801479,,Training;Visualization;Satellites;Costs;Annotations;Semantic segmentation;Semantics;Robot sensing systems;Robustness;Sensors,,1.0,,46,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/connorlee77/aerial-auto-segment,https://github.com/connorlee77/aerial-auto-segment
177,UMAD: University of Macau Anomaly Detection Benchmark Dataset,D. Li; L. Chen; C. -Z. Xu; H. Kong,"The State Key Laboratory of Internet of Things for Smart City (SKL-IOTSC), Faculty of Science and Technology, University of Macau, China; The State Key Laboratory of Internet of Things for Smart City (SKL-IOTSC), Faculty of Science and Technology, University of Macau, China; The State Key Laboratory of Internet of Things for Smart City (SKL-IOTSC), Faculty of Science and Technology, University of Macau, China; The State Key Laboratory of Internet of Things for Smart City (SKL-IOTSC), Faculty of Science and Technology, University of Macau, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,5836,5843,"Anomaly detection is critical in surveillance systems and patrol robots by identifying anomalous regions in images for early warning. Depending on whether reference data are utilized, anomaly detection can be categorized into anomaly detection with reference and anomaly detection without reference. Currently, anomaly detection without reference, which is closely related to out-of-distribution (OoD) object detection, struggles with learning anomalous patterns due to the difficulty of collecting sufficiently large and diverse anomaly datasets with the inherent rarity and novelty of anomalies. Alternatively, anomaly detection with reference employs the scheme of change detection to identify anomalies by comparing semantic changes between a reference image and a query one. However, there are very few ADr works due to the scarcity of public datasets in this domain. In this paper, we aim to address this gap by introducing the UMAD Benchmark Dataset. To our best knowledge, this is the first benchmark dataset designed specifically for anomaly detection with reference in robotic patrolling scenarios, e.g., where an autonomous robot is employed to detect anomalous objects by comparing a reference and a query video sequences. The reference sequences can be taken by the robot along a specified route when there are no anomalous objects in the scene. The query sequences are captured online by the robot when it is patrolling in the same scene following the same route. Our benchmark dataset is elaborated such that each query image can find a corresponding reference based on accurate robot localization along the same route in the pre-built 3D map, with which the reference and query images can be geometrically aligned using adaptive warping. Besides the proposed benchmark dataset, we evaluate the baseline models of ADr on this dataset. We hope this benchmark dataset will facilitate the advancement of ADr methods in the future. Our UMAD benchmark dataset will be publicly accessible at https://github.com/IMRL/UMAD.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802194,Science and Technology Development Fund; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802194,,Accuracy;Three-dimensional displays;Surveillance;Video sequences;Semantics;Object detection;Benchmark testing;Robot localization;Anomaly detection;Intelligent robots,,,,42,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/IMRL/UMAD,https://github.com/IMRL/UMAD
178,Representing 3D sparse map points and lines for camera relocalization,B. -T. Bui; H. -H. Bui; D. -T. Tran; J. -H. Lee,"Graduate School of Information Science and Engineering, Ritsumeikan University, Japan; Graduate School of Information Science and Engineering, Ritsumeikan University, Japan; College of Information Science and Engineering, Ritsumeikan University, Japan; College of Information Science and Engineering, Ritsumeikan University, Japan",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,8400,8407,"Recent advancements in visual localization and mapping have demonstrated considerable success in integrating point and line features. However, expanding the localization framework to include additional mapping components frequently results in increased demand for memory and computational resources dedicated to matching tasks. In this study, we show how a lightweight neural network can learn to represent both 3D point and line features, and exhibit leading pose accuracy by harnessing the power of multiple learned mappings. Specifically, we utilize a single transformer block to encode line features, effectively transforming them into distinctive point-like descriptors. Subsequently, we treat these point and line descriptor sets as distinct yet interconnected feature sets. Through the integration of self- and cross-attention within several graph layers, our method effectively refines each feature before regressing 3D maps using two simple MLPs. In comprehensive experiments, our indoor localization findings surpass those of Hloc and Limap across both point-based and line-assisted configurations. Moreover, in outdoor scenarios, our method secures a significant lead, marking the most considerable enhancement over state-of-the-art learning-based methodologies. The source code and demo videos of this work are publicly available at: https://thpjp.github.io/pl2map/.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802402,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802402,,Location awareness;Training;Visualization;Solid modeling;Three-dimensional displays;Source coding;Pipelines;Cameras;Transformers;Videos,,,,40,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://thpjp.github.io/pl2map,https://github.com/ais-lab/pl2map
179,Uncertainty-Aware Deployment of Pre-trained Language-Conditioned Imitation Learning Policies,B. Wu; B. D. Lee; K. Daniilidis; B. Bucher; N. Matni,"GRASP Lab, University of Pennsylvania; GRASP Lab, University of Pennsylvania; GRASP Lab, University of Pennsylvania; GRASP Lab, University of Pennsylvania; GRASP Lab, University of Pennsylvania",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,878,883,"Large-scale robotic policies trained on data from diverse tasks and robotic platforms hold great promise for enabling general-purpose robots; however, reliable generalization to new environment conditions remains a major challenge. Toward addressing this challenge, we propose a novel approach for uncertainty-aware deployment of pre-trained language-conditioned imitation learning agents. Specifically, we use temperature scaling to calibrate these models and exploit the calibrated model to make uncertainty-aware decisions by aggregating the local information of candidate actions. We implement our approach in simulation using three such pre-trained models, and showcase its potential to significantly enhance task completion rates. The accompanying code is accessible at the link: https://github.com/BobWu1998/uncertainty_quant_all.git",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802849,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802849,,Training;Codes;Foundation models;Imitation learning;Decision making;Calibration;Reliability;Intelligent robots,,,,25,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/BobWu1998/uncertainty_quant_all,https://github.com/BobWu1998/uncertainty_quant_all
180,AVM-SLAM: Semantic Visual SLAM with Multi-Sensor Fusion in a Bird’s Eye View for Automated Valet Parking,Y. Li; W. Yang; D. Lin; Q. Wang; Z. Cui; X. Qin,"Chengdu Institute of Computer Applications, Chinese Academy of Sciences, Chengdu, PR China; NA; Chengdu Institute of Computer Applications, Chinese Academy of Sciences, Chengdu, PR China; Chengdu Institute of Computer Applications, Chinese Academy of Sciences, Chengdu, PR China; Chengdu Institute of Computer Applications, Chinese Academy of Sciences, Chengdu, PR China; Chengdu Institute of Computer Applications, Chinese Academy of Sciences, Chengdu, PR China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,7937,7943,"Accurate localization in challenging garage environments—marked by poor lighting, sparse textures, repetitive structures, dynamic scenes, and the absence of GPS—is crucial for automated valet parking (AVP) tasks. Addressing these challenges, our research introduces AVM-SLAM, a cutting-edge semantic visual SLAM architecture with multi-sensor fusion in a bird’s eye view (BEV). This novel framework synergizes the capabilities of four fisheye cameras, wheel encoders, and an inertial measurement unit (IMU) to construct a robust SLAM system. Unique to our approach is the implementation of a flare removal technique within the BEV imagery, significantly enhancing road marking detection and semantic feature extraction by convolutional neural networks for superior mapping and localization. Our work also pioneers a semantic prequalification (SPQ) module, designed to adeptly handle the challenges posed by environments with repetitive textures, thereby enhancing loop detection and system robustness. To demonstrate the effectiveness and resilience of AVM-SLAM, we have released a specialized multi-sensor and high-resolution dataset of an underground garage, accessible at https://yale-cv.github.io/avm-slamdataset, encouraging further exploration and validation of our approach within similar settings.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802668,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802668,,Location awareness;Visualization;Simultaneous localization and mapping;Roads;Semantic segmentation;Semantics;Wheels;Feature extraction;Robustness;Resilience,,,,31,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://yale-cv.github.io/avm-slamdataset,
181,Neural Trajectory Model: Implicit Neural Trajectory Representation for Trajectories Generation,Z. Yu; Y. Tang,"Systems Hub, The Hong Kong University of Science and Technology(Guangzhou); International Digital Economy Academy",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,3049,3054,"The multi-agent trajectory planning problem is a difficult problem in robotics due to its computational complexity and real-world environment complexity with uncertainty, non-linearity, and real-time requirements. Many existing solutions are either search-based or optimization-based approaches with simplified assumptions of environment, limited planning speed, and limited scalability in the number of agents. In this work, we first attempt to reformulate single-agent and multi-agent trajectory planning problems as query problems over an implicit neural representation of trajectories. We formulate such implicit representations as Neural Trajectory Models (NTM) which can be queried to generate nearly optimal trajectory in complex environments. We conduct experiments in simulation environments and demonstrate that NTM achieve (1) sub-millisecond planning time using GPUs, (2) almost avoiding all collisions, and (3) generating almost shortest paths. We also demonstrate that the same NTM framework can also be used for refining low-quality and conflicting multi-agent trajectories into nearly optimal solutions efficiently. (Open source code is available at https://github.com/laser2099/neural-trajectory-model)",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802789,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802789,,Uncertainty;Trajectory planning;Source coding;Scalability;Refining;Real-time systems;Trajectory;Planning;Optimization;Intelligent robots,,,,19,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/laser2099/neural-trajectory-model,https://github.com/laser2099/neural-trajectory-model
182,FlexLoc: Conditional Neural Networks for Zero-Shot Sensor Perspective Invariance in Object Localization with Distributed Multimodal Sensors,J. Wu; Z. Wang; X. Ouyang; H. L. Jeong; C. Samplawski; L. M. Kaplan; B. Marlin; M. Srivastava,"University of California, Los Angeles; University of California, Los Angeles; University of California, Los Angeles; University of California, Los Angeles; University of Massachusetts, Amherst; Research Laboratory, US DEVCOM Army; University of Massachusetts, Amherst; University of California, Los Angeles",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,8563,8570,"Localization is a critical technology for various applications ranging from navigation and surveillance to assisted living. Localization systems typically fuse information from sensors viewing the scene from different perspectives to estimate the target location while also employing multiple modalities for enhanced robustness and accuracy. Recently, such systems have employed end-to-end deep neural models trained on large datasets due to their superior performance and ability to handle data from diverse sensor modalities. However, such neural models are often trained on data collected from a particular set of sensor poses (i.e., locations and orientations). During real-world deployments, slight deviations from these sensor poses can result in extreme inaccuracies. To address this challenge, we introduce FlexLoc, which employs conditional neural networks to inject node perspective information to adapt the localization pipeline. Specifically, a small subset of model weights are derived from node poses at run time, enabling accurate generalization to unseen perspectives with minimal additional overhead. Our evaluations on a multimodal, multi-view indoor tracking dataset showcase that FlexLoc improves the localization accuracy by almost 50% in the zero-shot case (no calibration data available) compared to the baselines. The source code of FlexLoc is available in https://github.com/nesl/FlexLoc.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802096,Army Research Laboratory; National Science Foundation; Air Force Office of Scientific Research; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802096,,Location awareness;Adaptation models;Accuracy;Surveillance;Neural networks;Pipelines;Sensor systems;Data models;Sensors;Calibration,,,,34,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/nesl/FlexLoc,https://github.com/nesl/FlexLoc
183,ODTFormer: Efficient Obstacle Detection and Tracking with Stereo Cameras Based on Transformer,T. Ding; H. Li; H. Jiang,"Northeastern University, Boston, MA; Brown University, Providence, RI; Northeastern University, Boston, MA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,9721,9728,"Obstacle detection and tracking represent a critical component in robot autonomous navigation. In this paper, we propose ODTFormer, a Transformer-based model that addresses both obstacle detection and tracking problems. For the detection task, our approach leverages deformable attention to construct a 3D cost volume, which is decoded progressively in the form of voxel occupancy grids. We further track the obstacles by matching the voxels between consecutive frames. The entire model can be optimized in an end-to-end manner. Through extensive experiments on DrivingStereo and KITTI benchmarks, our model achieves state-of-the-art performance in the obstacle detection task. We also report comparable accuracy to state-of-the-art obstacle tracking models while requiring only a fraction of their computation cost, typically ten-fold to twenty-fold less. Our code is available on https://github.com/neu-vi/ODTFormer.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802476,National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802476,,Costs;Accuracy;Three-dimensional displays;Quantization (signal);Tracking;Navigation;Computational modeling;Graphics processing units;Transformers;Intelligent robots,,,,42,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/neu-vi/ODTFormer,https://github.com/neu-vi/ODTFormer
184,SurrealDriver: Designing LLM-powered Generative Driver Agent Framework based on Human Drivers’ Driving-thinking Data,Y. Jin; R. Yang; Z. Yi; X. Shen; H. Peng; X. Liu; J. Qin; J. Li; J. Xie; P. Gao; G. Zhou; J. Gong,"Institute for AI Industry Research, Tsinghua University, Beijing, China; Institute for AI Industry Research, Tsinghua University, Beijing, China; Institute for AI Industry Research, Tsinghua University, Beijing, China; Institute for AI Industry Research, Tsinghua University, Beijing, China; Institute for AI Industry Research, Tsinghua University, Beijing, China; Institute for AI Industry Research, Tsinghua University, Beijing, China; Institute for AI Industry Research, Tsinghua University, Beijing, China; Institute for AI Industry Research, Tsinghua University, Beijing, China; Institute for AI Industry Research, Tsinghua University, Beijing, China; Institute for AI Industry Research, Tsinghua University, Beijing, China; Institute for AI Industry Research, Tsinghua University, Beijing, China; Institute for AI Industry Research, Tsinghua University, Beijing, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,966,971,"Leveraging advanced reasoning capabilities and extensive world knowledge of large language models (LLMs) to construct generative agents for solving complex real-world problems is a major trend. However, LLMs inherently lack embodiment as humans, resulting in suboptimal performance in many embodied decision-making tasks. In this paper, we introduce a framework for building human-like generative driving agents using post-driving self-report driving-thinking data from human drivers as both demonstration and feedback. To capture high-quality, natural language data from drivers, we conducted urban driving experiments, recording drivers’ verbalized thoughts under various conditions to serve as chain-of-thought prompts and demonstration examples for the LLM-Agent. The framework’s effectiveness was evaluated through simulations and human assessments. Results indicate that incorporating expert demonstration data significantly reduced collision rates by 81.04% and increased human likeness by 50% compared to a baseline LLM-based agent. Our study provides insights into using natural language-based human demonstration data for embodied tasks. The driving-thinking dataset is available at https://github.com/AIR-DISCOVER/Driving-Thinking-Dataset.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802229,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802229,,Training;Translation;Large language models;Reinforcement learning;Market research;Cognition;Recording;Driver behavior;Interviews;Vehicles,,,,24,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/AIR-DISCOVER/Driving-Thinking-Dataset,https://github.com/AIR-DISCOVER/Driving-Thinking-Dataset
185,Similarity Distance-Based Label Assignment for Tiny Object Detection,S. Shi; Q. Fang; X. Xu; T. Zhao,"National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,13711,13718,"Tiny object detection is becoming one of the most challenging tasks in computer vision because of the limited object size and lack of information. The label assignment strategy is a key factor affecting the accuracy of object detection. Although there are some effective label assignment strategies for tiny objects, most of them focus on reducing the sensitivity to the bounding boxes to increase the number of positive samples and have some fixed hyperparameters need to set. However, more positive samples may not necessarily lead to better detection results, in fact, excessive positive samples may lead to more false positives. In this paper, we introduce a simple but effective strategy named the Similarity Distance (SimD) to evaluate the similarity between bounding boxes. This proposed strategy not only considers both location and shape similarity but also learns hyperparameters adaptively, ensuring that it can adapt to different datasets and various object sizes in a dataset. Our approach can be simply applied in common anchor-based detectors in place of the IoU for label assignment and Non Maximum Suppression (NMS). Extensive experiments on four mainstream tiny object detection datasets demonstrate superior performance of our method, especially, 1.8 AP points and 4.1 AP points of very tiny higher than the state-of-the-art competitors on AI-TOD. Code is available at: https://github.com/cszzshi/simd.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801448,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801448,,Computer vision;Sensitivity;Codes;Accuracy;Shape;Object detection;Detectors;Intelligent robots,,,,41,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/cszzshi/simd,https://github.com/cszzshi/simd
186,Tightly-Coupled Factor Graph Formulation For Radar-Inertial Odometry,J. Michalczyk; J. Quell; F. Steidle; M. G. Müller; S. Weiss,"Control of Networked Systems Group, University of Klagenfurt, Austria; Institute of Robotics and Mechatronics, German Aerospace Center (DLR), Wessling, Germany; Institute of Robotics and Mechatronics, German Aerospace Center (DLR), Wessling, Germany; Institute of Robotics and Mechatronics, German Aerospace Center (DLR), Wessling, Germany; Control of Networked Systems Group, University of Klagenfurt, Austria",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,3364,3370,"In this paper, we present a Radar-Inertial Odometry (RIO) method based on the nonlinear optimization of factor graphs in a sliding window fashion. Our method makes use of a light-weight, low-power, inexpensive and commonly available hardware enabling easy deployment on small Unmanned Aerial Vehicles (UAV)s. We keep the state estimation problem bounded by employing partial marginalization of the oldest states, rendering the method real-time capable. We compare the implemented approach to the state-of-the-art multi-state Extended Kalman Filter (EKF)-based method in a one-to-one fashion. That is, we implemented in a single custom C++ RIO framework both estimation back-ends with all other parts shared and thus identical for a fair direct comparison. In the real-world flight experiments, we compare the two methods and show that both perform similarly in terms of accuracy when the linearization point is not far from the true state. Upon wrong initialization, the factor graph approach heavily outperforms the EKF approach. We also acknowledge that the influence of undetected outliers can overwhelm the inherent benefits of the nonlinear optimization approach leading to the insight that the estimator front-end has an important (and often underestimated) role in the overall performance. The open source code and datasets can be found here: https://github.com/aau-cns/aaucns_rio.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801945,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801945,,Accuracy;Uncertainty;Autonomous aerial vehicles;Software;Real-time systems;Vectors;Trajectory;Odometry;Optimization;Convergence,,,,26,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/aau-cns/aaucns_rio,https://github.com/aau-cns/aaucns_rio
187,Touch-GS: Visual-Tactile Supervised 3D Gaussian Splatting,A. Swann; M. Strong; W. K. Do; G. S. Camps; M. Schwager; M. Kennedy,"Department of Mechanical Engineering, Stanford University, Stanford, CA, USA; Department of Computer Science, Stanford University, Stanford, CA, USA; Department of Mechanical Engineering, Stanford University, Stanford, CA, USA; Department of Aeronautics and Astronautics, Stanford University, Stanford, CA, USA; Department of Aeronautics and Astronautics, Stanford University, Stanford, CA, USA; Department of Mechanical Engineering, Stanford University, Stanford, CA, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,10511,10518,"In this work, we propose a novel method to supervise 3D Gaussian Splatting (3DGS) scenes using optical tactile sensors. Optical tactile sensors have become widespread in their use in robotics for manipulation and object representation; however, raw optical tactile sensor data is unsuitable to directly supervise a 3DGS scene. Our representation leverages a Gaussian Process Implicit Surface to implicitly represent the object, combining many touches into a unified representation with uncertainty. We merge this model with a monocular depth estimation network, which is aligned in a two stage process, coarsely aligning with a depth camera and then finely adjusting to match our touch data. For every training image, our method produces a corresponding fused depth and uncertainty map. Utilizing this additional information, we propose a new loss function, variance-weighted depth supervised loss, for training the 3DGS scene model. We leverage the DenseTact optical tactile sensor and RealSense RGB-D camera to show that combining touch and vision in this manner leads to quantitatively and qualitatively better results than vision or touch alone in few-view scene synthesis on opaque, reflective and transparent objects. Please see our project page at armlabstanford.github.io/touchgs.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802412,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802412,,Training;Optical losses;Uncertainty;Three-dimensional displays;Fuses;Tactile sensors;Optical imaging;Cameras;Optical sensors;Optical reflection,,,,30,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://armlabstanford.github.io/touchgs,
188,DeepBHMR: Learning Bidirectional Hybrid Mixture Models for Generalized Rigid Point Set Registration,Z. Min; Z. Zhang; A. Zhang; R. Song; Y. Li; M. Q. . -H. Meng,"School of Control Science and Engineering, Shandong University, China; Department of Aeronautical and Aviation Engineering, The Hong Kong Polytechnic University; Yuanhua Robotics, Perception and AI Technologies Ltd, Shenzhen, China; School of Control Science and Engineering, Shandong University, China; School of Control Science and Engineering, Shandong University, China; Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,13160,13167,"In this paper, we introduce a novel normal-assisted learning-based rigid registration approach, i.e., Deep Bi-directional Hybrid Mixture Registration (DeepBHMR). Our approach utilises helpful normal vectors explicitly in both correspondence and transformation stages and formulates the optimization objective of registration in a bi-directional way that considers noise in both point sets. DeepBHMR consists of three modules: (1) the correspondence network that estimates the correspondence probability relating points within one generalized point set (i.e., positional and normal vectors) with components of Hybrid Mixture Models (HMMs) representing the other generalized point set; (2) the posterior module that computes HMMs parameters; (3) the transformation module that computes the rotation matrix and the translation vector given the estimated generalized-point to hybrid-distribution correspondences and HMMs parameters. DeepBHMR has been validated on 291 human femur and 260 hip models, and extensive experimental results demonstrate that DeepBHMR outperforms the state-of-the-art registration methods (p-value < 0.01). In the circumstance of femur bones, the mean rotation and translation error values are around 1° (i.e., 1.01°) and less than 1 mm (i.e., 0.36mm), respectively. Furthermore, even under the large transformation (i.e., in the range of [0,180]° and [0, 100] mm), the mean RMSE values being 3.05 mm is still satisfactory. Additionally, the results demonstrate the DeepBHMR’s favorable generalizability from femur shapes to hip shapes. We have carefully validated the significant benefits of incorporating normal vectors and the bidirectional mechanism. DeepBHMR can successfully handle the challenging scenario of large transformation and partial registration. The codes are available at https://github.com/zzyrobot/DeepBHMR.git.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802351,National Natural Science Foundation of China; Young Scientists Fund; Jinan Science and Technology Bureau; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802351,,Translation;Accuracy;Shape;Hidden Markov models;Mixture models;Bidirectional control;Bones;Vectors;Optimization;Hip,,,,25,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/zzyrobot/DeepBHMR,https://github.com/zzyrobot/DeepBHMR
189,SwinMTL: A Shared Architecture for Simultaneous Depth Estimation and Semantic Segmentation from Monocular Camera Images,P. Taghavi; R. Langari; G. Pandey,"Department of Mechanical Engineering, Texas A&M University, College Station, TX, USA; Department of Mechanical Engineering, Texas A&M University, College Station, TX, USA; Department of Engineering Technology & Industrial Distribution, Texas A&M University, College Station, TX, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,4957,4964,"This research paper presents an innovative multitask learning framework that allows concurrent depth estimation and semantic segmentation using a single camera. The proposed approach is based on a shared encoder-decoder architecture, which integrates various techniques to improve the accuracy of the depth estimation and semantic segmentation task without compromising computational efficiency. Additionally, the paper incorporates an adversarial training component, employing a Wasserstein GAN framework with a critic network, to refine model’s predictions. The framework is thoroughly evaluated on two datasets - the outdoor Cityscapes dataset and the indoor NYU Depth V2 dataset - and it outperforms existing state-of-the-art methods in both segmentation and depth estimation tasks. We also conducted ablation studies to analyze the contributions of different components, including pre-training strategies, the inclusion of critics, the use of logarithmic depth scaling, and advanced image augmentations, to provide a better understanding of the proposed framework. The accompanying source code is accessible at https://github.com/PardisTaghavi/SwinMTL.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802239,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802239,,Training;Accuracy;Depth measurement;Semantic segmentation;Source coding;Computer architecture;Streaming media;Multitasking;Cameras;Computational efficiency,,,,50,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/PardisTaghavi/SwinMTL,https://github.com/PardisTaghavi/SwinMTL
190,SCANet: Correcting LEGO Assembly Errors with Self-Correct Assembly Network,Y. Wan; K. Zhou; J. Chen; H. Dong,"School of Computer Science and Engineering, Southeast University; School of Computer Science, Peking University; School of Computer Science, Peking University; School of Computer Science, Peking University",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,5940,5947,"Autonomous assembly in robotics and 3D vision presents significant challenges, particularly in ensuring assembly correctness. Presently, predominant methods such as MEPNet focus on assembling components based on manually provided images. However, these approaches often fall short in achieving satisfactory results for tasks requiring long-term planning. Concurrently, we observe that integrating a self-correction module can partially alleviate such issues. Motivated by this concern, we introduce the Single-Step Assembly Error Correction Task, which involves identifying and rectifying misassembled components. To support research in this area, we present the LEGO Error Correction Assembly Dataset (LEGO-ECA), comprising manual images for assembly steps and instances of assembly failures. Additionally, we propose the Self-Correct Assembly Network (SCANet), a novel method to address this task. SCANet treats assembled components as queries, determining their correctness in manual images and providing corrections when necessary. Finally, we utilize SCANet to correct the assembly results of MEPNet. Experimental results demonstrate that SCANet can identify and correct MEPNet's misassembled results, significantly improving the correctness of assembly. Our code and dataset are available at https://github.com/Yaser-wyx/SCANet.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801934,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801934,,Three-dimensional displays;Codes;Manuals;Error correction;Planning;Assembly;Intelligent robots,,1.0,,47,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/Yaser-wyx/SCANet,https://github.com/Yaser-wyx/SCANet
191,CaT: Constraints as Terminations for Legged Locomotion Reinforcement Learning,E. Chane-Sane; P. -A. Leziart; T. Flayols; O. Stasse; P. Souères; N. Mansard,"LAAS-CNRS, Université de Toulouse, Toulouse, France; LAAS-CNRS, Université de Toulouse, Toulouse, France; LAAS-CNRS, Université de Toulouse, Toulouse, France; LAAS-CNRS, Université de Toulouse, Toulouse, France; LAAS-CNRS, Université de Toulouse, Toulouse, France; LAAS-CNRS, Université de Toulouse, Toulouse, France",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,13303,13310,"Deep Reinforcement Learning (RL) has demonstrated impressive results in solving complex robotic tasks such as quadruped locomotion. Yet, current solvers fail to produce efficient policies respecting hard constraints. In this work, we advocate for integrating constraints into robot learning and present Constraints as Terminations (CaT), a novel constrained RL algorithm. Departing from classical constrained RL formulations, we reformulate constraints through stochastic terminations during policy learning: any violation of a constraint triggers a probability of terminating potential future rewards the RL agent could attain. We propose an algorithmic approach to this formulation, by minimally modifying widely used off-the-shelf RL algorithms in robot learning (such as Proximal Policy Optimization). Our approach leads to excellent constraint adherence without introducing undue complexity and computational overhead, thus mitigating barriers to broader adoption. Through empirical evaluation on the real quadruped robot Solo crossing challenging obstacles, we demonstrate that CaT provides a compelling solution for incorporating constraints into RL frameworks. Videos and code are available at constraints-as-terminations.github.io.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802334,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802334,,Legged locomotion;Codes;Deep reinforcement learning;Robot learning;Safety;Quadrupedal robots;Standards;Optimization;Intelligent robots;Videos,,,,54,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
192,CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models,H. Huang; F. Lin; Y. Hu; S. Wang; Y. Gao,"Institute of Interdisciplinary Information Sciences, Tsinghua University; Institute of Interdisciplinary Information Sciences, Tsinghua University; Institute of Interdisciplinary Information Sciences, Tsinghua University; Institute of Interdisciplinary Information Sciences, Tsinghua University; Institute of Interdisciplinary Information Sciences, Tsinghua University",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,9488,9495,"Foundation models pre-trained on web-scale data are shown to encapsulate extensive world knowledge beneficial for robotic manipulation in the form of task planning. However, the actual physical implementation of these plans often relies on task-specific learning methods, which require significant data collection and struggle with generalizability. In this work, we introduce Robotic Manipulation through Spatial Constraints of Parts (CoPa), a novel framework that leverages the common sense knowledge embedded within foundation models to generate a sequence of 6-DoF end-effector poses for open-world robotic manipulation. Specifically, we decompose the manipulation process into two phases: task-oriented grasping and task-aware motion planning. In the task-oriented grasping phase, we employ foundation vision-language models (VLMs) to select the object’s grasping part through a novel coarse-to-fine grounding mechanism. During the task-aware motion planning phase, VLMs are utilized again to identify the spatial geometry constraints of task-relevant object parts, which are then used to derive post-grasp poses. We also demonstrate how CoPa can be seamlessly integrated with existing robotic planning algorithms to accomplish complex, long-horizon tasks. Our comprehensive real-world experiments show that CoPa possesses a fine-grained physical understanding of scenes, capable of handling open-set instructions and objects with minimal prompt engineering and without additional training. Project page: copa-2024.github.io",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801352,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801352,,Training;Learning systems;Knowledge engineering;Foundation models;Grasping;Robot sensing systems;Planning;Prompt engineering;Object recognition;Intelligent robots,,,,58,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
193,Continual Domain Randomization,J. Josifovski; S. Auddy; M. Malmir; J. Piater; A. Knoll; N. Navarro-Guerrero,"School of Computation Information and Technology, Technical University of Munich, Germany; Department of Computer Science, University of Innsbruck, Austria; School of Computation Information and Technology, Technical University of Munich, Germany; Department of Computer Science, University of Innsbruck, Austria; School of Computation Information and Technology, Technical University of Munich, Germany; L3S Research Center, Leibniz Universität, Hannover, Germany",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,4965,4972,"Domain Randomization (DR) is commonly used for sim2real transfer of reinforcement learning (RL) policies in robotics. Most DR approaches require a simulator with a fixed set of tunable parameters from the start of the training, from which the parameters are randomized simultaneously to train a robust model for use in the real world. However, the combined randomization of many parameters increases the task difficulty and might result in sub-optimal policies. To address this problem and to provide a more flexible training process, we propose Continual Domain Randomization (CDR) for RL that combines domain randomization with continual learning to enable sequential training in simulation on a subset of randomization parameters at a time. Starting from a model trained in a non-randomized simulation where the task is easier to solve, the model is trained on a sequence of randomizations, and continual learning is employed to remember the effects of previous randomizations. Our robotic reaching and grasping tasks experiments show that the model trained in this fashion learns effectively in simulation and performs robustly on the real robot while matching or outperforming baselines that employ combined randomization or sequential randomization without continual learning. Our code and videos are available at https://continual-dr.github.io/.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802060,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802060,Domain randomization;sim2real transfer;continual reinforcement learning;robotic manipulation,Training;Continuing education;Codes;Reinforcement learning;Grasping;Intelligent robots;Videos,,,,41,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
194,Polaris: Open-ended Interactive Robotic Manipulation via Syn2Real Visual Grounding and Large Language Models,T. Wang; H. Lin; J. Yu; Y. Fu,"Fudan University, China; Fudan University, China; Fudan University, China; Fudan University, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,9676,9683,"This paper investigates the task of the open-ended interactive robotic manipulation on table-top scenarios. While recent Large Language Models (LLMs) enhance robots' comprehension of user instructions, their lack of visual grounding constrains their ability to physically interact with the environment. This is because the robot needs to locate the target object for manipulation within the physical workspace. To this end, we introduce an interactive robotic manipulation framework called Polaris, which integrates perception and interaction by utilizing GPT-4 alongside grounded vision models. For precise manipulation, it is essential that such grounded vision models produce detailed object pose for the target object, rather than merely identifying pixels belonging to them in the image. Consequently, we propose a novel Synthetic-to-Real (Syn2Real) pose estimation pipeline. This pipeline utilizes rendered synthetic data for training and is then transferred to real-world manipulation tasks. The real-world performance demonstrates the efficacy of our proposed pipeline and underscores its potential for extension to more general categories. Moreover, real-robot experiments have showcased the impressive performance of our framework in grasping and executing multiple manipulation tasks. This indicates its potential to generalize to scenarios beyond the tabletop. More information and video results are available here: https://star-uu-wang.github.io/Polaris/.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801446,Shanghai Platform for Neuromorphic and AI Chip; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801446,,Training;Visualization;Grounding;Large language models;Pipelines;Pose estimation;Object recognition;Robots;Intelligent robots;Synthetic data,,,,62,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://star-uu-wang.github.io/Polaris,
195,Vertebrae-based Global X-ray to CT Registration for Thoracic Surgeries,L. Liu; Y. Jiao; Z. An; H. Ma; C. Zhou; H. Lu; J. Hu; R. Xiong; Y. Wang,"Department of Control Science and Engineering, Zhejiang University, Hangzhou, China; School of Information Science and Technology, Hangzhou Normal University, Hangzhou, China; Department of Thoracic Surgery, The First Affiliated Hospital, Zhejiang University School of Medicine, Hangzhou, China; Department of Thoracic Surgery, The First Affiliated Hospital, Zhejiang University School of Medicine, Hangzhou, China; Department of Control Science and Engineering, Zhejiang University, Hangzhou, China; Department of Control Science and Engineering, Zhejiang University, Hangzhou, China; Department of Thoracic Surgery, The First Affiliated Hospital, Zhejiang University School of Medicine, Hangzhou, China; Department of Control Science and Engineering, Zhejiang University, Hangzhou, China; Department of Control Science and Engineering, Zhejiang University, Hangzhou, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,7212,7219,"X-ray to CT registration is an essential technique to provide on-site guidance for clinicians and medical robots by aligning preoperative information with intraoperative images. Current methods focus on local registration with small capture ranges and necessitate a manual initial alignment before precise registration. Some existing global methods are likely to fail in thoracic surgeries because of the respiratory motion and the nearly colinear nature of vertebrae landmarks. In this study, we propose a vertebrae-based global X-ray to CT registration method with the assistance of clinical setups for thoracic surgeries. Firstly, vertebrae centroids are automatically localized by CNN-based networks in CT and X-ray for establishing 2D/3-D correspondences. Then, inspired by clinical setup, we address the degradation of colinear landmarks of 6-DoF pose estimation by introducing a 4-DoF solver. Considering the inaccurate priori and landmark mislocalization, the solver is embedded into the Adaptive Error-Aware Estimator (AE2) to simultaneously estimate weights and aggregate candidate poses. Finally, the whole method is trained in an end-to-end manner for better performance. Evaluations on both the public LIDC-IDRI dataset and clinical dataset demonstrate that our method outperforms existing optimization-based and learningbased approaches in terms of registration accuracy and success rate. Our code: https://github.com/LiuLiluZJU/2P-AE2",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10803049,National Science and Technology Major Project; National Natural Science Foundation of China; State Key Laboratory of Industrial Control Technology; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10803049,medical robot;registration;pose estimation;localization;deep learning;thoracic surgery,Training;Accuracy;Medical robotics;Computed tomography;Aggregates;Pose estimation;Surgery;Manuals;Robustness;X-ray imaging,,,,31,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/LiuLiluZJU/2P-AE2,https://github.com/LiuLiluZJU/2P-AE2
196,EverySync: An Open Hardware Time Synchronization Sensor Suite for Common Sensors in SLAM,X. Wu; H. Sun; R. Wu; Z. Fang,"Faculty of Robot Science and Engineering, Northeastern University, Shenyang, China; Faculty of Robot Science and Engineering, Northeastern University, Shenyang, China; Faculty of Robot Science and Engineering, Northeastern University, Shenyang, China; Faculty of Robot Science and Engineering, Northeastern University, Shenyang, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,12587,12593,"Multi-sensor fusion systems have been widely applied in various fields, including mobile robot, simultaneous localization and mapping (SLAM), and autonomous driving. For a tightly coupled multi-sensor fusion system, strict time synchronization between sensors will improve the accuracy of the system. However, there is currently a lack of open-source and general-purpose hardware synchronization systems for Cameras, IMUs, LiDARs, GNSS/RTK in the academic community. Therefore, we propose EverySync, an open hardware time synchronization system to address this gap. The synchronization accuracy of the system was evaluated through multiple experiments, achieving an accuracy of less than 1 ms. And, real-world experiments proved that hardware time synchronization improves the accuracy of the SLAM system. This open-source system is available on GitHub.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801968,National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801968,,Simultaneous localization and mapping;Accuracy;Laser radar;Robot vision systems;Sensor fusion;Cameras;Hardware;Sensor systems;Synchronization;Software development management,,,,28,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
197,Object Instance Retrieval in Assistive Robotics: Leveraging Fine-Tuned SimSiam with Multi-View Images Based on 3D Semantic Map,T. Sakaguchi; A. Taniguchi; Y. Hagiwara; L. El Hafi; S. Hasegawa; T. Taniguchi,"Ritsumeikan University, Osaka, Japan; Ritsumeikan University, Osaka, Japan; Ritsumeikan University, Osaka, Japan; Ritsumeikan University, Osaka, Japan; Ritsumeikan University, Osaka, Japan; Ritsumeikan University, Osaka, Japan",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,7817,7824,"Robots that assist humans in their daily lives should be able to locate specific instances of objects in an environment that match a user’s desired objects. This task is known as instance-specific image goal navigation (InstanceImageNav), which requires a model that can distinguish different instances of an object within the same class. A significant challenge in robotics is that when a robot observes the same object from various 3D viewpoints, its appearance may differ significantly, making it difficult to recognize and locate accurately. In this paper, we introduce a method called SimView, which leverages multi-view images based on a 3D semantic map of an environment and self-supervised learning using SimSiam to train an instance-identification model on-site. The effectiveness of our approach was validated using a photorealistic simulator, Habitat Matterport 3D, created by scanning actual home environments. Our results demonstrate a 1.7-fold improvement in task accuracy compared with contrastive language-image pre-training (CLIP), a pre-trained multimodal contrastive learning method for object searching. This improvement highlights the benefits of our proposed fine-tuning method in enhancing the performance of assistive robots in InstanceImageNav tasks. The project website is https://emergentsystemlabstudent.github.io/MultiViewRetrieve/.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802697,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802697,,Training;Solid modeling;Three-dimensional displays;Navigation;Semantics;Habitats;Contrastive learning;Search problems;Object recognition;Intelligent robots,,2.0,,27,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://emergentsystemlabstudent.github.io/MultiViewRetrieve,https://github.com/EmergentSystemLabStudent/MultiViewRetrieve
198,Learning Concept-Based Causal Transition and Symbolic Reasoning for Visual Planning,Y. Qian; P. Yu; Y. N. Wu; Y. Su; W. Wang; L. Fan,"State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI); Department of Statistics, University of California, Los Angeles (UCLA); Department of Statistics, University of California, Los Angeles (UCLA); State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI); State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI); State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI)",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,380,387,"Visual planning simulates how humans make decisions to achieve desired goals in the form of searching for visual causal transitions between an initial visual state and a final visual goal state. It has become increasingly important in egocentric vision with its advantages in guiding agents to perform daily tasks in complex environments. In this paper, we propose an interpretable and generalizable visual planning framework consisting of i) a novel Substitution-based Concept Learner (SCL) that abstracts visual inputs into disentangled concept representations, ii) symbol abstraction and reasoning that performs task planning via the learned symbols, and iii) a Visual Causal Transition model (ViCT) that grounds visual causal transitions to semantically similar real-world actions. Given an initial state, we perform goal-conditioned visual planning with a symbolic reasoning method fueled by the learned representations and causal transitions to reach the goal state. To verify the effectiveness of the proposed model, we collect a large-scale visual planning dataset based on AI2-THOR, dubbed as CCTP. Extensive experiments on this challenging dataset demonstrate the superior performance of our method in visual planning. Empirically, we show that our framework can generalize to unseen task trajectories, unseen object categories, and real-world data. Further details of this work are provided at https://fqyqc.github.io/ConTranPlan/.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802420,National Science and Technology Major Project; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802420,,Visualization;Disentangled representation learning;Symbols;Cognition;Planning;Trajectory;Intelligent robots,,,,41,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://fqyqc.github.io/ConTranPlan,https://github.com/FQYQC/ConTranPlan/edit/master/index.md
199,Efficient Trajectory Forecasting and Generation with Conditional Flow Matching,S. Ye; M. C. Gombolay,"Institute of Robotics and Intelligent Machines (IRIM), Georgia Institute of Technology, Atlanta, GA, USA; Institute of Robotics and Intelligent Machines (IRIM), Georgia Institute of Technology, Atlanta, GA, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,2816,2823,"Trajectory prediction and generation are crucial for autonomous robots in dynamic environments. While prior research has typically focused on either prediction or generation, our approach unifies these tasks to provide a versatile framework and achieve state-of-the-art performance. While diffusion models excel in trajectory generation, their iterative sampling process is computationally intensive, hindering robotic systems’ dynamic capabilities. We introduce Trajectory Conditional Flow Matching (T-CFM), a novel approach using flow matching techniques to learn a solver time-varying vector field for efficient, fast trajectory generation. T-CFM demonstrates effectiveness in adversarial tracking, real-world aircraft trajectory forecasting, and long-horizon planning, outperforming state-of-the-art baselines with 35% higher predictive accuracy and 142% improved planning performance. Crucially, T-CFM achieves up to 100× speed-up compared to diffusion models without sacrificing accuracy, enabling real-time decision making in robotics. Codebase: https://github.com/CORE-Robotics-Lab/TCFM",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802208,National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802208,,Accuracy;Diffusion models;Vectors;Real-time systems;Trajectory;Planning;Iterative methods;Forecasting;Aircraft;Intelligent robots,,,,27,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/CORE-Robotics-Lab/TCFM,https://github.com/CORE-Robotics-Lab/TCFM
200,MDHA: Multi-Scale Deformable Transformer with Hybrid Anchors for Multi-View 3D Object Detection,M. Adeline; J. Y. Loo; V. M. Baskaran,"School of Information Technology, Monash University, Malaysia; School of Information Technology, Monash University, Malaysia; School of Information Technology, Monash University, Malaysia",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,2668,2675,"Multi-view 3D object detection is a crucial component of autonomous driving systems. Contemporary query-based methods primarily depend either on dataset-specific initialization of 3D anchors, introducing bias, or utilize dense attention mechanisms, which are computationally inefficient and unscalable. To overcome these issues, we present MDHA, a novel sparse query-based framework, which constructs adaptive 3D output proposals using hybrid anchors from multi-view, multi-scale image input. Fixed 2D anchors are combined with depth predictions to form 2.5D anchors, which are projected to obtain 3D proposals. To ensure high efficiency, our proposed Anchor Encoder performs sparse refinement and selects the top-k anchors and features. Moreover, while existing multi-view attention mechanisms rely on projecting reference points to multiple images, our novel Circular Deformable Attention mechanism only projects to a single image but allows reference points to seamlessly attend to adjacent images, improving efficiency without compromising on performance. On the nuScenes val set, it achieves 46.4% mAP and 55.0% NDS with a ResNet101 backbone. MDHA significantly outperforms the baseline where anchor proposals are modelled as learnable embeddings. Code is available at https://github.com/NaomiEX/MDHA.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802056,Ministry of Higher Education; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802056,,Three-dimensional displays;Attention mechanisms;Object detection;Transformers;Hybrid power systems;Decoding;Proposals;Iterative methods;Iterative decoding;Intelligent robots,,,,30,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/NaomiEX/MDHA,https://github.com/NaomiEX/MDHA
201,Towards Dynamic and Small Objects Refinement for Unsupervised Domain Adaptative Nighttime Semantic Segmentation,J. Pan; S. Li; Y. Chen; J. Zhu; L. Wang,"AI Thrust, HKUST(GZ), Guangdong, China; AI Thrust, HKUST(GZ), Guangdong, China; AI Thrust, HKUST(GZ), Guangdong, China; AI Thrust, HKUST(GZ), Guangdong, China; AI Thrust, HKUST(GZ), Guangdong, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,2720,2727,"Nighttime semantic segmentation plays a crucial role in practical applications, such as autonomous driving, where it frequently encounters difficulties caused by inadequate illumination conditions and the absence of well-annotated datasets. Moreover, semantic segmentation models trained on daytime datasets often face difficulties in generalizing effectively to nighttime conditions. Unsupervised domain adaptation (UDA) has shown the potential to address the challenges and achieved remarkable results for nighttime semantic segmentation. However, existing methods still face limitations in 1) their reliance on style transfer or relighting models, which struggle to generalize to complex nighttime environments, and 2) their ignorance of dynamic and small objects like vehicles and poles, which are difficult to be directly learned from other domains. This paper proposes a novel UDA method that refines both label and feature levels for dynamic and small objects for nighttime semantic segmentation. First, we propose a dynamic and small object refinement module to complement the knowledge of dynamic and small objects from the source domain to target the nighttime domain. These dynamic and small objects are normally context-inconsistent in under-exposed conditions. Then, we design a feature prototype alignment module to reduce the domain gap by deploying contrastive learning between features and prototypes of the same class from different domains, while re-weighting the categories of dynamic and small objects. Extensive experiments on three benchmark datasets demonstrate that our method outperforms prior arts by a large margin for nighttime segmentation. Project page: https://rorisis.github.io/DSRNSS/.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801389,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801389,,Adaptation models;Heavily-tailed distribution;Semantic segmentation;Prototypes;Lighting;Focusing;Contrastive learning;Vehicle dynamics;Faces;Intelligent robots,,,,56,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://rorisis.github.io/DSRNSS,https://github.com/Rorisis/DSRNSS_codes
202,Exposing the Unseen: Exposure Time Emulation for Offline Benchmarking of Vision Algorithms,O. Gamache; J. -M. Fortin; M. Boxan; M. Vaidis; F. Pomerleau; P. Giguère,"Northern Robotics Laboratory, Université Laval, Québec City, Canada; NA; NA; NA; NA; Northern Robotics Laboratory, Université Laval, Québec City, Canada",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,11110,11117,"Visual Odometry (VO) is one of the fundamental tasks in computer vision for robotics. However, its performance is deeply affected by High Dynamic Range (HDR) scenes, omnipresent outdoor. While new Automatic-Exposure (AE) approaches to mitigate this have appeared, their comparison in a reproducible manner is problematic. This stems from the fact that the behavior of AE depends on the environment, and it affects the image acquisition process. Consequently, AE has traditionally only been benchmarked in an online manner, making the experiments non-reproducible. To solve this, we propose a new methodology based on an emulator that can generate images at any exposure time. It leverages BorealHDR, a unique multi-exposure stereo dataset collected over 10 km, on 55 trajectories with challenging illumination conditions. Moreover, it includes lidar-inertial-based global maps with pose estimation for each image frame as well as Global Navigation Satellite System (GNSS) data, for comparison. We show that using these images acquired at different exposure times, we can emulate realistic images, keeping a Root-Mean-Square Error (RMSE) below 1.78 % compared to ground truth images. To demonstrate the practicality of our approach for offline benchmarking, we compared three state-of-the-art AE algorithms on key elements of Visual Simultaneous Localization And Mapping (VSLAM) pipeline, against four baselines. Consequently, reproducible evaluation of AE is now possible, speeding up the development of future approaches. Our code and dataset are available on-line at this link: https://github.com/norlab-ulaval/BorealHDR",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10803057,Nature; Natural Sciences and Engineering Research Council of Canada; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10803057,,Global navigation satellite system;Visualization;Simultaneous localization and mapping;Pose estimation;Pipelines;Lighting;Benchmark testing;Trajectory;Intelligent robots;Visual odometry,,1.0,,35,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/norlab-ulaval/BorealHDR,https://github.com/norlab-ulaval/BorealHDR
203,Fast Spatial Reasoning of Implicit 3D Maps through Explicit Near-Far Sampling Range Prediction,C. Min; S. Cha; C. Won; J. Lim,"Department of Computer Science, Brown University; Advanced Robotics Lab., CTO Division, LG Electronics; MultiplEYE Co. Ltd; MultiplEYE Co. Ltd",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,5040,5047,"3D mapping is critical for many robotics applications, such as autonomous navigation and object manipulation. Recently, deep implicit mapping approaches have received much attention for their compactness and ability to represent fine-grained details. However, without explicit guidance, such implicit representations are often cumbersome for searching the full range on the rays to find the object surfaces. As a result, several approaches, including hierarchical sampling, occupancy grids, and zero-level set baking, have been proposed to improve sampling where costly forward passes of the neural network should be performed. However, hierarchical sampling is still suboptimal in that it requires uniform coarse samples. Discrete occupancy grids of Instant NGP and zero-level sets of various baking methods are less suitable for large and noisy real scenes.In this paper, we present a novel framework for adaptively predicting the near-far range for sampling the query positions of the deep implicit map. For this purpose, the truncated signed distance grid for the map is pre-constructed and used to provide hints for near-far prediction during rendering. In addition, our recovery algorithm automatically detects failed near-far predictions and recovers only those rays by directly using the implicit map. We conduct extensive experiments on a synthetic dataset, a public real dataset, and a real dataset captured by our multi-camera robot system. The experimental results show that our algorithm achieves the same rendering quality with surprisingly fewer samples compared to the existing methods, which means that the robot can reason about the image and depth properties of the scene much faster. Finally, a thorough analysis of the sample distribution along the rays is provided to give a better understanding of our method’s strong efficiency, adaptability, and robustness. https://chaerinmin.github.io/TSDF-sampling/",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802100,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802100,,Three-dimensional displays;Neural networks;Rendering (computer graphics);Prediction algorithms;Search problems;Robustness;Cognition;Noise measurement;Intelligent robots;Synthetic data,,,,26,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://chaerinmin.github.io/TSDF-sampling,https://github.com/ChaerinMin/TSDF-sampling
204,SWCF-Net: Similarity-weighted Convolution and Local-global Fusion for Efficient Large-scale Point Cloud Semantic Segmentation,Z. Lin; L. He; H. Yang; X. Sun; G. Zhang; W. Chen; Y. Guan; H. Zhang,"Biomimetic and Intelligent Robotics Lab (BIRL), School of Electromechanical Engineering, Guangdong University of Technology, Guangzhou, China; Department of Electronic and Electrical Engineering, Southern University of Science and Technology, China; Meituan Technology Co., Ltd, Shenzhen, China; Meituan Technology Co., Ltd, Shenzhen, China; Meituan Technology Co., Ltd, Shenzhen, China; Biomimetic and Intelligent Robotics Lab (BIRL), School of Electromechanical Engineering, Guangdong University of Technology, Guangzhou, China; Biomimetic and Intelligent Robotics Lab (BIRL), School of Electromechanical Engineering, Guangdong University of Technology, Guangzhou, China; Department of Electronic and Electrical Engineering, Southern University of Science and Technology, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,2602,2609,"Large-scale point cloud consists of a multitude of individual objects, thereby encompassing rich structural and underlying semantic contextual information, resulting in a challenging problem in efficiently segmenting a point cloud. Most existing researches mainly focus on capturing intricate local features without giving due consideration to global ones, thus failing to leverage semantic context. In this paper, we propose a Similarity-Weighted Convolution and local-global Fusion Network, named SWCF-Net, which takes into account both local and global features. We propose a Similarity-Weighted Convolution (SWConv) to effectively extract local features, where similarity weights are incorporated into the convolution operation to enhance the generalization capabilities. Then, we employ a downsampling operation on the K and V channels within the attention module, thereby reducing the quadratic complexity to linear, enabling Transformer to deal with large-scale point cloud. At last, orthogonal components are extracted in the global features and then aggregated with local features, thereby eliminating redundant information between local and global features and consequently promoting efficiency. We evaluate SWCF-Net on large-scale outdoor datasets SemanticKITTI and Toronto3D. Our experimental results demonstrate the effectiveness of the proposed network. Our method achieves a competitive result with less computational cost, and is able to handle large-scale point clouds efficiently. The code is available at https://github.com/Sylva-Lin/SWCF-Net.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801684,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801684,,Point cloud compression;Convolution;Semantic segmentation;Semantics;Feature extraction;Transformers;Computational efficiency;Complexity theory;Data mining;Intelligent robots,,,,34,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/Sylva-Lin/SWCF-Net,https://github.com/Sylva-Lin/SWCF-Net
205,RATE: Real-time Asynchronous Feature Tracking with Event Cameras,M. Ikura; C. L. Gentil; M. G. Müller; F. Schuler; A. Yamashita; W. Stürzl,"Department of Human and Engineered Environmental Studies, Graduate School of Frontier Sciences, The University of Tokyo, Kashiwa, Japan; Robotics Institute at the University of Technology Sydney, NSW, Australia; Institute of Robotics and Mechatronics, German Aerospace Center (DLR), Weßling, Germany; Institute of Robotics and Mechatronics, German Aerospace Center (DLR), Weßling, Germany; Department of Human and Engineered Environmental Studies, Graduate School of Frontier Sciences, The University of Tokyo, Kashiwa, Japan; Institute of Robotics and Mechatronics, German Aerospace Center (DLR), Weßling, Germany",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,11662,11669,"Vision-based self-localization is a crucial technology for enabling autonomous robot navigation in GPS-deprived environments. However, standard frame cameras are subject to motion blur and suffer from a limited dynamic range. This research focuses on efficient feature tracking for self-localization by using event-based cameras. Such cameras do not provide regular snapshots of the environment but asynchronously collect events that correspond to a small delta of illumination in each pixel independently, thus addressing the issue of motion blur during fast motion and high dynamic range. Specifically, we propose a continuous real-time asynchronous event-based feature tracking pipeline, named RATE. This pipeline integrates (i) a corner detector node utilizing a time slice of the Surface of Active Events to initialize trackers continuously, along with (ii) a tracker node with a proposed ""tracking manager"", consisting of a grid-based distributor to reduce redundant trackers and to remove feature tracks of poor quality. Evaluations using public datasets reveal that our method maintains a stable number of tracked features, and performs real-time tracking efficiently while maintaining or even improving tracking accuracy compared to state-of-the-art event-only tracking methods. Our ROS implementation is released as open-source: https://github.com/mikihiroikura/RATE",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802050,Australian Research Council; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802050,,Accuracy;Tracking;Event detection;Pipelines;Robot vision systems;Lighting;Detectors;Cameras;Real-time systems;Standards,,,,29,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/mikihiroikura/RATE,https://github.com/mikihiroikura/RATE
206,SOAR: Simultaneous Exploration and Photographing with Heterogeneous UAVs for Fast Autonomous Reconstruction,M. Zhang; C. Feng; Z. Li; G. Zheng; Y. Luo; Z. Wang; J. Zhou; S. Shen; B. Zhou,"School of Artificial Intelligence, Sun Yat-Sen University, Zhuhai, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; School of Artificial Intelligence, Sun Yat-Sen University, Zhuhai, China; School of Artificial Intelligence, Sun Yat-Sen University, Zhuhai, China; School of Artificial Intelligence, Sun Yat-Sen University, Zhuhai, China; Department of Automation, North China Electric Power University, Baoding, China; The Hong Kong University of Science and Technology, Guangzhou, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; School of Artificial Intelligence, Sun Yat-Sen University, Zhuhai, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,10975,10982,"Unmanned Aerial Vehicles (UAVs) have gained significant popularity in scene reconstruction. This paper presents SOAR, a LiDAR-Visual heterogeneous multi-UAV system specifically designed for fast autonomous reconstruction of complex environments. Our system comprises a LiDAR-equipped explorer with a large field-of-view (FoV), alongside photographers equipped with cameras. To ensure rapid acquisition of the scene’s surface geometry, we employ a surface frontier-based exploration strategy for the explorer. As the surface is progressively explored, we identify the uncovered areas and generate viewpoints incrementally. These viewpoints are then assigned to photographers through solving a Consistent Multiple Depot Multiple Traveling Salesman Problem (Consistent-MDMTSP), which optimizes scanning efficiency while ensuring task consistency. Finally, photographers utilize the assigned viewpoints to determine optimal coverage paths for acquiring images. We present extensive benchmarks in the realistic simulator, which validates the performance of SOAR compared with classical and state-of-the-art methods. For more details, please see our project page at sysu-star.github.io/SOAR.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801474,Research and Development; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801474,,Geometry;Surface reconstruction;Systems architecture;Traveling salesman problems;Benchmark testing;Cameras;Autonomous aerial vehicles;Image reconstruction;Intelligent robots,,,,22,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://sysu-star.github.io/SOAR,https://github.com/SYSU-STAR/SOAR
207,EasyHeC++: Fully Automatic Hand-Eye Calibration with Pretrained Image Models,Z. Hong; K. Zheng; L. Chen,Zhejiang University; Tsinghua University; Zhejiang University,2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,816,823,"Hand-eye calibration plays a fundamental role in robotics by directly influencing the efficiency of critical operations such as manipulation and grasping. In this work, we present a novel framework, EasyHeC++, designed for fully automatic hand-eye calibration. In contrast to previous methods that necessitate manual calibration, specialized markers, or the training of arm-specific neural networks, our approach is the first system that enables accurate calibration of any robot arm in a marker-free, training-free, and fully automatic manner. Our approach employs a two-step process. First, we initialize the camera pose using a sampling or feature-matching-based method with the aid of pretrained image models. Subsequently, we perform pose optimization through differentiable rendering. Extensive experiments demonstrate the system’s superior accuracy in both synthetic and real-world datasets across various robot arms and camera settings. Project page: https://ootts.github.io/easyhec_plus/.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801359,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801359,,Training;Accuracy;Robot vision systems;Grasping;Manipulators;Cameras;Rendering (computer graphics);Calibration;Robots;Optimization,,,,44,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://ootts.github.io/easyhec_plus,https://github.com/ootts/EasyHeC
208,Learning from Spatio-temporal Correlation for Semi-Supervised LiDAR Semantic Segmentation,S. Lee; H. Lee; H. Shim,Yonsei University; KAIST; KAIST,2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,14095,14102,"We address the challenges of the semi-supervised LiDAR segmentation (SSLS) problem, particularly in low-budget scenarios. The two main issues in low-budget SSLS are the poor-quality pseudo-labels for unlabeled data, and the performance drops due to the significant imbalance between ground-truth and pseudo-labels. This imbalance leads to a vicious training cycle. To overcome these challenges, we leverage the spatio-temporal prior by recognizing the substantial overlap between temporally adjacent LiDAR scans. We propose a proximity-based label estimation, which generates highly accurate pseudo-labels for unlabeled data by utilizing semantic consistency with adjacent labeled data. Additionally, we enhance this method by progressively expanding the pseudo-labels from the nearest unlabeled scans, which helps significantly reduce errors linked to dynamic classes. Additionally, we employ a dual-branch structure to mitigate performance degradation caused by data imbalance. Experimental results demonstrate remarkable performance in low-budget settings (i.e., ≤ 5%) and meaningful improvements in normal budget settings (i.e., 5 – 50%). Finally, our method has achieved new state-of-the-art results on SemanticKITTI and nuScenes in semi-supervised LiDAR segmentation. With only 5% labeled data, it offers competitive results against fully-supervised counterparts. Moreover, it surpasses the performance of the previous state-of-the-art at 100% labeled data (75.2%) using only 20% of labeled data (76.0%) on nuScenes. The code is available on https://github.com/halbielee/PLE.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801453,National Research Foundation of Korea; Samsung; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801453,,Training;Degradation;Laser radar;Correlation;Codes;Semantic segmentation;Semantics;Estimation;Focusing;Intelligent robots,,,,69,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/halbielee/PLE,https://github.com/halbielee/PLE
209,FusionTrack: An Online 3D Multi-object Tracking Framework Based on Camera-LiDAR Fusion,W. Zeng; J. Fan; X. Tian; H. Chu; B. Gao,"School of Automotive Studies, Tongji University, Shanghai, China; School of Automotive Studies, Tongji University, Shanghai, China; School of Automotive Studies, Tongji University, Shanghai, China; School of Automotive Studies, Tongji University, Shanghai, China; School of Automotive Studies, Tongji University, Shanghai, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,4920,4925,"3D multi-object tracking is an important component of the perception module in autonomous driving systems. Due to the limitations of a single sensor, tracking methods based on either LiDAR or cameras always have certain deficiencies. Fusion-based tracking methods have received increasing attention. However, existing fusion-based tracking methods often underutilize image information, ignore the respective effects of appearance information and 2D detection results, and lack further analysis on the simultaneous use of both. This paper proposes a novel camera-LiDAR fusion tracking framework that primarily relies on the motion model using 3D objects. It fully leverages the appearance information and 2D detection results simultaneously from images and introduces three modules to reduce the number of false positive samples, false negative samples and ID switches, respectively. Besides, the entire tracking process does not require global processing and achieves online tracking. The proposed method achieves competitive results on the KITTI tracking dataset with 78.50% HOTA. Compared with EagerMOT using the same 3D and 2D detectors, the HOTA metric improved by 4.11%. Code is available on https://github.com/zengwz/FusionTrack.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802805,Nature; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802805,,Measurement;Solid modeling;Three-dimensional displays;Laser radar;Codes;Tracking;Detectors;Robot sensing systems;Cameras;Intelligent robots,,,,28,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/zengwz/FusionTrack,https://github.com/zengwz/FusionTrack
210,DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping,K. Yılmaz; M. Nießner; A. Kornilova; A. Artemov,"Technical University of Munich, Garching, Germany; Technical University of Munich, Garching, Germany; Skolkovo Institute of Science and Technology, Moscow, Russia; Technical University of Munich, Garching, Germany",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,12279,12286,"Recently, significant progress has been achieved in sensing real large-scale outdoor 3D environments, particularly by using modern acquisition equipment such as LiDAR sensors. Unfortunately, they are fundamentally limited in their ability to produce dense, complete 3D scenes. To address this issue, recent learning-based methods integrate neural implicit representations and optimizable feature grids to approximate surfaces of 3D scenes. However, naively fitting samples along raw LiDAR rays leads to noisy 3D mapping results due to the nature of sparse, conflicting LiDAR measurements. Instead, in this work we depart from fitting LiDAR data exactly, instead letting the network optimize a non-metric monotonic implicit field defined in 3D space. To fit our field, we design a learning system integrating a monotonicity loss that enables optimizing neural monotonic fields and leverages recent progress in large-scale 3D mapping. Our algorithm achieves high-quality dense 3D mapping performance as cap¬tured by multiple quantitative and perceptual measures and visual results obtained for Mai City, Newer College, and KITTI benchmarks. The code of our approach is publicly available at https://github.com/artonson/deepmif.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801519,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801519,3D mapping;neural implicit representations,Learning systems;Visualization;Solid modeling;Three-dimensional displays;Laser radar;Urban areas;Fitting;Benchmark testing;Surface fitting;Sensors,,,,53,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/artonson/deepmif,https://github.com/artonson/deepmif
211,SSL-RGB2IR: Semi-supervised RGB-to-IR Image-to-Image Translation for Enhancing Visual Task Training in Semantic Segmentation and Object Detection,A. Sikdar; Q. Saadiyean; P. Anand; S. Sundaram,"Robert Bosch Centre for Cyber-Physical Systems, Indian Institute of Science, Bangalore; Department of Aerospace Engineering, Indian Institute of Science, Bangalore; School of Computer Science and Engineering, Vellore Institute of Technology, Vellore; Department of Aerospace Engineering, Indian Institute of Science, Bangalore",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,5017,5023,"The scarcity of annotated infrared (IR) image datasets limits deep learning networks from achieving performances comparable to those achieved with RGB data. To address this, we introduce a novel semi-supervised RGB-to-IR Image-to-Image Translation model (SSL-RGB2IR) that generates synthetic IR data from RGB images. Our model effectively preserves the IR characteristics in the generated images from both synthetic and real-world data. Compared to existing image-to-image translation techniques, training models on this generated IR data significantly improves performance in downstream tasks like segmentation and detection. Notably, in sim-to-real transfer, the segmentation model trained on SSL-RGB2IR generated IR images outperforms baselines and other Image-to-Image (I2I) models. Furthermore, for real-world applications utilizing EO/IR fusion images, this approach solves the well-known challenge of co-registering EO and IR images, which often have inherent misalignment’s due to differing sensor characteristics. Our code is available at https://github.com/prahlad-anand/ssl-rgb2ir https://github.com/prahlad-anand/ssl-rgb2ir.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802815,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802815,,Training;Deep learning;YOLO;Visualization;Translation;Supervised learning;Superresolution;Semisupervised learning;Data models;Unsupervised learning,,,,30,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/prahlad-anand/ssl-rgb2ir,https://github.com/prahlad-anand/ssl-rgb2ir
212,"VIRUS-NeRF - Vision, InfraRed and UltraSonic based Neural Radiance Fields",N. Schmid; C. Von Einem; C. Cadena; R. Siegwart; L. Hruby; F. Tschopp,"Autonomous Systems Lab, ETH Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Switzerland; Filics GmbH, Munich, Germany; Voliro AG, Zurich, Switzerland",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,12536,12543,"Autonomous mobile robots are an increasingly integral part of modern factory and warehouse operations. Obstacle detection, avoidance and path planning are critical safety-relevant tasks, which are often solved using expensive LiDAR sensors and depth cameras. We propose to use cost-effective low-resolution ranging sensors, such as ultrasonic and infrared time-of-flight sensors by developing VIRUS-NeRF - Vision, InfraRed, and UltraSonic based Neural Radiance Fields.Building upon Instant Neural Graphics Primitives with a Multiresolution Hash Encoding (Instant-NGP), VIRUS-NeRF incorporates depth measurements from ultrasonic and infrared sensors and utilizes them to update the occupancy grid used for ray marching. Experimental evaluation in 2D demonstrates that VIRUS-NeRF achieves comparable mapping performance to LiDAR point clouds regarding coverage. Notably, in small environments, its accuracy aligns with that of LiDAR measurements, while in larger ones, it is bounded by the utilized ultrasonic sensors. An in-depth ablation study reveals that adding ultrasonic and infrared sensors is highly effective when dealing with sparse data and low view variation. Further, the proposed occupancy grid of VIRUS-NeRF improves the mapping capabilities and increases the training speed by 46% compared to Instant-NGP. Overall, VIRUS-NeRF presents a promising approach for cost-effective local mapping in mobile robotics, with potential applications in safety and navigation tasks. The code can be found at https://github.com/ethz-asl/virus_nerf.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802852,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802852,,Training;Laser radar;Accuracy;Ultrasonic variables measurement;Neural radiance field;Infrared sensors;Cameras;Acoustics;Sensor systems;Intelligent sensors,,,,44,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/ethz-asl/virus_nerf,https://github.com/ethz-asl/virus_nerf
213,Stick Roller: Precise In-hand Stick Rolling with a Sample-Efficient Tactile Model,Y. Du; P. Zhou; M. Y. Wang; W. Lian; Y. She,"The Hong Kong University of Science and Technology, Hong Kong; Purdue University, West Lafayette, IN, USA; HKUST Shenzhen-Hong Kong Collaborative Innovation Research Institute, Shenzhen, China; University of Chinese Academy of Sciences, China; Purdue University, West Lafayette, IN, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,2312,2318,"In-hand manipulation is challenging in robotics due to the intricate contact dynamics and high degrees of control freedom. Precise manipulation with high accuracy often requires tactile perception, which adds further complexity to the system. Despite the challenges in perception and control, the rolling stick problem is an essential and practical motion primitive with many demanding industrial applications. This work aims to learn the high-resolution tactile dynamics of the rolling stick. Specifically, we try manipulating a small stick using the Allegro hand equipped with the Digit vision-based tactile sensor. The learning framework includes an action filtering module, tactile perception module, and learning with uncertainty module, all designed to operate in low data regimes. With only 2.3% amount of data and 5.7% model complexity of previous similar work, our learned contact dynamics model achieves better grasp stability, sub-millimeter precision, and promising zero-shot generalizability across novel objects. The proposed framework demonstrates the potential for precise in-hand manipulation with tactile feedback on real hardware. The project source code is available at: https://github.com/duyipai/Allegro_Digit. A video presentation is available here.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802003,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802003,,Hands;Accuracy;Uncertainty;Dynamics;Tactile sensors;Data models;Hardware;Stability analysis;Complexity theory;Manipulator dynamics,,,,28,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/duyipai/Allegro_Digit,https://github.com/duyipai/Allegro_Digit
214,OmniNxt: A Fully Open-source and Compact Aerial Robot with Omnidirectional Visual Perception,P. Liu; C. Feng; Y. Xu; Y. Ning; H. Xu; S. Shen,"Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; Division of Emerging Interdisciplinary Areas, The Hong Kong University of Science and Technology, Hong Kong, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,10605,10612,"Adopting omnidirectional Field of View (FoV) cameras in aerial robots vastly improves perception ability, significantly advancing aerial robotics’s capabilities in inspection, reconstruction, and rescue tasks. However, such sensors also elevate system complexity, e.g., hardware design, and corresponding algorithm, which limits researchers from utilizing aerial robots with omnidirectional FoV in their research. To bridge this gap, we propose OmniNxt, a fully open-source aerial robotics platform with omnidirectional perception. We design a high-performance flight controller Nxt-FC and a multi-fisheye camera set for OmniNxt. Meanwhile, the compatible software is carefully devised, which empowers OmniNxt to achieve accurate localization and real-time dense mapping with limited computation resource occupancy. We conducted extensive real-world experiments to validate the superior performance of OmniNxt in practical applications. All the hardware and software are open-access at3, and we provide docker images of each crucial module in the proposed system. Project page: https://hkust-aerial-robotics.github.io/OmniNxt.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802134,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802134,,Robot vision systems;Software algorithms;Autonomous aerial vehicles;Cameras;Software;Hardware;Sensor systems;Real-time systems;Sensors;Visual perception,,,,34,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://hkust-aerial-robotics.github.io/OmniNxt,https://github.com/HKUST-Aerial-Robotics/OmniNxt
215,Enhancing Online Road Network Perception and Reasoning with Standard Definition Maps,H. Zhang; D. Paz; Y. Guo; A. Das; X. Huang; K. Haug; H. I. Christensen; L. Ren,"Contextual Robotics Institute, UC San Diego, La Jolla, CA, USA; Bosch North America and Bosch Center for AI (BCAI), Sunnyvale, CA, USA; Bosch North America and Bosch Center for AI (BCAI), Sunnyvale, CA, USA; Bosch North America and Bosch Center for AI (BCAI), Sunnyvale, CA, USA; Bosch North America and Bosch Center for AI (BCAI), Sunnyvale, CA, USA; Robert Bosch GmbH, Stuttgart-Baihingen Bade-Wuerttemberg, Germany; Contextual Robotics Institute, UC San Diego, La Jolla, CA, USA; Bosch North America and Bosch Center for AI (BCAI), Sunnyvale, CA, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,1086,1093,"Autonomous driving for urban and highway driving applications often requires High Definition (HD) maps to generate a navigation plan. Nevertheless, various challenges arise when generating and maintaining HD maps at scale. While recent online mapping methods have started to emerge, their performance especially for longer ranges is limited by heavy occlusion in dynamic environments. With these considerations in mind, our work focuses on leveraging lightweight and scalable priors–Standard Definition (SD) maps–in the development of online vectorized HD map representations. We first examine the integration of prototypical rasterized SD map representations into various online mapping architectures. Furthermore, to identify lightweight strategies, we extend the OpenLane-V2 dataset with OpenStreetMaps and evaluate the benefits of graphical SD map representations. A key finding from designing SD map integration components is that SD map encoders are model agnostic and can be quickly adapted to new architectures that utilize bird’s eye view (BEV) encoders. Our results show that making use of SD maps as priors for the online mapping task can significantly speed up convergence and boost the performance of the online centerline perception task by 30% (mAP). Furthermore, we show that the introduction of the SD maps leads to a reduction of the number of parameters in the perception and reasoning task by leveraging SD map graphs while improving the overall performance. Project Page: https://henryzhangzhy.github.io/sdhdmap/.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801423,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801423,,Adaptation models;Navigation;Scalability;Roads;Cognition;Autonomous vehicles;Standards;Intelligent robots;Convergence,,,,39,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://henryzhangzhy.github.io/sdhdmap,https://github.com/eliahuhorwitz/Academic-project-page-template
216,Diffusion-PbD: Generalizable Robot Programming by Demonstration with Diffusion Features,M. Murray; E. Su; M. Cakmak,"Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA, USA; Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA, USA; Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,5168,5175,"Programming by Demonstration (PbD) is an intuitive technique for programming robot manipulation skills by demonstrating the desired behavior. However, most existing approaches either require extensive demonstrations or fail to generalize beyond their initial demonstration conditions. We introduce Diffusion-PbD, a novel approach to PbD that enables users to synthesize generalizable robot manipulation skills from a single demonstration by utilizing the representations captured by pre-trained visual foundation models. At demonstration time, hand and object detection priors are used to extract waypoints from the human demonstrations anchored to reference points in the scene. At execution time, features from pre-trained diffusion models are leveraged to identify corresponding reference points in new observations. We validate this approach through a series of real-world robot experiments, showing that Diffusion-PbD is applicable to a wide range of manipulation tasks and has strong ability to generalize to unseen objects, camera viewpoints, and scenes. Code and supplementary videos can be found at https://diffusion-pbd.github.io",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802625,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802625,,Visualization;Foundation models;Dynamics;Robot vision systems;Feature extraction;Robustness;Trajectory;Robots;Videos;Robot programming,,,,56,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
217,Enhancing Nighttime UAV Tracking with Light Distribution Suppression,L. Yao; C. Fu; Y. Wang; H. Zuo; K. Lu,"School of Mechanical Engineering, Tongji University, Shanghai, China; School of Mechanical Engineering, Tongji University, Shanghai, China; School of Mechanical Engineering, Tongji University, Shanghai, China; Department of Computer Science, University of Hong Kong, Hong Kong, China; School of Mechanical Engineering, Tongji University, Shanghai, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,5902,5909,"Visual object tracking has boosted extensive intelligent applications for unmanned aerial vehicles (UAVs). However, the state-of-the-art (SOTA) enhancers for nighttime UAV tracking always neglect the uneven light distribution in low-light images, inevitably leading to excessive enhancement in scenarios with complex illumination. To address these issues, this work proposes a novel enhancer, i.e., LDEnhancer, enhancing nighttime UAV tracking with light distribution suppression. Specifically, a novel image content refinement module is developed to decompose the light distribution information and image content information in the feature space, allowing for the targeted enhancement of the image content information. Then this work designs a new light distribution generation module to capture light distribution effectively. The features with light distribution information and image content information are fed into the different parameter estimation modules, respectively, for the parameter map prediction. Finally, leveraging two parameter maps, an innovative interweave iteration adjustment is proposed for the collaborative pixel-wise adjustment of low-light images. Additionally, a challenging nighttime UAV tracking dataset with uneven light distribution, namely NAT2024-2, is constructed to provide a comprehensive evaluation, which contains 40 challenging sequences with over 74K frames in total. Experimental results on the authoritative UAV benchmarks and the proposed NAT2024-2 demonstrate that LDEnhancer outperforms other SOTA low-light enhancers for nighttime UAV tracking. Furthermore, real-world tests on a typical UAV platform with an NVIDIA Orin NX confirm the practicality and efficiency of LDEnhancer. The code is available at https: //github.com/vision4robotics/LDEnhancer.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802568,National Natural Science Foundation of China; Natural Science Foundation of Shanghai; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802568,,Visualization;Parameter estimation;Target tracking;Codes;Collaboration;Lighting;Benchmark testing;Autonomous aerial vehicles;Object tracking;Intelligent robots,,,,34,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/vision4robotics/LDEnhancer,https://github.com/vision4robotics/LDEnhancer
218,Synergizing Morphological Computation and Generative Design: Automatic Synthesis of Tendon-Driven Grippers,K. D. Zharkov; M. E. Chaikovskii; Y. V. Osipov; R. Alshaowa; I. I. Borisov; S. A. Kolyubin,"Biomechatronics and Energy-Efficient Robotics Lab, ITMO University, Saint Petersburg, Russia; Biomechatronics and Energy-Efficient Robotics Lab, ITMO University, Saint Petersburg, Russia; Biomechatronics and Energy-Efficient Robotics Lab, ITMO University, Saint Petersburg, Russia; Biomechatronics and Energy-Efficient Robotics Lab, ITMO University, Saint Petersburg, Russia; Biomechatronics and Energy-Efficient Robotics Lab, ITMO University, Saint Petersburg, Russia; Biomechatronics and Energy-Efficient Robotics Lab, ITMO University, Saint Petersburg, Russia",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,10576,10581,"The design process of robotic systems is a complex journey that involves multiple phases. Throughout this process, the aim is to tackle various criteria simultaneously, even though they often contradict each other. The ultimate goal is to uncover the optimal solution that resolves these conflicting factors. Within this paper we propose a design methodology to generate linkage mechanisms for robots with morphological computation. We use a graph grammar and a heuristic search algorithm to create robot mechanism graphs that are converted into simulation models for testing the design output. To verify the design methodology we have applied it to a relatively simple quasi-static problem of object grasping. Designing a fully actuated gripper may seem simple, but we found a way to automatically design an underactuated tendon-driven gripper that can grasp a wide range of objects. This is possible because of its structure, not because of sophisticated planning or learning. To test the applicability of the proposed method in real engineering practice, we used it to create physical prototypes. Simulation results together with results of testing of physical prototypes are given at the end of the paper. The framework is open source and the link to GitHub is given in the paper.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801489,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801489,,Simulation;Pipelines;Prototypes;Kinematics;Software;Planning;Grippers;Testing;Tendons;Software development management,,,,19,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
219,Task and Domain Adaptive Reinforcement Learning for Robot Control,Y. T. Liu; N. Singh; A. Ahmad,"Max Planck Institute for Intelligent Systems, Tübingen, Germany; IIT, Kharagpur, West Bengal, India; University of Stuttgart, Stuttgart, Germany",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,656,663,"Deep reinforcement learning (DRL) has shown remarkable success in simulation domains, yet its application in designing robot controllers remains limited, due to its singletask orientation and insufficient adaptability to environmental changes. To overcome these limitations, we present a novel adaptive agent that leverages transfer learning techniques to dynamically adapt policy in response to different tasks and environmental conditions. The approach is validated through the blimp control challenge, where multitasking capabilities and environmental adaptability are essential. The agent is trained using a custom, highly parallelized simulator built on IsaacGym. We perform zero-shot transfer to fly the blimp in the real world to solve various tasks. We share our code at https://github.com/robot-perception-group/adaptive_agent/.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801963,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801963,,Codes;Adaptive systems;Transfer learning;Robot control;Multitasking;Deep reinforcement learning;Intelligent robots,,,,33,EU,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/robot-perception-group/adaptive_agent,https://github.com/robot-perception-group/adaptive_agent
220,Towards Accurate And Robust Dynamics and Reward Modeling for Model-Based Offline Inverse Reinforcement Learning,G. Zhang; Y. Yan,"Department of Computer Science, University of Illinois, Chicago, IL, USA; Department of Computer Science, University of Illinois, Chicago, IL, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,611,618,"This paper enhances model-based offline inverse reinforcement learning (IRL) by refining conservative Markov decision process (MDP) frameworks, traditionally employing uncertainty penalties to deter exploitation in uncertain areas. Existing methods, dependent on neural network ensembles to model MDP dynamics and quantify uncertainty through ensemble prediction heuristics, face limitations: they presume Gaussian-distributed state transitions, leading to simplified environmental representations. Additionally, ensemble modeling often results in high variance, indicating potential overfitting and a lack of generalizability. Moreover, the heuristic reliance for uncertainty quantification struggles to fully grasp environmental complexities, offering an incomplete foundation for informed decisions. Maintaining multiple models also demands substantial computational resources. Addressing these shortcomings, we propose leveraging score-based diffusion generative models for dynamic modeling. This method significantly broadens the scope of representable target distributions, surpassing Gaussian constraints. It not only improves the accuracy of transition modeling but also roots uncertainty quantification in diffusion models’ theoretical underpinnings, enabling more precise and dependable reward regularization. We further innovate by incorporating a transition stability regularizer (TSR) into the reward estimation. This novel element embeds stability into the reward learning process, diminishing the influence of transition variability and promoting more consistent policy optimization. Our empirical studies on diverse Mujoco robotic control tasks demonstrate that our diffusion-based methodology not only furnishes more accurate transition estimations but also surpasses conventional ensemble approaches in policy effectiveness. The addition of the TSR marks a distinctive advancement in offline IRL by enhancing the reward and policy learning efficacy. Code: https://github.com/GabrielZH/doc-irl.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802715,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802715,,Uncertainty;Accuracy;Computational modeling;Refining;Estimation;Stochastic processes;Reinforcement learning;Predictive models;Stability analysis;Space exploration,,,,34,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/GabrielZH/doc-irl,https://github.com/GabrielZH/doc-irl
221,Unsupervised 3D Part Decomposition via Leveraged Gaussian Splatting,J. G. Choy; G. Cha; H. Kee; S. Oh,"Sequor Robotics, Seoul, Korea; NAVER Cloud, Seongnam, Korea; Department of Electrical and Computer Engineering and ASRI, Seoul National University, Seoul, Korea; Department of Electrical and Computer Engineering and ASRI, Seoul National University, Seoul, Korea",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,2647,2652,"We propose a novel unsupervised method for motion-based 3D part decomposition of articulated objects using a single monocular video of a dynamic scene. In contrast to existing unsupervised methods relying on optical flow or tracking techniques, our approach addresses this problem without additional information by leveraging Gaussian splatting techniques. We generate a series of Gaussians from a monocular video and analyze their relationships to decompose the dynamic scene into motion-based parts. To decompose dynamic scenes consisting of articulated objects, we design an articulated deformation field suitable for the movement of articulated objects. And to effectively understand the relationships of Gaussians of different shapes, we propose a 3D reconstruction loss using 3D occupied voxel maps generated from the Gaussians. Experimental results demonstrate that our method outperforms existing approaches in terms of 3D part decomposition for articulated objects. More demos and code are available at https://choonsik93.github.io/artnerf/.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802165,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802165,,Training;Geometry;Three-dimensional displays;Codes;Tracking;Shape;Deformation;Dynamics;Optical flow;Intelligent robots,,,,32,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://choonsik93.github.io/artnerf,https://github.com/choonsik93/artnerf/blob/master/static/pdfs/2024_iros_u3dpd.pdf
222,ManiFoundation Model for General-Purpose Robotic Manipulation of Contact Synthesis with Arbitrary Objects and Robots,Z. Xu; C. Gao; Z. Liu; G. Yang; C. Tie; H. Zheng; H. Zhou; W. Peng; D. Wang; T. Hu; T. Chen; Z. Yu; L. Shao,"Department of Computer Science, National University of Singapore; Department of Computer Science, National University of Singapore; Tsinghua Shenzhen International Graduate School, Tsinghua University; Department of Computer Science, National University of Singapore; School of Electronics Engineering and Computer Science, Peking University; Department of Mathematics, National University of Singapore; Department of Mechanical Engineering, National University of Singapore; Department of Computer Science, National University of Singapore; Department of Computer Science, National University of Singapore; Department of Computer Science, National University of Singapore; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Division of Emerging Interdisciplinary Areas, Hongkong University of Science and Technology; Department of Computer Science, National University of Singapore",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,10905,10912,"To substantially enhance robot intelligence, there is a pressing need to develop a large model that enables general-purpose robots to proficiently undertake a broad spectrum of manipulation tasks, akin to the versatile task-planning ability exhibited by LLMs. The vast diversity in objects, robots, and manipulation tasks presents huge challenges. Our work introduces a comprehensive framework to develop a foundation model for general robotic manipulation that formalizes a manipulation task as contact synthesis. Specifically, our model takes as input object and robot manipulator point clouds, object physical attributes, target motions, and manipulation region masks. It outputs contact points on the object and associated contact forces or post-contact motions for robots to achieve the desired manipulation task. We perform extensive experiments both in the simulation and real-world settings, manipulating articulated rigid objects, rigid objects, and deformable objects that vary in dimensionality, ranging from one-dimensional objects like ropes to two-dimensional objects like cloth and extending to three-dimensional objects such as plasticine. Our model achieves average success rates of around 90%. Supplementary materials and videos are available on our project website at https://manifoundationmodel.github.io/.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801782,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801782,,Point cloud compression;Foundation models;Pressing;Manipulators;Distance measurement;Robots;Intelligent robots;Videos,,,,47,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
223,PICaSo: A Collaborative Robotics System for Inpainting on Physical Canvas using Marker and Eraser,S. Nasrat; J. -B. Yi; M. Jo; S. -j. Yi,"Faculty of Electrical Engineering, Pusan National University, Busan, South Korea; Faculty of Electrical Engineering, Pusan National University, Busan, South Korea; Faculty of Electrical Engineering, Pusan National University, Busan, South Korea; Faculty of Electrical Engineering, Pusan National University, Busan, South Korea",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,4739,4746,"Robotics collaborative drawing involves the inter-action between humans and robots to create of visual art using a variety of tools and materials, serving various functions such as communication, narration, and emotional representation. A creative technique within the human natural drawing process is known as inpainting, which involves reconstructing or editing elements in a drawing. This paper introduces PICaSo (Physical Inpainting on Canvas Solution), a robotic drawing system that enables multiple users to collaboratively create artwork on a canvas by integrating the inpainting process. PICaSo utilizes a fine-tuned text-to-image model to interpret natural language prompts into artistic renderings on canvas. Users guide the process by simple descriptive text and specifying desired drawing placement, empowering the robotic arm to autonomously translate these instructions into physical artworks. Our system’s innovation lies in its effective translation of digital inpainting processes into physical actions. By leveraging our erasing capability that enables selective removal of specific parts on the canvas without impacting neighboring areas, facilitating the creation of sequential drawings. This paper comprehensively outlines the capabilities of the proposed system, explores potential applications across various domains, and addresses technical challenges encountered during its development. Project website: shadynasrat.github.io/PICaSo",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801850,Korean National Police Agency; Korea Institute for Advancement of Technology; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801850,,Visualization;Technological innovation;Translation;Collaboration;Text to image;Rendering (computer graphics);Manipulators;Robots;Intelligent robots;Painting,,,,34,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://shadynasrat.github.io/PICaSo,https://github.com/shadynasrat/picaso
224,SceneSense: Diffusion Models for 3D Occupancy Synthesis from Partial Observation,A. Reed; B. Crowe; D. Albin; L. Achey; B. Hayes; C. Heckman,"Department of Computer Science, Intelligent Robotics Laboratory, University of Colorado Boulder; Department of Computer Science, Intelligent Robotics Laboratory, University of Colorado Boulder; Department of Computer Science, Intelligent Robotics Laboratory, University of Colorado Boulder; Department of Computer Science, Intelligent Robotics Laboratory, University of Colorado Boulder; Department of Computer Science, Intelligent Robotics Laboratory, University of Colorado Boulder; Department of Computer Science, Intelligent Robotics Laboratory, University of Colorado Boulder",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,7383,7390,"When exploring new areas, robotic systems generally exclusively plan and execute controls over geometry that has been directly measured. This planning paradigm can lead to unintuitive exploration or replanning latency when entering areas that were previous obstructed from view. To address this we present SceneSense, a real-time 3D diffusion model for synthesizing 3D occupancy information from partial observations that effectively predicts these occluded or out of view geometries for use in future planning and control frameworks. SceneSense uses a running occupancy map and a single RGB-D camera to generate predicted geometry around the platform at runtime, even when the geometry is occluded or out of view. Our architecture ensures that SceneSense never overwrites observed free or occupied space. By preserving the integrity of the observed map, SceneSense mitigates the risk of corrupting the observed space with generative predictions. While SceneSense is shown to operate well using a single RGB-D camera, the framework is flexible enough to extend to additional modalities. Unlike existing models that necessitate multiple views and offline scene synthesis, or are focused on filling gaps in observed data, our findings demonstrate that SceneSense is an effective approach to estimating unobserved local occupancy information at runtime. Local occupancy predictions from SceneSense are shown to better represent the ground truth occupancy distribution during the test exploration trajectories than the running occupancy map. The source code can be found on our website: https://arpg.github.io/scenesense/",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802589,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802589,,Geometry;Three-dimensional displays;Runtime;Source coding;Diffusion models;Cameras;Real-time systems;Planning;Trajectory;Intelligent robots,,,,44,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://arpg.github.io/scenesense,https://github.com/arpg/SceneSense
225,Multi-Robot Active Graph Exploration with Reduced Pose-SLAM Uncertainty via Submodular Optimization,R. Bai; S. Yuan; H. Guo; P. Yin; W. -Y. Yau; L. Xie,"School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Institute for Infocomm Research (I2R), Agency for Science, Technology and Research (A*STAR), Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Institute for Infocomm Research (I2R), Agency for Science, Technology and Research (A*STAR), Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,10229,10236,"This paper considers the multi-robot active graph exploration problem, where robots need to collaboratively cover a graph environment while maintaining reliable pose estimation in collaborative Simultaneous Localization and Mapping (SLAM). Considering both objectives presents challenges for multi-robot pathfinding, as it involves the expensive covariance propagation for SLAM uncertainty evaluation, especially when considering various combinations of robots’ paths. To reduce the computational complexity, we propose an efficient two-stage strategy where exploration paths are first generated for quick coverage, and then enhanced by adding informative loop-closing actions along the paths for reliable pose estimation. We formulate the latter problem as a non-monotone submodular maximization problem by relating SLAM uncertainty with pose graph topology, which (1) facilitates a more efficient evaluation of SLAM uncertainty than covariance inference, and (2) allows the employment of approximation algorithms in submodular optimization to provide suboptimality guarantees. We further introduce ordering heuristics to improve the objective values while preserving the optimality bound. Simulation experiments over randomly generated graph environments verify the effectiveness of our methods to achieve quick coverage and enhanced pose graph reliability, and benchmark the performance of the approximation algorithms and the greedy-based algorithm in the loop edge selection problem. Our implementations will be open-source at https://github.com/bairuofei/CGE.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802691,National Research Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802691,,Simultaneous localization and mapping;Uncertainty;Pose estimation;Employment;Approximation algorithms;Inference algorithms;Topology;Reliability;Optimization;Intelligent robots,,,,35,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/bairuofei/CGE,https://github.com/bairuofei/CGE
226,A Generic Trajectory Planning Method for Constrained All-Wheel-Steering Robots,R. Xin; H. Liu; Y. Chen; J. Cheng; S. Wang; J. Ma; M. Liu,"Division of Emerging Interdisciplinary Areas, The Hong Kong University of Science and Technology, Hong Kong, SAR China; Division of Emerging Interdisciplinary Areas, The Hong Kong University of Science and Technology, Hong Kong, SAR China; Division of Emerging Interdisciplinary Areas, The Hong Kong University of Science and Technology, Hong Kong, SAR China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, SAR China; Division of Emerging Interdisciplinary Areas, The Hong Kong University of Science and Technology, Hong Kong, SAR China; Robotics and Autonomous Systems Thrust, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; Robotics and Autonomous Systems Thrust, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,3084,3091,"This paper presents a generic trajectory planning method for wheeled robots with fixed steering axes while the steering angle of each wheel is constrained. In the existing literatures, All-Wheel-Steering (AWS) robots, incorporating modes such as rotation-free translation maneuvers, in-situ rotational maneuvers, and proportional steering, exhibit inefficient performance due to time-consuming mode switches. This inefficiency arises from wheel rotation constraints and inter-wheel cooperation requirements. The direct application of a holonomic moving strategy can lead to significant slip angles or even structural failure. Additionally, the limited steering range of AWS wheeled robots exacerbates non-linearity characteristics, thereby complicating control processes. To address these challenges, we developed a novel planning method termed Constrained AWS (C-AWS), which integrates second-order discrete search with predictive control techniques. Experimental results demonstrate that our method adeptly generates feasible and smooth trajectories for C-AWS while adhering to steering angle constraints. Code and video can be found at https://github.com/Rex-sys-hk/AWSPlanning.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801878,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801878,,Translation;Trajectory planning;Wheels;Transportation;Kinematics;Real-time systems;Trajectory;Mobile robots;Robots;Predictive control,,,,33,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/Rex-sys-hk/AWSPlanning,https://github.com/Rex-sys-hk/AWSPlanning
227,DaDiff: Domain-aware Diffusion Model for Nighttime UAV Tracking,H. Zuo; C. Fu; G. Zheng; L. Yao; K. Lu; J. Pan,"Department of Computer Science, University of Hong Kong, Centre for Transformative Garment Production, Hong Kong, China; School of Mechanical Engineering, Tongji University, Shanghai, China; Department of Computer Science, University of Hong Kong, Centre for Transformative Garment Production, Hong Kong, China; School of Mechanical Engineering, Tongji University, Shanghai, China; School of Mechanical Engineering, Tongji University, Shanghai, China; Department of Computer Science, University of Hong Kong, Centre for Transformative Garment Production, Hong Kong, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,11094,11101,"Domain adaptation is an inspiring solution to the misalignment issue of day/night image features for nighttime UAV tracking. However, the one-step adaptation paradigm is inadequate in addressing the prevalent difficulties posed by low-resolution (LR) objects when viewed from the UAVs at night, owing to the blurry edge contour and limited detail information. Moreover, these approaches struggle to perceive LR objects disturbed by nighttime noise. To address these challenges, this work proposes a novel progressive alignment paradigm, named domain-aware diffusion model (DaDiff), aligning nighttime LR object features to the daytime by virtue of progressive and stable generations. The proposed DaDiff includes an alignment encoder to enhance the detail information of nighttime LR objects, a tracking-oriented layer designed to achieve close collaboration with tracking tasks, and a successive distribution discriminator presented to distinguish different feature distributions at each diffusion timestep successively. Furthermore, an elaborate nighttime UAV tracking benchmark is constructed for LR objects, namely NUT-LR, consisting of 100 annotated sequences. Exhaustive experiments have demonstrated the robustness and feature alignment ability of the proposed DaDiff. The source code and video demo are available at https://github.com/vision4robotics/DaDiff.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802294,National Natural Science Foundation of China; Natural Science Foundation of Shanghai; Innovation and Technology Commission; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802294,,Visualization;Source coding;Noise;Collaboration;Lighting;Benchmark testing;Autonomous aerial vehicles;Diffusion models;Stability analysis;Robustness,,,,42,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/vision4robotics/DaDiff,https://github.com/vision4robotics/DaDiff
228,Automating ROS2 Security Policies Extraction through Static Analysis,G. Zanatta; G. Caiazza; P. Ferrara; L. Negrini; R. White,"Ca’ Foscari University of Venice, Italy; Ca’ Foscari University of Venice, Italy; Ca’ Foscari University of Venice, Italy; Ca’ Foscari University of Venice, Italy; White Robotics",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,3627,3634,"Cybersecurity in mission-critical robotic applications is a necessity to scale deployments securely. ROS2 builds upon DDS-Security specs in ROS Client Library (RCL) to implement its security features. Utilizing SROS2, developers have access to a set of utilities to help set up security in a way RCL can use. Through SROS2, security deployment is eased for developers. However, while access control is handled by DDS and consequently based on the SROS2-generated permission artifacts, the necessary authorization policies are manually generated by developers. This requires an entire system exercise to be sampled via live extraction and, per each node, list all the necessary Topics, Services, and Actions, which is a daunting and laborious process. Developers first have to generate tests. Then, they obtain a ’snapshot’ of the system for each test. Later, these snapshots must be collected and grouped into a policy by a minimum set of rules. All this procedure is quite error-prone. This paper introduces LiSA4ROS2, a tool for automatically extract the ROS2 computational graph via static analysis to derive a minimal correct configuration for ROS2 security policies. Our approach relies on the abstract interpretation theory to statically overapproximate all possible executions to extract a minimal and complete configuration per node. We evaluate our approach with minimal examples covering all the main communication patterns in ROS2 tutorials and all publicly available real-world ROS2 Python systems extracted from GitHub. The results of the minimal examples show that LiSA4ROS2 precisely supports all the main communication patterns. The extensive evaluation underlines that our prototype implementation of the analysis in LiSA4ROS2 is already able to precisely analyze 66% of existing repositories, automatically producing detailed computational graphs and access policies. All the results of the analysis, as well as a Docker artifact to reproduce them, are publicly available.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802507,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802507,,Mission critical systems;Prototypes;Static analysis;Tutorials;Reflection;Libraries;Security;Research and development;Intelligent robots;Software development management,,,,32,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
229,DAP: Diffusion-based Affordance Prediction for Multi-modality Storage,H. Chang; K. Boyalakuntla; Y. Liu; X. Zhang; L. Schramm; A. Boularias,"Department of Computer Science, Rutgers University, New Brunswick, USA; Department of Computer Science, Rutgers University, New Brunswick, USA; Department of Computer Science, Rutgers University, New Brunswick, USA; Department of Computer Science, Rutgers University, New Brunswick, USA; Department of Computer Science, Rutgers University, New Brunswick, USA; Department of Computer Science, Rutgers University, New Brunswick, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,9476,9481,"Solving storage problems—where objects must be accurately placed into containers with precise orientations and positions—presents a distinct challenge that extends beyond traditional rearrangement tasks. These challenges are primarily due to the need for fine-grained 6D manipulation and the inherent multi-modality of solution spaces, where multiple viable goal configurations exist for the same storage container. We present a novel Diffusion-based Affordance Prediction (DAP) pipeline for the multi-modal object storage problem. DAP leverages a two-step approach, initially identifying a placeable region on the container and then precisely computing the relative pose between the object and that region. Existing methods either struggle with multi-modality issues or computation-intensive training. Our experiments demonstrate DAP’s superior performance and training efficiency over the current state-of-the-art RPDiff, achieving remarkable results on the RPDiff benchmark. Additionally, our experiments showcase DAP’s data efficiency in real-world applications, an advancement over existing simulation-driven approaches. Our contribution fills a gap in robotic manipulation research by offering a solution that is both computationally efficient and capable of handling real-world variability. Code and supplementary material can be found at: https://github.com/changhaonan/DPS.git.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802575,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802575,,Training;Codes;Affordances;Pipelines;Pose estimation;Containers;Prediction algorithms;Diffusion models;Computational efficiency;Intelligent robots,,,,29,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/changhaonan/DPS,https://github.com/changhaonan/DPS
230,DTG : Diffusion-based Trajectory Generation for Mapless Global Navigation,J. Liang; A. Payandeh; D. Song; X. Xiao; D. Manocha,"University of Maryland, College Park; George Mason University; University of Maryland, College Park; George Mason University; University of Maryland, College Park",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,5340,5347,"We present a novel end-to-end diffusion-based trajectory generation method, DTG, for mapless global navigation in challenging outdoor scenarios with occlusions and unstructured off-road features like grass, buildings, bushes, etc. Given a distant goal, our approach computes a trajectory that satisfies the following goals: (1) minimize the travel distance to the goal; (2) maximize the traversability by choosing paths that do not lie in undesirable areas. Specifically, we present a novel Conditional RNN(CRNN) for diffusion models to efficiently generate trajectories. Furthermore, we propose an adaptive training method that ensures that the diffusion model generates more traversable trajectories. We evaluate our methods in various outdoor scenes and compare the performance with other global navigation algorithms on a Husky robot. In practice, we observe at least a 15% improvement in traveling distance and around a 7% improvement in traversability. Video and Code: https://github.com/jingGM/DTG.git.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802055,Arm; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802055,,Training;Codes;Navigation;Buildings;Diffusion models;Trajectory;Intelligent robots,,,,65,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/jingGM/DTG,https://github.com/jingGM/DTG
231,NeuSurfEmb: A Complete Pipeline for Dense Correspondence-based 6D Object Pose Estimation without CAD Models,F. Milano; J. J. Chung; H. Blum; R. Siegwart; L. Ott,"ETH, Zurich, Switzerland; The University of Queensland, Australia; ETH, Zurich, Switzerland; ETH, Zurich, Switzerland; ETH, Zurich, Switzerland",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,8882,8889,"State-of-the-art approaches for 6D object pose estimation assume the availability of CAD models and require the user to manually set up physically-based rendering (PBR) pipelines for synthetic training data generation. Both factors limit the application of these methods in real-world scenarios. In this work, we present a pipeline that does not require CAD models and allows training a state-of-the-art pose estimator requiring only a small set of real images as input. Our method is based on a NeuS2 [1] object representation, that we learn through a semi-automated procedure based on Structure-from-Motion (SfM) and object-agnostic segmentation. We exploit the novel-view synthesis ability of NeuS2 and simple cut-and-paste augmentation to automatically generate photorealistic object renderings, which we use to train the correspondence-based SurfEmb [2] pose estimator. We evaluate our method on the LINEMOD-Occlusion dataset, extensively studying the impact of its individual components and showing competitive performance with respect to approaches based on CAD models and PBR data. We additionally demonstrate the ease of use and effectiveness of our pipeline on self-collected real-world objects, showing that our method outperforms state-of-the-art CAD-model-free approaches, with better accuracy and robustness to mild occlusions. To allow the robotics community to benefit from this system, we will publicly release it at https://www.github.com/ethz-asl/neusurfemb.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801399,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801399,,Training;Solid modeling;Accuracy;Pipelines;Pose estimation;Training data;Rendering (computer graphics);Data models;Robustness;Synthetic data,,,,47,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/ethz-asl/neusurfemb,https://github.com/ethz-asl/neusurfemb
232,BEV2PR: BEV-Enhanced Visual Place Recognition with Structural Cues,F. Ge; Y. Zhang; S. Shen; W. Hu; Y. Wang; J. Gao,"State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), CASIA; State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), CASIA; State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), CASIA; State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), CASIA; State Key Laboratory of Industrial Control Technology and Institute of Cyber-Systems and Control, Zhejiang University; State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), CASIA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,13274,13281,"In this paper, we propose a new image-based visual place recognition (VPR) framework by exploiting the structural cues in bird’s-eye view (BEV) from a single monocular camera. The motivation arises from two key observations about place recognition methods based on both appearance and structure: 1) For the methods relying on LiDAR sensors, the integration of LiDAR in robotic systems has led to increased expenses, while the alignment of data between different sensors is also a major challenge. 2) Other image-/camera-based methods, involving integrating RGB images and their derived variants (e.g., pseudo depth images, pseudo 3D point clouds), exhibit several limitations, such as the failure to effectively exploit the explicit spatial relationships between different objects. To tackle the above issues, we design a new BEV-enhanced VPR framework, namely BEV2PR, generating a composite descriptor with both visual cues and spatial awareness based on a single camera. The key points lie in: 1) We use BEV features as an explicit source of structural knowledge in constructing global features. 2) The lower layers of the pretrained backbone from BEV generation are shared for visual and structural streams in VPR, facilitating the learning of fine-grained local features in the visual stream. 3) The complementary visual and structural features can jointly enhance VPR performance. Our BEV2PR framework enables consistent performance improvements over several popular aggregation modules for RGB global features. The experiments on our collected VPR-NuScenes dataset demonstrate an absolute gain of 2.47% on Recall@1 for the strong Conv-AP baseline to achieve the best performance in our setting, and notably, a 18.06% gain on the hard set. The code and dataset will be available at https://github.com/FudongGe/BEV2PR.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802401,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802401,,Image sensors;Visualization;Laser radar;Three-dimensional displays;Robot vision systems;Cameras;Sensor systems;Sensors;Streams;Visual place recognition,,,,31,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/FudongGe/BEV2PR,https://github.com/FudongGe/BEV2PR
233,PGA: Personalizing Grasping Agents with Single Human-Robot Interaction,J. Kim; G. -C. Kang; J. Kim; S. Yang; M. Jung; B. -T. Zhang,"AI Institute, Seoul National University; AI Institute, Seoul National University; AI Institute, Seoul National University; Division of Engineering Science, University of Toronto; AI Institute, Seoul National University; AI Institute, Seoul National University",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,9277,9284,"Language-Conditioned Robotic Grasping (LCRG) aims to develop robots that comprehend and grasp objects based on natural language instructions. While the ability to understand personal objects like my wallet facilitates more natural interaction with human users, current LCRG systems only allow generic language instructions, e.g., the black-colored wallet next to the laptop. To this end, we introduce a task scenario GraspMine alongside a novel dataset aimed at pinpointing and grasping personal objects given personal indicators via learning from a single human-robot interaction, rather than a large labeled dataset. Our proposed method, Personalized Grasping Agent (PGA), addresses GraspMine by leveraging the unlabeled image data of the user’s environment, called Reminiscence. Specifically, PGA acquires personal object information by a user presenting a personal object with its associated indicator, followed by PGA inspecting the object by rotating it. Based on the acquired information, PGA pseudo-labels objects in the Reminiscence by our proposed label propagation algorithm. Harnessing the information acquired from the interactions and the pseudo-labeled objects in the Reminiscence, PGA adapts the object grounding model to grasp personal objects. This results in significant eﬃciency while previous LCRG systems rely on resource-intensive human annotations—necessitating hundreds of labeled data to learn my wallet. Moreover, PGA outperforms baseline methods across all metrics and even shows comparable performance compared to the fully-supervised method, which learns from 9k annotated data samples. We further validate PGA’s real-world applicability by employing a physical robot to execute GrsapMine. Code and data are publicly available at https://github.com/JHKim-snu/PGA.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801347,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801347,,Measurement;Portable computers;Codes;Grounding;Natural languages;Human-robot interaction;Grasping;Robots;Intelligent robots;Electronics packaging,,,,39,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/JHKim-snu/PGA,https://github.com/JHKim-snu/PGA
234,Pre-training on Synthetic Driving Data for Trajectory Prediction,Y. Li; S. Z. Zhao; C. Xu; C. Tang; C. Li; M. Ding; M. Tomizuka; W. Zhan,"University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,5910,5917,"Accumulating substantial volumes of real-world driving data proves pivotal in the realm of trajectory forecasting for autonomous driving. Given the heavy reliance of current trajectory forecasting models on data-driven methodologies, we aim to tackle the challenge of learning general trajectory forecasting representations under limited data availability. We propose a pipeline-level solution to mitigate the issue of data scarcity in trajectory forecasting. The solution is composed of two parts: firstly, we adopt HD map augmentation and trajectory synthesis for generating driving data, and then we learn representations by pre-training on them. Specifically, we apply vector transformations to reshape the maps, and then employ a rule-based model to generate trajectories on both original and augmented scenes; thus enlarging the driving data without collecting additional real ones. To foster the learning of general representations within this augmented dataset, we comprehensively explore the different pre-training strategies, including extending the concept of a Masked AutoEncoder (MAE) for trajectory forecasting. Without bells and whistles, our proposed pipeline-level solution is general, simple, yet effective: we conduct extensive experiments to demonstrate the effectiveness of our data expansion and pre-training strategies, which outperform the baseline prediction model by large margins, e.g. 5.04%, 3.84% and 8.30% in terms of MR6, minADE6 and minFDE6. The pre-training dataset and the codes for pre-training and fine-tuning are released at https://github.com/yhli123/Pretraining_on_Synthetic_Driving_Data_for_Trajectory_Prediction.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802492,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802492,,Codes;Pipelines;Predictive models;Data collection;Data models;Vectors;Trajectory;Forecasting;Intelligent robots;Synthetic data,,,,40,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/yhli123/Pretraining_on_Synthetic_Driving_Data_for_Trajectory_Prediction,https://github.com/yhli123/Pretraining_on_Synthetic_Driving_Data_for_Trajectory_Prediction
235,LAC-Net: Linear-Fusion Attention-Guided Convolutional Network for Accurate Robotic Grasping Under the Occlusion,J. Zhang; Y. Gu; J. Gao; H. Lin; Q. Sun; X. Sun; X. Xue; Y. Fu,"School of Data Science, Fudan University, China; School of Data Science, Fudan University, China; School of Data Science, Fudan University, China; School of Data Science, Fudan University, China; School of Statistics and Information, Shanghai University of International Business and Economics, China; School of Data Science, Fudan University, China; School of Data Science, Fudan University, China; School of Data Science, Fudan University, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,10059,10065,"This paper addresses the challenge of perceiving complete object shapes through visual perception. While prior studies have demonstrated encouraging outcomes in segmenting the visible parts of objects within a scene, amodal segmentation, in particular, has the potential to allow robots to infer the occluded parts of objects. To this end, this paper introduces a new framework that explores amodal segmentation for robotic grasping in cluttered scenes, thus greatly enhancing robotic grasping abilities. Initially, we use a conventional segmentation algorithm to detect the visible segments of the target object, which provides shape priors for completing the full object mask. Particularly, to explore how to utilize semantic features from RGB images and geometric information from depth images, we propose a Linear-fusion Attention-guided Convolutional Network (LAC-Net). LAC-Net utilizes the linear-fusion strategy to effectively fuse this cross-modal data, and then uses the prior visible mask as attention map to guide the network to focus on target feature locations for further complete mask recovery. Using the amodal mask of the target object provides advantages in selecting more accurate and robust grasp points compared to relying solely on the visible segments. The results on different datasets show that our method achieves state-of-the-art performance. Furthermore, the robot experiments validate the feasibility and robustness of this method in the real world. Our code and demonstrations are available on the project page: https://jrryzh.github.io/LAC-Net.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802054,Shanghai Platform for Neuromorphic and AI Chip; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802054,,Accuracy;Shape;Semantics;Grasping;Feature extraction;Cleaning;Robustness;Convolutional neural networks;Robots;Visual perception,,,,47,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://jrryzh.github.io/LAC-Net,https://github.com/jrryzh/LAC-Net-repo/
236,Explicit Interaction for Fusion-Based Place Recognition,J. Xu; J. Ma; Q. Wu; Z. Zhou; Y. Wang; X. Chen; W. Yu; L. Pei,Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Beijing Institute of Technology; Zhejiang University; National University of Defense Technology; Shanghai Jiao Tong University; Shanghai Jiao Tong University,2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,3318,3325,"Fusion-based place recognition is an emerging technique jointly utilizing multi-modal perception data, to recognize previously visited places in GPS-denied scenarios for robots and autonomous vehicles. Recent fusion-based place recognition methods combine multi-modal features in implicit manners. While achieving remarkable results, they do not explicitly consider what the individual modality affords in the fusion system. Therefore, the benefit of multi-modal feature fusion may not be fully explored. In this paper, we propose a novel fusion-based network, dubbed EINet, to achieve explicit interaction of the two modalities. EINet uses LiDAR ranges to supervise more robust vision features for long time spans, and simultaneously uses camera RGB data to improve the discrimination of LiDAR point clouds. In addition, we develop a new benchmark for the place recognition task based on the nuScenes dataset. To establish this benchmark for future research with comprehensive comparisons, we introduce both supervised and self-supervised training schemes alongside evaluation protocols. We conduct extensive experiments on the proposed benchmark, and the experimental results show that our EINet exhibits better recognition performance as well as solid generalization ability compared to the state-of-the-art fusion-based place recognition approaches. Our open-source code and benchmark are released at: https://github.com/BIT-XJY/EINet.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802665,Nature; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802665,,Training;Point cloud compression;Laser radar;Protocols;Benchmark testing;Cameras;Solids;Rendering (computer graphics);Autonomous vehicles;Intelligent robots,,,,49,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/BIT-XJY/EINet,https://github.com/BIT-XJY/EINet
237,Quadruped robot traversing 3D complex environments with limited perception,Y. Cheng; H. Liu; G. Pan; H. Liu; L. Ye,"Tsinghua University, Beijing, China; University of Michigan, Ann Arbor, MI, USA; Tsinghua University, Beijing, China; Tsinghua University, Beijing, China; Shanghai University, Shanghai, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,9074,9081,"Traversing 3-D complex environments has always been a significant challenge for legged locomotion. Existing methods typically rely on external sensors such as vision and lidar to preemptively react to obstacles by acquiring environmental information. However, in scenarios like nighttime or dense forests, external sensors often fail to function properly, necessitating robots to rely on proprioceptive sensors to perceive diverse obstacles in the environment and respond promptly. This task is undeniably challenging. Our research finds that methods based on collision detection can enhance a robot’s perception of environmental obstacles. In this work, we propose an end-to-end learning-based quadruped robot motion controller that relies solely on proprioceptive sensing. This controller can accurately detect, localize, and agilely respond to collisions in unknown and complex 3D environments, thereby improving the robot’s traversability in complex environments. We demonstrate in both simulation and real-world experiments that our method enables quadruped robots to successfully traverse challenging obstacles in various complex environments. The videos and appendix can be found at Quad-Traverse-Go2.github.io",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801507,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801507,,Training;Three-dimensional displays;Accuracy;Computational modeling;Propioception;Sensors;Quadrupedal robots;Collision avoidance;Robots;Videos,,,,35,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
238,Safety-First Tracker: A Trajectory Planning Framework for Omnidirectional Robot Tracking,Y. Lin; Y. Liu; P. Zhang; X. Chen; D. Wang; H. Lu,"Dalian University of Technology, Dalian, China; Dalian University of Technology, Dalian, China; Dalian University of Technology, Dalian, China; Dalian University of Technology, Dalian, China; Dalian University of Technology, Dalian, China; Dalian University of Technology, Dalian, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,5416,5423,"This paper introduces a Safety-First Tracker (SF-Tracker) designed for omnidirectional autonomous tracking robots. The position and orientation of omnidirectional robots are decoupled for stepwise planning to ensure trajectory safety and maintain target visibility. SF-Tracker puts the trajectory safety in the first place. First, a collision-free and occlusion-free reference path is efficiently initialized by constructing a directed weighted graph. By building upon this path, safe trajectory optimization is implemented to ensure safe movement. Finally, an orientation planner is developed to achieve target visibility based on the safe trajectory. Extensive experimental evaluations in simulated environments and the real world demonstrate that the SF-Tracker outperforms state-of-the-art methods in terms trajectory safety and target visibility. Ablation experiments further demonstrate the significance of each step of the SF-Tracker. The source code and demonstration video can be found at https://github.com/Yue-0/SF-Tracker.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802592,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802592,,Target tracking;Trajectory planning;Source coding;Buildings;Safety;Planning;Collision avoidance;Trajectory optimization;Intelligent robots,,,,24,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/Yue-0/SF-Tracker,https://github.com/Yue-0/SF-Tracker
239,VIHE: Virtual In-Hand Eye Transformer for 3D Robotic Manipulation,W. Wang; Y. Lei; S. Jin; G. D. Hager; L. Zhang,"Department of Computer Science, Johns Hopkins University, Baltimore, USA; Baidu Research, Robotics and Autonomous Driving Lab (RAL), Sunnyvale, USA; Baidu Research, Robotics and Autonomous Driving Lab (RAL), Sunnyvale, USA; Department of Computer Science, Johns Hopkins University, Baltimore, USA; Baidu Research, Robotics and Autonomous Driving Lab (RAL), Sunnyvale, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,403,410,"In this work, we introduce the Virtual In-Hand Eye Transformer (VIHE), a novel method designed to enhance 3D manipulation capabilities through action-aware view rendering. VIHE autoregressively refines actions in multiple stages by conditioning on rendered views posed from action predictions in the earlier stages. These virtual in-hand views provide a strong inductive bias for effectively recognizing the correct pose for the hand, especially for challenging high-precision tasks such as peg insertion. On 18 manipulation tasks in RLBench simulated environments, VIHE achieves a new state-of-the-art, with a 12% absolute improvement, increasing from 65% to 77% over the existing state-of-the-art model using 100 demonstrations per task. In real-world scenarios, VIHE can learn manipulation tasks with just a handful of demonstrations, highlighting its practical utility. Videos and code implementation can be found at our project site: https://vihe-3d.github.io.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802366,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802366,,Point cloud compression;Hands;Three-dimensional displays;Codes;Robot vision systems;Rendering (computer graphics);Transformers;Cameras;Intelligent robots;Videos,,,,52,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
240,Progressive Query Refinement Framework for Bird's-Eye-View Semantic Segmentation from Surrounding Images,D. Choi; J. Kang; T. An; K. Ahn; K. Min,"Superintelligence Creative Research Laboratory, ETRI, South Korea; Superintelligence Creative Research Laboratory, ETRI, South Korea; Superintelligence Creative Research Laboratory, ETRI, South Korea; Superintelligence Creative Research Laboratory, ETRI, South Korea; Superintelligence Creative Research Laboratory, ETRI, South Korea",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,10807,10812,"Expressing images with Multi-Resolution (MR) features has been widely adopted in many computer vision tasks. In this paper, we introduce the MR concept into Bird's-Eye-View (BEV) semantic segmentation for autonomous driving. This introduction enhances our model's ability to capture both global and local characteristics of driving scenes through our proposed residual learning. Specifically, given a set of MR BEV query maps, the lowest resolution query map is initially updated using a View Transformation (VT) encoder. This updated query map is then upscaled and merged with a higher resolution query map to undergo further updates in a subsequent VT encoder. This process is repeated until the resolution of the updated query map reaches the target. Finally, the lowest resolution map is added to the target resolution to generate the final query map. During training, we enforce both the lowest and final query maps to align with the ground-truth BEV semantic map to help our model effectively capture the global and local characteristics. We also propose a visual feature interaction network that promotes interactions between features across images and across feature levels, thus highly contributing to the performance improvement. We evaluate our model on a large-scale real-world dataset. The experimental results show that our model outperforms the SOTA models in terms of IoU metric. Codes are available at https://github.com/d1024choi/ProgressiveQueryReneNet",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801357,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801357,,Training;Measurement;Visualization;Computer vision;Image resolution;Semantic segmentation;Semantics;Computer architecture;Predictive models;Intelligent robots,,,,33,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/d1024choi/ProgressiveQueryReneNet,https://github.com/d1024choi/ProgressiveQueryReneNet
241,CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking,N. Baumann; M. Baumgartner; E. Ghignone; J. Kühne; T. Fischer; Y. -H. Yang; M. Pollefeys; M. Magno,"Center for Project-Based Learning, D-ITET, ETH Zurich; Center for Project-Based Learning, D-ITET, ETH Zurich; Center for Project-Based Learning, D-ITET, ETH Zurich; Center for Project-Based Learning, D-ITET, ETH Zurich; Computer Vision and Geometry Group, D-INFK, ETH Zurich; Computer Vision and Geometry Group, D-INFK, ETH Zurich; Computer Vision and Geometry Group, D-INFK, ETH Zurich; Center for Project-Based Learning, D-ITET, ETH Zurich",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,4926,4933,"To enable self-driving vehicles accurate detection and tracking of surrounding objects is essential. While Light Detection and Ranging (LiDAR) sensors have set the benchmark for high-performance systems, the appeal of camera-only solutions lies in their cost-effectiveness. Notably, despite the prevalent use of Radio Detection and Ranging (RADAR) sensors in automotive systems, their potential in 3D detection and tracking has been largely disregarded due to data sparsity and measurement noise. As a recent development, the combination of RADARs and cameras is emerging as a promising solution. This paper presents Camera-RADAR 3D Detection and Tracking (CR3DT), a camera-RADAR fusion model for 3D object detection, and Multi-Object Tracking (MOT). Building upon the foundations of the State-of-the-Art (SotA) camera-only BEVDet architecture, CR3DT demonstrates substantial improvements in both detection and tracking capabilities, by incorporating the spatial and velocity information of the RADAR sensor. Experimental results demonstrate an absolute improvement in detection performance of 5.3% in mean Average Precision (mAP) and a 14.9% increase in Average Multi-Object Tracking Accuracy (AMOTA) on the nuScenes dataset when leveraging both modalities. CR3DT bridges the gap between high-performance and cost-effective perception systems in autonomous driving, by capitalizing on the ubiquitous presence of RADAR in automotive applications. The code is available at: https://github.com/ETH-PBL/CR3DT.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801848,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801848,,Solid modeling;Meteorological radar;Three-dimensional displays;Accuracy;Radar detection;Radar;Object detection;Radar tracking;Sensor systems;Sensors,,,,35,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/ETH-PBL/CR3DT,https://github.com/ETH-PBL/CR3DT
242,Optimal Robotic Assembly Sequence Planning (ORASP): A Sequential Decision-Making Approach,K. Nagpal; N. Mehr,"Department of Mechanical Engineering, University of California Berkeley, Berkeley, CA, USA; Faculty With the Department of Mechanical Engineering, University of California Berkeley, Berkeley, CA, USA",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,9847,9854,"The optimal robotic assembly planning problem entails determining the sequence of actions for a robot to feasibly assemble a product from its components which minimizes a given objective. This problem is made especially challenging as the number of potential sequences increase exponentially with respect to the number of parts in the assembly. Additionally, the optimal sequence must also consider and satisfy a selection of constraints such as attachment precedence or a maximum robot carry weight. Traditionally, robotic assembly planning problems have been solved using heuristics, but these methods are specific to a given robot or cost structure. In this paper, we propose to model robotic assembly planning as a decision-making problem which enables us to use tools from shortest path algorithms and reinforcement learning. We formulate assembly sequencing as a Markov Decision Process and use Dynamic Programming (DP) to find optimal assembly policies that far outperform the state-of-the-art in terms of speed. We further exploit the deterministic nature of assembly planning to introduce a class of optimal Graph Exploration Assembly Planners (GEAPs) and even propose our own ORASP Search Method. We further showcase how we can produce high-reward assembly plans for larger structures using our deep Reinforcement Learning (RL) method. We evaluate this method on large robotic assembly problems such as the assembly of the Hubble Space Telescope, the International Space Station, and the James Webb Space Telescope. We further discuss how our DP, GEAP, and RL methods are capable of finding optimal solutions under a variety of different objective functions and how we translate any form of precedence constraints to branch pruning and further improve performance. We have published our code at https://github.com/labicon/ORASP-Code.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802475,National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802475,,Robotic assembly;Sequential analysis;Uncertainty;Tracking;Search methods;Decision making;Telescopes;Deep reinforcement learning;Planning;Assembly,,,,48,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/labicon/ORASP-Code,https://github.com/labicon/ORASP-Code
243,HS3-Bench: A Benchmark and Strong Baseline for Hyperspectral Semantic Segmentation in Driving Scenarios,N. Theisen; R. Bartsch; D. Paulus; P. Neubert,"Institute of Computational Visualistics, University of Koblenz, Germany; Institute of Computational Visualistics, University of Koblenz, Germany; Institute of Computational Visualistics, University of Koblenz, Germany; Institute of Computational Visualistics, University of Koblenz, Germany",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,5895,5901,"Semantic segmentation is an essential step for many vision applications in order to understand a scene and the objects within. Recent progress in hyperspectral imaging technology enables the application in driving scenarios and the hope is that the devices perceptive abilities provide an advantage over RGB-cameras. Even though some datasets exist, there is no standard benchmark available to systematically measure progress on this task and evaluate the benefit of hyperspectral data. In this paper, we work towards closing this gap by providing the HyperSpectral Semantic Segmentation benchmark (HS3-Bench). It combines annotated hyperspectral images from three driving scenario datasets and provides standardized metrics, implementations, and evaluation protocols. We use the benchmark to derive two strong baseline models that surpass the previous state-of-the-art performances with and without pre-training on the individual datasets. Further, our results indicate that the existing learning-based methods benefit more from leveraging additional RGB training data than from leveraging the additional hyperspectral channels. This poses important questions for future research on hyperspectral imaging for semantic segmentation in driving scenarios. Code to run the benchmark and the strong baseline approaches are available under https://github.com/nickstheisen/hyperseg.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801768,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801768,,Measurement;Learning systems;Protocols;Codes;Semantic segmentation;Training data;Benchmark testing;Standards;Intelligent robots;Hyperspectral imaging,,,,22,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/nickstheisen/hyperseg,https://github.com/nickstheisen/hyperseg
244,SoftNeRF: A Self-Modeling Soft Robot Plugin for Various Tasks,J. Shan; Y. Li; Q. Feng; D. Li; L. Han; H. Wang,"Department of Automation, Key Laboratory of System Control and Information Processing of Ministry of Education, Key Laboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai Engineering Research Center of Intelligent Control and Management, Shanghai Jiao Tong University, Shanghai, China; School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Key Laboratory of System Control and Information Processing of Ministry of Education, Key Laboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai Engineering Research Center of Intelligent Control and Management, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Key Laboratory of System Control and Information Processing of Ministry of Education, Key Laboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai Engineering Research Center of Intelligent Control and Management, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Key Laboratory of System Control and Information Processing of Ministry of Education, Key Laboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai Engineering Research Center of Intelligent Control and Management, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Key Laboratory of System Control and Information Processing of Ministry of Education, Key Laboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai Engineering Research Center of Intelligent Control and Management, Shanghai Jiao Tong University, Shanghai, China",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,10558,10563,"Building a self-model for robots, enabling them to simulate their physical selves and predict future states without direct interaction with the physical world, is crucial for robot motion planning and control. Existing self-modeling methods primarily focus on rigid robots and typically require significant time, effort, and resources to gather training data. In this study, we introduce SoftNeRF, a self-supervised visual self-model designed for soft robots. We use a hybrid neural shape representation based on the Signed Distance Function (SDF) to capture both the geometry and complex nonlinear motion of soft robots. By leveraging differentiable rendering, our method learns a self-model from readily available RGB images, similar to how humans understand their physical state through reflection. To improve training efficiency and model accuracy, we propose an error-guided adaptive sampling strategy. SoftNeRF can serve as a plug-in for various downstream tasks, even when trained with data unrelated to those tasks. We demonstrate SoftNeRF’s ability to support shape prediction and motion planning for robots in both simulated and real-world environments. Furthermore, SoftNeRF excels in detecting and recovering from damage, thereby enhancing machine resilience. Code is available at: https://github.com/irmvlab/soft-nerf.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801344,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801344,,Training;Visualization;Adaptation models;Accuracy;Shape;Training data;Soft robotics;Rendering (computer graphics);Sampling methods;Planning,,,,27,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/irmvlab/soft-nerf,https://github.com/irmvlab/soft-nerf
245,Towards a Surgeon-in-the-Loop Ophthalmic Robotic Apprentice using Reinforcement and Imitation Learning,A. Gomaa; B. Mahdy; N. Kleer; A. Krüger,"German Research Center for Artificial Intelligence (DFKI) and Saarland Informatics Campus, Saarland University, Saarbrucken, Germany; German Research Center for Artificial Intelligence (DFKI) and Saarland Informatics Campus, Saarland University, Saarbrucken, Germany; German Research Center for Artificial Intelligence (DFKI) and Saarland Informatics Campus, Saarland University, Saarbrucken, Germany; German Research Center for Artificial Intelligence (DFKI) and Saarland Informatics Campus, Saarland University, Saarbrucken, Germany",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,6939,6946,"Robot-assisted surgical systems have demonstrated significant potential in enhancing surgical precision and minimizing human errors. However, existing systems cannot accommodate individual surgeons’ unique preferences and requirements. Additionally, they primarily focus on general surgeries (e.g., laparoscopy) and are unsuitable for highly precise microsurgeries, such as ophthalmic procedures. Thus, we propose an image-guided approach for surgeon-centered autonomous agents that can adapt to the individual surgeon’s skill level and preferred surgical techniques during ophthalmic cataract surgery. Our approach trains reinforcement and imitation learning agents simultaneously using curriculum learning approaches guided by image data to perform all tasks of the incision phase of cataract surgery. By integrating the surgeon’s actions and preferences into the training process, our approach enables the robot to implicitly learn and adapt to the individual surgeon’s unique techniques through surgeon-in-the-loop demonstrations. This results in a more intuitive and personalized surgical experience for the surgeon while ensuring consistent performance for the autonomous robotic apprentice. We define and evaluate the effectiveness of our approach in a simulated environment using our proposed metrics and highlight the trade-off between a generic agent and a surgeon-centered adapted agent. Finally, our approach has the potential to extend to other ophthalmic and microsurgical procedures, opening the door to a new generation of surgeon-in-the-loop autonomous surgical robots. We provide an open-source simulation framework for future development and reproducibility at https://github.com/amrgomaaelhady/CataractAdaptSurgRobot.",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10802574,Ministry of Education; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10802574,,Cataracts;Training;Laparoscopes;Medical robotics;Imitation learning;Microsurgery;Reproducibility of results;Autonomous agents;Intelligent robots,,,,48,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,https://github.com/amrgomaaelhady/CataractAdaptSurgRobot,https://github.com/amrgomaaelhady/CataractAdaptSurgRobot
246,QueSTMaps: Queryable Semantic Topological Maps for 3D Scene Understanding,Y. Mehan; K. Gupta; R. Jayanti; A. Govil; S. Garg; M. Krishna,"Robotics Research Center, India; Robotics Research Center, India; Robotics Research Center, India; Robotics Research Center, India; The University of Adelaide, Australia; Robotics Research Center, India",2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),25 Dec 2024,2024,,,13311,13317,"Robotic tasks such as planning and navigation require a hierarchical semantic understanding of a scene, which could include multiple floors and rooms. Current methods primarily focus on object segmentation for 3D scene understanding. However, such methods struggle to segment out topological regions like ""kitchen"" in the scene. In this work, we introduce a two-step pipeline to solve this problem. First, we extract a topological map, i.e., floorplan of the indoor scene using a novel multi-channel occupancy representation. Then, we generate CLIP-aligned features and semantic labels for every room instance based on the objects it contains using a self-attention transformer. Our language-topology alignment supports natural language querying, e.g., a ""place to cook"" locates the ""kitchen"". We outperform the current state-of-the-art on room segmentation by ~20% and room classification by ~12%. Our detailed qualitative analysis and ablation studies provide insights into the problem of joint structural and semantic 3D scene understanding. Project Page: quest-maps.github.io",2153-0866,979-8-3503-7770-5,10.1109/IROS58592.2024.10801814,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801814,,Three-dimensional displays;Navigation;Semantics;Pipelines;Natural languages;Object segmentation;Transformers;Planning;Intelligent robots;Floors,,,,40,IEEE,25 Dec 2024,,,IEEE,IEEE Conferences,,
247,GP-Guided MPPI for Efficient Navigation in Complex Unknown Cluttered Environments,I. S. Mohamed; M. Ali; L. Liu,"Luddy School of Informatics, Computing, and Engineering, Indiana University, Bloomington, IN, USA; Luddy School of Informatics, Computing, and Engineering, Indiana University, Bloomington, IN, USA; Luddy School of Informatics, Computing, and Engineering, Indiana University, Bloomington, IN, USA",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,7463,7470,"Robotic navigation in unknown, cluttered environ-ments with limited sensing capabilities poses significant chal-lenges in robotics. Local trajectory optimization methods, such as Model Predictive Path Intergal (MPPI), are a promising solution to this challenge. However, global guidance is required to ensure effective navigation, especially when encountering challenging environmental conditions or navigating beyond the planning horizon. This study presents the GP-MPPI, an online learning-based control strategy that integrates MPPI with a local perception model based on Sparse Gaussian Process (SGP). The key idea is to leverage the learning capability of SGP to construct a variance (uncertainty) surface, which enables the robot to learn about the navigable space surrounding it, identify a set of suggested subgoals, and ultimately recommend the optimal subgoal that minimizes a predefined cost function to the local MPPI planner. Afterward, MPPI computes the optimal control sequence that satisfies the robot and collision avoidance constraints. Such an approach eliminates the necessity of a global map of the environment or an offline training process. We validate the efficiency and robustness of our proposed control strategy through both simulated and real-world experiments of 2D autonomous navigation tasks in complex unknown en-vironments, demonstrating its superiority in guiding the robot safely towards its desired goal while avoiding obstacles and escaping entrapment in local minima. The GPU implementation of GP-MPPI, including the supplementary video, is available at https://github.com/IhabMohamed/GP-MPPI.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341382,"National Science Foundation(grant numbers:2006886,2047169); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341382,,Training;Uncertainty;Navigation;Optimal control;Aerospace electronics;Robot sensing systems;Cost function,,6.0,,29,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/IhabMohamed/GP-MPPI,https://github.com/IhabMohamed/GP-MPPI
248,Towards a Robust Adversarial Patch Attack Against Unmanned Aerial Vehicles Object Detection,S. Shrestha; S. Pathak; E. K. Viegas,"Secure Systems Research Center (SSRC) at Technology Innovation Institute (TII), Abu Dhabi, United Arab Emirates; Secure Systems Research Center (SSRC) at Technology Innovation Institute (TII), Abu Dhabi, United Arab Emirates; Secure Systems Research Center (SSRC) at Technology Innovation Institute (TII), Abu Dhabi, United Arab Emirates",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,3256,3263,"Object detection techniques for autonomous Un-manned Aerial Vehicles (UAV) are built upon Deep Neural Networks (DNN), which are known to be vulnerable to adversarial patch perturbation attacks that lead to object detection evasion. Yet, current adversarial patch generation schemes are not designed for UAV imagery settings. This paper proposes a new robust adversarial patch generation attack against object detection with UAVs. We build adversarial patches considering UAV-specific settings such as the UAV camera perspective, viewing angle, distance, and brightness changes. As a result, built patches can also degrade the accuracy of object detector models implemented with different initializations and architectures. Experiments conducted on the VisDrone dataset have shown the proposal's feasibility, achieving an attack success rate of up to 80% in a white-box setting. In addition, we also transfer the patch against DNN models with different initializations and different architectures, reaching attack success rates of up to 75% and 78%, respectively, in a gray-box setting. GitHub: https://github.com/SamSamhuns/yolov5_adversarial",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342460,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342460,,Perturbation methods;Object detection;Detectors;Artificial neural networks;Autonomous aerial vehicles;Robustness;Safety,,5.0,,27,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/SamSamhuns/yolov5_adversarial,https://github.com/SamSamhuns/yolov5_adversarial
249,CEFHRI: A Communication Efficient Federated Learning Framework for Recognizing Industrial Human-Robot Interaction,U. Khalid; H. Iqbal; S. Vahidian; J. Hua; C. Chen,"Center For Research in Computer Vision, University of Central Florida, Orlando, FL, USA; Dept. of Computer Science, Wayne State University, Detroit, MI, USA; Dept. of Electrical and Computer Engineering, Duke University, Durham, NC, USA; Dept. of Computer Science, Wayne State University, Detroit, MI, USA; Center For Research in Computer Vision, University of Central Florida, Orlando, FL, USA",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,10141,10148,"Human-robot interaction (HRI) is a rapidly growing field that encompasses social and industrial applications. Machine learning plays a vital role in industrial HRI by enhancing the adaptability and autonomy of robots in complex environments. However, data privacy is a crucial concern in the interaction between humans and robots, as companies need to protect sensitive data while machine learning algorithms require access to large datasets. Federated Learning (FL) offers a solution by enabling the distributed training of models without sharing raw data. Despite extensive research on Federated learning (FL) for tasks such as natural language processing (NLP) and image classification, the question of how to use FL for HRI remains an open research problem. The traditional FL approach involves transmitting large neural network parameter matrices between the server and clients, which can lead to high communication costs and often becomes a bottleneck in FL. This paper proposes a communication-efficient FL framework for human-robot interaction (CEFHRI) to address the challenges of data heterogeneity and communication costs. The framework leverages pre-trained models and introduces a trainable spatiotemporal adapter for video understanding tasks in HRI. Experimental results on three human-robot interaction benchmark datasets: HRI30, InHARD, and COIN demonstrate the superiority of CEFHRI over full fine-tuning in terms of communication costs. The proposed methodology provides a secure and efficient approach to HRI federated learning, particularly in industrial environments with data privacy concerns and limited communication bandwidth. Our code is available at https://github.com/umarkhalidAI/CEFHRI-Efficient-Federated-Learning.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341467,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341467,,Training;Data privacy;Adaptation models;Costs;Federated learning;Service robots;Human-robot interaction,,1.0,,48,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/umarkhalidAI/CEFHRI-Efficient-Federated-Learning,https://github.com/umarkhalidAI/CEFHRI-Efficient-Federated-Learning
250,Hybrid Map-Based Path Planning for Robot Navigation in Unstructured Environments,J. Liu; X. Chen; J. Xiao; S. Lin; Z. Zheng; H. Lu,"College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,2216,2223,"Fast and accurate path planning is important for ground robots to achieve safe and efficient autonomous navigation in unstructured outdoor environments. However, most existing methods exploiting either 2D or 2.5D maps struggle to balance the efficiency and safety for ground robots navigating in such challenging scenarios. In this paper, we propose a novel hybrid map representation by fusing a 2D grid and a 2.5D digital elevation map. Based on it, a novel path planning method is proposed, which considers the robot poses during traversability estimation. By doing so, our method explicitly takes safety as a planning constraint enabling robots to navigate unstructured environments smoothly. The proposed approach has been evaluated on both simulated datasets and a real robot platform. The experimental results demonstrate the efficiency and effectiveness of the proposed method. Compared to state-of-the-art baseline methods, the proposed approach consistently generates safer and easier paths for the robot in different unstructured outdoor environments. The implementation of our method is publicly available at https://github.com/nubot-nudt/T-Hybrid-planner.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341666,"National Science Foundation of China(grant numbers:U1913202,U22A2059,62203460); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341666,,Point cloud compression;Navigation;Robot kinematics;Path planning;Hybrid power systems;Safety;Planning,,8.0,,33,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/nubot-nudt/T-Hybrid-planner,https://github.com/nubot-nudt/T-Hybrid-planner
251,Autonomous Marker-Less Rapid Aerial Grasping,E. Bauer; B. G. Cangan; R. K. Katzschmann,"Soft Robotics Lab, ETH, Zurich, Switzerland; Soft Robotics Lab, ETH, Zurich, Switzerland; Soft Robotics Lab, ETH, Zurich, Switzerland",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,6395,6402,"In a future with autonomous robots, visual and spatial perception is of utmost importance for robotic systems. Particularly for aerial robotics, there are many applications where utilizing visual perception is necessary for any real-world scenarios. Robotic aerial grasping using drones promises fast pick-and-place solutions with a large increase in mobility over other robotic solutions. Utilizing Mask R-CNN scene segmentation (detectron2), we propose a vision-based system for autonomous rapid aerial grasping which does not rely on mark-ers for object localization and does not require the appearance of the object to be previously known. Combining segmented images with spatial information from a depth camera, we generate a dense point cloud of the detected objects and perform geometry - based grasp planning to determine grasping points on the objects. In real-world experiments on a dynamically grasping aerial platform, we show that our system can replicate the performance of a motion capture system for object local-ization up to 94.5 % of the baseline grasping success rate. With our results, we show the first use of geometry-based grasping techniques with a flying platform and aim to increase the autonomy of existing aerial manipulation platforms, bringing them further towards real-world applications in warehouses and similar environments.††Code: https://github.com/srl-ethz/detectron-realsense",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342033,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342033,,Location awareness;Point cloud compression;Image segmentation;Visualization;Shape;Grasping;Real-time systems,,1.0,,41,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/srl-ethz/detectron-realsense,https://github.com/srl-ethz/detectron-realsense
252,PanelPose: A 6D Pose Estimation of Highly-Variable Panel Object for Robotic Robust Cockpit Panel Inspection,H. Sun; P. Ni; Z. Li; Y. Wang; X. Zhu; Q. Cao,"School of Mechanical Engineering, Shanghai Jiao-tong University, Shanghai, China; School of Mechanical Engineering, Shanghai Jiao-tong University, Shanghai, China; School of Mechanical Engineering, Shanghai Jiao-tong University, Shanghai, China; School of Mechanical Engineering, Shanghai Jiao-tong University, Shanghai, China; School of Mechanical Engineering, Shanghai Jiao-tong University, Shanghai, China; School of Mechanical Engineering, Shanghai Jiao-tong University, Shanghai, China",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,3214,3221,"In robotic cockpit inspection scenarios, the 6D pose of highly-variable panel objects is necessary. However, the buttons with different states on the panel cause the variable texture and point cloud, which confuses the traditional invariable object pose estimation method. The bottleneck is the variable texture and point cloud. To address this issue, we propose a simple yet effective method denoted as PanelPose that leverages synthetic data and edge-line features. Specifically, we extract edge and line features of RGB images and fuse these feature maps as a multi-feature fusion map (MFF Map) to focus on the shape features of panel objects. Moreover, we design an effective keypoint selection algorithm considering the shape information of panel objects, which simplifies keypoint localization for precise pose estimation. Finally, the panel object pose is estimated via PNP/RANSAC, refined by the multi-state template (MST) and multi-scale ICP. We experimentally show that state-of-the-art 6D pose estimation methods alone are not sufficient to solve the cockpit panel inspection task but that our method significantly improves the performance. In cockpit inspection scenarios, the panel localization error is less than 3mm using our method. Code and data are available at https://github.com/sunhan1997/PaneIPose.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342304,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342304,,Point cloud compression;Location awareness;Shape;Image edge detection;Pose estimation;Inspection;Feature extraction,,1.0,,23,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/sunhan1997/PaneIPose,https://github.com/sunhan1997/PaneIPose
253,SDF-Pack: Towards Compact Bin Packing with Signed-Distance-Field Minimization,J. -H. Pan; K. -H. Hui; X. Gao; S. Zhu; Y. -H. Liu; P. -A. Heng; C. -W. Fu,"Department of Computer Science and Engineering, The Chinese University of Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong; Hong Kong Centre for Logistics Robotics; Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,10612,10619,"Robotic bin packing is very challenging, especially when considering practical needs such as object variety and packing compactness. This paper presents SDF-Pack, a new approach based on signed distance field (SDF) to model the geometric condition of objects in a container and compute the object placement locations and packing orders for achieving a more compact bin packing. Our method adopts a truncated SDF representation to localize the computation, and based on it, we formulate the SDF -minimization heuristic to find optimized placements to compactly pack objects with the existing ones. To further improve space utilization, if the packing sequence is controllable, our method can suggest which object to be packed next. Experimental results on a large variety of everyday objects show that our method can consistently achieve higher packing compactness over 1,000 packing cases, enabling us to pack more objects into the container, compared with the existing heuristics under various packing settings. The code is publicly available at: https://github.com/kwpoon/SDF-Pack.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341940,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341940,,Codes;Computational modeling;Reinforcement learning;Containers;Aerospace electronics;Minimization;Robustness,,3.0,,30,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/kwpoon/SDF-Pack,https://github.com/kwpoon/SDF-Pack
254,RGBD Fusion Grasp Network with Large-Scale Tableware Grasp Dataset,J. Yoon; J. Ahn; C. Ha; R. Chung; D. Park; H. Han; S. Kang,"Robot Center, Samsung Research, Samsung Electronics Co., Ltd., Seoul, Republic of Korea; Robot Center, Samsung Research, Samsung Electronics Co., Ltd., Seoul, Republic of Korea; Robot Center, Samsung Research, Samsung Electronics Co., Ltd., Seoul, Republic of Korea; Robot Center, Samsung Research, Samsung Electronics Co., Ltd., Seoul, Republic of Korea; Robot Center, Samsung Research, Samsung Electronics Co., Ltd., Seoul, Republic of Korea; Robot Center, Samsung Research, Samsung Electronics Co., Ltd., Seoul, Republic of Korea; Robot Center, Samsung Research, Samsung Electronics Co., Ltd., Seoul, Republic of Korea",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,2947,2954,"This paper proposes a novel approach to address the technical challenges of stable object grasping, particularly in the context of handling tableware in a home environment. Handling tableware is particularly important, yet challenging, due to the flat nature of most tableware objects and the need to maintain a stable posture to prevent spills. To address these challenges, we present three key contributions: 1) a large-scale tableware dataset, not commonly found in the previous datasets; 2) a novel sampling method for stable grasp pose generation; and 3) a multi-modal fusion grasp network that effectively learns 6- DoF grasp pose, including flat objects. Our dataset contains over 45 million grasp poses and 1 million RGBD images captured in 800 scenes, which include randomly selected 10–18 tableware objects under 4 different lighting conditions. The grasp poses in the dataset are generated using a novel sampling method that incorporates geometric analysis to ensure stable grasping with minimal object movement. Furthermore, we design an RGBD fusion grasp network (RGBD-FGN) that can combine information from RGB and depth images considering each characteristic. Our experimental results demonstrate the superior performance of our approach over existing techniques, which is a significant contribution towards developing a multitasking home robot. Our dataset and source code can be accessed at https://github.com/SamsungLabs/RGBD-FGN.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341357,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341357,,Fuses;Source coding;Lighting;Grasping;Benchmark testing;Sampling methods;Multitasking,,,,28,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/SamsungLabs/RGBD-FGN,https://github.com/SamsungLabs/RGBD-FGN
255,Converting Depth Images and Point Clouds for Feature-Based Pose Estimation,R. Lösch; M. Sastuba; J. Toth; B. Jung,"Institute of Computer Science, Technical University Bergakademie, Freiberg, Germany; German Centre for Rail Traffic Research at the Federal Railway Authority, Germany; Institute of Computer Science, Technical University Bergakademie, Freiberg, Germany; Institute of Computer Science, Technical University Bergakademie, Freiberg, Germany",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,3422,3428,"In recent years, depth sensors have become more and more affordable and have found their way into a growing amount of robotic systems. However, mono- or multi-modal sensor registration, often a necessary step for further pro-cessing, faces many challenges on raw depth images or point clouds. This paper presents a method of converting depth data into images capable of visualizing spatial details that are basically hidden in traditional depth images. After noise removal, a neighborhood of points forms two normal vectors whose difference is encoded into this new conversion. Compared to Bearing Angle images, our method yields brighter, higher-contrast images with more visible contours and more details. We tested feature-based pose estimation of both conversions in a visual odometry task and RGB-D SLAM. For all tested features, AKAZE, ORB, SIFT, and SURF, our new Flexion images yield better results than Bearing Angle images and show great potential to bridge the gap between depth data and classical computer vision. Source code is available here: https://rlsch.github.io/depth-flexion-conversion.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341758,European Institute of Innovation and Technology (EIT); EU Framework Programme for Research and Innovation(grant numbers:17019); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341758,,Point cloud compression;Simultaneous localization and mapping;Source coding;Multimodal sensors;Pose estimation;Sensor systems;Task analysis,,2.0,,24,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://rlsch.github.io/depth-flexion-conversion,https://github.com/rlsch/depth-flexion-conversion
256,CompUDA: Compositional Unsupervised Domain Adaptation for Semantic Segmentation Under Adverse Conditions,Z. Zhengl; Y. Chen; B. -S. Hua; S. -K. Yeung,"Department of Computer Science and Engineering, Hong Kong University of Science and Technology; Department of Computer Science and Engineering, Hong Kong University of Science and Technology; VinAI Research, Hanoi, Vietnam; Department of Computer Science and Engineering, Hong Kong University of Science and Technology",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,7675,7681,"In autonomous driving, performing robust semantic segmentation under adverse weather conditions is a long-standing challenge. Imperfect camera observations under adverse conditions result in images with reduced visibility, which hinders label annotation and semantic scene understanding based on these images. A common solution is to adopt semantic segmentation models trained in a source domain with ground truth labels and perform unsupervised domain adaptation (UDA) from the source domain to an unlabeled target domain that has adverse conditions. Due to imperfect visual observations in the target domain, such adaptation needs special treatment to achieve good performance. In this paper, we propose a new compositional unsupervised domain adaptation (CompUDA) method that disentangles the domain gap based on multiple factors including style, visibility, and image quality. The domain gaps caused by these individual factors can then be addressed separately by introducing the intermediate domains. Specifically, 1) to address the style gap, we perform source-to-intermediate domain adaptation and generate pseudo-labels for self-training in the target domain; 2) to address the visibility gap, we perform a geometry-aligned normal-to-adverse image translation and introduce a synthetic domain; 3) finally, to address the image quality gap between the synthetic and target domain, we perform a synthetic-to-real adaptation based on the generated pseudo-labels. Our compositional unsupervised domain adaptation can be used in conjunction with a wide variety of semantic segmentation methods and result in significant performance improvement across datasets. The codes are available at https://github.com/zhengziqiang/CompUDA.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342102,HKUST(grant numbers:R9429); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342102,,Image quality;Visualization;Codes;Image synthesis;Semantic segmentation;Semantics;Performance gain,,1.0,,31,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/zhengziqiang/CompUDA,https://github.com/zhengziqiang/CompUDA
257,Trajectory Tracking via Multiscale Continuous Attractor Networks,T. Joseph; T. Fischer; M. Milford,"QUT Centre for Robotics, School of Electrical Engineering and Robotics, Queensland University of Technology, Brisbane, QLD, Australia; QUT Centre for Robotics, School of Electrical Engineering and Robotics, Queensland University of Technology, Brisbane, QLD, Australia; QUT Centre for Robotics, School of Electrical Engineering and Robotics, Queensland University of Technology, Brisbane, QLD, Australia",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,368,375,"Animals and insects showcase remarkably robust and adept navigational abilities, up to literally circumnavigating the globe. Primary progress in robotics inspired by these natural systems has occurred in two areas: highly theoretical computational neuroscience models, and handcrafted systems like RatSLAM and NeuroSLAM. In this research, we present work bridging the gap between the two, in the form of Multiscale Continuous Attractor Networks (MCAN), that combine the multiscale parallel spatial neural networks of the previous theoretical models with the real-world robustness of the robot-targeted systems, to enable trajectory tracking over large velocity ranges. To overcome the limitations of the reliance of previous systems on hand-tuned parameters, we present a genetic algorithm-based approach for automated tuning of these networks, substantially improving their usability. To provide challenging navigational scale ranges, we open source a flexible city-scale navigation simulator that adapts to any street network, enabling high throughput experimentation11https://github.com/theresejoseph/Trajectory_ Tracking_via_MCAN/ • In extensive experiments using the city-scale navigation environment and Kitti, we show that the system is capable of stable dead reckoning over a wide range of velocities and environmental scales, where a single-scale approach fails.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341938,Queensland University of Technology (QUT); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341938,,Dead reckoning;Trajectory tracking;Computational modeling;Insects;Throughput;Robustness;Usability,,1.0,,41,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/theresejoseph/Trajectory_,https://github.com/theresejoseph/Trajectory_
258,See What the Robot Can't See: Learning Cooperative Perception for Visual Navigation,J. Blumenkamp; Q. Li; B. Wang; Z. Liu; A. Prorok,"Department of Computer Science and Technology, University of Cambridge, United Kingdom; Department of Computer Science and Technology, University of Cambridge, United Kingdom; NA; Department of Computer Science and Technology, University of Cambridge, United Kingdom; Department of Computer Science and Technology, University of Cambridge, United Kingdom",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,7333,7340,"We consider the problem of navigating a mobile robot towards a target in an unknown environment that is endowed with visual sensors, where neither the robot nor the sensors have access to global positioning information and only use first-person- view images. In order to overcome the need for positioning, we train the sensors to encode and communicate relevant viewpoint information to the mobile robot, whose objective it is to use this information to navigate to the target along the shortest path. We overcome the challenge of enabling all the sensors (even those that cannot directly see the target) to predict the direction along the shortest path to the target by implementing a neighborhood-based feature aggregation module using a Graph Neural Network (GNN) architecture. In our experiments, we first demonstrate generalizability to previously unseen environments with various sensor layouts. Our results show that by using communication between the sensors and the robot, we achieve up to 2.0 × improvement in SPL (Success weighted by Path Length) when compared to a communication-free baseline. This is done without requiring a global map, positioning data, nor pre-calibration of the sensor network. Second, we perform a zero-shot transfer of our model from simulation to the real world. Laboratory experiments demonstrate the feasibility of our approach in various cluttered environments. Finally, we showcase examples of successful navigation to the target while both the sensor network layout as well as obstacles are dynamically reconfigured as the robot navigates. We provide a video demo11https://www.youtube.com/watch?v=kcrnr6RUgucw, the dataset, trained models, and source code22https://github.com/proroklab/sensor-guided-visual-nav.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342535,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342535,,Visualization;Navigation;Layout;Robot sensing systems;Graph neural networks;Sensor systems;Sensors,,,,30,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/proroklab/sensor-guided-visual-nav,https://github.com/proroklab/sensor-guided-visual-nav
259,Thoracic Cartilage Ultrasound-CT Registration Using Dense Skeleton Graph,Z. Jiang; C. Li; X. Lil; N. Navab,"Chair for Computer Aided Medical Procedures and Augmented Reality, Technical University of Munich, Germany; Chair for Computer Aided Medical Procedures and Augmented Reality, Technical University of Munich, Germany; Chair for Computer Aided Medical Procedures and Augmented Reality, Technical University of Munich, Germany; Chair for Computer Aided Medical Procedures and Augmented Reality, Technical University of Munich, Germany",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,6586,6592,"Autonomous ultrasound (US) imaging has gained increased interest recently, and it has been seen as a potential solution to overcome the limitations of free-hand US exami-nations, such as inter-operator variations. However, it is still challenging to accurately map planned paths from a generic atlas to individual patients, particularly for thoracic applications with high acoustic-impedance bone structures below the skin. To address this challenge, a dense graph-based non-rigid registration is proposed to transfer planned paths from the atlas to the current setup by explicitly considering subcutaneous bone surface. To this end, the sternum and cartilage branches are segmented using a template matching to assist coarse alignment of US and CT point clouds. Afterward, a directed graph is generated based on the CT template. Then, the self-organizing map using geographical distance is successively performed twice to extract the optimal graph representations for CT and US point clouds, individually. To evaluate the proposed approach, five cartilage point clouds from distinct patients are employed. The results demonstrate that the proposed graph-based registration can effectively map trajectories from CT to the current setup to do US examination through limited intercostal space. The non-rigid registration results in terms of Hausdorff distance (Mean±SD) is $9.48 \pm 0.27$ mm and the path transferring error in terms of Euclidean distance is $2.21\pm 1.11\ mm$. The code11https://github.com/marslicy/Cartilage-graph-based-US-CT-Registration and video22Video: https://www.youtube.com/watch?v=QJz2fkwgbP8 can be publicly accessed.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341575,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341575,,Point cloud compression;Self-organizing feature maps;Ultrasonic imaging;Computed tomography;Sternum;Euclidean distance;Bones,,1.0,,25,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/marslicy/Cartilage-graph-based-US-CT-Registration,https://github.com/marslicy/Cartilage-graph-based-US-CT-Registration
260,Depth-Based 6DoF Object Pose Estimation Using Swin Transformer,Z. Li; I. Stamos,"The Graduate Center, City University of New York; The Graduate Center, City University of New York",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,1185,1191,"Accurately estimating the 6D pose of objects is crucial for many applications, such as robotic grasping, autonomous driving, and augmented reality. However, this task becomes more challenging in poor lighting conditions or when dealing with textureless objects. To address this issue, depth images are becoming an increasingly popular choice due to their invariance to a scene's appearance and the implicit incorporation of essential geometric characteristics. However, fully leveraging depth information to improve the performance of pose estimation remains a difficult and under-investigated problem. To tackle this challenge, we propose a novel framework called SwinDePose, that uses only geometric information from depth images to achieve accurate 6D pose estimation. SwinDePose first calculates the angles between each normal vector defined in a depth image and the three coordinate axes in the camera coordinate system. The resulting angles are then formed into an image, which is encoded using Swin Transformer. Additionally, we apply RandLA-Net to learn the representations from point clouds. The resulting image and point clouds embeddings are concatenated and fed into a semantic segmentation module and a 3D keypoints localization module. Finally, we estimate 6D poses using a least-square fitting approach based on the target object's predicted semantic mask and 3D keypoints. In experiments on the LineMod and Occlusion LineMod, SwinDePose outperforms existing state-of-the-art methods for 6D object pose estimation using depth images. We also provide competitive results on the YCB-Video dataset even without post-processing. This demonstrates the effectiveness of our approach and highlights its potential for improving performance in real-world scenarios. Our code is at https://github.com/zhujunli1993/SwinDePose.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342215,NSF(grant numbers:CNS1625843); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342215,,Point cloud compression;Location awareness;Three-dimensional displays;Semantic segmentation;Robot kinematics;Pose estimation;Semantics,,6.0,,36,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/zhujunli1993/SwinDePose,https://github.com/zhujunli1993/SwinDePose
261,Kinematics-Only Differential Flatness Based Trajectory Tracking for Autonomous Racing,Y. Dighe; Y. Kim; S. Rajguru; Y. Turkar; T. Singh; K. Dantu,"University at Buffalo, NY, USA; University at Buffalo, NY, USA; University at Buffalo, NY, USA; University at Buffalo, NY, USA; University at Buffalo, NY, USA; University at Buffalo, NY, USA",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,1629,1636,"In autonomous racing, accurately tracking the race line at the limits of handling is essential to guarantee competitiveness. In this study, we show the effectiveness of Differential Flatness based control for high-speed trajectory tracking for car-like robots. We compare the tracking performance of our controller against Nonlinear Model Predictive Control and resource use while running on embedded hardware and show that on average KFC reduces the computation resource usage by 50 % while performing on par with NMPC. Our implementation of the proposed controller, the simulation environment and detailed results is open-sourced on https://github.com/droneslab/.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341603,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341603,,Trajectory tracking;Hardware;Intelligent robots;Predictive control,,1.0,,33,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/droneslab,https://github.com/droneslab
262,Rollvox: Real-Time and High-Quality LiDAR Colorization with Rolling Shutter Camera,S. Hong; C. Zheng; H. Yin; S. Shen,"Department of Electronic Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong SAR, China; Department of Mechanical Engineering, The University of Hong Kong, Hong Kong SAR, China; Department of Electronic Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong SAR, China; Department of Electronic Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong SAR, China",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,7195,7201,"In this study, we propose a novel system for real-time coloring LiDAR point clouds with a low-cost RS camera. The main challenges are dealing with the motion distortion of the RS camera and the multi-sensor time synchronization. To tackle these challenges, we carefully design a hardware synchronizer to ensure the strict alignment of the LiDAR, inertial measurement unit, and RS camera. With accurate timestamps, we first use LiDAR-inertial odometry (LIO) for pose estimation, and the poses of image line exposure are calculated by forward propagation based on a constant velocity motion model. Then, we propose our method based on the RS constraint for colorizing the LiDAR point cloud. For comparison, we colorize the LiDAR point cloud with conventional rolling shutter image undistortion. In the real-world tests, The results show that our proposed method produces more accurate and efficient colorization of point clouds. Besides, considering the situation of readout time not being provided, we propose a method to calibrate the readout time by minimizing the reprojection error of LIO's inter-frame pose and image optical flows. We release our code and self-collected datasets on Github33https://github.com/sheng00125/Rollvox to benefit the community.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342172,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342172,,Point cloud compression;Laser radar;Robot vision systems;Pose estimation;Cameras;Real-time systems;Hardware,,1.0,,21,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/sheng00125/Rollvox,https://github.com/sheng00125/Rollvox
263,Uncertainty-Aware Lidar Place Recognition in Novel Environments,K. Mason; J. Knights; M. Ramezani; P. Moghadam; D. Miller,"Robotics and Autonomous Systems, DATA61, CSIRO, Brisbane, QLD, Australia; Robotics and Autonomous Systems, DATA61, CSIRO, Brisbane, QLD, Australia; Robotics and Autonomous Systems, DATA61, CSIRO, Brisbane, QLD, Australia; Robotics and Autonomous Systems, DATA61, CSIRO, Brisbane, QLD, Australia; School of Electrical Engineering, Queensland University of Technology (QUT), Brisbane, Australia",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,3366,3373,"State-of-the-art lidar place recognition models exhibit unreliable performance when tested on environments different from their training dataset, which limits their use in complex and evolving environments. To address this issue, we investigate the task of uncertainty-aware lidar place recognition, where each predicted place must have an associated uncertainty that can be used to identify and reject incorrect predictions. We introduce a novel evaluation protocol and present the first comprehensive benchmark for this task, testing across five uncertainty estimation techniques and three large-scale datasets. Our results show that an Ensembles approach is the highest performing technique, consistently improving the performance of lidar place recognition and uncertainty estimation in novel environments, though it incurs a computational cost. Code is publicly available at https://github.com/csiro-robotics/Uncertainty-LPR.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341383,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341383,,Training;Uncertainty;Laser radar;Protocols;Pose estimation;Benchmark testing;Real-time systems,,2.0,,42,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/csiro-robotics/Uncertainty-LPR,https://github.com/csiro-robotics/Uncertainty-LPR
264,360FusionNeRF: Panoramic Neural Radiance Fields with Joint Guidance,S. Kulkarni; P. Yin; S. Scherer,"Department of Mechanical Engineering, Indian Institute of Technology, Madras; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,7202,7209,"Based on the neural radiance fields (NeRF), we present a pipeline for generating novel views from a single 360° panoramic image. Prior research relied on the neighborhood interpolation capability of multi-layer perceptions to complete missing regions caused by occlusion. This resulted in artifacts in their predictions. We propose 360FusionNeRF, a semi-supervised learning framework that employs geometric supervision and semantic consistency to guide the progressive training process. Firstly, the input image is reprojected to 360° images, and depth maps are extracted at different camera positions. In addition to the NeRF color guidance, the depth supervision enhances the geometry of the synthesized views. Furthermore, we include a semantic consistency loss that encourages realistic renderings of novel views. We extract these semantic features using a pre-trained visual encoder CLIP, a Vision Transformer (ViT) trained on hundreds of millions of diverse 2D photographs mined from the web with natural language supervision. Experiments indicate that our proposed method is capable of producing realistic completions of unobserved regions while preserving the features of the scene. 360FusionNeRF consistently delivers state-of-the-art performance when transferring to synthetic Structured3D dataset (PSNR ~ 5%, SSIM ~3% LPIPS ~13%), real-world Matterport3D dataset (PSNR ~3%, SSIM ~3% LPIPS ~9%) and Replica360 dataset (PSNR ~8%, SSIM ~2% LPIPS ~18%). We provide the source code at https://github.com/MetaSLAM/360FusionNeRF.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341346,NVIDIA; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341346,Scene representation;View synthesis;Neural Radiance Field;360° image;3D deep learning,Training;Visualization;Image color analysis;Source coding;Semantics;Semisupervised learning;Feature extraction,,5.0,,45,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/MetaSLAM/360FusionNeRF,https://github.com/MetaSLAM/360FusionNeRF
265,Principled ICP Covariance Modelling in Perceptually Degraded Environments for the EELS Mission Concept,W. Talbot; J. Nash; M. Paton; E. Ambrose; B. Metz; R. Thakker; R. Etheredge; M. Ono; V. Ila,"Jet Propulsion Laboratory, California Institute of Technology, USA; Jet Propulsion Laboratory, California Institute of Technology, USA; Jet Propulsion Laboratory, California Institute of Technology, USA; Jet Propulsion Laboratory, California Institute of Technology, USA; Jet Propulsion Laboratory, California Institute of Technology, USA; Jet Propulsion Laboratory, California Institute of Technology, USA; Jet Propulsion Laboratory, California Institute of Technology, USA; Jet Propulsion Laboratory, California Institute of Technology, USA; School of Aerospace, Mechanical and Mechatronic Engineering, University of Sydney, Australia",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,10763,10770,"The Exobiology Extant Life Surveyor (EELS) is a snake-like mobile instruments platform under development at Jet Propulsion Laboratory (JPL) for a mission concept to find evidence of life on Saturn's sixth largest moon, Enceladus. To conduct a life surveying mission there, the EELS platform must first traverse an unknown icy surface terrain before undertaking a controlled descent into a cryovolcanic vent. The remoteness of Enceladus and the icy nature of its terrain demands a level of autonomy in navigation significantly higher than previous rover missions. The perception system onboard EELS must be highly resilient to perceptually-degraded environments such as flat, open ice fields, icy plumes, and repeating geometries in vents. EELS' perception system is implemented as a multi-sensor Simultaneous Localisation And Mapping (SLAM) solution called SERPENT. State Estimation through Robust Perception in Extreme and Novel Terrains (SERPENT) estimates the robot trajectory and maintains a map database, from which dense global or local maps can be obtained on demand for downstream planning algorithms. This system opts to incorporate measurements from many sensor modalities (laser scans, images, IMU, altimeter, etc.), solving the SLAM problem through joint optimisation, and thus requires that the contribution of each sensor be balanced through careful modelling of their uncertainties. With a specific focus on Light Detection And Ranging (LiDAR) in this context, this paper proposes a principled approach to model the covariances of point-to-plane Iterative Closest Point (ICP). It performs a rigorous comparative analysis of new and existing covariance models, and is the first time some of these have been tested within a complete SLAM pipeline. These models are evaluated on perceptually challenging datasets collected in glacial environments by the EELS sensor suite (see Figures 1, 2). SERPENT is open-sourced at https://github.com/jpl-eels/serpent.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341455,Jet Propulsion Laboratory; California Institute of Technology; National Aeronautics and Space Administration(grant numbers:80NM0018D0004); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341455,,Simultaneous localization and mapping;Uncertainty;Saturn;Vents;Pipelines;Optimized production technology;Propulsion,,2.0,,51,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/jpl-eels/serpent,https://github.com/jpl-eels/serpent
266,Shape Completion with Prediction of Uncertain Regions,M. Humt; D. Winkelbauer; U. Hillenbrand,DLR Institute of Robotics and Mechatronics; DLR Institute of Robotics and Mechatronics; DLR Institute of Robotics and Mechatronics,2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,1215,1221,"Shape completion, i.e., predicting the complete geometry of an object from a partial observation, is highly relevant for several downstream tasks, most notably robotic manipulation. When basing planning or prediction of real grasps on object shape reconstruction, an indication of severe geometric uncertainty is indispensable. In particular, there can be an irreducible uncertainty in extended regions about the presence of entire object parts when given ambiguous object views. To treat this important case, we propose two novel methods for predicting such uncertain regions as straightforward extensions of any method for predicting local spatial occupancy, one through postprocessing occupancy scores, the other through direct prediction of an uncertainty indicator. We compare these methods together with two known approaches to probabilistic shape completion. Moreover, we generate a dataset, derived from ShapeNet [1], of realistically rendered depth images of object views with ground-truth annotations for the uncertain regions. We train on this dataset and test each method in shape completion and prediction of uncertain regions for known and novel object instances and on synthetic and real data. While direct uncertainty prediction is by far the most accurate in the segmentation of uncertain regions, both novel methods outperform the two baselines in shape completion and uncertain region prediction, and avoiding the predicted uncertain regions increases the quality of grasps for all tested methods. Web: https://github.com/DLR-RM/shape-completion",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342487,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342487,,Geometry;Uncertainty;Shape;Grasping;Probabilistic logic;Safety;Planning,,2.0,,38,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/DLR-RM/shape-completion,https://github.com/DLR-RM/shape-completion
267,RAMP: Hierarchical Reactive Motion Planning for Manipulation Tasks Using Implicit Signed Distance Functions,V. Vasilopoulos; S. Garg; P. Piacenza; J. Huh; V. Isler,"Samsung AI Center NY, New York, NY; Samsung AI Center NY, New York, NY; Samsung AI Center NY, New York, NY; Samsung AI Center NY, New York, NY; Samsung AI Center NY, New York, NY",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,10551,10558,"We introduce Reactive Action and Motion Planner (RAMP), which combines the strengths of sampling-based and reactive approaches for motion planning. In essence, RAMP is a hierarchical approach where a novel variant of a Model Predictive Path Integral (MPPI) controller is used to generate trajectories which are then followed asynchronously by a local vector field controller. We demonstrate, in the context of a table clearing application, that RAMP can rapidly find paths in the robot's configuration space, satisfy task and robot-specific constraints, and provide safety by reacting to static or dynamically moving obstacles. RAMP achieves superior performance through a number of key innovations: we use Signed Distance Function (SDF) representations directly from the robot configuration space, both for collision checking and reactive control. The use of SDFs allows for a smoother definition of collision cost when planning for a trajectory, and is critical in ensuring safety while following trajectories. In addition, we introduce a novel variant of MPPI which, combined with the safety guarantees of the vector field trajectory follower, performs incremental real-time global trajectory planning. Simulation results establish that our method can generate paths that are comparable to traditional and state-of-the-art approaches in terms of total trajectory length while being up to 30 times faster. Real-world experiments demonstrate the safety and effectiveness of our approach in challenging table clearing scenarios. Videos and code are available at: https://samsunglabs.github.io/RAMP-project-page/",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342397,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342397,,Technological innovation;Trajectory planning;Aerospace electronics;Real-time systems;Trajectory;Safety;Planning,,4.0,,25,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://samsunglabs.github.io/RAMP-project-page,https://github.com/SamsungLabs/RAMP
268,The Audio-Visual BatVision Dataset for Research on Sight and Sound,A. Brunetto; S. Hornauer; S. X. Yu; F. Moutarde,"Center for Robotics, MINES Paris, Université PSL, Paris, France; Center for Robotics, MINES Paris, Université PSL, Paris, France; University of Michigan, Ann Arbor, United States of America; Center for Robotics, MINES Paris, Université PSL, Paris, France",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,1,8,"Vision research showed remarkable success in understanding our world, propelled by datasets of images and videos. Sensor data from radar, LiDAR and cameras supports research in robotics and autonomous driving for at least a decade. However, while visual sensors may fail in some conditions, sound has recently shown potential to complement sensor data. Simulated room impulse responses (RIR) in 3D apartment-models became a benchmark dataset for the community, fostering a range of audiovisual research. In simulation, depth is predictable from sound, by learning bat-like perception with a neural network. Concurrently, the same was achieved in reality by using RGB-D images and echoes of chirping sounds. Biomimicking bat perception is an exciting new direction but needs dedicated datasets to explore the potential. Therefore, we collected the BatVision dataset to provide large-scale echoes in complex real-world scenes to the community. We equipped a robot with a speaker to emit chirps and a binaural microphone to record their echoes. Synchronized RGB-D images from the same perspective provide visual labels of traversed spaces. We sampled modern US office spaces to historic French university grounds, indoor and outdoor with large architectural variety. This dataset will allow research on robot echolocation, general audio-visual tasks and sound phænomena unavailable in simulated data. We show promising results for audio-only depth prediction and show how state-of-the-art work developed for simulated data can also succeed on our dataset. Project page: https://amandinebtto.github.io/Batvision-Dataset/",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341715,US National Science Foundation (NSF)(grant numbers:2215542); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341715,,Training;Visualization;Ultrasonic imaging;Three-dimensional displays;Chirp;Robot vision systems;Radar imaging,,1.0,,33,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://amandinebtto.github.io/Batvision-Dataset,https://github.com/AmandineBtto/Batvision-Dataset
269,Multi-Scale Point Octree Encoding Network for Point Cloud Based Place Recognition,Z. Tang; H. Ye; H. Zhang,"Department of Electrical and Electronic Engineering, Shenzhen Key Laboratory of Robotics and Computer Vision, Southern University of Science and Technology (SUSTech), SUSTech, Shenzhen, China; Department of Electrical and Electronic Engineering, Shenzhen Key Laboratory of Robotics and Computer Vision, Southern University of Science and Technology (SUSTech), SUSTech, Shenzhen, China; Department of Electrical and Electronic Engineering, Shenzhen Key Laboratory of Robotics and Computer Vision, Southern University of Science and Technology (SUSTech), SUSTech, Shenzhen, China",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,9191,9197,"Over the past decades, point cloud-based place recognition has garnered significant attention. This research paper presents a pioneering approach, denoted as the Multi-scale Point Octree Encoding Network (MPOE-Net), designed to acquire a discriminative global descriptor for efficient retrieval of places. The key element of the MPOE-Net is the point octree encoding module, which adeptly captures local information for each point by considering its nearest and farthest neighbors. Further enhancing local relationships, a multi-transformer network is introduced, utilizing a novel grouped offset-attention mechanism. To amalgamate the multi-scale attention maps into a comprehensive global descriptor, a multi-NetVLAD layer is incorporated. Through rigorous experimentation across diverse benchmark datasets, our proposed method unequivocally outperforms existing techniques in the realm of point cloud-based place recognition tasks, achieving state-of-the-art results. Our code is released publicly at https://github.com/Zhilong-Tang/MPOE-Net.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341943,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341943,,Point cloud compression;Codes;Octrees;Benchmark testing;Encoding;Data mining;Task analysis,,,,26,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/Zhilong-Tang/MPOE-Net,https://github.com/Zhilong-Tang/MPOE-Net
270,A Grasp Pose is All You Need: Learning Multi-Fingered Grasping with Deep Reinforcement Learning from Vision and Touch,F. Ceola; E. Maiettini; L. Rosasco; L. Natale,"Humanoid Sensing and Perception (HSP), Istituto Italiano di Tecnologia (IIT), Genoa, Italy; Humanoid Sensing and Perception (HSP), Istituto Italiano di Tecnologia (IIT), Genoa, Italy; Dipartimento di Informatica, Bioingegneria, Robotica e Ingegneria dei Sistemi (DIBRIS), Laboratory for Computational and Statistical Learning (LCSL) and Machine Learning Genoa Center (MaLGa), University of Genoa, Genoa, Italy; Humanoid Sensing and Perception (HSP), Istituto Italiano di Tecnologia (IIT), Genoa, Italy",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,2985,2992,"Multi-fingered robotic hands have potential to enable robots to perform sophisticated manipulation tasks. However, teaching a robot to grasp objects with an anthropomorphic hand is an arduous problem due to the high dimensionality of state and action spaces. Deep Reinforcement Learning (DRL) offers techniques to design control policies for this kind of problems without explicit environment or hand modeling. However, state-of-the-art model-free algorithms have proven inefficient for learning such policies. The main problem is that the exploration of the environment is unfeasible for such high-dimensional problems, thus hampering the initial phases of policy optimization. One possibility to address this is to rely on off-line task demonstrations, but, oftentimes, this is too demanding in terms of time and computational resources. To address these problems, we propose the A Grasp Pose is All You Need (G-PAYN) method for the anthropomorphic hand of the iCub humanoid. We develop an approach to automatically collect task demonstrations to initialize the training of the policy. The proposed grasping pipeline starts from a grasp pose generated by an external algorithm, used to initiate the movement. Then a control policy (previously trained with the proposed G-PAYN) is used to reach and grab the object. We deployed the iCub into the MuJoCo simulator and use it to test our approach with objects from the YCB-Video dataset. Results show that G-PAYN outperforms current DRL techniques in the considered setting in terms of success rate and execution time with respect to the baselines. The code to reproduce the experiments is released together with the paper with an open source license11https://github.com/hsp-iit/rl-icub-dexterous-manipulation.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341776,"European Research Council(grant numbers:SLING 819789); AFOSR projects(grant numbers:FA9550-18-1-7009,FA9550-17-1-0390,BAA-AFRL-AFOSR-2016-0007); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341776,,Training;Deep learning;Visualization;Pipelines;Propioception;Grasping;Reinforcement learning,,3.0,,37,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/hsp-iit/rl-icub-dexterous-manipulation,https://github.com/hsp-iit/rl-icub-dexterous-manipulation
271,Recurrent Macro Actions Generator for POMDP Planning,Y. Liang; H. Kurniawati,"School of Computing, Australian National University; School of Computing, Australian National University",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,2026,2033,"Many planning problems in robotics require long planning horizon and uncertain in nature. The Par-tially Observable Markov Descision Process (POMDP) is a mathematically principled framework for planning under uncertainty. To alleviate the difficulties of computing good approximate POMDP solutions for long horizon problems, one often plans using macro actions, where each macro action is a chain of primitive actions. Such a strategy reduces the effective planning horizon of the problem, and hence reduces the computational complexity for solving. The difficulty is in generating a set of suitable macro actions. In this paper, we present a simple recurrent neural network that learns to generate suitable sets of candidate macro actions that exploits environment information. Key to this learning method is to represent the raw partial information from the environment as a latent problem instance, and sequentially generate macro actions conditioned on the past information. We compare our proposed method with state-of-the-art [1] on four dif-ferent long horizon planning tasks with various difficulties. The results indicate the quality of the policies computed using macro actions generated by our proposed method consistently exceeds benchmarks. Our implementation can be accessed at https://github.com/YC-Liang/Recurrent-Macro-Action-Generator.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341759,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341759,,Learning systems;Uncertainty;Recurrent neural networks;Scalability;Computer architecture;Markov processes;Generators,,,,34,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/YC-Liang/Recurrent-Macro-Action-Generator,https://github.com/YC-Liang/Recurrent-Macro-Action-Generator
272,Adaptive PD Control Using Deep Reinforcement Learning for Local-Remote Teleoperation with Stochastic Time Delays,L. McCutcheon; S. Fallah,"Department of Mechanical Engineering Sciences, CAV-Lab, University of Surrey; Department of Mechanical Engineering Sciences, CAV-Lab, University of Surrey",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,7046,7053,"Local-remote systems allow robots to execute complex tasks in hazardous environments such as space and nuclear power stations. However, establishing accurate positional mapping between local and remote devices can be difficult due to time delays that can compromise system performance and stability. Enhancing the synchronicity and stability of localremote systems is vital for enabling robots to interact with environments at greater distances and under highly challenging network conditions, including time delays. We introduce an adaptive control method employing reinforcement learning to tackle the time-delayed control problem. By adjusting controller parameters in real-time, this adaptive controller compensates for stochastic delays and improves synchronicity between local and remote robotic manipulators. To improve the adaptive PD controller's performance, we devise a model-based reinforcement learning approach that effectively incorporates multi-step delays into the learning framework. Utilizing this proposed technique, the local-remote system's performance is stabilized for stochastic communication time-delays of up to 290ms. Our results demonstrate that the suggested model-based reinforcement learning method surpasses the Soft-Actor Critic and augmented state Soft-Actor Critic techniques. Access the code at: https://github.com/CAV-Research-Lab/Predictive-Model-Delay-Correction",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341953,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341953,,Adaptation models;Delay effects;System performance;Computational modeling;Stochastic processes;Reinforcement learning;Stability analysis,,,,30,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/CAV-Research-Lab/Predictive-Model-Delay-Correction,https://github.com/CAV-Research-Lab/Predictive-Model-Delay-Correction
273,Learning-Based Real-Time Torque Prediction for Grasping Unknown Objects with a Multi-Fingered Hand,D. Winkelbauer; B. Bäuml; R. Triebel,"DLR Institute of Robotics & Mechatronics, Germany; DLR Institute of Robotics & Mechatronics, Germany; DLR Institute of Robotics & Mechatronics, Germany",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,2979,2984,"When grasping objects with a multi-finger hand, it is crucial for the grasp stability to apply the correct torques at each joint so that external forces are countered. Most current systems use simple heuristics instead of modeling the required torque correctly. Instead, we propose a learning-based approach that is able to predict torques for grasps on unknown objects in real-time. The neural network, trained end-to-end using supervised learning, is shown to predict torques that are more efficient, and the objects are held with less involuntary movement compared to all tested heuristic baselines. Specifically, for 90 % of the grasps the translational deviation of the object is below 2.9 mm and the rotational below 3.1°. To generate training data, we formulate the analytical computation of torques as an optimization problem and handle the indeterminacy of multi-contacts using an elastic model. We further show that the network generalizes to predict torques for unknown objects on the real robot system with an inference time of 1.5 ms. Website: dlr-alr.github.io/grasping/",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341970,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341970,,Analytical models;Torque;Computational modeling;Neural networks;Supervised learning;Training data;Grasping,,1.0,,25,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://dlr-alr.github.io/grasping,
274,A Game-Theoretic Framework for Joint Forecasting and Planning,K. Kedia; P. Dan; S. Choudhury,"Cornell University, Ithaca, New York, USA; Cornell University, Ithaca, New York, USA; Cornell University, Ithaca, New York, USA",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,6773,6778,"Planning safe robot motions in the presence of humans requires reliable forecasts of future human motion. However, simply predicting the most likely motion from prior interactions does not guarantee safety. Such forecasts fail to model the long tail of possible events, which are rarely observed in limited datasets. On the other hand, planning for worst-case motions leads to overtly conservative behavior and a “frozen robot”. Instead, we aim to learn forecasts that predict counterfactuals that humans guard against. We propose a novel game-theoretic framework for joint planning and forecasting with the payoff being the performance of the planner against the demonstrator, and present practical algorithms to train models in an end-to-end fashion. We demonstrate that our proposed algorithm results in safer plans in a crowd navigation simulator and real-world datasets of pedestrian motion. We release our code at https://github.com/portal-cornell/Game-Theoretic-Forecasting-Planning.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341265,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341265,,Robot motion;Pedestrians;Navigation;Tail;Predictive models;Prediction algorithms;Planning,,1.0,,25,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/portal-cornell/Game-Theoretic-Forecasting-Planning,https://github.com/portal-cornell/Game-Theoretic-Forecasting-Planning
275,SMART-Degradation: A Dataset for LiDAR Degradation Evaluation in Rain,C. Zhang; Z. Huang; B. X. L. Tung; M. H. Ang; D. Rus,"National University of Singapore, Singapore; National University of Singapore, Singapore; National University of Singapore, Singapore; National University of Singapore, Singapore; Massachusetts Institute of Technology, Cambridge, MA, USA",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,7400,7406,"Sensor degradation is one of the major challenges for autonomous driving. During the rain, the interference from raindrops can negatively influence LiDAR measurements. For example, valid measurements could be reduced during the rain, and some measurements may become noisy. Unreliable measurements can lead to potential safety issues if autonomous driving systems are unaware of these changes. In this work, we will release a naturalistic driving dataset to advance the research in studying LiDAR degradation. Our dataset consists of 3D LiDAR scans collected by a data collection vehicle under various rainy conditions. Besides these raw scans, we also release LiDAR scan pairs (each pair consists of one scan from rainy weather and one scan from clear weather at the same location). These LiDAR pairs are developed to help researchers identify LiDAR degradation. Finally, we will release a toolbox integrated with mapping, localization, and scan synthesis functions used to create this dataset. This toolbox can facilitate dataset creation for studying degradation in other harsh weather conditions. More information can be found at https://smart-rain-dataset.github.io/.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342323,National Research Foundation; Singapore-MIT Alliance for Research and Technology (SMART); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342323,,Degradation;Location awareness;Laser radar;Rain;Three-dimensional displays;Safety;Noise measurement,,2.0,,31,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,,
276,Unsupervised OmniMVS: Efficient Omnidirectional Depth Inference via Establishing Pseudo-Stereo Supervision,Z. Chen; C. Lin; L. Nie; K. Liao; Y. Zhao,"Institute of Information Science, Beijing Jiaotong University, Beijing, China; Institute of Information Science, Beijing Jiaotong University, Beijing, China; Institute of Information Science, Beijing Jiaotong University, Beijing, China; Institute of Information Science, Beijing Jiaotong University, Beijing, China; Institute of Information Science, Beijing Jiaotong University, Beijing, China",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,10873,10879,"Omnidirectional multi-view stereo (MVS) vision is attractive for its ultra-wide field-of-view (FoV), enabling machines to perceive 360°3D surroundings. However, the existing solutions require expensive dense depth labels for supervision, making them impractical in real-world applications. In this paper, we propose the first unsupervised omnidirectional MVS framework based on multiple fisheye images. To this end, we project all images to a virtual view center and composite two panoramic images with spherical geometry from two pairs of back-to-back fisheye images. The two 360° images formulate a stereo pair with a special pose, and the photometric consistency is leveraged to establish the unsupervised constraint, which we term “Pseudo-Stereo Supervision”. In addition, we propose Un-OmniMVS, an efficient unsupervised omnidirectional MVS network, to facilitate the inference speed with two efficient components. First, a novel feature extractor with frequency attention is proposed to simultaneously capture the non-local Fourier features and local spatial features, explicitly facilitating the feature representation. Then, a variance-based light cost volume is put forward to reduce the computational complexity. Experiments exhibit that the performance of our unsupervised solution is competitive to that of the state-of-the-art (SoTA) supervised methods with better generalization in real-world data. The code will be available at https://github.com/Chen-z-s/Un-OmniMVS.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342332,"National Natural Science Foundation of China(grant numbers:62172032,62120106009); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342332,,Geometry;Costs;Codes;Feature extraction;Cameras;Computational complexity;Intelligent robots,,2.0,,32,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/Chen-z-s/Un-OmniMVS,https://github.com/Chen-z-s/Un-OmniMVS
277,Autonomous Exploration and Mapping for Mobile Robots via Cumulative Curriculum Reinforcement Learning,Z. Li; J. Xin; N. Li,"Department of Automation, Shanghai Jiao Tong University, Shanghai, P.R. China; Ministry of Education of China, Key Laboratory of System Control and Information Processing, Shanghai, China; Shanghai Engi-neering Research Center of Intelligent Control and Management, Shanghai, China",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,7495,7500,"Deep reinforcement learning (DRL) has been widely applied in autonomous exploration and mapping tasks, but often struggles with the challenges of sampling efficiency, poor adaptability to unknown map sizes, and slow simulation speed. To speed up convergence, we combine curriculum learning (CL) with DRL, and first propose a Cumulative Curriculum Reinforcement Learning (CCRL) training framework to alleviate the issue of catastrophic forgetting faced by general CL. Besides, we present a novel state representation, which considers a local egocentric map and a global exploration map resized to the fixed dimension, so as to flexibly adapt to environments with various sizes and shapes. Additionally, for facilitating the fast training of DRL models, we develop a lightweight grid-based simulator, which can substantially accelerate simulation compared to popular robot simulation platforms such as Gazebo. Based on the customized simulator, comprehensive experiments have been conducted, and the results show that the CCRL framework not only mitigates the catastrophic forgetting problem, but also improves the sample efficiency and generalization of DRL models, compared to general CL as well as without a curriculum. Our code is available at https://github.com/BeamanLi/CCRL_Exploration.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342066,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342066,,Training;Deep learning;Adaptation models;Codes;Shape;Reinforcement learning;Mobile robots;Task analysis;Intelligent robots;Convergence,,1.0,,31,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/BeamanLi/CCRL_Exploration,https://github.com/BeamanLi/CCRL_Exploration
278,Task-Oriented Grasping with Point Cloud Representation of Objects,A. Patankar; K. Phi; D. Mahalingam; N. Chakraborty; I. Ramakrishnan,"Department of Mechanical Engineering, Stony Brook University, USA; Department of Computer Science, Stony Brook University, USA; Department of Mechanical Engineering, Stony Brook University, USA; Department of Mechanical Engineering, Stony Brook University, USA; Department of Computer Science, Stony Brook University, USA",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,6853,6860,"In this paper, we study the problem of task-oriented grasp synthesis from partial point cloud data using an eye-in-hand camera configuration. In task-oriented grasp synthesis, a grasp has to be selected so that the object is not lost during manipulation, and it is also ensured that adequate force/moment can be applied to perform the task. We formalize the notion of a gross manipulation task as a constant screw motion (or a sequence of constant screw motions) to be applied to the object after grasping. Using this notion of task, and a corresponding grasp quality metric developed in our prior work, we use a neural network to approximate a function for predicting the grasp quality metric on a cuboid shape. We show that by using a bounding box obtained from the partial point cloud of an object, and the grasp quality metric mentioned above, we can generate a good grasping region on the bounding box that can be used to compute an antipodal grasp on the actual object. Our algorithm does not use any manually labeled data or grasping simulator, thus making it very efficient to implement and integrate with screw linear interpolation-based motion planners. We present simulation as well as experimental results that show the effectiveness of our approach. Website: https://irsl-sbu.github.io/Task-Oriented-Grasping-from-Point-Cloud-Representation/.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342318,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342318,,Point cloud compression;Measurement;Shape;Neural networks;Grasping;Fasteners;Cameras,,1.0,,36,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://irsl-sbu.github.io/Task-Oriented-Grasping-from-Point-Cloud-Representation,https://github.com/irsl-sbu/Task-Oriented-Grasping-from-Point-Cloud-Representation
279,VDBblox: Accurate and Efficient Distance Fields for Path Planning and Mesh Reconstruction,Y. Bai; Z. Miao; X. Wang; Y. Liu; H. Wang; Y. Wang,"Department of Electrical and Information Engineering, Hunan University, Changsha, China; Department of Electrical and Information Engineering, Hunan University, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Electrical and Information Engineering, Hunan University, Changsha, China",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,7187,7194,"Highly accurate and efficient map in unknown and complex environments is essential for robotics navigation. Traditionally, mobile robot platforms are often computationally constrained when using multiple sensors to process large amounts of input data. In previous works, some of them have been deployed to embedded platforms in real-time. However, how to balance accuracy and efficiency while reducing the computational resources and the memory footprint is still the bottleneck. Motivated by these challenges, we proposed a mapping framework called VDBblox to incrementally build Euclidean Signed Distance Fields (ESDFs) map from Truncated Signed Distance Fields (TSDFs) mapping. We use a novel weight function to update the non-projective TSDFs, thus improving the quality of the mesh reconstruction with higher accuracy than up-to-date methods. Meanwhile, the generated ESDFs map is maintained by the least recently used (LRU) cache to dynamically handle the obstacle changes with less runtime than state-of-the-art. We show VDBblox performance in terms of accuracy and efficiency by benchmark comparison on RGB-D and LiDAR public datasets. Moreover, we demonstrate that VDBblox can be integrated into a completed quadrotor system as a sub-module. Then we validate it through online obstacle avoidance and high-quality mesh reconstruction in real-world experiments. Finally, we release our method as open-source code to the community11Code - https://github.com/yinloonga/vdbblox.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342123,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342123,,Runtime;Laser radar;Navigation;Memory management;Robot sensing systems;Real-time systems;Path planning,,2.0,,30,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/yinloonga/vdbblox,https://github.com/yinloonga/vdbblox
280,InsMOS: Instance-Aware Moving Object Segmentation in LiDAR Data,N. Wang; C. Shi; R. Guo; H. Lu; Z. Zheng; X. Chen,"College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,7598,7605,"Identifying moving objects is a crucial capability for autonomous navigation, consistent map generation, and future trajectory prediction of objects. In this paper, we propose a novel network that addresses the challenge of segmenting moving objects in 3D LiDAR scans. Our approach not only predicts point-wise moving labels but also detects instance information of main traffic participants. Such a design helps determine which instances are actually moving and which ones are temporarily static in the current scene. Our method exploits a sequence of point clouds as input and quantifies them into 4D voxels. We use 4D sparse convolutions to extract motion features from the 4D voxels and inject them into the current scan. Then, we extract spatio-temporal features from the current scan for instance detection and feature fusion. Finally, we design an upsample fusion module to output point-wise labels by fusing the spatio-temporal features and predicted instance information. We evaluated our approach on the LiDAR-MOS benchmark based on SemanticKITTI and achieved better moving object segmentation performance compared to state-of-the-art methods, demonstrating the effectiveness of our approach in integrating instance information for moving object segmentation. Furthermore, our method shows superior performance on the Apollo dataset with a pre-trained model on SemanticKITTI, indicating that our method generalizes well in different scenes. The code and pre-trained models of our method will be released at https://github.com/nubot-nudt/InsMOS.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342277,"National Science Foundation of China(grant numbers:U1913202,U22A2059,62203460); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342277,,Point cloud compression;Laser radar;Three-dimensional displays;Motion segmentation;Object segmentation;Feature extraction;Prediction algorithms,,16.0,,38,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/nubot-nudt/InsMOS,https://github.com/nubot-nudt/InsMOS
281,Revisiting Deformable Convolution for Depth Completion,X. Sun; J. Ponce; Y. -X. Wang,"Department of Computer Science, Stanford University; Computer Science Department of Ecole normale superieure (ENS-PSLCNRS, Inria), Global Distinguished Professor in the Courant Institute and the Center for Data Science at New York University; Department of Computer Science, University of Illinois Urbana-Champaign",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,1300,1306,"Depth completion, which aims to generate high-quality dense depth maps from sparse depth maps, has attracted increasing attention in recent years. Previous work usually employs RGB images as guidance, and introduces iterative spatial propagation to refine estimated coarse depth maps. However, most of the propagation refinement methods require several iterations and suffer from a fixed receptive field, which may contain irrelevant and useless information with very sparse input. In this paper, we address these two challenges simultaneously by revisiting the idea of deformable convolution. We propose an effective architecture that leverages deformable kernel convolution as a single-pass refinement module, and empirically demonstrate its superiority. To better understand the function of deformable convolution and exploit it for depth completion, we further systematically investigate a variety of representative strategies. Our study reveals that, different from prior work, deformable convolution needs to be applied on an estimated depth map with a relatively high density for better performance. We evaluate our model on the large-scale KITTI dataset and achieve state-of-the-art level performance in both accuracy and inference speed. Our code is available at https://github.com/AlexSunNiklReDC.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342026,NSF(grant numbers:2106825); NIFA(grant numbers:2020-67021-32799); French government under management of Agence Nationale de la Recherche as part of the Investissements d'avenir program(grant numbers:ANR19-P3IA0001); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342026,,Deformable models;Codes;Convolution;Iterative methods;Kernel;Intelligent robots,,,,57,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/AlexSunNiklReDC,https://github.com/AlexSunNiklReDC
282,Exploring Kinodynamic Fabrics for Reactive Whole-Body Control of Underactuated Humanoid Robots,A. Adu-Bredu; G. Gibson; J. Grizzle,"Robotics Department, University of Michigan, Ann Arbor, MI, USA; Robotics Department, University of Michigan, Ann Arbor, MI, USA; Robotics Department, University of Michigan, Ann Arbor, MI, USA",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,10397,10404,"For bipedal humanoid robots to successfully operate in the real world, they must be competent at simultaneously executing multiple motion tasks while reacting to unforeseen external disturbances in real-time. We propose Kinodynamic Fabrics as an approach for the specification, solution and simultaneous execution of multiple motion tasks in real-time while being reactive to dynamism in the environment. Kinodynamic Fabrics allows for the specification of prioritized motion tasks as forced spectral semi-sprays and solves for desired robot joint accelerations at real-time frequencies. We evaluate the capabilities of Kinodynamic fabrics on diverse physically-challenging whole-body control tasks with a bipedal humanoid robot both in simulation and in the real-world. Kinodynamic Fabrics outperforms the state-of-the-art Quadratic Program based whole-body controller on a variety of whole-body control tasks on run-time and reactivity metrics in our experiments. Our open-source implementation of Kinodynamic Fabrics as well as robot demonstration videos can be found at this url: https://adubredu.github.io/kinofabs",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342091,Toyota Research Institute; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342091,,Measurement;Humanoid robots;Fabrics;Real-time systems;Task analysis;Manipulator dynamics;Intelligent robots,,4.0,,25,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://adubredu.github.io/kinofabs,https://github.com/adubredu/KinodynamicFabrics.jl
283,Online Adaptive Disparity Estimation for Dynamic Scenes in Structured Light Systems,R. Qiao; H. Kawasaki; H. Zha,"Key Lab of Machine Perception (MOE), School of Intelligence Science and Technology, Peking University, Beijing, China; Graduate School and Faculty of Information Science and Electrical Engineering, Kyushu University, Fukuoka, Japan; Key Lab of Machine Perception (MOE), School of Intelligence Science and Technology, Peking University, Beijing, China",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,11186,11193,"In recent years, deep neural networks have shown remarkable progress in dense disparity estimation from dynamic scenes in monocular structured light systems. However, their performance significantly drops when applied in unseen environments. To address this issue, self-supervised online adaptation has been proposed as a solution to bridge this performance gap. Unlike traditional fine-tuning processes, online adaptation performs test-time optimization to adapt networks to new domains. Therefore, achieving fast convergence during the adaptation process is critical for attaining satisfactory accuracy. In this paper, we propose an unsupervised loss function based on long sequential inputs. It ensures better gradient directions and faster convergence. Our loss function is designed using a multi-frame pattern flow, which comprises a set of sparse trajectories of the projected pattern along the sequence. We estimate the sparse pseudo ground truth with a confidence mask using a filter-based method, which guides the online adaptation process. Our proposed framework significantly improves the online adaptation speed and achieves superior performance on unseen data. The code is available on https://github.com/CodePointer/TIDENet.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342000,National Natural Science Foundation of China(grant numbers:62176010); Joint Funds of the National Natural Science Foundation of China(grant numbers:U22A2061); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342000,,Bridges;Codes;Estimation;Computer architecture;Artificial neural networks;Trajectory;Safety,,,,29,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/CodePointer/TIDENet,https://github.com/CodePointer/TIDENet
284,Weakly Supervised Referring Expression Grounding via Dynamic Self-Knowledge Distillation,J. Mi; Z. Chen; J. Zhang,"Institute of Machine Intelligence (IMI), University of Shanghai for Science and Technology, China; Institute of Machine Intelligence (IMI), University of Shanghai for Science and Technology, China; Department of Informatics, Technical Aspects of Multimodal Systems (TAMS), University of Hamburg, Germany",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,1254,1260,"Weakly supervised referring expression grounding (WREG) is an attractive and challenging task for grounding target regions in images by understanding given referring expressions. WREG learns to ground target objects without the manual annotations between image regions and referring expressions during the model training phase. Different from the predominant grounding pattern of existing models, which locates target objects by reconstructing the region-expression correspondence, we investigate WREG from a novel perspective and enrich the prevailing pattern with self-knowledge distillation. Specifically, we propose a target-guided self-knowledge distillation approach that adopts the target prediction knowledge learned from the previous training iterations as the teacher to guide the subsequent training procedure. In order to avoid the misleading caused by the teacher knowledge with low prediction confidence, we present an uncertaintyaware knowledge refinement strategy to adaptively rectify the teacher knowledge by learning dynamic threshold values based on the model prediction uncertainty. To validate the proposed approach, we implement extensive experiments on three benchmark datasets, i.e., Ref Coco, RefCOCO+, and RefCOCOg. Our approach achieves new state-of-the-art results on several splits of the benchmark datasets, showcasing the advantage of the proposed framework for WREG. The implementation codes and trained models are available at: https://github.com/dami23IWREG.sar_KD.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341909,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341909,,Training;Uncertainty;Codes;Grounding;Manuals;Benchmark testing;Predictive models,,1.0,,43,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/dami23IWREG,https://github.com/dami23IWREG
285,From Temporal-Evolving to Spatial-Fixing: A Keypoints-Based Learning Paradigm for Visual Robotic Manipulation,K. Riou; K. Dong; K. Subrin; Y. Sun; P. L. Callet,"Ecole Centrale Nantes, CNRS, LS2N, UMR 6004, Nantes Université, Nantes, France; School of Information and Control Engineering, China University of Mining and Technology, Xuzhou, China; Ecole Centrale Nantes, CNRS, LS2N, UMR 6004, Nantes Université, Nantes, France; School of Information and Control Engineering, China University of Mining and Technology, Xuzhou, China; Ecole Centrale Nantes, CNRS, LS2N, UMR 6004, Nantes Université, Nantes, France",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,1728,1734,"The current learning pipelines for robotics manipulation infer movement primitives sequentially along the temporal-evolving axis, which can result in an accumulation of prediction errors and subsequently cause the visual observations to fall out of the training distribution. This paper proposes a novel hierarchical behavior cloning approach which tries to dissociate standard behaviour cloning (BC) pipeline to two stages. The intuition of this approach is to eliminate accumu-lation errors using a fixed spatial representation. At first stage, a high-level planner will be employed to translate the initial observation of the scene into task-specific spatial waypoints. Then, a low-level robotic path planner takes over the task of guiding the robot by executing a set of pre-defined elementary movements or actions known as primitives, with the goal of reaching the previously predicted waypoints. Our hierarchical keypoints-based paradigm aims to simplify existing temporal-evolving approach to a more simple way: directly spatialize the whole sequential primitives as a set of 8D waypoints only from the very first observation. Plentiful experiments demon-strate that our paradigm can achieve comparable results with Reinforcement Learning (RL) and outperforms existing offline BC approaches, with only a single-shot inference from the initial observation. Code and models are available at: https://github.com/KevinRiou22/spatial-fixing-il",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341397,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341397,,Training;Visualization;Pipelines;Cloning;Reinforcement learning;Behavioral sciences;Trajectory,,,,26,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/KevinRiou22/spatial-fixing-il,https://github.com/KevinRiou22/spatial-fixing-il
286,LiDAR Meta Depth Completion,W. Boettcher; L. Hoyer; O. Unal; K. Li; D. Dai,"ETH Zurich, Switzerland; ETH Zurich, Switzerland; ETH Zurich, Switzerland; ETH Zurich, Switzerland; Huawei Technologies, Zurich Research Center",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,7750,7756,"Depth estimation is one of the essential tasks to be addressed when creating mobile autonomous systems. While monocular depth estimation methods have improved in recent times, depth completion provides more accurate and reliable depth maps by additionally using sparse depth information from other sensors such as LiDAR. However, current methods are specifically trained for a single LiDAR sensor. As the scanning pattern differs between sensors, every new sensor would require re- training a specialized depth completion model, which is computationally inefficient and not flexible. Therefore, we propose to dynamically adapt the depth completion model to the used sensor type enabling LiDAR adaptive depth completion. Specifically, we propose a meta depth completion network that uses data patterns derived from the data to learn a task network to alter weights of the main depth completion network to solve a given depth completion task effectively. The method demonstrates a strong capability to work on multiple LiDAR scanning patterns and can also generalize to scanning patterns that are unseen during training. While using a single model, our method yields significantly better results than a non-adaptive baseline trained on different LiDAR patterns. It outperforms LiDAR-specific expert models for very sparse cases. These advantages allow flexible deployment of a single depth completion model on different sensors, which could also prove valuable to process the input of nascent LiDAR technology with adaptive instead of fixed scanning patterns. The source code is available at github.com/wbkit/ResLAN",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341349,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341349,,Training;Adaptation models;Laser radar;Computational modeling;Source coding;Estimation;Sensors,,,,47,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/wbkit/ResLAN,https://github.com/wbkit/ResLAN
287,SMART-Rain: A Degradation Evaluation Dataset for Autonomous Driving in Rain,C. Zhang; Z. Huang; H. Guo; L. Qin; M. H. Ang; D. Rus,"National University of Singapore, Singapore; Singapore-MIT Alliance for Research and Technology, Singapore; Singapore-MIT Alliance for Research and Technology, Singapore; Singapore-MIT Alliance for Research and Technology, Singapore; National University of Singapore, Singapore; Massachusetts Institute of Technology, Cambridge, MA, USA",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,9691,9698,"Autonomous driving in the rain remains a challenge. One main problem is performance degradation caused by rain. This work introduces a new dataset to study this problem. Our dataset is collected from a full-scale vehicle equipped with a 3D LiDAR sensor and multiple forward-facing cameras under various rainy conditions. In addition, rainfall intensity is recorded in real-time from a rain sensor. The combination of sensor and rainfall intensity measurement is designed for studying algorithm performance under different levels of rainfall. In this work, in addition to presenting dataset creation details, we also introduce three degradation evaluation tasks with baseline results, including rainfall intensity estimation, LiDAR degradation estimation, and 2D object detection evaluation. This dataset, development kit, and baseline codes will be made available at https://smart-rain-dataset.github.io/",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342015,"National Research Foundation, Prime Minister's Office, Singapore, under its CREATE programme; Singapore-MIT Alliance for Research and Technology (SMART); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342015,,Degradation;Rain;Laser radar;Three-dimensional displays;Urban areas;Estimation;Robot sensing systems,,,,39,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,,
288,SSC-RS: Elevate LiDAR Semantic Scene Completion with Representation Separation and BEV Fusion,J. Mei; Y. Yang; M. Wang; T. Huang; X. Yang; Y. Liu,"Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Shanghai Artificial Intelligence Laboratory, Shanghai, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,1,8,"Semantic scene completion (SSC) jointly predicts the semantics and geometry of the entire 3D scene, which plays an essential role in 3D scene understanding for autonomous driving systems. SSC has achieved rapid progress with the help of semantic context in segmentation. However, how to effectively exploit the relationships between the semantic context in semantic segmentation and geometric structure in scene completion remains under exploration. In this paper, we propose to solve outdoor SSC from the perspective of representation separation and BEV fusion. Specifically, we present the network, named SSC-RS, which uses separate branches with deep supervision to explicitly disentangle the learning procedure of the semantic and geometric representations. And a BEV fusion network equipped with the proposed Adaptive Representation Fusion (ARF) module is presented to aggregate the multi-scale features effectively and efficiently. Due to the low computational burden and powerful representation ability, our model has good generality while running in real-time. Extensive experiments on SemanticKITTI demonstrate our SSC-RS achieves state-of-the-art performance. Code is available at https://github.com/Jieqianyu/SSC-RS.git.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341742,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341742,,Geometry;Visualization;Three-dimensional displays;Adaptive systems;Laser radar;Fuses;Semantic segmentation,,9.0,,41,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/Jieqianyu/SSC-RS,https://github.com/Jieqianyu/SSC-RS
289,RFDNet: Real-Time 3D Object Detection Via Range Feature Decoration,H. Chang; L. Wang; J. Cheng,"Northeastern University, Shenyang, China; Northeastern University, Shenyang, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,5715,5721,"High-performance real-time 3D object detection is crucial in autonomous driving perception systems. Voxel-or point-based 3D object detectors are highly accurate but inefficient and difficult to deploy, while other methods use 2D projection views to improve efficiency, but information loss usually degrades performance. To balance effectiveness and efficiency, we propose a scheme called RFDNet that uses range features to decorate points. Specifically, RFDNet adaptively aggregates point features projected to independent grids and nearby regions via Dilated Grid Feature Encoding (DGFE) to generate a range view, which can handle occlusion and multi-frame inputs while the established geometric correlation between grid with surrounding space weakens the effects of scale distortion. We also propose a Soft Box Regression (SBR) strategy that supervises 3D box regression on a more extensive range than conventional methods to enhance model robustness. In addition, RFDNet benefits from our designed Semantic-assisted Ground-truth Sample (SA-GTS) data augmentation, which additionally considers collisions and spatial distributions of objects. Experiments on the nuScenes benchmark show that RFDNet outperforms all LiDAR-only non-ensemble 3D object detectors and runs at high speed of 20 FPS, achieving a better effectiveness-efficiency trade-off. Code is available at https://github.com/wy17646051/RFDNet.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341482,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341482,,Solid modeling;Three-dimensional displays;Graphical models;Detectors;Object detection;Feature extraction;Real-time systems,,,,32,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/wy17646051/RFDNet,https://github.com/wy17646051/RFDNet
290,Generating Executable Action Plans with Environmentally-Aware Language Models,M. Gramopadhye; D. Szafir,"University of North Carolina at Chapel Hill, United States; University of North Carolina at Chapel Hill, United States",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,3568,3575,"Large Language Models (LLMs) trained using massive text datasets have recently shown promise in generating action plans for robotic agents from high-level text queries. However, these models typically do not consider the robot's environment, resulting in generated plans that may not actually be executable, due to ambiguities in the planned actions or environmental constraints. In this paper, we propose an approach to generate environmentally-aware action plans that agents are better able to execute. Our approach involves integrating environmental objects and object relations as additional inputs into LLM action plan generation to provide the system with an awareness of its surroundings, resulting in plans where each generated action is mapped to objects present in the scene. We also design a novel scoring function that, along with generating the action steps and associating them with objects, helps the system disambiguate among object instances and take into account their states. We evaluated our approach using the VirtualHome simulator and the ActivityPrograms knowledge base and found that action plans generated from our system had a 310% improvement in executability and a 147% improvement in correctness over prior work. The complete code and a demo of our method is publicly available at https://github.com/hri-ironlab/scene_aware_language_planner.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341989,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341989,,Codes;Affordances;Knowledge based systems;Cognition;Task analysis;Intelligent robots,,8.0,,70,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/hri-ironlab/scene_aware_language_planner,https://github.com/hri-ironlab/scene_aware_language_planner
291,"HANDAL: A Dataset of Real-World Manipulable Object Categories with Pose Annotations, Affordances, and Reconstructions",A. Guo; B. Wen; J. Yuan; J. Tremblay; S. Tyree; J. Smith; S. Birchfield,NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA,2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,11428,11435,"We present the HANDAL dataset for category-level object pose estimation and affordance prediction. Unlike previous datasets, ours is focused on robotics-ready manipulable objects that are of the proper size and shape for functional grasping by robot manipulators, such as pliers, utensils, and screwdrivers. Our annotation process is streamlined, requiring only a single off-the-shelf camera and semi-automated processing, allowing us to produce high-quality 3D annotations without crowd-sourcing. The dataset consists of 308k annotated image frames from 2.2k videos of 212 real-world objects in 17 categories. We focus on hardware and kitchen tool objects to facilitate research in practical scenarios in which a robot manipulator needs to interact with the environment beyond simple pushing or indiscriminate grasping. We outline the usefulness of our dataset for 6-DoF category-level pose+scale estimation and related tasks. We also provide 3D reconstructed meshes of all objects, and we outline some of the bottlenecks to be addressed for democratizing the collection of datasets like this one. Project website: https://nvlabs.github.io/HANDAL/",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341672,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341672,,Three-dimensional displays;Annotations;Shape;Affordances;Grasping;Streaming media;6-DOF,,6.0,,44,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://nvlabs.github.io/HANDAL,https://github.com/NVlabs/HANDAL
292,Quantized Distillation: Optimizing Driver Activity Recognition Models for Resource-Constrained Environments,C. Tanama; K. Peng; Z. Marinov; R. Stiefelhagen; A. Roitberg,"Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology; Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology; Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology; Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology; Institute for Artificial Intelligence, University of Stuttgart",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,5479,5486,"Deep learning-based models are at the top of most driver observation benchmarks due to their remarkable accuracies but come with a high computational cost, while the resources are often limited in real-world driving scenarios. This paper presents a lightweight framework for resource- efficient driver activity recognition. We enhance 3D MobileNet, a speed-optimized neural architecture for video classification, with two paradigms for improving the trade-off between model accuracy and computational efficiency: knowledge distillation and model quantization. Knowledge distillation prevents large drops in accuracy when reducing the model size by harvesting knowledge from a large teacher model (I3D) via soft labels instead of using the original ground truth. Quantization further drastically reduces the memory and computation requirements by representing the model weights and activations using lower precision integers. Extensive experiments on a public dataset for in-vehicle monitoring during autonomous driving show that our proposed framework leads to an 3- fold reduction in model size and 1.4-fold improvement in inference time compared to an already speed-optimized architecture. Our code is available at https://github.com/calvintanama/qd-driver-activity-reco.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342203,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342203,,Solid modeling;Quantization (signal);Three-dimensional displays;Computational modeling;Memory management;Activity recognition;Computational efficiency,,1.0,,61,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/calvintanama/qd-driver-activity-reco,https://github.com/calvintanama/qd-driver-activity-reco
293,T-UDA: Temporal Unsupervised Domain Adaptation in Sequential Point Clouds,A. H. Gebrehiwot; D. Hurych; K. Zimmermann; P. Pérez; T. Svoboda,"Faculty of Electrical Engineering, Czech Technical University, Prague, Czech Republic; Valeo.ai; Faculty of Electrical Engineering, Czech Technical University, Prague, Czech Republic; Valeo.ai; Faculty of Electrical Engineering, Czech Technical University, Prague, Czech Republic",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,7643,7650,"Deep perception models have to reliably cope with an open-world setting of domain shifts induced by different geographic regions, sensor properties, mounting positions, and several other reasons. Since covering all domains with annotated data is technically intractable due to the endless possible variations, researchers focus on unsupervised domain adaptation (UDA) methods that adapt models trained on one (source) domain with annotations available to another (target) domain for which only unannotated data are available. Current predominant methods either leverage semi-supervised approaches, e.g., teacher-student setup, or exploit privileged data, such as other sensor modalities or temporal data consistency. We introduce a novel domain adaptation method that leverages the best of both approaches. Our approach combines input data's temporal and cross-sensor geometric consistency with the mean teacher method. Dubbed T-UDA for “temporal UDA”, such a combination yields massive performance gains for the task of 3D semantic segmentation of driving scenes. Experiments are conducted on Waymo Open Dataset, nuScenes, and SemanticKITTI, for two popular 3D point cloud architectures, Cylinder3D and MinkowskiNet. Our codes are publicly available on https://github.com/ctu-vras/T-UDA.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341446,CSF Project(grant numbers:20-29531S); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341446,,Point cloud compression;Measurement;Adaptation models;Three-dimensional displays;Semantic segmentation;Performance gain;Robot sensing systems,,1.0,,34,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/ctu-vras/T-UDA,https://github.com/ctu-vras/T-UDA
294,PANet: LiDAR Panoptic Segmentation with Sparse Instance Proposal and Aggregation,J. Mei; Y. Yang; M. Wang; X. Hou; L. Li; Y. Liu,"Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,7726,7733,"Reliable LiDAR panoptic segmentation (LPS), including both semantic and instance segmentation, is vital for many robotic applications, such as autonomous driving. This work proposes a new LPS framework named PANet to eliminate the dependency on the offset branch and improve the performance on large objects, which are always over-segmented by clustering algorithms. Firstly, we propose a non-learning Sparse Instance Proposal (SIP) module with the “sampling-shifting-grouping” scheme to directly group thing points into instances from the raw point cloud efficiently. More specifically, balanced point sampling is introduced to generate sparse seed points with more uniform point distribution over the distance range. And a shift module, termed bubble shifting, is proposed to shrink the seed points to the clustered centers. Then we utilize the connected component label algorithm to generate instance proposals. Furthermore, an instance aggregation module is devised to integrate potentially fragmented instances, improving the performance of the SIP module on large objects. Extensive experiments show that PANet achieves state-of-the-art performance among published works on the SemanticKITII validation and nuScenes validation for the panoptic segmentation task. Code is available at https://github.com/Jieqianyu/PANet.git.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342468,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342468,,Point cloud compression;Training;Instance segmentation;Laser radar;Semantics;Clustering algorithms;Proposals,,,,47,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/Jieqianyu/PANet,https://github.com/Jieqianyu/PANet
295,RACECAR - The Dataset for High-Speed Autonomous Racing,A. Kulkarni; J. Chrosniak; E. Ducote; F. Sauerbeck; A. Saba; U. Chirimar; J. Link; M. Behl; M. Cellina,University of Virginia; University of Virginia; University of Virginia; Technical University of Munich; Carnegie Mellon University; University of Virginia; University of Virginia; University of Virginia; Politecnico di Milano,2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,11458,11463,"This paper describes RACECAR, the first open dataset for full-scale and high-speed autonomous racing. Multi-modal sensor data was collected from fully autonomous Indy race cars operating at speeds of up to 170 mph (273 kph). Six teams who raced in the Indy Autonomous Challenge (2021–2022) have contributed to this dataset. The dataset spans 11 racing scenarios across two race tracks which include solo laps, multi-agent laps, overtaking situations, high-accelerations, banked tracks, obstacle avoidance, pit entry and exit at different speeds. The dataset contains 27 racing sessions across 11 scenarios with over 6.5 hours of autonomous racing. The data has been released in both ROS 2 and nuScenes format. We have also developed the ROSbag2nuScenes conversion library to achieve this. The RACECAR data is unique because of the high-speed environment of autonomous racing. We present several benchmark problems on localization, object detection and tracking (LiDAR, Radar, and Camera), and mapping to explore issues that arise at the limits of operation of the vehicle. RACECAR data can be accessed at https://github.com/linklab-uva/RACECAR_DATA.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342053,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342053,,Location awareness;Laser radar;Multimodal sensors;Radar detection;Object detection;Benchmark testing;Radar tracking,,6.0,,20,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/linklab-uva/RACECAR_DATA,https://github.com/linklab-uva/RACECAR_DATA
296,BlinkFlow: A Dataset to Push the Limits of Event-Based Optical Flow Estimation,Y. Li; Z. Huang; S. Chen; X. Shi; H. Li; H. Bao; Z. Cui; G. Zhang,"State Key Lab of CAD&CG, Zhejiang University; Multimedia Laboratory, The Chinese University of Hong Kong; State Key Lab of CAD&CG, Zhejiang University; Multimedia Laboratory, The Chinese University of Hong Kong; Multimedia Laboratory, The Chinese University of Hong Kong; State Key Lab of CAD&CG, Zhejiang University; State Key Lab of CAD&CG, Zhejiang University; State Key Lab of CAD&CG, Zhejiang University",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,3881,3888,"Event cameras provide high temporal precision, low data rates, and high dynamic range visual perception, which are well-suited for optical flow estimation. While data-driven optical flow estimation has obtained great success in RGB cameras, its generalization performance is seriously hindered in event cameras mainly due to the limited and biased training data. In this paper, we present a novel simulator, BlinkSim, for the fast generation of large-scale data for event-based optical flow. BlinkSim incorporates a configurable rendering engine alongside an event simulation suite. By leveraging the wealth of current 3D assets, the rendering engine enables us to automatically build up thousands of scenes with different objects, textures, and motion patterns and render very high-frequency images for realistic event data simulation. Based on BlinkSim, we construct a large training dataset and evaluation benchmark BlinkFlow that contains sufficient, diversiform, and challenging event data with optical flow ground truth. Experiments show that BlinkFlow improves the generalization performance of state-of-the-art methods by more than 40% on average and up to 90%. Moreover, we further propose an Event-based optical Flow transFormer (E-FlowFormer) architecture. Powered by our BlinkFlow, E-FlowFormer outperforms the SOTA methods by up to 91% on the MVSEC dataset and 14% on the DSEC dataset and presents the best generalization performance. The source code and data are available at https://zju3dv.github.io/blinkflow/.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341802,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341802,,Training;Estimation;Training data;Benchmark testing;Cameras;Transformers;Rendering (computer graphics),,9.0,,44,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://zju3dv.github.io/blinkflow,https://github.com/zju3dv/blink_sim
297,Enhancing State Estimation in Robots: A Data-Driven Approach with Differentiable Ensemble Kalman Filters,X. Liu; G. Clark; J. Campbell; Y. Zhou; H. B. Amor,"SCAI, Arizona State University, USA; SCAI, Arizona State University, USA; RI at Carnegie Mellon University, USA; SCAI, Arizona State University, USA; SCAI, Arizona State University, USA",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,1947,1954,"This paper introduces a novel state estimation framework for robots using differentiable ensemble Kalman filters (DEnKF). DEnKF is a reformulation of the traditional ensemble Kalman filter that employs stochastic neural networks to model the process noise implicitly. Our work is an extension of previous research on differentiable filters, which has provided a strong foundation for our modular and end-to-end differentiable framework. This framework enables each component of the system to function independently, leading to improved flexibility and versatility in implementation. Through a series of experiments, we demonstrate the flexibility of this model across a diverse set of real-world tracking tasks, including visual odometry and robot manipulation. Moreover, we show that our model effectively handles noisy observations, is robust in the absence of observations, and outperforms state-of-the-art differentiable filters in terms of error metrics. Specifically, we observe a significant improvement of at least 59% in translational error when using DEnKF with noisy observations. Our results underscore the potential of DEnKF in advancing state estimation for robotics. Code for DEnKF is available at https://github.com/ir-lab/DEnKF",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341617,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341617,,Visualization;Uncertainty;Perturbation methods;Robot sensing systems;Kalman filters;Noise measurement;Task analysis,,4.0,,46,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/ir-lab/DEnKF,https://github.com/ir-lab/DEnKF
298,Efficient and Feasible Robotic Assembly Sequence Planning via Graph Representation Learning,M. Atad; J. Feng; I. Rodríguez; M. Durner; R. Triebel,"Department of Informatics, Technical University of Munich, Garching, Germany; German Aerospace Center (DLR), Institute of Robotics and Mechatronics, Wessling, Germany; German Aerospace Center (DLR), Institute of Robotics and Mechatronics, Wessling, Germany; German Aerospace Center (DLR), Institute of Robotics and Mechatronics, Wessling, Germany; German Aerospace Center (DLR), Institute of Robotics and Mechatronics, Wessling, Germany",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,8262,8269,"Automatic Robotic Assembly Sequence Planning (RASP) can significantly improve productivity and resilience in modern manufacturing along with the growing need for greater product customization. One of the main challenges in realizing such automation resides in efficiently finding solutions from a growing number of potential sequences for increasingly complex assemblies. Besides, costly feasibility checks are always required for the robotic system. To address this, we propose a holistic graphical approach including a graph representation called Assembly Graph for product assemblies and a policy architecture, Graph Assembly Processing Network, dubbed GRACE for assembly sequence generation. With GRACE, we are able to extract meaningful information from the graph input and predict assembly sequences in a step-by-step manner. In experiments, we show that our approach can predict feasible assembly sequences across product variants of aluminum profiles based on data collected in simulation of a dual-armed robotic system. We further demonstrate that our method is capable of detecting infeasible assemblies, substantially alleviating the undesirable impacts from false predictions, and hence facilitating real-world deployment soon. Code and training data are available at https://github.com/DLR-RM/GRACE.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342352,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342352,,Robotic assembly;Representation learning;Three-dimensional displays;Aluminum;Training data;Solids;Planning,,5.0,,36,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/DLR-RM/GRACE,https://github.com/DLR-RM/GRACE
299,Stackelberg Meta-Learning for Strategic Guidance in Multi-Robot Trajectory Planning,Y. Zhao; Q. Zhu,"Department of Electrical and Computer Engineering, New York University, Brooklyn, NY, USA; Department of Electrical and Computer Engineering, New York University, Brooklyn, NY, USA",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,11342,11347,"Trajectory guidance requires a leader robotic agent to assist a follower robotic agent to cooperatively reach the target destination. However, planning cooperation becomes difficult when the leader serves a family of different followers and has incomplete information about the followers. There is a need for learning and fast adaptation of different cooperation plans. We develop a Stackelberg meta-learning approach to address this challenge. We first formulate the guided trajectory planning problem as a dynamic Stackelberg game to capture the leader-follower interactions. Then, we leverage meta-learning to develop cooperative strategies for different followers. The leader learns a meta-best-response model from a prescribed set of followers. When a specific follower initiates a guidance query, the leader quickly adapts to the follower-specific model with a small amount of learning data and uses it to perform trajectory guidance. We use simulations to elaborate that our method provides a better generalization and adaptation per-formance on learning followers' behavior than other learning approaches. The value and the effectiveness of guidance are also demonstrated by the comparison with zero guidance scenarios11The simulation codes are available at https://github.com/yuhan16/Stackelberg-Meta-Learning..",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342202,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342202,,Metalearning;Adaptation models;Trajectory planning;Human-robot interaction;Games;Trajectory;Planning,,3.0,,25,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/yuhan16/Stackelberg-Meta-Learning,https://github.com/yuhan16/Stackelberg-Meta-Learning
300,LIO-PPF: Fast LiDAR-Inertial Odometry via Incremental Plane Pre-Fitting and Skeleton Tracking,X. Chen; P. Wu; G. Li; T. H. Li,"School of Electronic and Computer Engineering, Peking University, China; School of Electronic and Computer Engineering, Peking University, China; School of Electronic and Computer Engineering, Peking University, China; School of Electronic and Computer Engineering, Peking University, China",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,1458,1465,"As a crucial infrastructure of intelligent mobile robots, LiDAR-Inertial odometry (LIO) provides the basic capability of state estimation by tracking LiDAR scans. The high-accuracy tracking generally involves the $k\text{NN}$ search, which is used with minimizing the point-to-plane distance. The cost for this, however, is maintaining a large local map and performing $k\text{NN}$ plane fit for each point. In this work, we reduce both time and space complexity of LIO by saving these unnecessary costs. Technically, we design a plane pre-fitting (PPF) pipeline to track the basic skeleton of the 3D scene. In PPF, planes are not fitted individually for each scan, let alone for each point, but are updated incrementally as the scene ‘flows’. Unlike $k\text{NN}$, the PPF is more robust to noisy and non-strict planes with our iterative Principal Component Analyse (iPCA) refinement. Moreover, a simple yet effective sandwich layer is introduced to eliminate false point-to-plane matches. Our method was extensively tested on a total number of 22 sequences across 5 open datasets, and evaluated in 3 existing state-of-the-art LIO systems. By contrast, LIO-PPF can consume only 36% of the original local map size to achieve up to 4x faster residual computing and 1.92x overall FPS, while maintaining the same level of accuracy. We fully open source our implementation at https://github.com/xingyuuchen/LIO-PPF.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341524,National Natural Science Foundation of China(grant numbers:62172021); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341524,,Laser radar;Costs;Three-dimensional displays;Simultaneous localization and mapping;Pipelines;Redundancy;Skeleton,,2.0,,34,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/xingyuuchen/LIO-PPF,https://github.com/xingyuuchen/LIO-PPF
301,Boosting Feedback Efficiency of Interactive Reinforcement Learning by Adaptive Learning from Scores,S. Liu; C. Wu; Y. Li; L. Zhang,"Department of Engineering Mathematics, University of Bristol, UK; RAL, Baidu Research; School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China; RAL, Baidu Research",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,7561,7567,"Interactive reinforcement learning has shown promise in learning complex robotic tasks. However, the process can be human-intensive due to the requirement of a large amount of interactive feedback. This paper presents a new method that uses scores provided by humans instead of pairwise preferences to improve the feedback efficiency of interactive reinforcement learning. Our key insight is that scores can yield significantly more data than pairwise preferences. Specifically, we require a teacher to interactively score the full trajectories of an agent to train a behavioral policy in a sparse reward environment. To avoid unstable scores given by humans negatively impacting the training process, we propose an adaptive learning scheme. This enables the learning paradigm to be insensitive to imperfect or unreliable scores. We extensively evaluate our method for robotic locomotion and manipulation tasks. The results show that the proposed method can efficiently learn near-optimal policies by adaptive learning from scores while requiring less feedback compared to pairwise preference learning methods. The source codes are publicly available at https://github.com/SSKKai/Interactive-Scoring-IRL.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341990,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341990,,Training;Learning systems;Adaptive learning;Source coding;Reinforcement learning;Boosting;Behavioral sciences,,,,47,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/SSKKai/Interactive-Scoring-IRL,https://github.com/SSKKai/Interactive-Scoring-IRL
302,Streaming Motion Forecasting for Autonomous Driving,Z. Pang; D. Ramanan; M. Li; Y. -X. Wang,University of Illinois Urbana-Champaign; Cargenie Mellon University; Cargenie Mellon University; University of Illinois Urbana-Champaign,2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,7407,7414,"Trajectory forecasting is a widely-studied problem for autonomous navigation. However, existing benchmarks evaluate forecasting based on independent snapshots of trajectories, which are not representative of real-world applications that operate on a continuous stream of data. To bridge this gap, we introduce a benchmark that continuously queries future trajectories on streaming data and we refer to it as “streaming forecasting.” Our benchmark inherently captures the disappearance and re-appearance of agents, presenting the emergent challenge of forecasting for occluded agents, which is a safetycritical problem yet overlooked by snapshot-based benchmarks. Moreover, forecasting in the context of continuous timestamps naturally asks for temporal coherence between predictions from adjacent timestamps. Based on this benchmark, we further provide solutions and analysis for streaming forecasting. We propose a plug-and-play meta-algorithm called “Predictive Streamer” that can adapt any snapshot-based forecaster into a streaming forecaster. Our algorithm estimates the states of occluded agents by propagating their positions with multi-modal trajectories, and leverages differentiable filters to ensure temporal consistency. Both occlusion reasoning and temporal coherence strategies significantly improve forecasting quality, resulting in 25% smaller endpoint errors for occluded agents and 10-20% smaller fluctuations of trajectories. Our work is intended to generate interest within the community by highlighting the importance of addressing motion forecasting in its intrinsic streaming setting. Code is available at https://github.com/ziqipang/StreamingForecasting.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341894,NSF(grant numbers:2106825); NIFA(grant numbers:2020-67021-32799); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341894,,Fluctuations;Codes;Coherence;Benchmark testing;Filtering algorithms;Prediction algorithms;Cognition,,3.0,,34,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/ziqipang/StreamingForecasting,https://github.com/ziqipang/StreamingForecasting
303,Poly-MOT: A Polyhedral Framework For 3D Multi-Object Tracking,X. Li; T. Xie; D. Liu; J. Gao; K. Dai; Z. Jiang; L. Zhao; K. Wang,"State Key Laboratory of Robotics and System, Harbin Institute of Technology, Harbin, China; State Key Laboratory of Robotics and System, Harbin Institute of Technology, Harbin, China; State Key Laboratory of Robotics and System, Harbin Institute of Technology, Harbin, China; State Key Laboratory of Robotics and System, Harbin Institute of Technology, Harbin, China; State Key Laboratory of Robotics and System, Harbin Institute of Technology, Harbin, China; State Key Laboratory of Robotics and System, Harbin Institute of Technology, Harbin, China; State Key Laboratory of Robotics and System, Harbin Institute of Technology, Harbin, China; State Key Laboratory of Robotics and System, Harbin Institute of Technology, Harbin, China",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,9391,9398,"3D Multi-object tracking (MOT) empowers mobile robots to accomplish well-informed motion planning and navigation tasks by providing motion trajectories of surrounding objects. However, existing 3D MOT methods typically employ a single similarity metric and physical model to perform data association and state estimation for all objects. With large-scale modern datasets and real scenes, there are a variety of object categories that commonly exhibit distinctive geometric properties and motion patterns. In this way, such distinctions would enable various object categories to behave differently under the same standard, resulting in erroneous matches between trajectories and detections, and jeopardizing the reliability of downstream tasks (navigation, etc.). Towards this end, we propose Poly-MOT, an efficient 3D MOT method based on the Tracking-By-Detection framework that enables the tracker to choose the most appropriate tracking criteria for each object category. Specifically, Poly-MOT leverages different motion models for various object categories to characterize distinct types of motion accurately. We also introduce the constraint of the rigid structure of objects into a specific motion model to accurately describe the highly nonlinear motion of the object. Additionally, we introduce a two-stage data association strategy to ensure that objects can find the optimal similarity metric from three custom metrics for their categories and reduce missing matches. On the NuScenes dataset, our proposed method achieves state-of-the-art performance with 75.4% AMOTA. The code is available at https://github.com/lixiaoyu20001P0Iy-MOT.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341778,National Natural Science Foundation of China(grant numbers:62073101); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341778,,Measurement;Training;Three-dimensional displays;Tracking;Navigation;Detectors;Trajectory,,17.0,,27,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/lixiaoyu20001P0Iy-MOT,https://github.com/lixiaoyu20001P0Iy-MOT
304,CoFlyers: A Universal Platform for Collective Flying of Swarm Drones,J. Huang; F. Wang; T. Hu,"Machine Intelligence and Collective Robotics (MICRO) Lab, School of Aeronautics and Astronautics, Sun Yat-sen University, Shenzhen, China; Machine Intelligence and Collective Robotics (MICRO) Lab, School of Aeronautics and Astronautics, Sun Yat-sen University, Shenzhen, China; Machine Intelligence and Collective Robotics (MICRO) Lab, School of Aeronautics and Astronautics, Sun Yat-sen University, Shenzhen, China",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,8808,8813,"Swarm drones flying is a very attractive field of robotics research, motivated by natural bird flocking or other animal collective behaviors. In this paper, we propose and develop an open-source11https://github.com/micros-uav/CoFlyers universal platform CoFlyers for end-to-end whole-chain development from flocking-inspired models to real-drone swarm flying. In particular, CoFlyers is more user-friendly with only a unified programming language of MATLAB&Simulink, rather than several existing platforms with mixed programming languages or more efforts on raw functional modules. The prototype simulator of CoFlyers is implemented in MATLAB, allowing users to quickly develop and prototype swarm flying algorithms, and to conduct task-oriented parameter auto-tuning and batch processing within reproducible scenarios. Moreover, a real-world verification module of swarm drones is developed in Simulink as well, which directly calls the prototype simulator modules for code reuse. It connects the external platforms via a standardized user-datagram-protocol communication in-terface. As a case study, CoFlyers is utilized into a multi-drone collective flying scenario in confined environments, by implementing ROS&PX4&Gazebo for high-fidelity simulation and Optitrack&Tello-drones for experiments. Eventually, both simulation and experimental results have demonstrated and validated the user-friendly practicability of CoFlyers.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342485,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342485,,Computer languages;Codes;Software packages;Prototypes;Birds;Behavioral sciences;Task analysis,,1.0,,21,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/micros-uav/CoFlyers,https://github.com/micros-uav/CoFlyers
305,ElC-OIS: Ellipsoidal Clustering for Open-World Instance Segmentation on LiDAR Data,W. Deng; K. Huang; Q. Yu; H. Lu; Z. Zheng; X. Chen,"College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,7606,7613,"Open-world Instance Segmentation (OIS) is a challenging task that aims to accurately segment every object instance appearing in the current observation, regardless of whether these instances have been labeled in the training set. This is important for safety-critical applications such as robust autonomous navigation. In this paper, we present a flexible and effective OIS framework for LiDAR point cloud that can accurately segment both known and unknown instances (i.e., seen and unseen instance categories during training). It first identifies points belonging to known classes and removes the back-ground by leveraging close-set panoptic segmentation networks. Then, we propose a novel ellipsoidal clustering method that is more adapted to the characteristic of LiDAR scans and allows precise segmentation of unknown instances. Furthermore, a diffuse searching method is proposed to handle the common over-segmentation problem presented in the known instances. With the combination of these techniques, we are able to achieve accurate segmentation for both known and unknown instances. We evaluated our method on the SemanticKITTI open-world LiDAR instance segmentation dataset. The experimental results suggest that it outperforms current state-of-the-art methods, especially with a 10.0% improvement in association quality. The source code of our method will be publicly available at https://github.com/nubot-nudt/ElC-OIS.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342356,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342356,,Instance segmentation;Training;Point cloud compression;Laser radar;Clustering methods;Source coding;Benchmark testing,,1.0,,39,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/nubot-nudt/ElC-OIS,https://github.com/nubot-nudt/ElC-OIS
306,DualCross: Cross-Modality Cross-Domain Adaptation for Monocular BEV Perception,Y. Man; L. Gui; Y. -X. Wang,University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign,2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,10910,10917,"Closing the domain gap between training and deployment and incorporating multiple sensor modalities are two challenging yet critical topics for self-driving. Existing work only focuses on single one of the above topics, overlooking the simultaneous domain and modality shift which pervasively exists in real-world scenarios. A model trained with multi-sensor data collected in Europe may need to run in Asia with a subset of input sensors available. In this work, we propose DualCross, a cross-modality cross-domain adaptation framework to facilitate the learning of a more robust monocular bird's-eye-view (BEV) perception model, which transfers the point cloud knowledge from a LiDAR sensor in one domain during the training phase to the camera-only testing scenario in a different domain. This work results in the first open analysis of cross-domain cross-sensor perception and adaptation for monocular 3D tasks in the wild. We benchmark our approach on large-scale datasets under a wide range of domain shifts and show state-of-the-art results against various baselines. Our project webpage is at https://yunzeman.github.io/DualCross.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341473,NSF(grant numbers:2106825); NIFA(grant numbers:2020-67021-32799); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341473,,Training;Point cloud compression;Adaptation models;Three-dimensional displays;Laser radar;Europe;Robot sensing systems,,1.0,,47,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://yunzeman.github.io/DualCross,https://github.com/YunzeMan/DualCross
307,T2FPV: Dataset and Method for Correcting First-Person View Errors in Pedestrian Trajectory Prediction,B. Stoler; M. Jana; S. Hwang; J. Oh,"Computer Science Dept., Carnegie Mellon University, Pittsburgh, PA; Mechanical Engineering Dept., Carnegie Mellon University, Pittsburgh, PA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,4037,4044,"Predicting pedestrian motion is essential for developing socially-aware robots that interact in a crowded environment. While the natural visual perspective for a social interaction setting is an egocentric view, the majority of existing work in trajectory prediction therein has been investigated purely in the top-down trajectory space. To support first-person view trajectory prediction research, we present T2FPV, a method for constructing high-fidelity first-person view (FPV) datasets given a real-world, top-down trajectory dataset; we showcase our approach on the ETH/UCY pedestrian dataset to generate the egocentric visual data of all interacting pedestrians, creating the T2FPV-ETH dataset. In this setting, FPV-specific errors arise due to imperfect detection and tracking, occlusions, and field-of-view (FOV) limitations of the camera. To address these errors, we propose CoFE, a module that further refines the imputation of missing data in an end-to-end manner with trajectory forecasting algorithms. Our method reduces the impact of such FPV errors on downstream prediction performance, decreasing displacement error by more than 10% on average. To facilitate research engagement, we release our T2FPV-ETH dataset and software tools§§https://github.com/cmubig/T2FPV.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341874,"Ministry of Trade, Industry and Energy (MOTIE); Korea Institute of Advancement of Technology (KIAT); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341874,,Training;Visualization;Pedestrians;Navigation;Tracking;Software algorithms;Prediction algorithms,,3.0,,39,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/cmubig/T2FPV,https://github.com/cmubig/T2FPV
308,An Open-Source Robotic Chinese Chess Player,S. An; G. Che; J. Guo; Y. Xu; G. Wang; K. A. Tsintotas; F. Zhang; J. Ye; C. Fu; H. Zhu; H. Zhang,"JD Health International Inc., Beijing, China; JD.COM Inc, Beijing, China; JD.COM Inc, Beijing, China; Puncture (Shanghai) Intelligent Medical Technology Co., Ltd, Shanghai, China; College of Biomedical Engineering & Instrument Science, Zhejiang University, Hangzhou, China; Department of Production and Management Engineering, Democritus University of Thrace, Xanthi, Greece; Citrus Research Institute, Southwest University, Chongqing, China; School of Mechanical Engineering, Tongji University, Shanghai, China; School of Mechanical Engineering, Tongji University, Shanghai, China; State Key Laboratory of Software Development Environment, Beihang University, Beijing, China; Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen, China",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,6238,6245,"Consumer robots can accompany children growing up, improving their abilities while playing and entertaining. This paper presents an open-source, practical, low-cost robotic Chinese chess player. The proposed system includes an elaborate mechanical structure, a simple kinematic solution, a novel robot operating system, real-time and accurate chess recognition. Regarding its mechanical design, it combines a magnetism structure and mechanical cam drive, while the overall system has just three servo motors. At the same time, its control strategy is simple and effective. Furthermore, a lightweight robot message communication mechanism, entitled TinyROS, is developed for computing resource-limited embedded chips. Concerning the recognition process, our CNNbased object detector determines chess and achieves accurate identification. As a result, our robotic Chinese chess player is exquisite and easy for large-scale promotion while improving users' chess skills. Aiming to facilitate future consumer robot research and popularize customer robots, the model's mechanical and software design and the TinyROS protocol are open-sourced at https://github.com/Star-Robot/chinese-chess-robot.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341697,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341697,,Visualization;Protocols;Software design;Operating systems;Detectors;Real-time systems;Reliability,,,,40,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/Star-Robot/chinese-chess-robot,https://github.com/Star-Robot/chinese-chess-robot
309,SpinDOE: A Ball Spin Estimation Method for Table Tennis Robot,T. Gossard; J. Tebbe; A. Ziegler; A. Zell,"Dept. Informatics, Cognitive Systems Group, University of Tuebingen; Dept. Informatics, Cognitive Systems Group, University of Tuebingen; Dept. Informatics, Cognitive Systems Group, University of Tuebingen; Dept. Informatics, Cognitive Systems Group, University of Tuebingen",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,5744,5750,"Spin plays a considerable role in table tennis, making a shot's trajectory harder to read and predict. However, the spin is challenging to measure because of the ball's high velocity and the magnitude of the spin values. Existing methods either require extremely high framerate cameras or are unreliable because they use the ball's logo, which may not always be visible. Because of this, many table tennis-playing robots ignore the spin, which severely limits their capabilities. This paper proposes an easily implementable and reliable spin estimation method. We developed a dotted-ball orientation estimation (DOE) method, that can then be used to estimate the spin. The dots are first localized on the image using a CNN and then identified using geometric hashing. The spin is finally regressed from the estimated orientations. Using our algorithm, the ball's orientation can be estimated with a mean error of 2.4° and the spin estimation has an relative error lower than 1%. Spins up to 175 rps are measurable with a camera of 350 fps in real time. Using our method, we generated a dataset of table tennis ball trajectories with position and spin, available on our project page. Project page: https://cogsys-tuebingen.github.io/spindoe/.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342178,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342178,,Torque;Sports equipment;Robot vision systems;Estimation;Cameras;Time measurement;Trajectory,,4.0,,26,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://cogsys-tuebingen.github.io/spindoe,https://github.com/cogsys-tuebingen/spindoe
310,Interactive Spatiotemporal Token Attention Network for Skeleton-Based General Interactive Action Recognition,Y. Wen; Z. Tang; Y. Pang; B. Ding; M. Liu,"Sun Yat-sen University, Shenzhen, China; Sun Yat-sen University, Shenzhen, China; Tencent Technology (Shenzhen) Co., Ltd., China; Sun Yat-sen University, Shenzhen, China; Key Laboratory of Machine Perception, Shenzhen Graduate School, Peking University, Shenzhen, China",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,7886,7892,"Recognizing interactive action plays an important role in human-robot interaction and collaboration. Previous methods use late fusion and co-attention mechanism to capture interactive relations, which have limited learning capability or inefficiency to adapt to more interacting entities. With assumption that priors of each entity are already known, they also lack evaluations on a more general setting addressing the diversity of subjects. To address these problems, we propose an Interactive Spatiotemporal Token Attention Network (ISTA-Net), which simultaneously model spatial, temporal, and interactive relations. Specifically, our network contains a tokenizer to partition Interactive Spatiotemporal Tokens (ISTs), which is a unified way to represent motions of multiple diverse entities. By extending the entity dimension, ISTs provide better interactive representations. To jointly learn along three dimensions in ISTs, multi-head self-attention blocks integrated with 3D convolutions are designed to capture inter-token correlations. When modeling correlations, a strict entity ordering is usually irrelevant for recognizing interactive actions. To this end, Entity Rearrangement is proposed to eliminate the orderliness in ISTs for interchangeable entities. Extensive experiments on four datasets verify the effectiveness of ISTA-Net by outperforming state-of-the-art methods. Our code is publicly available at https://github.com/Necolizer/ISTA-Net.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342472,"National Natural Science Foundation of China(grant numbers:62203476,52105079); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342472,,Knowledge engineering;Convolutional codes;Correlation;Three-dimensional displays;Human-robot interaction;Collaboration;Benchmark testing,,14.0,,34,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/Necolizer/ISTA-Net,https://github.com/Necolizer/ISTA-Net
311,Few-Shot Segmentation and Semantic Segmentation for Underwater Imagery,I. Kabir; S. Shaurya; V. Maigur; N. Thakurdesai; M. Latnekar; M. Raunak; D. Crandall; M. A. Reza,"College of Information Sciences and Technology, Pennsylvania State University, PA, USA; Luddy School of Informatics, Computing, and Engineering, Indiana University, Bloomington, IN, USA; Luddy School of Informatics, Computing, and Engineering, Indiana University, Bloomington, IN, USA; Luddy School of Informatics, Computing, and Engineering, Indiana University, Bloomington, IN, USA; Luddy School of Informatics, Computing, and Engineering, Indiana University, Bloomington, IN, USA; Luddy School of Informatics, Computing, and Engineering, Indiana University, Bloomington, IN, USA; Luddy School of Informatics, Computing, and Engineering, Indiana University, Bloomington, IN, USA; Department of Mathematics and Computer Science, Drake University, Des Moines, IA, USA",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,11451,11457,"This paper tackles image segmentation problems for underwater environments. First, we introduce a novel under-water animal-centric dataset with dense pixel-level annotations containing diverse fine-grained animal categories to mitigate the lack of diverse categories in the existing benchmarks. Then, we solve two image segmentation tasks using underwater images in this dataset: (i) few-shot segmentation, and (ii) semantic segmentation. For the segmentation task in a few-shot learning framework, we propose a novel attention-guided deep neural network architecture by infusing attention modules in various stages of our proposed network. We systematically explore how the learned attention maps can improve few-shot segmentation performance for underwater imagery. Finally, we assess the semantic segmentation problem on our proposed dataset by benchmarking it with two state-of-the-art semantic segmentation methods. We believe our new problem setup, i.e., few-shot segmentation for underwater environments, will be a valuable addition to the existing underwater semantic segmentation task. We believe our novel dataset will pave the way for developing better algorithms and exploring new research directions for marine robotics and underwater image understanding. We publicly release our dataset and the code to advance image understanding research in underwater environments: https://github.com/Imran220S/uwsnet.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342227,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342227,,Codes;Annotations;Animals;Semantic segmentation;Artificial neural networks;Benchmark testing;Task analysis,,1.0,,38,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/Imran220S/uwsnet,https://github.com/Imran220S/uwsnet
312,Object Manipulation Through Contact Configuration Regulation: Multiple and Intermittent Contacts,O. Taylor; N. Doshi; A. Rodriguez,Massachusetts Institute of Technology; Massachusetts Institute of Technology; Massachusetts Institute of Technology,2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,8735,8743,"In this work, we build on our method for manipulating unknown objects via contact configuration regulation: the estimation and control of the location, geometry, and mode of all contacts between the robot, object, and environment. We further develop our estimator and controller to enable manipulation through more complex contact interactions, including intermittent contact between the robot/object, and multiple contacts between the object/environment. In addition, we support a larger set of contact geometries at each interface. This is accomplished through a factor graph based estimation framework that reasons about the complementary kinematic and wrench constraints of contact to predict the current contact configuration. We are aided by the incorporation of a limited amount of visual feedback; which when combined with the available F/T sensing and robot proprioception, allows us to differentiate contact modes that were previously indistinguishable. We implement this revamped framework on our manipulation platform, and demonstrate that it allows the robot to perform a wider set of manipulation tasks. This includes, using a wall as a support to re-orient an object, or regulating the contact geometry between the object and the ground. Finally, we conduct ablation studies to understand the contributions from visual and tactile feedback in our manipulation framework. Our code can be found at: https://github.com/mcubelab/pbal.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341362,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341362,,Geometry;Visualization;Estimation;Tactile sensors;Kinematics;Regulation;Sensors,,,,36,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/mcubelab/pbal,https://github.com/mcubelab/pbal
313,Ungar - A C++ Framework for Real-Time Optimal Control Using Template Metaprogramming,F. De Vincenti; S. Coros,"Computational Robotics Lab, ETH Zurich, Switzerland; Computational Robotics Lab, ETH Zurich, Switzerland",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,6297,6303,"We present Ungar, an open-source library to aid the implementation of high-dimensional optimal control problems (OCPs). We adopt modern template metaprogramming techniques to enable the compile-time modeling of complex systems while retaining maximum runtime efficiency. Our framework provides syntactic sugar to allow for expressive formulations of a rich set of structured dynamical systems. While the core modules depend only on the header-only Eigen and Boost.Hana libraries, we bundle our codebase with optional packages and custom wrappers for automatic differentiation, code generation, and nonlinear programming. Finally, we demonstrate the versatility of Ungar in various model predictive control applications, namely, four-legged locomotion and collaborative loco-manipulation with multiple one-armed quadruped robots. Ungar is available under the Apache License 2.0 at https://github.com/fdevinc/ungar.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341365,Swiss National Science Foundation(grant numbers:200021200644); European Research Council (ERC)(grant numbers:866480); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341365,,Codes;Optimal control;Collaboration;C++ languages;Syntactics;Libraries;Real-time systems,,,,25,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/fdevinc/ungar,https://github.com/fdevinc/ungar
314,BSH-Det3D: Improving 3D Object Detection with BEV Shape Heatmap,Y. Shen; Y. Zhang; Y. Wu; Z. Wang; L. Yang; S. Coleman; D. Kerr,"College of Information Science and Engineering, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China; School of Electronic and Computer Engineering, Peking University, Shenzhen, China; Department of Electronic and Computer Engineering, Faculty of Robotics and Engineering, North-eastern University, Technical University in Munich, Germany, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China; School of Computing, Engineering and Intelligent Systems, Ulster University, N. Ireland, UK; School of Computing, Engineering and Intelligent Systems, Ulster University, N. Ireland, UK",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,5730,5737,"The progress of LiDAR-based 3D object detection has significantly enhanced developments in autonomous driving and robotics. However, due to the limitations of LiDAR sensors, object shapes suffer from deterioration in occluded and distant areas, which creates a fundamental challenge to 3D perception. Existing methods estimate specific 3D shapes and achieve remarkable performance. However, these methods rely on extensive computation and memory, causing imbalances between accuracy and real-time performance. To tackle this challenge, we propose a novel LiDAR-based 3D object detection model named BSH-Det3D, which applies an effective way to enhance spatial features by estimating complete shapes from a bird's eye view (BEV). Specifically, we design the Pillar-based Shape Completion (PSC) module to predict the probability of occupancy whether a pillar contains object shapes. The PSC module generates a BEV shape heatmap for each scene. After integrating with heatmaps, BSH-Det3D can provide additional information in shape deterioration areas and generate high-quality 3D proposals. We also design an attention-based densification fusion module (ADF) to adaptively associate the sparse features with heatmaps and raw points. The ADF module integrates the advantages of points and shapes knowledge with negligible overheads. Extensive experiments on the KITTI benchmark achieve state-of-the-art (SOTA) performance in terms of accuracy and speed, demonstrating the efficiency and flexibility of BSH-Det3D. The source code is available on https://github.com/mystorm16/BSH-Det3D.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341930,National Natural Science Foundation of China(grant numbers:61973066); Fundamental Research Funds for the Central Universities(grant numbers:N2004022); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341930,,Heating systems;Solid modeling;Three-dimensional displays;Shape;Source coding;Object detection;Benchmark testing,,4.0,,37,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/mystorm16/BSH-Det3D,https://github.com/mystorm16/BSH-Det3D
315,Efficient Heuristics for Multi-Robot Path Planning in Crowded Environments,T. Guo; J. Yu,"Department of Computer Science, Rutgers, the State University of New Jersey, Piscataway, NJ, USA; Department of Computer Science, Rutgers, the State University of New Jersey, Piscataway, NJ, USA",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,6749,6756,"Optimal Multi-Robot Path Planning (MRPP) has garnered significant attention due to its many applications in domains including warehouse automation, transportation, and swarm robotics. Current MRPP solvers can be divided into reduction-based, search-based, and rule-based categories, each with their strengths and limitations. Regardless of the methodology, however, the issue of handling dense MRPP instances remains a significant challenge, where existing approaches generally demonstrate a dichotomy regarding solution optimality and efficiency. This study seeks to bridge the gap in optimal MRPP resolution for dense, highly-entangled scenarios, with potential applications to high-density storage systems and traffic congestion control. Toward that goal, we analyze the behaviors of SOTA MRPP algorithms in dense settings and develop two hybrid algorithms leveraging the strengths of existing SOTA algorithms: DCBS (database-accelerated enhanced conflict-based search) and SCBS (sparsified enhanced conflict-based search). Experimental validations demonstrate that DCBS and SCBS deliver a significant reduction in computational time compared to existing bounded-suboptimal methods and improve solution quality compared to existing rule-based methods, achieving a desirable balance between computational efficiency and solution optimality. As a result, DCBS and SCBS are particularly suitable for quickly computing good-quality solutions for multi-robot routing in dense settings. Simulation video https://youtu.be/dZxMPUr7Bqg Upon the publication of the manuscript source code and data will be released at https://github.com/arc-l/dcbs",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341800,NSF(grant numbers:IIS-1845888); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341800,,Automation;Source coding;Transportation;Swarm robotics;Routing;Path planning;Computational efficiency,,1.0,,42,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/arc-l/dcbs,https://github.com/arc-l/dcbs
316,SSGM: Spatial Semantic Graph Matching for Loop Closure Detection in Indoor Environments,Y. Tang; M. Wang; Y. Deng; Y. Yang; Y. Yue,"School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,9163,9168,"Capturing the semantics of objects and the topological relationship allows the robot to describe the scene more intelligently like a human and measure the similarity between scenes (loop closure detection) more accurately. However, many current semantic graph matching methods are based on walk descriptors, which only extract adjacency relations between objects. In such way, the comprehensive information in the semantic graph is not fully exploited, which may lead to false closed-loop detection. This paper proposes a novel spatial semantic graph matching method (SSGM) in indoor environments, which considers multifaceted information of the semantic graphs. Firstly, two semantic graphs are aligned in the same coordinate space contributed by the second-order spatial compatibility metric between objects and local graph features of objects in semantic graphs. Secondly, the similarity of the spatial distribution of overall semantic graphs is further evaluated. The proposed algorithm is validated on public datasets and compared with the latest semantic graph matching methods, demonstrating improved accuracy and efficiency in loop closure detection. The code is available at https://github.com/BIT-TYJ/SSGM.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342317,"National Natural Science Foundation of China(grant numbers:NSFC 62233002,62003039,61973034,U1913203); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342317,,Measurement;Graphical models;Codes;Robot kinematics;Current measurement;Semantics;Indoor environment,,1.0,,28,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/BIT-TYJ/SSGM,https://github.com/BIT-TYJ/SSGM
317,Driver Distraction Detection for Daytime and Nighttime with Unpaired Visible and Infrared Image Translation,H. -Z. Shen; H. -Y. Lin,"Department of Electrical Engineering, National Chung Cheng University, Taiwan; Department of Computer Science and Information Engineering, National Taipei, University of Technology, Taipei, Taiwan",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,9027,9034,"Driver distraction detection is an important function of driver monitoring systems and intelligent vehicles. Most previous research only focuses on the system development for daytime operations. In this paper, we propose a network model, V2IA-Net, which is able to use the daytime visible and nighttime infrared images for the driver distraction detection task. With the visible-infrared image translation, driver action recognition and head pose detection, the driver distraction behavior can be analyzed in real-time performance. To provide realistic driving scenes for network training and testing, a visible-infrared image dataset, VID, is created. The proposed V2IA-Net is trained on the unpaired images, and capable of common feature extraction for visible-infrared image conversion. In the experiments, our technique is compared with various driver distraction detection models. The results have demonstrated the effectiveness of the proposed method. Source code and datasets are available at https://github.com/kk2487/V2IA-Net.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342206,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342206,,Training;Image recognition;Head;Source coding;Network architecture;Real-time systems;Behavioral sciences,,1.0,,31,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/kk2487/V2IA-Net,https://github.com/kk2487/V2IA-Net
318,Optical Flow Boosts Unsupervised Localization and Segmentation,X. Zhang; A. Boularias,"Department of Computer Science, Rutgers University; Department of Computer Science, Rutgers University",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,7635,7642,"Unsupervised localization and segmentation are long-standing robot vision challenges that describe the critical ability for an autonomous robot to learn to decompose images into individual objects without labeled data. These tasks are important because of the limited availability of dense image manual annotation and the promising vision of adapting to an evolving set of object categories in lifelong learning. Most recent methods focus on using visual appearance continuity as object cues by spatially clustering features obtained from self-supervised vision transformers (ViT). In this work, we leverage motion cues, inspired by the common fate principle that pixels that share similar movements tend to belong to the same object. We propose a new loss term formulation that uses optical flow in unlabeled videos to encourage self-supervised ViT features to become closer to each other if their corresponding spatial locations share similar movements, and vice versa. We use the proposed loss function to finetune vision transformers that were originally trained on static images. Our fine-tuning procedure outperforms state-of-the-art techniques for unsupervised semantic segmentation through linear probing, without the use of any labeled data. This procedure also demonstrates increased performance over original ViT networks across unsupervised object localization and semantic segmentation benchmarks. Our code is available at https://github.com/mlzxy/flowdino.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342195,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342195,,Location awareness;Optical losses;Visualization;Semantic segmentation;Robot vision systems;Object segmentation;Transformers,,2.0,,69,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/mlzxy/flowdino,https://github.com/mlzxy/flowdino
319,Image Restoration via UAVFormer for Under-Display Camera of UAV,Z. Zheng; X. Jia,"School of Computer Science and Engineering, Nanjing University of Science and Technology, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, China",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,3222,3227,"The exposed cameras of UAVs can shake, shift, or even malfunction under the influence of harsh weather, while the add-on devices (Dupont lines) are very vulnerable to dam-age. Although we can place a low-cost transparent film overlay around the camera to protect it, this would also introduce image degradation issues (such as oversaturation, astigmatism, etc). To tackle the image degradation problem caused by overlaying transparent film, in this paper we propose a novel method to enhance the visual experience by adapting a deep network with UAV characteristics. Specifically, we propose a customized Transformer named UAVFormer to recover the image, which has a key module at each stage based on the Swin Transformer with local awareness (LAT). In the end, we use an evidential fusion algorithm to integrate the generated images at each stage to obtain a high-quality result. Furthermore, we create a high-resolution under-display camera dataset to support the training and testing of compared models. Our model can conduct high-quality recovery of images of 2K resolution on some embedded devices (Raspberry Pi 4b) in realtime. The URL for the code at https://github.com/zzr-idam/UAVFormer.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342454,National Natural Science Foundation of China(grant numbers:62176123); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342454,,Degradation;Uniform resource locators;Training;Visualization;Cameras;Transformers;Autonomous aerial vehicles,,,,51,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/zzr-idam/UAVFormer,https://github.com/zzr-idam/UAVFormer
320,Accurate and Interactive Visual-Inertial Sensor Calibration with Next-Best-View and Next-Best-Trajectory Suggestion,C. L. Choi; B. Xu; S. Leutenegger,"Department of Computing, Smart Robotics Lab, Imperial College London, United Kingdom; Department of Computing, Smart Robotics Lab, Imperial College London, United Kingdom; Department of Computing, Smart Robotics Lab, Imperial College London, United Kingdom",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,1759,1766,"Visual-Inertial (VI) sensors are popular in robotics, self-driving vehicles, and augmented and virtual reality applications. In order to use them for any computer vision or state-estimation task, a good calibration is essential. However, collecting informative calibration data in order to render the calibration parameters observable is not trivial for a non-expert. In this work, we introduce a novel VI calibration pipeline that guides a non-expert with the use of a graphical user interface and information theory in collecting informative calibration data with Next-Best-View and Next-Best-Trajectory suggestions to calibrate the intrinsics, extrinsics, and temporal misalignment of a VI sensor. We show through experiments that our method is faster, more accurate, and more consistent than state-of-the-art alternatives. Specifically, we show how calibrations with our proposed method achieve higher accuracy estimation results when used by state-of-the-art VI Odometry as well as VI-SLAM approaches. The source code of our software can be found on: https://github.com/chutsu/yac.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341815,Imperial College London; Technical University of Munich; EPSRC(grant numbers:ABM EP/N018494/1); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341815,,Visualization;Simultaneous localization and mapping;Source coding;Software algorithms;Virtual reality;Software;Calibration,,,,24,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/chutsu/yac,https://github.com/chutsu/yac
321,ANEC: Adaptive Neural Ensemble Controller for Mitigating Latency Problems in Vision-Based Autonomous Driving,A. Khalil; J. Kwon,"Department of Electrical and Computer Engineering, University of Michigan-Dearborn, Dearborn, MI, USA; Department of Electrical and Computer Engineering, University of Michigan-Dearborn, Dearborn, MI, USA",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,9340,9346,"Humans have latency in their visual perception system between observation and action. Any action we take is based on an earlier observation since, by the time we act, the state has already changed, and we got a new observation. In autonomous driving, this latency is also present, determined by the amount of time the control algorithm needs to process information before acting. This algorithmic perception latency can be reduced by massive computing power via GPUs and FPGAs, which is improbable in automobile platforms. Thus, it is a reasonable assumption that the algorithmic perception latency is inevitable. Many researchers have developed different neural network driving models without consideration of the algorithmic perception latency. This paper studies the latency effect on vision-based neural network autonomous driving in the lane-keeping task and proposes a vision-based novel neural network controller, the Adaptive Neural Ensemble Controller (ANEC) that is inspired by the near/far gaze distribution of human drivers during lane-keeping. ANEC was tested in Gazebo 3D simulation environment with Robot Operating System (ROS) which showed the effectiveness of ANEC in dealing with algorithmic latency. The source code is available at https://github.com/jrkwon/oscar/tree/devel_anec.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342520,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342520,Autonomous Vehicle Navigation;Machine Learning for Robot Control;Imitation Learning,Adaptive systems;Three-dimensional displays;Source coding;Operating systems;Neural networks;Process control;Task analysis,,,,33,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/jrkwon/oscar,https://github.com/jrkwon/oscar
322,EventTransAct: A Video Transformer-Based Framework for Event-Camera Based Action Recognition,T. d. Blegiers; I. R. Dave; A. Yousaf; M. Shah,"Center for Research in Computer Vision lab (CRCV), University of Central Florida, USA; Center for Research in Computer Vision lab (CRCV), University of Central Florida, USA; Center for Research in Computer Vision lab (CRCV), University of Central Florida, USA; Center for Research in Computer Vision lab (CRCV), University of Central Florida, USA",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,1,7,"Recognizing and comprehending human actions and gestures is a crucial perception requirement for robots to interact with humans and carry out tasks in diverse domains, including service robotics, healthcare, and manufacturing. Event cameras, with their ability to capture fast-moving objects at a high temporal resolution, offer new opportunities compared to standard action recognition in RGB videos. However, previous research on event camera action recognition has primarily focused on sensor-specific network architectures and image encoding, which may not be suitable for new sensors and limit the use of recent advancement in transformer-based architectures. In this study, we employ using a computationally efficient model, namely the video transformer network (VTN), which initially acquires spatial embeddings per event-frame and then utilizes a temporal self-attention mechanism. This approach separates the spatial and temporal operations, resulting in VTN being more computationally efficient than other video transformers that process spatio-temporal volumes directly. In order to better adopt the VTN for the sparse and finegrained nature of event data, we design Event-Contrastive Loss $\left(\mathscr{L}_{E C}\right)$ and event specific augmentations. Proposed $\left(\mathscr{L}_{E C}\right)$ promotes learning fine-grained spatial cues in the spatial backbone of VTN by contrasting temporally misaligned frames. We evaluate our method on real-world action recognition of N-EPIC Kitchens dataset, and achieve state-of-the-art results on both protocols - testing in seen kitchen (74.9% accuracy) and testing in unseen kitchens (42.43% and 46.66% Accuracy). Our approach also takes less computation time compared to competitive prior approaches. We also evaluate our method on the standard DVS Gesture recognition dataset, achieving a competitive accuracy of 97.9% compared to prior work that uses dedicated architectures and image-encoding for the DVS dataset. These results demonstrate the potential of our framework EventTransAct for real-world applications of event-camera based action recognition. Project Page: https://tristandb8.github.io/EventTransAct_webpage/",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341740,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341740,,Robot vision systems;Gesture recognition;Transformers;Cameras;Data models;Voltage control;Task analysis,,7.0,,58,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://tristandb8.github.io/EventTransAct_webpage,https://github.com/tristandb8/EventTransAct
323,An MCTS-DRL Based Obstacle and Occlusion Avoidance Methodology in Robotic Follow-Ahead Applications,S. Leisiazar; E. J. Park; A. Lim; M. Chen,"School of Mechatronics Systems Engineering, Simon Fraser University, Canada; School of Mechatronics Systems Engineering, Simon Fraser University, Canada; School of Computing Sciences, Simon Fraser University, Canada; School of Computing Sciences, Simon Fraser University, Canada",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,221,228,"We propose a novel methodology for robotic follow-ahead applications that address the critical challenge of obstacle and occlusion avoidance. Our approach effectively navigates the robot while ensuring avoidance of collisions and occlusions caused by surrounding objects. To achieve this, we developed a high-level decision-making algorithm that generates short-term navigational goals for the mobile robot. Monte Carlo Tree Search is integrated with a Deep Reinforcement Learning method to enhance the performance of the decision-making process and generate more reliable navigational goals. Through extensive experimentation and analysis, we demonstrate the effectiveness and superiority of our proposed approach in comparison to the existing follow-ahead human-following robotic methods. Our code is available at https://github.com/saharLeisiazar/follow-ahead-ros.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342150,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342150,,Deep learning;Monte Carlo methods;Navigation;Decision making;Reinforcement learning;Reliability;Mobile robots,,,,23,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/saharLeisiazar/follow-ahead-ros,https://github.com/saharLeisiazar/follow-ahead-ros
324,Chat with the Environment: Interactive Multimodal Perception Using Large Language Models,X. Zhao; M. Li; C. Weber; M. B. Hafez; S. Wermter,"Department of Informatics, Knowledge Technology Group, Universität Hamburg, Hamburg, Germany; Department of Informatics, Knowledge Technology Group, Universität Hamburg, Hamburg, Germany; Department of Informatics, Knowledge Technology Group, Universität Hamburg, Hamburg, Germany; Department of Informatics, Knowledge Technology Group, Universität Hamburg, Hamburg, Germany; Department of Informatics, Knowledge Technology Group, Universität Hamburg, Hamburg, Germany",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,3590,3596,"Programming robot behavior in a complex world faces challenges on multiple levels, from dextrous low-level skills to high-level planning and reasoning. Recent pre-trained Large Language Models (LLMs) have shown remarkable reasoning ability in few-shot robotic planning. However, it remains challenging to ground LLMs in multimodal sensory input and continuous action output, while enabling a robot to interact with its environment and acquire novel information as its policies unfold. We develop a robot interaction scenario with a partially observable state, which necessitates a robot to decide on a range of epistemic actions in order to sample sensory information among multiple modalities, before being able to execute the task correctly. An interactive perception framework is therefore proposed with an LLM as its backbone, whose ability is exploited to instruct epistemic actions and to reason over the resulting multimodal sensations (vision, sound, haptics, proprioception), as well as to plan an entire task execution based on the interactively acquired information. Our study demonstrates that LLMs can provide high-level planning and reasoning skills and control interactive robot behavior in a multimodal environment, while multimodal modules with the context of the environmental state help ground the LLMs and extend their processing ability. The project website can be found at https://matcha-model.github.io/.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342363,German Research Foundation (DFG) in the project Crossmodal Learning(grant numbers:TRR-169); China Scholarship Council (CSC); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342363,,Training;Computational modeling;Memory management;Process control;Programming;Robot sensing systems;Planning,,17.0,,30,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,,
325,Transparent Object Tracking with Enhanced Fusion Module,K. Garigapati; E. Blasch; J. Wei; H. Ling,"Stony Brook University, Stony Brook, NY, USA; Air Force Research Lab, Arlington, VA, USA; City College of New York, New York, NY, USA; Stony Brook University, Stony Brook, NY, USA",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,7696,7703,"Accurate tracking of transparent objects, such as glasses, plays a critical role in many robotic tasks such as robot-assisted living. Due to the adaptive and often reflective texture of such objects, traditional tracking algorithms that rely on general-purpose learned features suffer from reduced performance. Recent research has proposed to instill trans-parency awareness into existing general object trackers by fusing purpose-built features. However, with the existing fusion techniques, the addition of new features causes a change in the latent space making it impossible to incorporate transparency awareness on trackers with fixed latent spaces. For example, many of the current days' transformer-based trackers are fully pre-trained and are sensitive to any latent space perturbations. In this paper, we present a new feature fusion technique that integrates transparency information into a fixed feature space, enabling its use in a broader range of trackers. Our proposed fusion module, composed of a transformer encoder and an MLP module, leverages key query-based transformations to embed the transparency information into the tracking pipeline. We also present a new two-step training strategy for our fusion module to effectively merge transparency features. We propose a new tracker architecture that uses our fusion techniques to achieve superior results for transparent object tracking. Our proposed method achieves competitive results with state-of-the-art trackers on TOTB, which is the largest transparent object tracking benchmark recently released. Our results and the implementation of code will be made publicly available at https://github.com/kalyan0510/TOTEM.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341597,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341597,,Training;Perturbation methods;Pipelines;Object segmentation;Glass;Transformers;Object tracking,,,,27,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/kalyan0510/TOTEM,https://github.com/kalyan0510/TOTEM
326,"Exploring Visual Pre-training for Robot Manipulation: Datasets, Models and Methods",Y. Jing; X. Zhu; X. Liu; Q. Sima; T. Yang; Y. Feng; T. Kong,ByteDance Research; ByteDance Research; ByteDance Research; ByteDance Research; ByteDance Research; ByteDance Research; ByteDance Research,2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,11390,11395,"Visual pre-training with large-scale real-world data has made great progress in recent years, showing great potential in robot learning with pixel observations. However, the recipes of visual pre-training for robot manipulation tasks are yet to be built. In this paper, we thoroughly investigate the effects of visual pre-training strategies on robot manipulation tasks from three fundamental perspectives: pre-training datasets, model architectures and training methods. Several significant experimental findings are provided that are beneficial for robot learning. Further, we propose a visual pre-training scheme for robot manipulation termed Vi-PRoM, which combines self-supervised learning and supervised learning. Concretely, the former employs contrastive learning to acquire underlying patterns from large-scale unlabeled data, while the latter aims learning visual semantics and temporal dynamics. Extensive experiments on robot manipulations in various simulation environments and the real robot demonstrate the superiority of the proposed scheme. Videos and more details can be found on https://explore-pretrain-robot.github.io.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342201,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342201,,Training;Visualization;Supervised learning;Semantics;Self-supervised learning;Robot learning;Task analysis,,2.0,,35,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,,
327,Efficient Visuo-Haptic Object Shape Completion for Robot Manipulation,L. Rustler; J. Matas; M. Hoffmann,"Department of Cybernetics, Faculty of Electrical Engineering, Czech Technical University in Prague; Department of Cybernetics, Faculty of Electrical Engineering, Czech Technical University in Prague; Department of Cybernetics, Faculty of Electrical Engineering, Czech Technical University in Prague",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,3121,3128,"For robot manipulation, a complete and accurate object shape is desirable. Here, we present a method that combines visual and haptic reconstruction in a closed-loop pipeline. From an initial viewpoint, the object shape is reconstructed using an implicit surface deep neural network. The location with highest uncertainty is selected for haptic exploration, the object is touched, the new information from touch and a new point cloud from the camera are added, object position is re-estimated and the cycle is repeated. We extend Rustler et al. (2022) by using a new theoretically grounded method to determine the points with highest uncertainty, and we increase the yield of every haptic exploration by adding not only the contact points to the point cloud but also incorporating the empty space established through the robot movement to the object. Additionally, the solution is compact in that the jaws of a closed two-finger gripper are directly used for exploration. The object position is re-estimated after every robot action and multiple objects can be present simultaneously on the table. We achieve a steady improvement with every touch using three different metrics and demonstrate the utility of the better shape reconstruction in grasping experiments on the real robot. On average, grasp success rate increases from 63.3 % to 70.4 % after a single exploratory touch and to 82.7% after five touches. The collected data and code are publicly available (https://osf.io/j6rkd/, https://github.com/ctu-vras/vishac).",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342200,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342200,,Point cloud compression;Measurement;Surface reconstruction;Uncertainty;Codes;Shape;Pipelines,,3.0,,45,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/ctu-vras/vishac,https://github.com/ctu-vras/vishac
328,Online Self-Supervised Thermal Water Segmentation for Aerial Vehicles,C. Lee; J. G. Frennert; L. Gan; M. Anderson; S. -J. Chung,California Institute of Technology; California Institute of Technology; California Institute of Technology; California Institute of Technology; California Institute of Technology,2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,7734,7741,"We present a new method to adapt an RGB-trained water segmentation network to target-domain aerial thermal imagery using online self-supervision by leveraging texture and motion cues as supervisory signals. This new thermal capability enables current autonomous aerial robots operating in near-shore environments to perform tasks such as visual navigation, bathymetry, and flow tracking at night. Our method overcomes the problem of scarce and difficult-to-obtain near-shore thermal data that prevents the application of conventional supervised and unsupervised methods. In this work, we curate the first aerial thermal near-shore dataset, show that our approach outperforms fully-supervised segmentation models trained on limited target-domain thermal data, and demonstrate real-time capabilities onboard an Nvidia Jetson embedded computing platform. Code and datasets used in this work will be available at: https://github.com/connorlee77/uav-thermal-water-segmentation.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342016,Office of Naval Research; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342016,,Training;Visualization;Target tracking;Navigation;Motion segmentation;Autonomous aerial vehicles;Bathymetry,,4.0,,46,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/connorlee77/uav-thermal-water-segmentation,https://github.com/connorlee77/uav-thermal-water-segmentation
329,MIMIR-UW: A Multipurpose Synthetic Dataset for Underwater Navigation and Inspection,O. Álvarez-Tuñón; H. Kanner; L. R. Marnet; H. X. Pham; J. le Fevre Sejersen; Y. Brodskiy; E. Kayacan,"Department of Electrical and Computer Engineering, Artificial Intelligence in Robotics Laboratory (AiRLab), Aarhus University, Aarhus C, Denmark; EIVA a/s, Skanderborg, Denmark; EIVA a/s, Skanderborg, Denmark; Department of Electrical and Computer Engineering, Artificial Intelligence in Robotics Laboratory (AiRLab), Aarhus University, Aarhus C, Denmark; Department of Electrical and Computer Engineering, Artificial Intelligence in Robotics Laboratory (AiRLab), Aarhus University, Aarhus C, Denmark; EIVA a/s, Skanderborg, Denmark; Automatic Control Group (RAT), Paderborn University, Paderborn, Germany",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,6141,6148,"This paper presents MIMIR-UW, a multipurpose underwater synthetic dataset for SLAM, depth estimation, and object segmentation to bridge the gap between theory and application in underwater environments. MIMIR-UW integrates three camera sensors, inertial measurements, and ground truth for robot pose, image depth, and object segmentation. The underwater robot is deployed within a pipe exploration scenario, carrying artificial lights that create uneven lighting, in addition to natural artefacts such as reflections from natural light and backscattering effects. Four environments totalling eleven tracks are provided, with various difficulties regarding light conditions or dynamic elements. Two metrics for dataset evaluation are proposed, allowing MIMIR-UW to be compared with other datasets. State-of-art methods on SLAM, segmentation and depth estimation are deployed and benchmarked on MIMIR-UW. Moreover, the dataset's potential for sim-to-real transfer is demonstrated by leveraging the segmentation and depth estimation models trained on MIMIR-UW in a real pipeline inspection scenario. To the best of the authors' knowledge, this is the first underwater dataset targeted for such a variety of methods. The dataset is publicly available online. https://github.com/remaro-network/MIMIR-UW/",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341436,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341436,,Measurement;Simultaneous localization and mapping;Robot vision systems;Pipelines;Estimation;Object segmentation;Inspection,,6.0,,36,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/remaro-network/MIMIR-UW,https://github.com/remaro-network/MIMIR-UW
330,MDT3D: Multi-Dataset Training for LiDAR 3D Object Detection Generalization,L. Soum-Fontez; J. -E. Deschaud; F. Goulette,"Centre for Robotics, Mines Paris - PSL, PSL University, Paris, France; Centre for Robotics, Mines Paris - PSL, PSL University, Paris, France; Centre for Robotics, Mines Paris - PSL, PSL University, Paris, France",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,5765,5772,"Supervised 3D Object Detection models have been displaying increasingly better performance in single-domain cases where the training data comes from the same environment and sensor as the testing data. However, in real-world scenarios data from the target domain may not be available for finetuning or for domain adaptation methods. Indeed, 3D object detection models trained on a source dataset with a specific point distribution have shown difficulties in generalizing to unseen datasets. Therefore, we decided to leverage the information available from several annotated source datasets with our Multi-Dataset Training for 3D Object Detection (MDT3D) method to increase the robustness of 3D object detection models when tested in a new environment with a different sensor configuration. To tackle the labelling gap between datasets, we used a new label mapping based on coarse labels. Furthermore, we show how we managed the mix of datasets during training and finally introduce a new cross-dataset augmentation method: crossdataset object injection. We demonstrate that this training paradigm shows improvements for different types of 3D object detection models. The source code and additional results for this research project will be publicly available on GitHub for interested parties to access and utilize: https://github.com/LouisSF/MDT3D",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341614,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341614,,Training;Solid modeling;Adaptation models;Three-dimensional displays;Laser radar;Training data;Object detection,,2.0,,26,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/LouisSF/MDT3D,https://github.com/LouisSF/MDT3D
331,IDA: Informed Domain Adaptive Semantic Segmentation,Z. Chen; Z. Ding; J. M. Gregory; L. Liu,"Luddy School of Informatics, Computing, and Engineering, Indiana University, Bloomington, IN, USA; Department of Computer Science, Tulane University; DEVCOM Army Research Laboratory, USA; Luddy School of Informatics, Computing, and Engineering, Indiana University, Bloomington, IN, USA",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,90,97,"Mixup-based data augmentation has been validated to be a critical stage in the self-training framework for unsupervised domain adaptive semantic segmentation (UDASS), which aims to transfer knowledge from a well-annotated (source) domain to an unlabeled (target) domain. Existing self-training methods usually adopt the popular region-based mixup techniques with a random sampling strategy, which unfortunately ignores the dynamic evolution of different semantics across various domains as training proceeds. To improve the UDA-SS performance, we propose an Informed Domain Adaptation (IDA) model, a self-training framework that mixes the data based on class-level segmentation performance, which aims to emphasize small-region semantics during mixup. In our IDA model, the class-level performance is tracked by an expected confidence score (ECS). We then use a dynamic schedule to determine the mixing ratio for data in different domains. Extensive experimental results reveal that our proposed method is able to outperform the state-of-the-art UDA-SS method by a margin of 1.1 mIoU in the adaptation of GTA-V to Cityscapes and of 0.9 mIoU in the adaptation of SYNTHIA to Cityscapes. Code link: https://github.com/ArlenCHEN/IDA.git",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342254,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342254,,Training;Adaptation models;Schedules;Codes;Semantic segmentation;Semantics;Dynamic scheduling,,5.0,,41,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/ArlenCHEN/IDA,https://github.com/ArlenCHEN/IDA
332,Joint Out-of-Distribution Detection and Uncertainty Estimation for Trajectory Prediction,J. Wiederer; J. Schmidt; U. Kressel; K. Dietmayer; V. Belagiannis,"Mercedes-Benz Group AG, Stuttgart, Germany; Mercedes-Benz Group AG, Stuttgart, Germany; Mercedes-Benz Group AG, Stuttgart, Germany; Institute of Measurement, Control and Microtechnology, University Ulm, Ulm, Germany; Department of Multi-media Communication and Signal Processing, Friedrich-Alexander-Universität, Erlangen, Germany",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,5487,5494,"Despite the significant research efforts on trajectory prediction for automated driving, limited work exists on assessing the prediction reliability. To address this limitation we propose an approach that covers two sources of error, namely novel situations with out-of-distribution (OOD) detection and the complexity in in-distribution (ID) situations with uncertainty estimation. We introduce two modules next to an encoder-decoder network for trajectory prediction. Firstly, a Gaussian mixture model learns the probability density function of the ID encoder features during training, and then it is used to detect the OOD samples in regions of the feature space with low likelihood. Secondly, an error regression network is applied to the encoder, which learns to estimate the trajectory prediction error in supervised training. During inference, the estimated prediction error is used as the uncertainty. In our experiments, the combination of both modules outperforms the prior work in OOD detection and uncertainty estimation, on the Shifts robust trajectory prediction dataset by 2.8 % and 10.1%, respectively. The code is publicly available44project page: https://github.com/againerju/joodu.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341616,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341616,,Training;Uncertainty;Computational modeling;Estimation;Predictive models;Probability density function;Feature extraction,,,,34,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/againerju/joodu,https://github.com/againerju/joodu
333,SeasonDepth: Cross-Season Monocular Depth Prediction Dataset and Benchmark Under Multiple Environments,H. Hu; B. Yang; Z. Qiao; S. Liu; J. Zhu; Z. Liu; W. Ding; D. Zhao; H. Wang,"Carnegie Mellon University, Pittsburgh, USA; Shanghai Jiao Tong University, Shanghai, China; Hong Kong University of Science and Technology, Hong Kong, China; Carnegie Mellon University, Pittsburgh, USA; Carnegie Mellon University, Pittsburgh, USA; Carnegie Mellon University, Pittsburgh, USA; Carnegie Mellon University, Pittsburgh, USA; Carnegie Mellon University, Pittsburgh, USA; Shanghai Jiao Tong University, Shanghai, China",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,11384,11389,"Different environments pose a great challenge to the outdoor robust visual perception for long-term autonomous driving, and the generalization of learning-based algorithms on different environments is still an open problem. Although monocular depth prediction has been well studied recently, few works focus on the robustness of learning-based depth prediction across different environments, e.g. changing illumination and seasons, owing to the lack of such a multi-environment real-world dataset and benchmark. To this end, the cross-season monocular depth prediction dataset and benchmark, SeasonDepth, is introduced to benchmark the depth estimation performance under different environments. We investigate several state-of-the-art representative open-source supervised and self-supervised depth prediction methods using newly-formulated metrics. Through extensive experimental evaluation on the proposed dataset and cross-dataset evaluation with current autonomous driving datasets, the performance and robustness against the influence of multiple environments are analyzed qualitatively and quantitatively. We show that long-term monocular depth prediction is still challenging and believe our work can boost further research on the long-term robustness and generalization for outdoor visual perception. The dataset is available on https://seasondepth.github.io.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341917,"Natural Science Foundation of China(grant numbers:62225309,62073222,U21A20480,U1913204); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341917,,Measurement;Lighting;Estimation;Prediction methods;Benchmark testing;Prediction algorithms;Robustness,,4.0,,44,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,,
334,An Attentional Recurrent Neural Network for Occlusion-Aware Proactive Anomaly Detection in Field Robot Navigation,A. Schreiber; T. Ji; D. L. McPherson; K. Driggs-Campbell,"Coordinated Science Laboratory, University of Illinois at Urbana-Champaign, Champaign, IL, USA; Coordinated Science Laboratory, University of Illinois at Urbana-Champaign, Champaign, IL, USA; Coordinated Science Laboratory, University of Illinois at Urbana-Champaign, Champaign, IL, USA; Coordinated Science Laboratory, University of Illinois at Urbana-Champaign, Champaign, IL, USA",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,8038,8045,"The use of mobile robots in unstructured environments like the agricultural field is becoming increasingly common. The ability for such field robots to proactively identify and avoid failures is thus crucial for ensuring efficiency and avoiding damage. However, the cluttered field environment introduces various sources of noise (such as sensor occlusions) that make proactive anomaly detection difficult. Existing approaches can show poor performance in sensor occlusion scenarios as they typically do not explicitly model occlusions and only leverage current sensory inputs. In this work, we present an attention-based recurrent neural network architecture for proactive anomaly detection that fuses current sensory inputs and planned control actions with a latent representation of prior robot state. We enhance our model with an explicitly-learned model of sensor occlusion that is used to modulate the use of our latent representation of prior robot state. Our method shows improved anomaly detection performance and enables mobile field robots to display increased resilience to predicting false positives regarding navigation failure during periods of sensor occlusion, particularly in cases where all sensors are briefly occluded. Our code is available at: https://github.com/andreschreiber/roar.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341852,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341852,,Recurrent neural networks;Navigation;Fuses;Supervised learning;Robot sensing systems;Robustness;Trajectory,,1.0,,32,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/andreschreiber/roar,https://github.com/andreschreiber/roar
335,MOMA-Force: Visual-Force Imitation for Real-World Mobile Manipulation,T. Yang; Y. Jing; H. Wu; J. Xu; K. Sima; G. Chen; Q. Sima; T. Kong,ByteDance Research; ByteDance Research; ByteDance Research; ByteDance Research; ByteDance Research; ByteDance Research; ByteDance Research; ByteDance Research,2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,6847,6852,"In this paper, we present a novel method for mobile manipulators to perform multiple contact-rich manipulation tasks. While learning-based methods have the potential to generate actions in an end-to-end manner, they often suffer from insufficient action accuracy and robustness against noise. On the other hand, classical control-based methods can enhance system robustness, but at the cost of extensive parameter tuning. To address these challenges, we present MOMA-Force, a visual-force imitation method that seamlessly combines representation learning for perception, imitation learning for complex motion generation, and admittance whole-body control for system robustness and controllability. MOMA-Force enables a mobile manipulator to learn multiple complex contact-rich tasks with high success rates and small contact forces. In a real household setting, our method outperforms baseline methods in terms of task success rates. Moreover, our method achieves smaller contact forces and smaller force variances compared to baseline methods without force imitation. Overall, we offer a promising approach for efficient and robust mobile manipulation in the real world. Videos and more details can be found on https://visual-force-imitation.github.io.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342371,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342371,,Learning systems;Visualization;Force;Manipulators;Robustness;Trajectory;Admittance,,2.0,,35,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,,
336,InteractionNet: Joint Planning and Prediction for Autonomous Driving with Transformers,J. Fu; Y. Shen; Z. Jian; S. Chen; J. Xin; N. Zheng,"Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, Xi'an, Shaanxi, P.R. China; Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, Xi'an, Shaanxi, P.R. China; Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, Xi'an, Shaanxi, P.R. China; Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, Xi'an, Shaanxi, P.R. China; Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, Xi'an, Shaanxi, P.R. China; Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, Xi'an, Shaanxi, P.R. China",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,9332,9339,"Planning and prediction are two important modules of autonomous driving and have experienced tremendous advancement recently. Nevertheless, most existing methods regard planning and prediction as independent and ignore the correlation between them, leading to the lack of consideration for interaction and dynamic changes of traffic scenarios. To address this challenge, we propose InteractionNet, which leverages transformer to share global contextual reasoning among all traffic participants to capture interaction and interconnect planning and prediction to achieve joint. Besides, InteractionNet deploys another transformer to help the model pay extra attention to the perceived region containing critical or unseen vehicles. InteractionNet outperforms other baselines in several benchmarks, especially in terms of safety, which benefits from the joint consideration of planning and forecasting. The code will be available at https://github.com/fujiawei0724/InteractionNet.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342367,National Key Research and Development Program of China(grant numbers:SQ2022YFB2500007); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342367,,Correlation;Collaboration;Benchmark testing;Transformers;Planning;Safety;Vehicle dynamics,,2.0,,46,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/fujiawei0724/InteractionNet,https://github.com/fujiawei0724/InteractionNet
337,RaPlace: Place Recognition for Imaging Radar using Radon Transform and Mutable Threshold,H. Jang; M. Jung; A. Kim,"Department of Mechanical Engineering, SNU, Seoul, S. Korea; Department of Mechanical Engineering, SNU, Seoul, S. Korea; Department of Mechanical Engineering, SNU, Seoul, S. Korea",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,11194,11201,"Due to the robustness in sensing, radar has been highlighted, overcoming harsh weather conditions such as fog and heavy snow. In this paper, we present a novel radar-only place recognition that measures the similarity score by utilizing Radon-transformed sinogram images and cross-correlation in frequency domain. Doing so achieves rigid transform invariance during place recognition, while ignoring the effects of radar multipath and ring noises. In addition, we compute the radar similarity distance using mutable threshold to mitigate variability of the similarity score, and reduce the time complexity of processing a copious radar data with hierarchical retrieval. We demonstrate the matching performance for both intra-session loop-closure detection and global place recognition using a publicly available imaging radar datasets. We verify reliable performance compared to existing stable radar place recognition method. Furthermore, codes for the proposed imaging radar place recognition is released for community https://github.com/hyesu-jang/RaPlace.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341883,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341883,,Image recognition;Simultaneous localization and mapping;Snow;Imaging;Radar;Transforms;Radar imaging,,8.0,,27,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/hyesu-jang/RaPlace,https://github.com/hyesu-jang/RaPlace
338,"UnLoc: A Universal Localization Method for Autonomous Vehicles using LiDAR, Radar and/or Camera Input",M. Ibrahim; N. Akhtar; S. Anwar; A. Mian,"Department of Computer Science, The University of Western, Australia; Department of Computer Science, The University of Western, Australia; Information and Computer Science, King Fahad University of Petroleum and Minerals (KFUPM), Dhahran, KSA; Department of Computer Science, The University of Western, Australia",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,5187,5194,"Localization is a fundamental task in robotics for autonomous navigation. Existing localization methods rely on a single input data modality or train several computational models to process different modalities. This leads to stringent computational requirements and sub-optimal results that fail to capitalize on the complementary information in other data streams. This paper proposes UnLoc, a novel unified neural modeling approach for localization with multi-sensor input in all weather conditions. Our multi-stream network can handle LiDAR, Camera and RADAR inputs for localization on demand, i.e., it can work with one or more input sensors, making it robust to sensor failure. UnLoc uses 3D sparse convolutions and cylindrical partitioning of the space to process LiDAR frames and implements ResNet blocks with a slot attention-based feature filtering module for the Radar and image modalities. We introduce a unique learnable modality encoding scheme to distinguish between the input sensor data. Our method is extensively evaluated on Oxford Radar RobotCar, ApolloSouthBay and Perth-WA datasets. The results ascertain the efficacy of our technique. The dataset, results, and codes are available at https://github.com/IbrahimUWA/UnLoc",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342046,Australian Research Council Future Fellowship Award(grant numbers:FT210100268); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342046,,Location awareness;Laser radar;Three-dimensional displays;Filtering;Radar;Radar imaging;Cameras,,1.0,,40,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/IbrahimUWA/UnLoc,https://github.com/IbrahimUWA/UnLoc
339,Language Guided Robotic Grasping with Fine-Grained Instructions,Q. Sun; H. Lin; Y. Fu; Y. Fu; X. Xue,"school of Data Science and Shanghai Key Lab of Intelligent Information Processingxs, Fudan University and Fudan ISTBI-ZJNU Algorithm Centre for Brain-inspired Intelligence, Zhejiang Normal University, Jinhua, China; school of Data Science and Shanghai Key Lab of Intelligent Information Processingxs, Fudan University and Fudan ISTBI-ZJNU Algorithm Centre for Brain-inspired Intelligence, Zhejiang Normal University, Jinhua, China; Beijing Institute of Technology, China; school of Data Science and Shanghai Key Lab of Intelligent Information Processingxs, Fudan University and Fudan ISTBI-ZJNU Algorithm Centre for Brain-inspired Intelligence, Zhejiang Normal University, Jinhua, China; school of Data Science and Shanghai Key Lab of Intelligent Information Processingxs, Fudan University and Fudan ISTBI-ZJNU Algorithm Centre for Brain-inspired Intelligence, Zhejiang Normal University, Jinhua, China",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,1319,1326,"Given a single RGB image and the attribute-rich language instructions, this paper investigates the novel problem of using Fine-grained instructions for the Language guided robotic Grasping (FLarG). This problem is made challenging by learning fine-grained language descriptions to ground target objects. Recent advances have been made in visually grounding the objects simply by several coarse attributes [1]. However, these methods have poor performance as they cannot well align the multi-modal features, and do not make the best of recent powerful large pre-trained vision and language models, e.g., CLIP. To this end, this paper proposes a FLarG pipeline including stages of CLIP-guided object localization, and 6-DoF category-level object pose estimation for grasping. Specially, we first take the CLIP-based segmentation model CRIS as the backbone and propose an end-to-end DyCRIS model that uses a novel dynamic mask strategy to well fuse the multi-level language and vision features. Then, the well-trained instance segmentation backbone Mask R-CNN is adopted to further improve the predicted mask of our DyCRIS. Finally, the target object pose is inferred for the robotics grasping by using the recent 6-DoF object pose estimation method. To validate our CLIP-enhanced pipeline, we also construct a validation dataset for our FLarG task and name it RefNOCS. Extensive results on RefNOCS have shown the utility and effectiveness of our proposed method. The project homepage is available at https://sunqiang85.github.ioIFLarG/.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342331,NSFC(grant numbers:62176061); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342331,,Location awareness;Instance segmentation;Point cloud compression;Grounding;Pipelines;Pose estimation;Grasping,,5.0,,39,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,,
340,Keypoints-Based Adaptive Visual Servoing for Control of Robotic Manipulators in Configuration Space,S. Chatterjee; A. C. Karade; A. Gandhi; B. Calli,"Department of Robotics Engineering, Worcester Polytechnic In-stitute, Worcester, MA, USA; Department of Robotics Engineering, Worcester Polytechnic In-stitute, Worcester, MA, USA; Department of Robotics Engineering, Worcester Polytechnic In-stitute, Worcester, MA, USA; Department of Robotics Engineering, Worcester Polytechnic In-stitute, Worcester, MA, USA",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,6387,6394,"This paper presents a visual servoing method for controlling a robot in the configuration space by purely using its natural features. We first created a data collection pipeline that uses camera intrinsics, extrinsics, and forward kinematics to generate 2D projections of a robot's joint locations (keypoints) in image space. Using this pipeline, we are able to collect large sets of real-robot data, which we use to train realtime keypoint detectors. The inferred keypoints from the trained model are used as control features in an adaptive visual servoing scheme that estimates, in runtime, the Jacobian relating the changes of the keypoints and joint velocities. We compared the 2D configuration control performance of this method to the skeleton-based visual servoing method (the only other algorithm for purely vision-based configuration space visual servoing), and demonstrated that the keypoints provide more robust and less noisy features, which result in better transient response. We also demonstrate the first vision-based 3D configuration space control results in the literature, and discuss its limitations. Our data collection pipeline is available at https://github.com/JaniC-WPI/KPDataGenerator.git which can be utilized to collect image datasets and train realtime keypoint detectors for various robots and environments.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342503,National Science Foundation(grant numbers:IIS-1900953); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342503,,Transient response;Three-dimensional displays;Runtime;Pipelines;Detectors;Aerospace electronics;Data collection,,1.0,,23,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/JaniC-WPI/KPDataGenerator,https://github.com/JaniC-WPI/KPDataGenerator
341,Arena-Rosnav 2.0: A Development and Benchmarking Platform for Robot Navigation in Highly Dynamic Environments,L. Kästner; R. Carstens; H. Zeng; J. Kmiecik; T. Bhuiyan; N. Khorsandhi; V. Shcherbyna; J. Lambrecht,"The Chair of Industry Grade Networks and Clouds, Technical University Berlin (TUB), Germany; The Chair of Industry Grade Networks and Clouds, Technical University Berlin (TUB), Germany; The Chair of Robotics, Artificial Intelligence and Real-time Systems, Technical University Munich (TUM), Germany; The Chair of Industry Grade Networks and Clouds, Technical University Berlin (TUB), Germany; The Chair of Industry Grade Networks and Clouds, Technical University Berlin (TUB), Germany; The Chair of Industry Grade Networks and Clouds, Technical University Berlin (TUB), Germany; The Chair of Industry Grade Networks and Clouds, Technical University Berlin (TUB), Germany; The Chair of Industry Grade Networks and Clouds, Technical University Berlin (TUB), Germany",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,11257,11264,"Following up on our previous works, in this paper, we present Arena-Rosnav 2.0 an extension to our previous works Arena-Bench [1] and Arena-Rosnav [2], which adds a variety of additional modules for developing and benchmarking robotic navigation approaches. The platform is fundamentally restructured and provides unified APIs to add additional functionalities such as planning algorithms, simulators, or evaluation functionalities. We have included more realistic simulation and pedestrian behavior and provide a profound documentation to lower the entry barrier. We evaluated our system by first, conducting a user study in which we asked experienced researchers as well as new practitioners and students to test our system. The feedback was mostly positive and a high number of participants are utilizing our system for other research endeavors. Finally, we demonstrate the feasibility of our system by integrating two new simulators and a variety of state of the art navigation approaches and benchmark them against one another. The platform is openly available at https://github.com/Arena-Rosnav.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342152,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342152,,Training;Navigation;Documentation;Tutorials;Benchmark testing;User interfaces;Robot sensing systems,,3.0,,34,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/Arena-Rosnav,https://github.com/Arena-Rosnav
342,PuSHR: A Multirobot System for Nonprehensile Rearrangement,S. Talia; A. Thareja; C. Mavrogiannis; M. Schmittle; S. S. Srinivasa,"Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA, USA; Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA, USA; Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA, USA; Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA, USA; Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA, USA",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,5380,5387,"We focus on the problem of rearranging a set of objects with a team of car-like robot pushers built using off-the-shelf components. Maintaining control of pushed objects while avoiding collisions in a tight space demands highly coordinated motion that is challenging to execute on constrained hardware. Centralized replanning approaches become intractable even for small-sized problems whereas decentralized approaches often get stuck in deadlocks. Our key insight is that by carefully assigning pushing tasks to robots, we could reduce the complexity of the rearrangement task, enabling robust performance via scalable decentralized control. Based on this insight, we built PuSHR, a system that optimally assigns pushing tasks and trajectories to robots offline, and performs trajectory tracking via decentralized control online. Through an ablation study in simulation, we demonstrate that PuSHR dominates baselines ranging from purely centralized to fully decentralized in terms of success rate and time efficiency across challenging tasks with up to 4 robots. Hardware experiments demonstrate the transfer of our system to the real world and highlight its robustness to model inaccuracies. Our code can be found at https://github.com/prl-mushr/pushr, and videos from our experiments at https://youtu.be/nyUn9mHoR8Y.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341853,"National Science Foundation(grant numbers:2132848); CHS(grant numbers:2007011); DARPA(grant numbers:HR0011-21-C-0171); Office of Naval Research(grant numbers:N00014-17-1-2617-P00004,2022-016-01 UW); Amazon; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341853,,Trajectory tracking;Robot kinematics;Decentralized control;System recovery;Hardware;Robustness;Trajectory,,,,40,USGov,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/prl-mushr/pushr,https://github.com/prl-mushr/pushr
343,LocalViT: Analyzing Locality in Vision Transformers,Y. Li; K. Zhang; J. Cao; R. Timofte; M. Magno; L. Benini; L. Van Goo,"Computer Vision Lab, D-ITET, ETH, Zurich, Switzerland; Computer Vision Lab, D-ITET, ETH, Zurich, Switzerland; Computer Vision Lab, D-ITET, ETH, Zurich, Switzerland; Computer Vision Lab, D-ITET, ETH, Zurich, Switzerland; Center for Project-Based Learning, D-ITET, ETH, Zurich, Switzerland; Integrated Systems Laboratory, D-ITET, ETH, Zurich, Switzerland; Processing Speech and Images (PSI), KU Leuven, Belgium",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,9598,9605,"The aim of this paper is to study the influence of locality mechanisms in vision transformers. Transformers originated from machine translation and are particularly good at modelling long-range dependencies within a long sequence. Although the global interaction between the token embeddings could be well modelled by the self-attention mechanism of transformers, what is lacking is a locality mechanism for infor-mation exchange within a local region. In this paper, locality mechanism is systematically investigated by carefully designed controlled experiments. We add locality to vision transformers into the feed-forward network. This seemingly simple solution is inspired by the comparison between feed-forward networks and inverted residual blocks. The importance of locality mechanisms is validated in two ways: 1) A wide range of design choices (activation function, layer placement, expansion ratio) are available for incorporating locality mechanisms and proper choices can lead to a performance gain over the baseline, and 2) The same locality mechanism is successfully applied to vision transformers with different architecture designs, which shows the generalization of the locality concept. For ImageNet2012 classification, the locality-enhanced transformers outperform the baselines Swin-T [1], DeiT-T [2] and PVT-T [3] by 1.0%, 2.6 % and 3.1 % with a negligible increase in the number of parameters and computational effort. Code is available at https://github.com/ofsoundof/LocalViT.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342025,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342025,,Convolutional codes;Lattices;Computer architecture;Performance gain;Transformers;Machine translation;Intelligent robots,,49.0,,45,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/ofsoundof/LocalViT,https://github.com/ofsoundof/LocalViT
344,Visual Pre-Training for Navigation: What Can We Learn from Noise?,Y. Wang; C. -Y. Ko; P. Agrawal,"Department of Electrical Engineering and Computer Science, MIT; Department of Electrical Engineering and Computer Science, MIT; Department of Electrical Engineering and Computer Science, MIT",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,3897,3902,"One powerful paradigm in visual navigation is to predict actions from observations directly. Training such an end-to-end system allows representations useful for downstream tasks to emerge automatically. However, the lack of inductive bias makes this system data inefficient. We hypothesize a sufficient representation of the current view and the goal view for a navigation policy can be learned by predicting the location and size of a crop of the current view that corresponds to the goal. We further show that training such random crop prediction in a self-supervised fashion purely on synthetic noise images transfers well to natural home images. The learned representation can then be bootstrapped to learn a navigation policy efficiently with little interaction data. The code is available at https://yanweiw.github.io/noise2ptz/",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342521,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342521,,Training;Visualization;Codes;Navigation;Crops;Task analysis;Intelligent robots,,1.0,,22,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://yanweiw.github.io/noise2ptz,https://github.com/yanweiw/noise2ptz
345,LAMP: Leveraging Language Prompts for Multi-Person Pose Estimation,S. Hu; C. Zheng; Z. Zhou; C. Chen; G. Sukthankar,"University of Central Florida, Orlando, FL, USA; University of Central Florida, Orlando, FL, USA; University of Central Florida, Orlando, FL, USA; University of Central Florida, Orlando, FL, USA; University of Central Florida, Orlando, FL, USA",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,3759,3766,"Human-centric visual understanding is an important desideratum for effective human-robot interaction. In order to navigate crowded public places, social robots must be able to interpret the activity of the surrounding humans. This paper addresses one key aspect of human-centric visual understanding, multi-person pose estimation. Achieving good performance on multi-person pose estimation in crowded scenes is difficult due to the challenges of occluded joints and instance separation. In order to tackle these challenges and overcome the limitations of image features in representing invisible body parts, we propose a novel prompt-based pose inference strategy called LAMP (Language Assisted Multi-person Pose estimation). By utilizing the text representations generated by a well-trained language model (CLIP), LAMP can facilitate the understanding of poses on the instance and joint levels, and learn more robust visual representations that are less susceptible to occlusion. This paper demonstrates that language-supervised training boosts the performance of single-stage multi-person pose estimation, and both instance-level and joint-level prompts are valuable for training. The code is available at https://github.com/shengnanh20/LAMP.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341430,Lockheed Martin Corporation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341430,,Training;Visualization;Codes;Navigation;Pose estimation;Social robots;Intelligent robots,,1.0,,52,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/shengnanh20/LAMP,https://github.com/shengnanh20/LAMP
346,Learning Soft Robot Dynamics Using Differentiable Kalman Filters and Spatio-Temporal Embeddings,X. Liu; S. Ikemoto; Y. Yoshimitsu; H. B. Amor,"School of Computing and Augmented Intelligence, Arizona State University; Department of Human Intelligence Systems, Graduate School of Life Science and Systems Engineering, Kyushu Institute of Technology; Department of Human Intelligence Systems, Graduate School of Life Science and Systems Engineering, Kyushu Institute of Technology; School of Computing and Augmented Intelligence, Arizona State University",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,2550,2557,"This paper introduces a novel approach for modeling the dynamics of soft robots, utilizing a differentiable filter architecture. The proposed approach enables end-to-end training to learn system dynamics, noise characteristics, and temporal behavior of the robot. A novel spatio-temporal embedding process is discussed to handle observations with varying sensor placements and sampling frequencies. The efficacy of this approach is demonstrated on a tensegrity robot arm by learning end-effector dynamics from demonstrations with complex bending motions. The model is proven to be robust against missing modalities, diverse sensor placement, and varying sampling rates. Additionally, the proposed framework is shown to identify physical interactions with humans during motion. The utilization of a differentiable filter presents a novel solution to the difficulties of modeling soft robot dynamics. Our approach shows substantial improvement in accuracy compared to state-of-the-art filtering methods, with at least a 24% reduction in mean absolute error (MAE) observed. Furthermore, the predicted end-effector positions show an average MAE of 25.77mm from the ground truth, highlighting the advantage of our approach. The code is available at https://github.com/ir-lab/soft_robot_DEnKF.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341856,"KAKENHI(grant numbers:19K0285,19H01122,21H03524); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341856,,Measurement;Sensor placement;Analytical models;Deformation;Force;Dynamics;Soft robotics,,2.0,,32,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/ir-lab/soft_robot_DEnKF,https://github.com/ir-lab/soft_robot_DEnKF
347,navlie: A Python Package for State Estimation on Lie Groups,C. C. Cossette; M. Cohen; V. Korotkine; A. Del Castillo Bernal; M. A. Shalaby; J. R. Forbes,"Department of Mechanical Engineering, McGill University, Montreal, QC, Canada; Department of Mechanical Engineering, McGill University, Montreal, QC, Canada; Department of Mechanical Engineering, McGill University, Montreal, QC, Canada; Department of Mechanical Engineering, McGill University, Montreal, QC, Canada; Department of Mechanical Engineering, McGill University, Montreal, QC, Canada; Department of Mechanical Engineering, McGill University, Montreal, QC, Canada",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,5282,5287,"The ability to rapidly test a variety of algorithms for an arbitrary state estimation task is valuable in the prototyping phase of navigation systems. Lie group theory is now mainstream in the robotics community, and hence estimation prototyping tools should allow state definitions that belong to manifolds. A new package, called navlie, provides a framework that allows a user to model a large class of problems by implementing a set of classes complying with a generic interface. Once accomplished, navlie provides a variety of on-manifold estimation algorithms that can run directly on these classes. The package also provides a built-in library of common models, as well as many useful utilities. The open-source project can be found at https://github.com/decargroup/navlie",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342362,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342362,Localization;Sensor Fusion;Software Tools for Robot Programming,Manifolds;Estimation error;Navigation;Prototypes;Libraries;State estimation;Task analysis,,1.0,,23,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/decargroup/navlie,https://github.com/decargroup/navlie
348,WIT-UAS: A Wildland-Fire Infrared Thermal Dataset to Detect Crew Assets from Aerial Views,A. Jong; M. Yu; D. Dhrafani; S. Kailas; B. Moon; K. Sycara; S. Scherer,"Carnegie Mellon University, School of Computer Science, the Robotics Institute, Pittsburgh, PA, USA; Carnegie Mellon University, School of Computer Science, the Robotics Institute, Pittsburgh, PA, USA; Department of Mechanical Engineering, Carnegie Mellon University, College of Engineering, Pittsburgh, PA, USA; Carnegie Mellon University, School of Computer Science, the Robotics Institute, Pittsburgh, PA, USA; Carnegie Mellon University, School of Computer Science, the Robotics Institute, Pittsburgh, PA, USA; Carnegie Mellon University, School of Computer Science, the Robotics Institute, Pittsburgh, PA, USA; Carnegie Mellon University, School of Computer Science, the Robotics Institute, Pittsburgh, PA, USA",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,11464,11471,"We present the Wildland-fire Infrared Thermal (WIT-UAS) dataset for long-wave infrared sensing of crew and vehicle assets amidst prescribed wildland fire environments. While such a dataset is crucial for safety monitoring in wildland fire applications, to the authors' awareness, no such dataset focusing on assets near fire is publicly available. Presumably, this is due to the barrier to entry of collaborating with fire management personnel. We present two related data subsets: WIT-UAS-ROS consists of full ROS bag files containing sensor and robot data of UAS flight over the fire, and WIT-UAS-Image contains hand-labeled long-wave infrared (LWIR) images extracted from WIT-UAS-ROS. Our dataset is the first to focus on asset detection in a wildland fire environment. We show that thermal detection models trained without fire data frequently detect false positives by classifying fire as people. By adding our dataset to training, we show that the false positive rate is reduced significantly. Yet asset detection in wildland fire environments is still significantly more challenging than detection in urban environments, due to dense obscuring trees, greater heat variation, and overbearing thermal signal of the fire. We publicize this dataset to encourage the community to study more advanced models to tackle this challenging environment. The dataset, code and pretrained models are available at https://github.com/castacks/WIT-UAS-Dataset.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341683,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341683,,Training;Analytical models;Wildfires;Urban areas;Training data;Robot sensing systems;Data models,,2.0,,22,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/castacks/WIT-UAS-Dataset,https://github.com/castacks/WIT-UAS-Dataset
349,Value-Informed Skill Chaining for Policy Learning of Long-Horizon Tasks with Surgical Robot,T. Huang; K. Chen; W. Wei; J. Li; Y. Long; Q. Dou,"The Department of Computer Science and Engineering, The Chinese University of Hong Kong; The Department of Computer Science and Engineering, The Chinese University of Hong Kong; The Department of Computer Science and Engineering, The Chinese University of Hong Kong; The Department of Computer Science and Engineering, The Chinese University of Hong Kong; The Department of Computer Science and Engineering, The Chinese University of Hong Kong; The Department of Computer Science and Engineering, The Chinese University of Hong Kong",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,8495,8501,"Reinforcement learning is still struggling with solving long-horizon surgical robot tasks which involve multiple steps over an extended duration of time due to the policy exploration challenge. Recent methods try to tackle this problem by skill chaining, in which the long-horizon task is decomposed into multiple subtasks for easing the exploration burden and subtask policies are temporally connected to complete the whole long-horizon task. However, smoothly connecting all subtask policies is difficult for surgical robot scenarios. Not all states are equally suitable for connecting two adjacent subtasks. An undesired terminate state of the previous subtask would make the current subtask policy unstable and result in a failed execution. In this work, we introduce value-informed skill chaining (ViSkill), a novel reinforcement learning framework for long-horizon surgical robot tasks. The core idea is to distinguish which terminal state is suitable for starting all the following subtask policies. To achieve this target, we introduce a state value function that estimates the expected success probability of the entire task given a state. Based on this value function, a chaining policy is learned to instruct subtask policies to terminate at the state with the highest value so that all subsequent policies are more likely to be connected for accomplishing the task. We demonstrate the effectiveness of our method on three complex surgical robot tasks from SurRoL, a comprehensive surgical simulation platform, achieving high task success rates and execution efficiency. Code is available at https: / /github. com/med-air/ViSkill.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342180,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342180,,Medical robotics;Codes;Reinforcement learning;Task analysis;Intelligent robots;Software development management,,1.0,,38,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,,
350,RVWO: A Robust Visual-Wheel SLAM System for Mobile Robots in Dynamic Environments,J. Mahmoud; A. Penkovskiy; H. T. Long Vuong; A. Burkov; S. Kolyubin,"Faculty of Control Systems and Robotics, ITMO University, St. Petersburg, Russia; Faculty of Control Systems and Robotics, ITMO University, St. Petersburg, Russia; Faculty of Control Systems and Robotics, ITMO University, St. Petersburg, Russia; Faculty of Control Systems and Robotics, ITMO University, St. Petersburg, Russia; Faculty of Control Systems and Robotics, ITMO University, St. Petersburg, Russia",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,3468,3474,"This paper presents RVWO, a system designed to provide robust localization and mapping for wheeled mobile robots in challenging scenarios. The proposed approach leverages a probabilistic framework that incorporates semantic prior information about landmarks and visual re-projection error to create a landmark reliability model, which acts as an adaptive kernel for the visual residuals in optimization. Additionally, we fuse visual residuals with wheel odometry measurements, taking advantage of the planar motion assumption. The RVWO system is designed to be robust against wrong data association due to moving objects, poor visual texture, bad illumination, and wheel slippage. Evaluation results demonstrate that the proposed system shows competitive results in dynamic environments and outperforms existing approaches on both public benchmarks and our custom hardware setup. We also provide the code as an open-source contribution to the robotics community22https://github.com/be2rlab/rvwo.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342183,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342183,,Visualization;Simultaneous localization and mapping;Semantics;Dynamics;Pose estimation;Wheels;Probabilistic logic,,3.0,,23,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/be2rlab/rvwo,https://github.com/be2rlab/rvwo
351,Hybrid Object Tracking with Events and Frames,Z. Li; N. A. Piga; F. Di Pietro; M. Iacono; A. Glover; L. Natale; C. Bartolozzi,"Humanoid Sensing and Perception, Istituto Italiano di Tecnologia, Italy; Humanoid Sensing and Perception, Istituto Italiano di Tecnologia, Italy; Event-driven Perception for Robotics, Istituto Italiano di Tecnologia, Italy; Event-driven Perception for Robotics, Istituto Italiano di Tecnologia, Italy; Event-driven Perception for Robotics, Istituto Italiano di Tecnologia, Italy; Humanoid Sensing and Perception, Istituto Italiano di Tecnologia, Italy; Event-driven Perception for Robotics, Istituto Italiano di Tecnologia, Italy",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,9057,9064,"Robust object pose tracking plays an important role in robot manipulation, but it is still an open issue for quickly moving targets as motion blur and low frequency detection can reduce pose estimation accuracy even for state-of-the-art RGB-D-based methods. An event-camera is a low-latency vision sensor that can act complementary to RGB-D. Specifically, its sub-millisecond temporal resolution can be exploited to correct for pose estimation inaccuracies due to low frequency RGB-D based detection. To do so, we propose a dual Kalman filter: the first filter estimates an object's velocity from the spatiotemporal patterns of “events”, the second filter fuses the tracked object velocity with a low-frequency object pose estimated from a deep neural network using RGB-D data. The full system outputs high frequency, accurate object poses also for fast moving objects. The proposed method works towards low-power robotics by replacing high-cost GPU-based optical flow used in prior work with event-cameras that inherently extract the required signal without costly processing. The proposed algorithm achieves comparable or better performance when compared to two state-of-the-art 6-DoF object pose estimation algorithms and one hybrid event/RGB-D algorithm on benchmarks with simulated and real data. We discuss the benefits and tradeoffs for using the event-camera and contribute algorithm, code, and datasets to the community. The code and datasets are available at https://github.com/event-driven-robotics/Hybrid-object-tracking-with-events-and-frames.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342300,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342300,,Optical filters;Training;Codes;Heuristic algorithms;Pose estimation;Vision sensors;6-DOF,,1.0,,31,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/event-driven-robotics/Hybrid-object-tracking-with-events-and-frames,https://github.com/event-driven-robotics/Hybrid-object-tracking-with-events-and-frames
352,Optimal and Stable Multi-Layer Object Rearrangement on a Tabletop,A. Xu; K. Gao; S. W. Feng; J. Yu,"Department of Computer Science, Rutgers, the State University of New Jersey, Piscataway, NJ, USA; Department of Computer Science, Rutgers, the State University of New Jersey, Piscataway, NJ, USA; Department of Computer Science, Rutgers, the State University of New Jersey, Piscataway, NJ, USA; Department of Computer Science, Rutgers, the State University of New Jersey, Piscataway, NJ, USA",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,2078,2085,"Object rearrangement is a fundamental sub-task in accomplishing a great many physical tasks. As such, effectively executing rearrangement is an important skill for intelligent robots to master. In this study, we conduct the first algorithmic study on optimally solving the problem of Multi-layer Object Rearrangement on a Tabletop (MORT), in which one object may be relocated at a time, and an object can only be moved if other objects do not block its top surface. In addition, any intermediate structure during the reconfiguration process must be physically stable, i.e., it should stand without external support. To tackle the dual challenges of untangling the dependencies between objects and ensuring structural stability, we develop an algorithm that interleaves the computation of the optimal rearrangement plan and structural stability checking. Using a carefully constructed integer linear programming (ILP) model, our algorithm, Stability-Aware Rearrangement Programming (SARP), readily scales to optimally solve complex rearrangement problems of 3D structures with over 60 building blocks, with solution quality significantly outperforming natural greedy best-first approaches. Upon the publication of the manuscript source code and data will be available at https//github.com/arc-1/mort/.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342446,"NSF(grant numbers:IIS-1845888,IIS-2132972); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342446,,Solid modeling;Three-dimensional displays;Source coding;Computational modeling;Integer linear programming;Data models;Structural engineering,,,,38,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/arc-1/mort,https://github.com/arc-1/mort
353,Multi-View Robust Collaborative Localization in High Outlier Ratio Scenes Based on Semantic Features,Y. Tang; M. Wang; Y. Deng; Y. Yang; Z. Lan; Y. Yue,"School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; Inceptio Technology Co. Ltd.; School of Automation, Beijing Institute of Technology, Beijing, China",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,11042,11047,"Filtering out outlier data associations between local maps can improve the robustness and accuracy of multi-robot localization. When the overlap is low and the field of view difference is large, it is likely to produce outlier data associations between local maps, which will reduce the matching accuracy and even lead to the failure of collaborative localization. To solve this problem, this paper proposes a novel outdoor robust collaborative localization algorithm (HORCL) capable for high outlier ratio scenes. The Mixture Probability Model (MPM) and the Hierarchical EM (Expectation Maximization) algorithm in HORCL are applied to screen two levels of outliers (loop closure constraints and point pairs) and improve localization performance. Specifically, the inlier probabilities of data associations are calculated in MPM to identify outliers by considering geometric distances, semantic consistency, and spatial consistency. Then, outlier loop closures and outlier point pairs in inlier constraints are filtered by applying the Hierarchical EM algorithm, thereby relieving the adverse effect of outliers on localization accuracy. The proposed algorithm is validated on public datasets and compared with the latest methods, demonstrating the improvement in localization accuracy and robustness. The code is available at https://github.com/BIT-TYJ/HORCL.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342524,"National Natural Science Foundation of China(grant numbers:NSFC 62233002,62003039,61973034,U1913203); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342524,,Location awareness;Semantics;Collaboration;Probability;Filtering algorithms;Robustness;Data models,,3.0,,30,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/BIT-TYJ/HORCL,https://github.com/BIT-TYJ/HORCL
354,DR-Pose: A Two-Stage Deformation-and-Registration Pipeline for Category-Level 6D Object Pose Estimation,L. Zhou; Z. Liu; R. Gan; H. Wang; M. H. Ang,"Department of Mechanical Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,1192,1199,"Category-level object pose estimation involves estimating the 6D pose and the 3D metric size of objects from predetermined categories. While recent approaches take categorical shape prior information as reference to improve pose estimation accuracy, the single-stage network design and training manner lead to sub-optimal performance since there are two distinct tasks in the pipeline. In this paper, the advantage of two-stage pipeline over single-stage design is discussed. To this end, we propose a two-stage deformation-and-registration pipeline called DR-Pose, which consists of completion-aided deformation stage and scaled registration stage. The first stage uses a point cloud completion method to generate unseen parts of target object, guiding subsequent deformation on the shape prior. In the second stage, a novel registration network is designed to extract pose-sensitive features and predict the representation of object partial point cloud in canonical space based on the deformation results from the first stage. DR-Pose produces superior results to the state-of-the-art shape prior-based methods on both CAMERA25 and REAL275 benchmarks. Codes are available at https://github.com/Zray26/DR-Pose.git.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341552,"Agency for Science, Technology and Research (A* STAR)(grant numbers:A18A2b0046); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341552,,Point cloud compression;Training;Three-dimensional displays;Shape;Deformation;Pipelines;Pose estimation,,4.0,,29,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/Zray26/DR-Pose,https://github.com/Zray26/DR-Pose
355,RADA: Robust Adversarial Data Augmentation for Camera Localization in Challenging Conditions,J. Wang; M. R. U. Saputra; C. Xiaoxuan Lu; N. Trigoni; A. Markham,"Department of Computer Science, University of Oxford, UK; Data Science Department, Monash University, Indonesia; School of Informatics, the University of Edinburgh, UK; Department of Computer Science, University of Oxford, UK; Department of Computer Science, University of Oxford, UK",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,3335,3342,"Camera localization is a fundamental problem for many applications in computer vision, robotics, and autonomy. Despite recent deep learning-based approaches, the lack of robustness in challenging conditions persists due to changes in appearance caused by texture-less planes, repeating structures, reflective surfaces, motion blur, and illumination changes. Data augmentation is an attractive solution, but standard image perturbation methods fail to improve localization robustness. To address this, we propose RADA, which concentrates on perturbing the most vulnerable pixels to generate relatively less image perturbations that perplex the network. Our method outperforms previous augmentation techniques, achieving up to twice the accuracy of state-of-the-art models even under ‘unseen’ challenging weather conditions. Videos of our results can be found at https://youtu.be/niOv7-fJeCA. The source code for RADA is publicly available at https://github.com/jialuwang123321/RADA.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341653,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341653,,Location awareness;Training;Perturbation methods;Robot vision systems;Cameras;Data augmentation;Robustness,,1.0,,37,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/jialuwang123321/RADA,https://github.com/jialuwang123321/RADA
356,F2BEV: Bird's Eye View Generation from Surround-View Fisheye Camera Images for Automated Driving,E. U. Samani; F. Tao; H. R. Dasari; S. Ding; A. G. Banerjee,"Department of Mechanical Engineering, University of Washington, Seattle, WA, USA; Volvo Cars, Sunnyvale, CA, USA; Volvo Cars, Sunnyvale, CA, USA; Volvo Cars, Sunnyvale, CA, USA; Department of Industrial & Systems Engineering, Department of Mechanical Engineering, University of Washington, Seattle, WA, USA",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,9367,9374,"Bird's Eye View (BEV) representations are tremendously useful for perception-related automated driving tasks. However, generating BEVs from surround-view fisheye camera images is challenging due to the strong distortions introduced by such wide-angle lenses. We take the first step in addressing this challenge and introduce a baseline, F2BEV, to generate discretized BEV height maps and BEV semantic segmentation maps from fisheye images. F2BEV consists of a distortion-aware spatial cross attention module for querying and consolidating spatial information from fisheye image features in a transformer-style architecture followed by a task-specific head. We evaluate single-task and multi-task variants of F2BEV on our synthetic FB-SSEM dataset, all of which generate better BEV height and segmentation maps (in terms of the IoU) than a state-of-the-art BEV generation method operating on undistorted fisheye images. We also demonstrate discretized height map generation from real-world fisheye images using F2BEV. Our dataset is publicly available at https://github.com/volvo-cars/FB-SSEM-dataset",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341862,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341862,,Head;Semantic segmentation;Robot vision systems;Cameras;Transformers;Multitasking;Distortion,,2.0,,45,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/volvo-cars/FB-SSEM-dataset,https://github.com/volvo-cars/FB-SSEM-dataset
357,Primitive Skill-Based Robot Learning from Human Evaluative Feedback,A. Hiranaka; M. Hwang; S. Lee; C. Wang; L. Fei-Fei; J. Wu; R. Zhang,"Department of Mechanical Engineering; Department of Computer Science, Stanford University, CA, USA; Department of Computer Science, Stanford University, CA, USA; Department of Computer Science, Stanford University, CA, USA; Department of Computer Science, Stanford University, CA, USA; Department of Computer Science, Stanford University, CA, USA; Department of Computer Science, Stanford University, CA, USA",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,7817,7824,"Reinforcement learning (RL) algorithms face significant challenges when dealing with long-horizon robot manipulation tasks in real-world environments due to sample inefficiency and safety issues. To overcome these challenges, we propose a novel framework, SEED, which leverages two approaches: reinforcement learning from human feedback (RLHF) and primitive skill-based reinforcement learning. Both approaches are particularly effective in addressing sparse reward issues and the complexities involved in long-horizon tasks. By combining them, SEED reduces the human effort required in RLHF and increases safety in training robot manipulation with RL in real-world settings. Additionally, parameterized skills provide a clear view of the agent's high-level intentions, allowing humans to evaluate skill choices before they are executed. This feature makes the training process even safer and more efficient. To evaluate the performance of SEED, we conducted extensive experiments on five manipulation tasks with varying levels of complexity. Our results show that SEED significantly outperforms state-of-the-art RL algorithms in sample efficiency and safety. In addition, SEED also exhibits a substantial reduction of human effort compared to other RLHF methods. Further details and video results can be found at https://seediros23.github.io/.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341912,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341912,,Training;Costs;Reinforcement learning;Robot learning;Safety;Complexity theory;Task analysis,,1.0,,44,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,,
358,Online Monocular Lane Mapping Using Catmull-Rom Spline,Z. Qiao; Z. Yu; H. Yin; S. Shen,"Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, China",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,7179,7186,"In this study, we introduce an online monocular lane mapping approach that solely relies on a single camera and odometry for generating spline-based maps. Our proposed technique models the lane association process as an assignment issue utilizing a bipartite graph, and assigns weights to the edges by incorporating Chamfer distance, pose uncertainty, and lateral sequence consistency. Furthermore, we meticulously design control point initialization, spline parameterization, and optimization to progressively create, expand, and refine splines. In contrast to prior research that assessed performance using self-constructed datasets, our experiments are conducted on the openly accessible OpenLane dataset. The experimental outcomes reveal that our suggested approach enhances lane association and odometry precision, as well as overall lane map quality. We have open-sourced out code11https://github.com/HKUST-Aerial-Robotics/MonoLaneMapping for this project.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341707,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341707,,Uncertainty;Cameras;Bipartite graph;Odometry;Splines (mathematics);Optimization;Intelligent robots,,5.0,,39,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/HKUST-Aerial-Robotics/MonoLaneMapping,https://github.com/HKUST-Aerial-Robotics/MonoLaneMapping
359,Open-Vocabulary Affordance Detection in 3D Point Clouds,T. Nguyen; M. N. Vu; A. Vuong; D. Nguyen; T. Vo; N. Le; A. Nguyen,"FPT Software AI Center, Vietnam; Automation & Control Institute (ACIN), TU Wien, Vienna, Austria; FPT Software AI Center, Vietnam; FPT Software AI Center, Vietnam; Faculty of Mathematics and Statistics, Ton Duc Thang University, Ho Chi Minh City, Vietnam; Department of Computer Science & Computer Engineering, University of Arkansas; Department of Computer Science, University of Liverpool, UK",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,5692,5698,"Affordance detection is a challenging problem with a wide variety of robotic applications. Traditional affordance detection methods are limited to a predefined set of affordance labels, hence potentially restricting the adaptability of intelligent robots in complex and dynamic environments. In this paper, we present the Open-Vocabulary Affordance Detection (OpenAD) method, which is capable of detecting an unbounded number of affordances in 3D point clouds. By simultaneously learning the affordance text and the point feature, OpenAD successfully exploits the semantic relationships between affordances. Therefore, our proposed method enables zero-shot detection and can be able to detect previously unseen affordances without a single annotation example. Intensive experimental results show that OpenAD works effectively on a wide range of affordance detection setups and outperforms other baselines by a large margin. Additionally, we demonstrate the practicality of the proposed OpenAD in real-world robotic applications with a fast inference speed. Our project is available at https://openad2023.github.io.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341553,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341553,,Point cloud compression;Three-dimensional displays;Annotations;Affordances;Semantics;Usability;Intelligent robots,,9.0,,60,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,,
360,RaSpectLoc: RAman SPECTroscopy-dependent robot LOCalisation,C. Thirgood; O. Mendez; E. C. Ling; J. Storey; S. Hadfield,"CVSSP, Computer Science and Electronic Engineering, University of Surrey, Guildford, Surrey, United Kingdom; CVSSP, Computer Science and Electronic Engineering, University of Surrey, Guildford, Surrey, United Kingdom; Surrey Institute for People-Centered Artificial Intelligence, University of Surrey, Guildford, Surrey, United Kingdom; Industrial 3D Robotics, Tonbridge, Kent, UK; CVSSP, Computer Science and Electronic Engineering, University of Surrey, Guildford, Surrey, United Kingdom",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,5296,5303,"This paper presents a new information source for supporting robot localisation: material composition. The proposed method complements the existing visual, structural, and semantic cues utilized in the literature. However, it has a distinct advantage in its ability to differentiate structurally [23], visually [25] or categorically [1] similar objects such as different doors, by using Raman spectrometers. Such devices can identify the material of objects it probes through the bonds between the material's molecules. Unlike similar sensors, such as mass spectroscopy, it does so without damaging the material or environment. In addition to introducing the first material-based localisation algorithm, this paper supports the future growth of the field by presenting a gazebo plugin for Raman spectrometers, material sensing demonstrations, as well as the first-ever localisation data-set with benchmarks for material-based localisation. This benchmarking shows that the proposed technique results in a significant improvement over current state-of-the-art localisation techniques, achieving 16 % more accurate localisation than the leading baseline. The code and dataset will be released at: https://github.com/ThirgoodC/RaSpectLoc",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342198,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342198,,Visualization;Simultaneous localization and mapping;Navigation;Semantics;Benchmark testing;Mass spectroscopy;Sensors,,,,32,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/ThirgoodC/RaSpectLoc,https://github.com/ThirgoodC/RaSpectLoc
361,"UMIRobot: An Open-{Software, Hardware} Low-Cost Robotic Manipulator for Education",M. M. Marinho; H. -C. Lin; J. Zhao,"Department of Mechanical Engineering, the University of Tokyo, Tokyo, Japan; Department of Mechanical Engineering, the University of Tokyo, Tokyo, Japan; Department of Mechanical Engineering, the University of Tokyo, Tokyo, Japan",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,4464,4471,"Robot teleoperation has been studied for the past 70 years and is relevant in many contexts, such as in the handling of hazardous materials and telesurgery. The COVID19 pandemic has rekindled interest in this topic, but the existing robotic education kits fall short of being suitable for teleoperated robotic manipulator learning. In addition, the global restrictions of motion motivated large investments in online/hybrid education. In this work, a newly developed robotics education kit and its ecosystem are presented which is used as the backbone of an online/hybrid course in teleoperated robots. The students are divided into teams. Each team designs, fabricates (3D printing and assembling), and implements a control strategy for a master device and gripper. Coupling those with the UMIRobot, provided as a kit, the students compete in a teleoperation challenge. The kit is low cost (<100USD), which allows higher-learning institutions to provide one kit per student and they can learn in a risk-free environment. As of now, 73 such kits have been assembled and sent to course participants in eight countries. As major success stories, we show an example of gripper and master designed for the proposed course. In addition, we show a teleoperated task between Japan and Bangladesh executed by course participants. Design files, videos, source code, and more information are available at https://mmmarinho.github.io/UMIRobot/",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341347,University of Tokyo; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341347,,Temperature sensors;Three-dimensional displays;Education;Ecosystems;Manipulators;Three-dimensional printing;Hardware,,,,19,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://mmmarinho.github.io/UMIRobot,https://github.com/mmmarinho/umirobot-arduino
362,Multiplanar Self-Calibration for Mobile Cobot 3D Object Manipulation Using 2D Detectors and Depth Estimation,T. Dang; K. Nguyen; M. Huber,"Department of Computer Science and Engineering, Learning and Adaptive Robotics Laboratory, University of Texas at Arlington, Arlington, TX, USA; Department of Computer Science and Engineering, Learning and Adaptive Robotics Laboratory, University of Texas at Arlington, Arlington, TX, USA; Department of Computer Science and Engineering, Learning and Adaptive Robotics Laboratory, University of Texas at Arlington, Arlington, TX, USA",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,1782,1788,"Calibration is the first and foremost step in dealing with sensor displacement errors that can appear during extended operation and off-time periods to enable robot object manipulation with precision. In this paper, we present a novel multiplanar self-calibration between the camera system and the robot's end-effector for 3D object manipulation. Our approach first takes the robot end-effector as ground truth to calibrate the camera's position and orientation while the robot arm moves the object in multiple planes in 3D space, and a 2D state-of-the-art vision detector identifies the object's center in the image coordinates system. The transformation between world coordinates and image coordinates is then computed using 2D pixels from the detector and 3D known points obtained by robot kinematics. Next, an integrated stereo-vision system estimates the distance between the camera and the object, resulting in 3D object localization. We test our proposed method on the Baxter robot with two 7-DOF arms and a 2D detector that can run in real time on an onboard GPU. After self-calibrating, our robot can localize objects in 3D using an RGB camera and depth image. The source code is available at https://github.com/tuantdang/calib_cobot.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341911,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341911,,Three-dimensional displays;Robot kinematics;Source coding;Robot vision systems;Detectors;Cameras;End effectors,,,,30,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/tuantdang/calib_cobot,https://github.com/tuantdang/calib_cobot
363,VL-Grasp: a 6-Dof Interactive Grasp Policy for Language-Oriented Objects in Cluttered Indoor Scenes,Y. Lu; Y. Fan; B. Deng; F. Liu; Y. Li; S. Wang,Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University,2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,976,983,"Robotic grasping faces new challenges in human-robot-interaction scenarios. We consider the task that the robot grasps a target object designated by human's language directives. The robot not only needs to locate a target based on vision-and-language information, but also needs to predict the reasonable grasp pose candidate at various views and postures. In this work, we propose a novel interactive grasp policy, named Visual-Lingual-Grasp (VL-Grasp), to grasp the target specified by human language. First, we build a new challenging visual grounding dataset to provide functional training data for robotic interactive perception in indoor environments. Second, we propose a 6- Dof interactive grasp policy combined with visual grounding and 6- Dof grasp pose detection to extend the universality of interactive grasping. Third, we design a grasp pose filter module to enhance the performance of the policy. Experiments demonstrate the effectiveness and extendibility of the VL-Grasp in real world. The VL-Grasp achieves a success rate of 72.5 % in different indoor scenes. The code and dataset is available at https://github.com/luyh20/VL-Grasp.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341379,Tsinghua University; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341379,,Point cloud compression;Visualization;Grounding;Training data;Grasping;Indoor environment;Task analysis,,5.0,,64,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/luyh20/VL-Grasp,https://github.com/luyh20/VL-Grasp
364,VIW-Fusion: Extrinsic Calibration and Pose Estimation for Visual-IMU-Wheel Encoder System,C. Qiao; S. Zhao; Y. Zhang; Y. Wang; D. Zhang,"College of Information Science and Engineering, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China; UISEE Technology, Beijing, China; UISEE Technology, Beijing, China",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,1489,1496,"The data fusion of camera, IMU, and wheel encoder measurements has proved its effectiveness in localizing ground robots, and obtaining accurate sensor extrinsic parameters is its premise. We propose an extrinsic parameter calibration algorithm and a multi-sensor-based pose estimation algorithm for the camera-IMU-wheel encoder system. First, we propose a joint calibration algorithm for the extrinsic parameters of the camera-IMU-wheel encoder system, which improves the accuracy and robustness of the camera-wheel encoder calibration. We then extend the visual-inertial odometry (VIO) to incorporate the measurements from the wheel encoder and weight the wheel encoder measurements according to angular velocity in global optimization to improve the performance. We further propose a novel method for VIO initialization by integrating wheel encoder information, which significantly reduces the scale error in initialization. We conduct extrinsic parameter calibration experiments on a real self-driving car and validate the performance of our multi-sensor-based localization system on the KAIST dataset and a dataset collected by our self-driving vehicles by performing an exhaust comparison with the state-of-the-art algorithms. Our implementations are open source11https://github.com/chunxiaoqiao/VIW-Fusion.git.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341453,National Natural Science Foundation of China(grant numbers:61973066); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341453,,Weight measurement;Simultaneous localization and mapping;Pose estimation;Robot vision systems;Wheels;Data integration;Cameras,,1.0,,28,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/chunxiaoqiao/VIW-Fusion,https://github.com/chunxiaoqiao/VIW-Fusion
365,Walking in Narrow Spaces: Safety-Critical Locomotion Control for Quadrupedal Robots with Duality-Based Optimization,Q. Liao; Z. Li; A. Thirugnanam; J. Zeng; K. Sreenath,"University of California, Berkeley, USA; University of California, Berkeley, USA; University of California, Berkeley, USA; University of California, Berkeley, USA; University of California, Berkeley, USA",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,2723,2730,"This paper presents a safety-critical locomotion control framework for quadrupedal robots. Our goal is to enable quadrupedal robots to safely navigate in cluttered environments. To tackle this, we introduce exponential Discrete Control Barrier Functions (exponential DCBFs) with duality-based obstacle avoidance constraints into a Non-linear Model Predictive Control (NMPC) with Whole-Body Control (WBC) framework for quadrupedal locomotion control. This enables us to use polytopes to describe the shapes of the robot and obstacles for collision avoidance while doing locomotion control of quadrupedal robots. Compared to most prior work, especially using CBFs, that utilize spherical and conservative approximation for obstacle avoidance, this work demonstrates a quadrupedal robot autonomously and safely navigating through very tight spaces in the real world. (Our open-source code is available at https://github.com/HybridRobotics/quadruped_nmpc_dcbf_duality, and the video is available at https://youtu.be/plgSQjwXm1Q.)",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341896,"NSF(grant numbers:CMMI-1931853,CMMI-1944722); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341896,,Legged locomotion;Codes;Navigation;Shape;Aerospace electronics;Quadrupedal robots;Collision avoidance,,4.0,,38,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/HybridRobotics/quadruped_nmpc_dcbf_duality,https://github.com/HybridRobotics/quadruped_nmpc_dcbf_duality
366,EVOLIN Benchmark: Evaluation of Line Detection and Association,K. Ivanov; G. Ferrer; A. Kornilova,"Skolkovo Institute of Science and Technology (Skoltech), Center for AI Technology (CAIT); Skolkovo Institute of Science and Technology (Skoltech), Center for AI Technology (CAIT); Skolkovo Institute of Science and Technology (Skoltech), Center for AI Technology (CAIT)",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,11412,11419,"Lines are interesting geometrical features commonly seen in indoor and urban environments. There is missing a complete benchmark where one can evaluate lines from a sequential stream of images in all its stages: Line detection, Line Association and Pose error. To do so, we present a complete and exhaustive benchmark for visual lines in a SLAM front-end, both for RGB and RGBD, by providing a plethora of complementary metrics. We have also labeled data from well-known SLAM datasets in order to have all in one poses and accurately annotated lines. In particular, we have evaluated 17 line detection algorithms, 5 line associations methods and the resultant pose error for aligning a pair of frames with several combinations of detector-association. We have packaged all methods and evaluations metrics and made them publicly available on web-page33https://prime-slam.github.io/evolin/.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342185,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342185,,Measurement;Visualization;Simultaneous localization and mapping;Urban areas;Benchmark testing;Streaming media;Solids,,2.0,,74,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://prime-slam.github.io/evolin,https://github.com/prime-slam/evolin
367,RMBench: Benchmarking Deep Reinforcement Learning for Robotic Manipulator Control,Y. Xiang; X. Wang; S. Hu; B. Zhu; X. Huang; X. Wu; S. Lyu,"Department of Earth System Science, Ministry of Education, Key Laboratory for Earth System Modeling, Institute for Global Change Studies, Tsinghua University, Beijing, China; University at Albany, State University of New York, USA; Carnegie Mellon University, USA; Microsoft Research Asia, China; Department of Earth System Science, Ministry of Education, Key Laboratory for Earth System Modeling, Institute for Global Change Studies, Tsinghua University, Beijing, China; Chengdu University of Information Technology, China; University at Buffalo, State University of New York, USA",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,1207,1214,"Reinforcement learning is used to tackle complex tasks with high-dimensional sensory inputs. Over the past decade, a wide range of reinforcement learning algorithms have been developed, with recent progress benefiting from deep learning for raw sensory signal representation. This raises a natural question: how well do these algorithms perform across different robotic manipulation tasks? To objectively compare algorithms, benchmarks use performance metrics. Benchmarks use objective performance metrics to offer a scientific way to compare algorithms. In this paper, we introduce RMBench, the first benchmark for robotic manipulations with high-dimensional continuous action and state spaces. We implement and evaluate reinforcement learning algorithms that take observed pixels as inputs and report their average performance and learning curves to demonstrate their performance and training stability. Our study concludes that none of the evaluated algorithms can handle all tasks well, with soft Actor-Critic outperforming most algorithms in terms of average reward and stability, and an algorithm combined with data augmentation potentially facilitating learning policies. Our code is publicly available at https://github.com/xiangyanfei212/RMBench-2022.git, including all benchmark tasks and studied algorithms.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342479,"National Natural Science Foundation of China(grant numbers:42125503,42075137); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342479,,Measurement;Deep learning;Training;Reinforcement learning;Benchmark testing;Robot sensing systems;Manipulators,,,,53,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/xiangyanfei212/RMBench-2022,https://github.com/xiangyanfei212/RMBench-2022
368,Effectively Rearranging Heterogeneous Objects on Cluttered Tabletops,K. Gao; J. Yu; T. S. Punjabi; J. Yu,"Department of Computer Science, Rutgers, the State University of New Jersey, Piscataway, NJ, USA; Department of Computer Science, Rutgers, the State University of New Jersey, Piscataway, NJ, USA; Department of Computer Science, Rutgers, the State University of New Jersey, Piscataway, NJ, USA; Department of Computer Science, Rutgers, the State University of New Jersey, Piscataway, NJ, USA",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,2057,2064,"Effectively rearranging heterogeneous objects constitutes a high-utility skill that an intelligent robot should master. Whereas significant work has been devoted to the grasp synthesis of heterogeneous objects, little attention has been given to the planning for sequentially manipulating such objects. In this work, we examine the long-horizon sequential rearrangement of heterogeneous objects in a tabletop setting, addressing not just generating feasible plans but near-optimal ones. Toward that end, and building on previous methods, including combinatorial algorithms and Monte Carlo tree search-based solutions, we develop state-of-the-art solvers for optimizing two practical objective functions considering key object properties such as size and weight. Thorough simulation studies show that our methods provide significant advantages in handling challenging heterogeneous object rearrangement problems, especially in cluttered settings. Real robot experiments further demonstrate and confirm these advantages. Source code and evaluation data associated with this research will be available at https//github.com/arc-l/TRLB upon the publication of this manuscript",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342164,"NSF(grant numbers:IIS-1845888,IIS-2132972); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342164,,Costs;Monte Carlo methods;Shape;Heuristic algorithms;Search problems;Linear programming;Planning,,2.0,,39,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/arc-l/TRLB,https://github.com/arc-l/TRLB
369,On Designing a Learning Robot: Improving Morphology for Enhanced Task Performance and Learning,M. Sorokin; C. Fu; J. Tan; C. K. Liu; Y. Bai; W. Lu; S. Ha; M. Khansari,Everyday Robots; Everyday Robots; Robotics at Google; Stanford University; Work done while at Everyday Robots; Everyday Robots; Georgia Institute of Technology; Everyday Robots,2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,487,494,"As robots become more prevalent, optimizing their design for better performance and efficiency is becoming increasingly important. However, current robot design practices overlook the impact of perception and design choices on a robot's learning capabilities. To address this gap, we propose a comprehensive methodology that accounts for the interplay between the robot's perception, hardware characteristics, and task requirements. Our approach optimizes the robot's morphology holistically, leading to improved learning and task execution proficiency. To achieve this, we introduce a Morphology-AGnostIc Controller (MAGIC), which helps with the rapid assessment of different robot designs. The MAGIC policy is efficiently trained through a novel PRIvileged Single-stage learning via latent alignMent (PRISM) framework, which also encourages behaviors that are typical of robot onboard observation. Our simulation-based results demonstrate that morphologies optimized holistically improve the robot performance by 15-20% on various manipulation tasks, and require 25x less data to match human-expert made morphology performance. In summary, our work contributes to the growing trend of learning-based approaches in robotics and emphasizes the potential in designing robots that facilitate better learning. The project's website can be found at learning-robot.github.io",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341905,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341905,,Training;Morphology;Market research;Hardware;Robot learning;Behavioral sciences;Task analysis,,,,45,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,,
370,Learning to Solve Tasks with Exploring Prior Behaviours,R. Zhu; S. Li; T. Dai; C. Zhang; O. Celiktutan,"Department of Engineering, King's College London; School of Computer Science and Technology, Harbin Institute of Technology; Department of Computing Science, University of Aberdeen; Institute for Interdisciplinary Information Sciences (IIIS), Tsinghua University; Department of Engineering, King's College London",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,7501,7507,"Demonstrations are widely used in Deep Reinforcement Learning (DRL) for facilitating solving tasks with sparse rewards. However, the tasks in real-world scenarios can often have varied initial conditions from the demonstration, which would require additional prior behaviours. For example, consider we are given the demonstration for the task of picking up an object from an open drawer, but the drawer is closed in the training. Without acquiring the prior behaviours of opening the drawer, the robot is unlikely to solve the task. To address this, in this paper we propose an Intrinsic Reward Driven Example-based Control (IRDEC). Our method can endow agents with the ability to explore and acquire the required prior behaviours and then connect to the task-specific behaviours in the demonstration to solve sparse-reward tasks without requiring additional demonstration of the prior behaviours. The performance of our method outperforms other baselines on three navigation tasks and one robotic manipulation task with sparse rewards. Codes are available at https://github.com/Ricky-Zhu/IRDEC.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342272,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342272,,Training;Deep learning;Codes;Navigation;Reinforcement learning;Task analysis;Intelligent robots,,1.0,,30,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/Ricky-Zhu/IRDEC,https://github.com/Ricky-Zhu/IRDEC
371,PoseFusion: Robust Object-in-Hand Pose Estimation with SelectLSTM,Y. Tu; J. Jiang; S. Li; N. Hendrich; M. Li; J. Zhang,"Department of Informatics, Universiät Hamburg, Hamburg, Germany; Institute of Technological Sciences, Wuhan University, Hubei, China; Department of Informatics, Universiät Hamburg, Hamburg, Germany; Department of Informatics, Universiät Hamburg, Hamburg, Germany; Institute of Technological Sciences, Wuhan University, Hubei, China; Department of Informatics, Universiät Hamburg, Hamburg, Germany",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,6839,6846,"Accurate estimation of the relative pose between an object and a robot hand is critical for many manipulation tasks. However, most of the existing object-in-hand pose datasets use two-finger grippers and also assume that the object remains fixed in the hand without any relative movements, which is not representative of real-world scenarios. To address this issue, a 6D object-in-hand pose dataset is proposed using a teleoperation method with an anthropomorphic Shadow Dexterous hand. Our dataset comprises RGB-D images, proprioception and tactile data, covering diverse grasping poses, finger contact states, and object occlusions. To overcome the significant hand occlusion and limited tactile sensor contact in real-world scenarios, we propose PoseFusion, a hybrid multi-modal fusion approach that integrates the information from visual and tactile perception channels. PoseFusion generates three candidate object poses from three estimators (tactile only, visual only, and visuo-tactile fusion), which are then filtered by a SelectLSTM network to select the optimal pose, avoiding inferior fusion poses resulting from modality collapse. Extensive experiments demonstrate the robustness and advantages of our framework. All data and codes are available on the project website: https://elevenjiang1.github.io/ObjectlnHand-Dataset/.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341688,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341688,,Training;Visualization;Pose estimation;Tactile sensors;Grasping;Information filters;Robustness,,5.0,,38,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://elevenjiang1.github.io/ObjectlnHand-Dataset,
372,ConSOR: A Context-Aware Semantic Object Rearrangement Framework for Partially Arranged Scenes,K. Ramachandruni; M. Zuo; S. Chernova,"Georgia Institute of Technology, Atlanta, Georgia, United States; Georgia Institute of Technology, Atlanta, Georgia, United States; Georgia Institute of Technology, Atlanta, Georgia, United States",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,82,89,"Object rearrangement is the problem of enabling a robot to identify the correct object placement in a complex environment. Prior work on object rearrangement has explored a diverse set of techniques for following user instructions to achieve some desired goal state. Logical predicates, images of the goal scene, and natural language descriptions have all been used to instruct a robot in how to arrange objects. In this work, we argue that burdening the user with specifying goal scenes is not necessary in partially-arranged environments, such as common household settings. Instead, we show that contextual cues from partially arranged scenes (i.e., the placement of some number of pre-arranged objects in the environment) provide sufficient context to enable robots to perform object rearrangement without any explicit user goal specification. We introduce ConSOR, a Context-aware Semantic Object Rearrangement framework that utilizes contextual cues from a partially arranged initial state of the environment to complete the arrangement of new objects, without explicit goal specification from the user. We demonstrate that ConSOR strongly outperforms two baselines in generalizing to novel object arrangements and unseen object categories. The code and data are available at https://github.com/kartikvrama/consor.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10341873,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341873,,Codes;Semantics;Natural languages;Object recognition;Intelligent robots,,1.0,,30,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/kartikvrama/consor,https://github.com/kartikvrama/consor
373,A Multiplicative Value Function for Safe and Efficient Reinforcement Learning,N. Bührer; Z. Zhang; A. Liniger; F. Yu; L. Van Gool,"Computer Vision Lab, ETH Zurich, Switzerland; Computer Vision Lab, ETH Zurich, Switzerland; Computer Vision Lab, ETH Zurich, Switzerland; Computer Vision Lab, ETH Zurich, Switzerland; Computer Vision Lab, ETH Zurich, Switzerland",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,5582,5589,"An emerging field of sequential decision problems is safe Reinforcement Learning (RL), where the objective is to maximize the reward while obeying safety constraints. Being able to handle constraints is essential for deploying RL agents in real-world environments, where constraint violations can harm the agent and the environment. To this end, we propose a safe model-free RL algorithm with a novel multiplicative value function consisting of a safety critic and a reward critic. The safety critic predicts the probability of constraint violation and discounts the reward critic that only estimates constraint-free returns. By splitting responsibilities, we facilitate the learning task leading to increased sample efficiency. We integrate our approach into two popular RL algorithms, Proximal Policy Optimization and Soft Actor-Critic, and evaluate our method in four safety-focused environments, including classical RL benchmarks augmented with safety constraints and robot navigation tasks with images and raw Lidar scans as observations. Finally, we make the zero-shot sim-to-real transfer where a differential drive robot has to navigate through a cluttered room. Our code can be found at https://github.com/nikeke19/Safe-Mult-RL.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342288,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342288,,Laser radar;Codes;Navigation;Reinforcement learning;Benchmark testing;Prediction algorithms;Safety,,,,45,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/nikeke19/Safe-Mult-RL,https://github.com/nikeke19/Safe-Mult-RL
374,Learning Bifunctional Push-Grasping Synergistic Strategy for Goal-Agnostic and Goal-Oriented Tasks,D. Ren; S. Wu; X. Wang; Y. Peng; X. Ren,"School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China; Huawei Noah's Ark Lab, Hong Kong SAR, China; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China; Institute of Artificial Intelligence, Shanghai University, Shanghai, China; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China",2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),13 Dec 2023,2023,,,2909,2916,"Both goal-agnostic and goal-oriented tasks have practical value for robotic grasping: goal-agnostic tasks target all objects in the workspace, while goal-oriented tasks aim at grasping pre-assigned goal objects. However, most current grasping methods are only better at coping with one task. In this work, we propose a bifunctional push-grasping synergistic strategy for goal-agnostic and goal-oriented grasping tasks. Our method integrates pushing along with grasping to pick up all objects or pre-assigned goal objects with high action efficiency depending on the task requirement. We introduce a bifunctional network, which takes in visual observations and outputs dense pixel-wise maps of $Q$ values for pushing and grasping primitive actions, to increase the available samples in the action space. Then we propose a hierarchical reinforcement learning framework to coordinate the two tasks by considering the goal-agnostic task as a combination of multiple goal-oriented tasks. To reduce the training difficulty of the hierarchical framework, we design a two-stage training method to train the two types of tasks separately. We perform pre-training of the model in simulation, and then transfer the learned model to the real world without any additional real-world fine-tuning. Experimental results show that the proposed approach outperforms existing methods in task completion rate and grasp success rate with less motion number. Supplementary material is available at https://github.com/DafaRen/Learning_Bifunctional_Push-grasping_Synergistic_Strategy_for_Goal-agnostic_and_Goal-oriented_Tasks.",2153-0866,978-1-6654-9190-7,10.1109/IROS55552.2023.10342533,National Natural Science Foundation of China(grant numbers:62273223); Project of Science and Technology Commission of Shanghai Municipality(grant numbers:22JC1401401); Shanghai Education Development Foundation(grant numbers:20SG40); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342533,,Training;Visualization;Robot kinematics;Grasping;Reinforcement learning;Task analysis;Clutter,,2.0,,26,IEEE,13 Dec 2023,,,IEEE,IEEE Conferences,https://github.com/DafaRen/Learning_Bifunctional_Push-grasping_Synergistic_Strategy_for_Goal-agnostic_and_Goal-oriented_Tasks,https://github.com/DafaRen/Learning_Bifunctional_Push-grasping_Synergistic_Strategy_for_Goal-agnostic_and_Goal-oriented_Tasks
375,CROON: Automatic Multi-LiDAR Calibration and Refinement Method in Road Scene,P. Wei; G. Yan; Y. Li; K. Fang; X. Cai; J. Yang; W. Liu,"Department of Automation, Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, China; Shanghai AI Laboratory, Autonomous Driving Group, China; Shanghai AI Laboratory, Autonomous Driving Group, China; Department of Automation, Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, China; Shanghai AI Laboratory, Autonomous Driving Group, China; Department of Automation, Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, China; Department of Automation, Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, China",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,12857,12863,"Sensor-based environmental perception is a crucial part of the autonomous driving system. In order to get an excellent perception of the surrounding environment, an intelligent system would configure multiple LiDARs (3D Light Detection and Ranging) to cover the distant and near space of the car. The precision of perception relies on the quality of sensor calibration. This research aims at developing an accurate, automatic, and robust calibration strategy for multiple LiDAR systems in the general road scene. We thus propose CROON (automatic multi-LiDAR Calibration and Refinement methOd in rOad sceNe), a two-stage method including rough and refinement calibration. The first stage can calibrate the sensor from an arbitrary initial pose, and the second stage is able to precisely calibrate the sensor iteratively. Specifically, CROON utilize the nature characteristics of road scene so that it is independent and easy to apply in large-scale conditions. Experimental results on real-world and simulated data sets demonstrate the reliability and accuracy of our method. All the related data sets and codes are open-sourced on the Github website https://github.com/OpenCalib/LiDAR2LiDAR.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981558,"National Key RD Program of China(grant numbers:2019YFBI311503); NSFC, China(grant numbers:61876107,UI803261,61977046); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981558,,Laser radar;Three-dimensional displays;Roads;Source coding;Robot sensing systems;Distance measurement;Calibration,,8.0,,31,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/OpenCalib/LiDAR2LiDAR,https://github.com/OpenCalib/LiDAR2LiDAR
376,6-DoF Pose Estimation of Household Objects for Robotic Manipulation: An Accessible Dataset and Benchmark,S. Tyree; J. Tremblay; T. To; J. Cheng; T. Mosier; J. Smith; S. Birchfield,NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA,2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,13081,13088,"We present a new dataset for 6-DoF pose estimation of known objects, with a focus on robotic manipulation research. We propose a set of toy grocery objects, whose physical instantiations are readily available for purchase and are appropriately sized for robotic grasping and manipulation. We provide 3D scanned textured models of these objects, suitable for generating synthetic training data, as well as RGBD images of the objects in challenging, cluttered scenes exhibiting partial occlusion, extreme lighting variations, multiple instances per image, and a large variety of poses. Using semi-automated RGBD-to-model texture correspondences, the images are annotated with ground truth poses accurate within a few millimeters. We also propose a new pose evaluation metric called ADD-H based on the Hungarian assignment algorithm that is robust to symmetries in object geometry without requiring their explicit enumeration. We share pre-trained pose estimators for all the toy grocery objects, along with their baseline performance on both validation and test sets. We offer this dataset to the community to help connect the efforts of computer vision researchers with the needs of roboticists.11https://github.com/swtyree/hope-dataset",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981838,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981838,,Measurement;Solid modeling;Computer vision;Three-dimensional displays;Pose estimation;Toy manufacturing industry;Training data,,47.0,,38,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/swtyree/hope-dataset,https://github.com/swtyree/hope-dataset
377,Unbiased Active Inference for Classical Control,M. Baioumy; C. Pezzato; R. Ferrari; N. Hawes,"Oxford Robotics Institute, Oxford University; Cognitive Robotics Department, TU Delft; Department of Systems and Control, TU Delft; Oxford Robotics Institute, Oxford University",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,12787,12794,"Active inference is a mathematical framework that originated in computational neuroscience. Recently, it has been demonstrated as a promising approach for constructing goal-driven behavior in robotics. Specifically, the active inference controller (AIC) has been successful on several continuous control and state-estimation tasks. Despite its relative success, some established design choices lead to a number of practical limitations for robot control. These include having a biased estimate of the state, and only an implicit model of control actions. In this paper, we highlight these limitations and propose an extended version of the unbiased active inference controller (u-AIC). The u-AIC maintains all the compelling benefits of the AIC and removes its limitations. Simulation results on a 2-DOF arm and experiments on a real 7-DOF manipulator show the improved performance of the u-AIC with respect to the standard AIC. The code can be found at https://github.com/cpezzato/unbiasedaic.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981095,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981095,,Simulation;Robot control;Computational neuroscience;Human-robot interaction;Manipulators;State estimation;Task analysis,,3.0,,32,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/cpezzato/unbiasedaic,https://github.com/cpezzato/unbiasedaic
378,Detecting Adversarial Perturbations in Multi-Task Perception,M. Klingner; V. R. Kumar; S. Yogamani; A. Bär; T. Fingscheidt,"Institute for Communications Technology, Technische Universität Braunschweig, Braunschweig, Germany; Valeo DAR, Kronach, Germany; Valeo Vision Systems, Tuam, Ireland; Institute for Communications Technology, Technische Universität Braunschweig, Braunschweig, Germany; Institute for Communications Technology, Technische Universität Braunschweig, Braunschweig, Germany",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,13050,13057,"While deep neural networks (DNNs) achieve impressive performance on environment perception tasks, their sensitivity to adversarial perturbations limits their use in practical applications. In this paper, we (i) propose a novel adversarial perturbation detection scheme based on multi-task perception of complex vision tasks (i.e., depth estimation and semantic segmentation). Specifically, adversarial perturbations are detected by inconsistencies between extracted edges of the input image, the depth output, and the segmentation output. To further improve this technique, we (ii) develop a novel edge consistency loss between all three modalities, thereby improving their initial consistency which in turn supports our detection scheme. We verify our detection scheme's effectiveness by employing various known attacks and image noises. In addition, we (iii) develop a multi-task adversarial attack, aiming at fooling both tasks as well as our detection scheme. Experimental evaluation on the Cityscapes and KITTI datasets shows that under an assumption of a 5% false positive rate up to 100% of images are correctly detected as adversarially perturbed, depending on the strength of the perturbation. Code is available at https://github.com/ifnspaml/AdvAttackDet. A short video at https://youtu.be/KKa6gOyWmH4 provides qualitative results.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981559,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981559,,Training;Sensitivity;Perturbation methods;Image edge detection;Semantic segmentation;Neural networks;Estimation,,9.0,,39,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/ifnspaml/AdvAttackDet,https://github.com/ifnspaml/AdvAttackDet
379,Siamese Object Tracking for Vision-Based UAM Approaching with Pairwise Scale-Channel Attention,G. Zheng; C. Fu; J. Ye; B. Li; G. Lu; J. Pan,"School of Mechanical Engineering, Tongji University, Shanghai, China; School of Mechanical Engineering, Tongji University, Shanghai, China; School of Mechanical Engineering, Tongji University, Shanghai, China; School of Mechanical Engineering, Tongji University, Shanghai, China; Department of Automation, Tsinghua University, Beijing, China; Department of Computer Science, University of Hong Kong, Hong Kong, China",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,10486,10492,"Although the manipulating of the unmanned aerial manipulator (UAM) has been widely studied, vision-based UAM approaching, which is crucial to the subsequent manipulating, generally lacks effective design. The key to the visual UAM approaching lies in object tracking, while current UAM tracking typically relies on costly model-based methods. Besides, UAM approaching often confronts more severe object scale variation issues, which makes it inappro-priate to directly employ state-of-the-art model-free Siamese-based methods from the object tracking field. To address the above problems, this work proposes a novel Siamese network with pairwise scale-channel attention (SiamSA) for vision-based UAM approaching. Specifically, SiamSA consists of a pairwise scale-channel attention network (PSAN) and a scale-aware anchor proposal network (SA-APN). PSAN acquires valuable scale information for feature processing, while SA-APN mainly attaches scale awareness to anchor proposing. Moreover, a new tracking benchmark for UAM approaching, namely UAMT100, is recorded with 35K frames on a flying UAM platform for evaluation. Exhaustive experiments on the benchmarks and real-world tests validate the efficiency and practicality of SiamSA with a promising speed. Both the code and UAMT100 benchmark are now available at https://github.com/vision4robotics/SiamSA.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9982189,National Natural Science Foundation of China(grant numbers:62173249); Natural Science Foundation of Shanghai(grant numbers:20ZR1460100); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982189,,Visualization;Codes;Robot vision systems;Benchmark testing;Manipulators;Cameras;Object tracking,,10.0,,36,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/vision4robotics/SiamSA,https://github.com/vision4robotics/SiamSA
380,Physical Neural Cellular Automata for 2D Shape Classification,K. Walker; R. B. Palm; R. Moreno; A. Faina; K. Stoy; S. Risi,IT University of Copenhagen; IT University of Copenhagen; IT University of Copenhagen; IT University of Copenhagen; IT University of Copenhagen; IT University of Copenhagen,2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,12667,12673,"Materials with the ability to self-classify their own shape have the potential to advance a wide range of engineering applications and industries. Biological systems possess the ability not only to self-reconfigure but also to self-classify themselves to determine a general shape and function. Previous work into modular robotics systems has only enabled self-recognition and self-reconfiguration into a specific target shape, missing the inherent robustness present in nature to self-classify. In this paper we therefore take advantage of recent advances in deep learning and neural cellular automata, and present a simple modular 2D robotic system that can infer its own class of shape through the local communication of its components. Furthermore, we show that our system can be successfully transferred to hardware which thus opens op-portunities for future self-classifying machines. Code available at https://github.com/kattwalker/projectcube. Video available at https://youtu.be/0TCOkE4keyc.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981214,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981214,,Industries;Deep learning;Three-dimensional displays;Shape;Service robots;Learning automata;Automata,,3.0,,38,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/kattwalker/projectcube,https://github.com/kattwalker/projectcube
381,MAPFASTER: A Faster and Simpler take on Multi-Agent Path Finding Algorithm Selection,J. -M. Alkazzi; A. Rizk; M. Salomon; A. Makhoul,"IDEALworks GmbH, Munich, Germany; IDEALworks GmbH, Munich, Germany; FEMTO-ST Institute, UMR 6174 CNRS, Univ. Bourgogne Franche-Comté, Belfort, France; FEMTO-ST Institute, UMR 6174 CNRS, Univ. Bourgogne Franche-Comté, Belfort, France",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,10088,10093,"Portfolio-based algorithm selection can help in choosing the best suited algorithm for a given task while leveraging the complementary strengths of the candidates. Solving the Multi-Agent Path Finding (MAPF) problem optimally has been proven to be NP-Hard. Furthermore, no single optimal algorithm has been shown to have the fastest runtime for all MAPF problem instances, and there are no proven approaches for when to use each algorithm. To address these challenges, we develop MAPFASTER, a smaller and more accurate deep learning based architecture aiming to be deployed in fleet management systems to select the fastest MAPF solver in a multi-robot setting. MAPF problem instances are encoded as images and passed to the model for classification into one of the portfolio's candidates. We evaluate our model against state-of-the-art Optimal-MAPF-Algorithm selectors, showing +5.42% improvement in accuracy while being 7.1× faster to train. The dataset, code and analysis used in this research can be found at https://github.com/jeanmarcalkazzi/mapfaster.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981981,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981981,,Deep learning;Runtime;Codes;Classification algorithms;Task analysis;Intelligent robots,,2.0,,37,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/jeanmarcalkazzi/mapfaster,https://github.com/jeanmarcalkazzi/mapfaster
382,Active Mapping via Gradient Ascent Optimization of Shannon Mutual Information over Continuous SE(3) Trajectories,A. Asgharivaskasi; S. Koga; N. Atanasov,"Department of Electrical and Computer Engineering, University of California San Diego, CA, USA; Department of Electrical and Computer Engineering, University of California San Diego, CA, USA; Department of Electrical and Computer Engineering, University of California San Diego, CA, USA",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,12994,13001,"The problem of active mapping aims to plan an informative sequence of sensing views given a limited budget such as distance traveled. This paper considers active occupancy grid mapping using a range sensor, such as LiDAR or depth camera. State-of-the-art methods optimize information-theoretic measures relating the occupancy grid probabilities with the range sensor measurements. The non-smooth nature of ray-tracing within a grid representation makes the objective function non-differentiable, forcing existing methods to search over a discrete space of candidate trajectories. This work proposes a differentiable approximation of the Shannon mutual information between a grid map and ray-based observations that enables gradient ascent optimization in the continuous space of SE(3) sensor poses. Our gradient-based formulation leads to more informative sensing trajectories, while avoiding occlusions and collisions. The proposed method is demonstrated in simulated and real-world experiments in 2-D and 3-D environments. Materials supplementing this paper are available at: https://arashasgharivaskasi-bc.github.io/grad_active_mapping/",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981875,NSF(grant numbers:2045945); ONR(grant numbers:N00014-18-1-2828); ARL(grant numbers:W911NF-17-2-0181); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981875,,Ray tracing;Robot sensing systems;Search problems;Probabilistic logic;Linear programming;Trajectory;Sensors,,7.0,,22,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://arashasgharivaskasi-bc.github.io/grad_active_mapping,
383,TransDARC: Transformer-based Driver Activity Recognition with Latent Space Feature Calibration,K. Peng; A. Roitberg; K. Yang; J. Zhang; R. Stiefelhagen,"Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology; Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology; Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology; Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology; Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,278,285,"Traditional video-based human activity recognition has experienced remarkable progress linked to the rise of deep learning, but this effect was slower as it comes to the downstream task of driver behavior understanding. Understanding the situation inside the vehicle cabin is essential for Advanced Driving Assistant System (ADAS) as it enables identifying distraction, predicting driver's intent and leads to more convenient human-vehicle interaction. At the same time, driver observation systems face substantial obstacles as they need to capture different granularities of driver states, while the complexity of such secondary activities grows with the rising automation and increased driver freedom. Furthermore, a model is rarely deployed under conditions identical to the ones in the training set, as sensor placements and types vary from vehicle to vehicle, constituting a substantial obstacle for real-life deployment of data-driven models. In this work, we present a novel vision-based framework for recognizing secondary driver behaviours based on visual transformers and an additional augmented feature distribution calibration module. This module operates in the latent feature-space enriching and diversifying the training set at feature-level in order to improve generalization to novel data appearances, (e.g., sensor changes) and general feature quality. Our framework consistently leads to better recognition rates, surpassing previous state-of-the-art results of the public Drive&Act benchmark on all granularity levels. Our code will be made publicly available at https://github.com/KPeng9510/TransDARC.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981445,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981445,,Training;Visualization;Sensor placement;Benchmark testing;Transformers;Robot sensing systems;Calibration,,17.0,,50,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/KPeng9510/TransDARC,https://github.com/KPeng9510/TransDARC
384,Application of Ghost-DeblurGAN to Fiducial Marker Detection,Y. Liu; A. Haridevan; H. Schofield; J. Shan,"Department of Earth and Space Science and Engineering, York University, Toronto, Ontario, Canada; Department of Earth and Space Science and Engineering, York University, Toronto, Ontario, Canada; Department of Earth and Space Science and Engineering, York University, Toronto, Ontario, Canada; Department of Earth and Space Science and Engineering, York University, Toronto, Ontario, Canada",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,6827,6832,"Feature extraction or localization based on the fiducial marker could fail due to motion blur in real-world robotic applications. To solve this problem, a lightweight generative adversarial network, named Ghost-DeblurGAN, for real-time motion deblurring is developed in this paper. Furthermore, on account that there is no existing deblurring benchmark for such task, a new large-scale dataset, York-Tag, is proposed that provides pairs of sharp/blurred images containing fiducial markers. With the proposed model trained and tested on YorkTag, it is demonstrated that when applied along with fiducial marker systems to motion-blurred images, Ghost-DeblurGAN improves the marker detection significantly. The datasets and codes used in this paper are available at: https://github.com/York-SDCNLab/Ghost-DeblurGAN.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981701,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981701,,Measurement;Location awareness;Simultaneous localization and mapping;Benchmark testing;Generative adversarial networks;Fiducial markers;Real-time systems,,7.0,,29,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/York-SDCNLab/Ghost-DeblurGAN,https://github.com/York-SDCNLab/Ghost-DeblurGAN
385,Robust Counterexample-guided Optimization for Planning from Differentiable Temporal Logic,C. Dawson; C. Fan,"Department of Aeronautics and Astronautics, Massachusetts Institute of Technology, Cambridge, USA; Department of Aeronautics and Astronautics, Massachusetts Institute of Technology, Cambridge, USA",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,7205,7212,"Signal temporal logic (STL) provides a powerful, flexible framework for specifying complex autonomy tasks; however, existing methods for planning based on STL specifications have difficulty scaling to long-horizon tasks and are not robust to external disturbances. In this paper, we present an algorithm for finding robust plans that satisfy STL specifications. Our method alternates between local optimization and local falsification, using automatically differentiable temporal logic to iteratively optimize its plan in response to counterexamples found during the falsification process. We benchmark our counterexample-guided planning method against state-of-the-art planning methods on two long-horizon satellite rendezvous missions, showing that our method finds high-quality plans that satisfy STL specifications despite adversarial disturbances. We find that our method consistently finds plans that are robust to adversarial disturbances and requires less than half the time of competing methods. We provide an implementation of our planner at https://github.com/MIT-REALM/architect.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981382,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981382,formal methods;differentiable programming;temporal logic,Satellites;Source coding;Human-robot interaction;Programming;Model checking;Iterative algorithms;Planning,,5.0,,26,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/MIT-REALM/architect,https://github.com/MIT-REALM/architect
386,Optimal Constrained Task Planning as Mixed Integer Programming,A. Adu-Bredu; N. Devraj; O. C. Jenkins,"Department of Electrical Engineering and Computer Science, Robotics Institute, University of Michigan, Ann Arbor, MI, USA; Department of Electrical Engineering and Computer Science, Robotics Institute, University of Michigan, Ann Arbor, MI, USA; Department of Electrical Engineering and Computer Science, Robotics Institute, University of Michigan, Ann Arbor, MI, USA",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,12029,12036,"For robots to successfully execute tasks as-signed to them, they must be capable of planning the right sequence of actions. These actions must be both optimal with respect to a specified objective and satisfy whatever constraints exist in their world. We propose an approach for robot task planning that is capable of planning the optimal sequence of grounded actions to accomplish a task given a specific objective function while satisfying all specified numerical constraints. Our approach accomplishes this by encoding the entire task planning problem as a single mixed integer convex program, which it then solves using an off-the-shelf Mixed Integer Program-ming solver. We evaluate our approach on several mobile manipulation tasks in both simulation and on a physical humanoid robot. Our approach is able to consistently produce optimal plans while accounting for all specified numerical constraints in the mobile manipulation tasks. Open-source implementations of the components of our approach as well as videos of robots executing planned grounded actions in both simulation and the physical world can be found at this url: https://adubredu.github.io/gtpmip",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981237,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981237,,Integer programming;Humanoid robots;Kinematics;Encoding;Planning;Numerical models;Task analysis,,3.0,,26,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://adubredu.github.io/gtpmip,https://github.com/adubredu/HPD.jl
387,HighlightNet: Highlighting Low-Light Potential Features for Real-Time UAV Tracking,C. Fu; H. Dong; J. Ye; G. Zheng; S. Li; J. Zhao,"School of Mechanical Engineering, Tongji University, Shanghai, China; School of Mechanical Engineering, Tongji University, Shanghai, China; School of Mechanical Engineering, Tongji University, Shanghai, China; School of Mechanical Engineering, Tongji University, Shanghai, China; School of Mechanical Engineering, Tongji University, Shanghai, China; School of Mechanical Engineering, Tongji University, Shanghai, China",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,12146,12153,"Low-light environments have posed a formidable challenge for robust unmanned aerial vehicle (UAV) tracking even with state-of-the-art (SOTA) trackers since the poten-tial image features are hard to extract under adverse light conditions. Besides, due to the low visibility, accurate online selection of the object also becomes extremely difficult for human monitors to initialize UAV tracking in ground con-trol stations. To solve these problems, this work proposes a novel enhancer, i.e., HighlightNet, to light up potential objects for both human operators and UAV trackers. By employing Transformer, HighlightNet can adjust enhancement parameters according to global features and is thus adaptive for the illumination variation. Pixel-level range mask is introduced to make HighlightNet more focused on the enhancement of the tracking object and regions without light sources. Furthermore, a soft truncation mechanism is built to prevent background noise from being mistaken for crucial features. Evaluations on image enhancement benchmarks demonstrate HighlightNet has advantages in facilitating human perception. Experiments on the public UAVDark135 benchmark show that HightlightNet is more suitable for UAV tracking tasks than other state-of-the-art (SOTA) low-light enhancers. In addition, real-world tests on a typical UAV platform verify HightlightNet's practicability and efficiency in nighttime aerial tracking-related applications. The code and demo videos are available at https://github.com/vision4robotics/HighlightNet.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981070,National Natural Science Foundation of China(grant numbers:62173249); Natural Science Foundation of Shanghai(grant numbers:20ZR1460100); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981070,,Target tracking;Benchmark testing;Autonomous aerial vehicles;Feature extraction;Transformers;Real-time systems;Task analysis,,8.0,,33,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/vision4robotics/HighlightNet,https://github.com/vision4robotics/HighlightNet
388,Grasp Pre-shape Selection by Synthetic Training: Eye-in-hand Shared Control on the Hannes Prosthesis,F. Vasile; E. Maiettini; G. Pasquale; A. Florio; N. Boccardo; L. Natale,"Humanoid Sensing and Perception, Istituto Italiano di Tecnologia, Genoa, Italy; Humanoid Sensing and Perception, Istituto Italiano di Tecnologia, Genoa, Italy; Humanoid Sensing and Perception, Istituto Italiano di Tecnologia, Genoa, Italy; Rehab Technologies Lab, Istituto Italiano di Tecnologia, Genoa, Italy; Rehab Technologies Lab, Istituto Italiano di Tecnologia, Genoa, Italy; Humanoid Sensing and Perception, Istituto Italiano di Tecnologia, Genoa, Italy",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,13112,13119,"We consider the task of object grasping with a prosthetic hand capable of multiple grasp types. In this setting, communicating the intended grasp type often requires a high user cognitive load which can be reduced adopting shared autonomy frameworks. Among these, so-called eye-in-hand systems automatically control the hand pre-shaping before the grasp, based on visual input coming from a camera on the wrist. In this paper, we present an eye-in-hand learning-based approach for hand pre-shape classification from RGB sequences. Differently from previous work, we design the system to support the possibility to grasp each considered object part with a different grasp type. In order to overcome the lack of data of this kind and reduce the need for tedious data collection sessions for training the system, we devise a pipeline for rendering synthetic visual sequences of hand trajectories. We develop a sensorized setup to acquire real human grasping sequences for benchmarking and show that, compared on practical use cases, models trained with our synthetic dataset achieve better generalization performance than models trained on real data. We finally integrate our model on the Hannes prosthetic hand and show its practical effectiveness. We make publicly available the code and dataset to reproduce the presented results11https://github.com/hsp-iit/prosthetic-grasping-simulation.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981035,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981035,,Wrist;Training;Visualization;Pipelines;Grasping;Data models;Trajectory,,10.0,,41,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/hsp-iit/prosthetic-grasping-simulation,https://github.com/hsp-iit/prosthetic-grasping-simulation
389,SLAM-Supported Self-Training for 6D Object Pose Estimation,Z. Lu; Y. Zhang; K. Doherty; O. Severinsen; E. Yang; J. Leonard,"Computer Science and Artificial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology (MIT), Cambridge, MA; Computer Science and Artificial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology (MIT), Cambridge, MA; Computer Science and Artificial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology (MIT), Cambridge, MA; Computer Science and Artificial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology (MIT), Cambridge, MA; Computer Science and Artificial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology (MIT), Cambridge, MA; Computer Science and Artificial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology (MIT), Cambridge, MA",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,2833,2840,"Recent progress in object pose prediction provides a promising path for robots to build object-level scene representations during navigation. However, as we deploy a robot in novel environments, the out-of-distribution data can degrade the prediction performance. To mitigate the domain gap, we can potentially perform self-training in the target domain, using predictions on robot-captured images as pseudo labels to fine-tune the object pose estimator. Unfortunately, the pose predictions are typically outlier-corrupted, and it is hard to quantify their uncertainties, which can result in low-quality pseudo-labeled data. To address the problem, we propose a SLAM-supported self-training method, leveraging robot understanding of the 3D scene geometry to enhance the object pose inference performance. Combining the pose predictions with robot odometry, we formulate and solve pose graph optimization to refine the object pose estimates and make pseudo labels more consistent across frames. We incorporate the pose prediction covariances as variables into the optimization to automatically model their uncertainties. This automatic covariance tuning (ACT) process can fit 6D pose prediction noise at the component level, leading to higher-quality pseudo training data. We test our method with the deep object pose estimator (DOPE) on the YCB video dataset and in real robot experiments. It achieves respectively 34.3% and 17.8% accuracy enhancements in pose prediction on the two tests. Our code is available at https://github.com/520xyxyzq/slam-super-6d.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981145,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981145,,Uncertainty;Three-dimensional displays;Simultaneous localization and mapping;Pose estimation;Training data;Predictive models;Robots,,4.0,,35,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/520xyxyzq/slam-super-6d,https://github.com/520xyxyzq/slam-super-6d
390,A Dataset and Benchmark for Learning the Kinematics of Concentric Tube Continuum Robots,R. M. Grassmann; R. Z. Chen; N. Liang; J. Burgner-Kahrs,"Department of Mathematical and Computational Sciences, Continuum Robotics Laboratory, University of Toronto, Mississauga, ON, Canda; Department of Mathematical and Computational Sciences, Continuum Robotics Laboratory, University of Toronto, Mississauga, ON, Canda; Autonomous Systems and Biomechatronics Laboratory, Mechanical and Industrial Engineering, University of Toronto, Toronto, ON, Canada; Department of Mathematical and Computational Sciences, Continuum Robotics Laboratory, University of Toronto, Mississauga, ON, Canda",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,9550,9557,"Establishing a physics-based model capturing the kinetostatic behavior of concentric tube continuum robots is challenging as elastic interactions between the flexible tubes constituting the robot result in a highly non-linear problem. The Goldstandard physics-based model using the Cosserat theory of elastic rods achieves reasonable approximations with 1.5 - 3 % with respect to the robot's length, if well-calibrated. Learning-based models of concentric tube continuum robots have been shown to outperform the Goldstandard model with approximation errors below 1 %. Yet, the merits of learning-based models remain largely unexplored as no common dataset and benchmark exist. In this paper, we present a dataset captured from a three-tube concentric tube continuum robot for use in learning-based kinematics research. The dataset consists of 100 000 joint configurations and the corresponding four 6 dof sensors in SE(3) measured with an electromagnetic tracking system (github.com/ContinuumRoboticsLab/CRL-Dataset-CTCR-Pose). With our dataset, we empower the continuum robotics and machine learning community to advance the field. We share our insights and lessons learned on joint space representation, shape representation in task space, and sampling strategies. Furthermore, we provide benchmark results for learning the forward kinematics using a simple, shallow feedforward neural network. The benchmark results for the tip error are 0.74 mm w.r.t. position (0.4 % of total robot length) and 6.49° w.r.t. orientation.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981719,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981719,,Shape;Kinematics;Machine learning;Benchmark testing;Robot sensing systems;Sensor systems;Electron tubes,,5.0,,25,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/ContinuumRoboticsLab/CRL-Dataset-CTCR-Pose,https://github.com/ContinuumRoboticsLab/CRL-Dataset-CTCR-Pose
391,CloudAttention: Efficient Multi-Scale Attention Scheme For 3D Point Cloud Learning,M. Saleh; Y. Wang; N. Navab; B. Busam; F. Tombari,"Faculty of Computer Science, Technische Universität München (TUM), Garching bei München, Germany; Faculty of Computer Science, Technische Universität München (TUM), Garching bei München, Germany; Faculty of Computer Science, Technische Universität München (TUM), Garching bei München, Germany; Faculty of Computer Science, Technische Universität München (TUM), Garching bei München, Germany; Faculty of Computer Science, Technische Universität München (TUM), Garching bei München, Germany",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,1986,1992,"Processing 3D data efficiently has always been a challenge. Spatial operations on large-scale point clouds, stored as sparse data, require extra cost. Attracted by the success of transformers, researchers are using multi-head attention for vision tasks. However, attention calculations in transformers come with quadratic complexity in the number of inputs and miss spatial intuition on sets like point clouds. We redesign set transformers in this work and incorporate them into a hierarchical framework for shape classification and part and scene segmentation. We propose our local attention unit, which captures features in a spatial neighborhood. We also compute efficient and dynamic global cross attentions by leveraging sampling and grouping at each iteration. Finally, to mitigate the non-heterogeneity of point clouds, we propose an efficient Multi-Scale Tokenization (MST), which extracts scale-invariant tokens for attention operations. The proposed hierarchical model achieves state-of-the-art shape classification in mean accuracy and yields results on par with the previous segmentation methods while requiring significantly fewer computations. Our proposed architecture predicts segmentation labels with around half the latency and parameter count of the previous most effi-cient method with comparable performance. The code is available at https://github.com/YigeWang-WHU/CloudAttention.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9982276,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982276,,Point cloud compression;Three-dimensional displays;Shape;Pipelines;Semantics;Transformers;Tokenization,,5.0,,34,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/YigeWang-WHU/CloudAttention,https://github.com/YigeWang-WHU/CloudAttention
392,A Deep-Learning-based System for Indoor Active Cleaning,Y. Yun; L. Hou; Z. Feng; W. Jin; Y. Liu; H. Wang; R. He; W. Guo; B. Han; B. Qin; J. Li,Gaussian Robotics Pte. Ltd.; Gaussian Robotics Pte. Ltd.; Gaussian Robotics Pte. Ltd.; Gaussian Robotics Pte. Ltd.; Gaussian Robotics Pte. Ltd.; Gaussian Robotics Pte. Ltd.; Gaussian Robotics Pte. Ltd.; Gaussian Robotics Pte. Ltd.; Gaussian Robotics Pte. Ltd.; Gaussian Robotics Pte. Ltd.; Gaussian Robotics Pte. Ltd.,2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,7803,7808,"Cleaning public areas like commercial complexes is challenging due to their sophisticated surroundings and the vast kinds of real-life dirt. Robots are required to distinguish dirts and apply corresponding cleaning strategies. In this work, we proposed an active-cleaning framework by utilizing deep-learning methods for both solid wastes detection and liquid stains segmentation. Our system consists of 4 components: a Perception module integrated with deep-learning models, a Post-processing module for projection, a Tracking module for map localization, and a Planning and Control module for cleaning strategies. Compared with classic approaches, our vision-based system significantly improves cleaning efficiency. Besides, we released the largest real-world indoor hybrid dirt cleaning dataset (HD10K) containing 10K labeled images, together with a track-level evaluation metric for better cleaning performance measurement. The proposed deep-learning based system is verified with extensive experiments on our dataset, and deployed to Gaussian Robotics's robots operating globally. Dataset is available at: https://gaussianopensource.github.io/projects/active_cleaning.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9982137,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982137,,Measurement;Location awareness;Solid modeling;Liquids;Solids;Cleaning;Planning,,1.0,,29,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://gaussianopensource.github.io/projects/active_cleaning,https://github.com/gaussianopensource/dl_active_cleaning
393,IndoLayout: Leveraging Attention for Extended Indoor Layout Estimation from an RGB Image,S. Singh; J. Shriram; S. Kulkarni; B. Bhowmick; K. M. Krishna,"IIIT, Robotics Research Center, Hyderabad; IIIT, Robotics Research Center, Hyderabad; IIIT, Robotics Research Center, Hyderabad; TCS Research, Kolkata, India; IIIT, Robotics Research Center, Hyderabad",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,13128,13135,"In this work, we propose IndoLayout, a novel real-time approach for generating high-quality occupancy maps from an RGB image for indoor scenes. Such occupancy maps are often crucial for path-planning and mapping in indoor environments but are often built using only information contained in the ego view. In contrast, our approach also predicts occupancy values beyond immediately visible regions from just a monocular image, leveraging learnt priors from indoor scenes. Hence, our proposed network can produce a hallucinated, amodal scene layout that includes areas occluded in the RGB image, such as a navigable floor behind a desk. Specifically, we propose a novel architecture that uses self-attention and adversarial learning to vastly improve the quality of the predicted layout. We evaluate our model on several photorealistic indoor datasets and outperform previous relevant work on all metrics that measure layout quality, including newly adopted ones. Finally, we demonstrate the effectiveness of our method by showing significant improvements on the PointNav task over similar approaches using IndoLayout. For more details, please refer to the project page: https://indolayout.github.io/.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9982106,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982106,,Measurement;Layout;Estimation;Real-time systems;Adversarial machine learning;Indoor environment;Task analysis,,,,45,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,,
394,End-to-End Feature Decontaminated Network for UAV Tracking,H. Zuo; C. Fu; S. Li; J. Ye; G. Zheng,"School of Mechanical Engineering, Tongji University, Shanghai, China; School of Mechanical Engineering, Tongji University, Shanghai, China; School of Mechanical Engineering, Tongji University, Shanghai, China; School of Mechanical Engineering, Tongji University, Shanghai, China; School of Mechanical Engineering, Tongji University, Shanghai, China",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,12130,12137,"Object feature pollution is one of the burning issues in vision-based UAV tracking, commonly caused by occlusion, fast motion, and illumination variation. Due to the contaminated information in the polluted object features, most trackers fail to precisely estimate the object location and scale. To address the above disturbing issue, this work proposes a novel end-to-end feature decontaminated network for efficient and effective UAV tracking, i.e., FDNT. FDNT mainly includes two modules: a decontaminated downsampling network and a decontaminated upsampling network. The former reduces the interference information of the feature pollution and enhanced the expression of the object location information with two asymmetric convolution branches. The latter restores the object scale information with the super-resolution technology-based low-to-high encoder, achieving a further decontamination effect. Moreover, a novel pooling distance loss is carefully developed to assist the decontaminated downsampling network in concentrating on the critical regions with the object information. Exhaustive experiments on three well-known benchmarks validate the effectiveness of FDNT, especially on the sequences with feature pollution. In addition, real-world tests show the efficiency of FDNT with 31.4 frames per second. The code and demo videos are available at https://github.com/vision4robotics/FDNT.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981882,National Natural Science Foundation of China(grant numbers:62173249); Natural Science Foundation of Shanghai(grant numbers:20ZR1460100); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981882,,Visualization;Pollution;Convolution;Decontamination;Superresolution;Lighting;Interference,,4.0,,39,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/vision4robotics/FDNT,https://github.com/vision4robotics/FDNT
395,Visual-Tactile Multimodality for Following Deformable Linear Objects Using Reinforcement Learning,L. Pecyna; S. Dong; S. Luo,"Department of Computer Science, University of Liverpool, Liverpool, UK; School of Computer Science & Engineering, University of Washington, Seattle, WA, USA; Department of Engineering, King's College London, London, UK",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,3987,3994,"Manipulation of deformable objects is a challenging task for a robot. It would be problematic to use a single sensory input to track the behaviour of such objects: vision can be subjected to occlusions, whereas tactile inputs cannot capture the global information that is useful for the task. In this paper, we study the problem of using vision and tactile inputs together to complete the task of following deformable linear objects, for the first time. We create a Reinforcement Learning agent using different sensing modalities and investigate how its behaviour can be boosted using visual-tactile fusion, compared to using a single sensing modality. To this end, we developed a benchmark in simulation for manipulating the deformable linear objects using multimodal sensing inputs. The policy of the agent uses distilled information, e.g., the pose of the object in both visual and tactile perspectives, instead of the raw sensing signals, so that it can be directly transferred to real environments. In this way, we disentangle the perception system and the learned control policy. Our extensive experiments show that the use of both vision and tactile inputs, together with proprioception, allows the agent to complete the task in up to 92% of cases, compared to 77% when only one of the signals is given. Our results can provide valuable insights for the future design of tactile sensors and for deformable objects manipulation. Code and videos can be found at: https://github.com/lpecyna/SoftSlidingGym.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9982218,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982218,,Deformable models;Visualization;Codes;Multimodal sensors;Tactile sensors;Reinforcement learning;Benchmark testing,,14.0,,28,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/lpecyna/SoftSlidingGym,https://github.com/lpecyna/SoftSlidingGym
396,RAPTOR: Rapid Aerial Pickup and Transport of Objects by Robots,A. X. Appius; E. Bauer; M. Blöchlinger; A. Kalra; R. Oberson; A. Raayatsanati; P. Strauch; S. Suresh; M. von Salis; R. K. Katzschmann,"Soft Robotics Lab, ETH Zurich, Switzerland; Soft Robotics Lab, ETH Zurich, Switzerland; Soft Robotics Lab, ETH Zurich, Switzerland; Soft Robotics Lab, ETH Zurich, Switzerland; Soft Robotics Lab, ETH Zurich, Switzerland; Soft Robotics Lab, ETH Zurich, Switzerland; Soft Robotics Lab, ETH Zurich, Switzerland; Soft Robotics Lab, ETH Zurich, Switzerland; Soft Robotics Lab, ETH Zurich, Switzerland; Soft Robotics Lab, ETH Zurich, Switzerland",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,349,355,"Rapid aerial grasping through robots can lead to many applications that utilize fast and dynamic picking and placing of objects. Rigid grippers traditionally used in aerial manipulators require high precision and specific object geometries for successful grasping. We propose RAPTOR, a quadcopter platform combined with a custom Fin Ray®gripper to enable more flexible grasping of objects with different geometries, leveraging the properties of soft materials to increase the contact surface between the gripper and the objects. To reduce the communication latency, we present a new lightweight middleware solution based on Fast DDS (Data Distribution Service) as an alternative to ROS (Robot Operating System). We show that RAPTOR achieves an average of 83% grasping efficacy in a real-world setting for four different object geometries while moving at an average velocity of 1 m/s during grasping. In a high-velocity setting, RAPTOR supports up to four times the payload compared to previous works. Our results highlight the potential of aerial drones in automated warehouses and other manipulation applications where speed, swiftness, and robustness are essential while operating in hard-to-reach places.11Code: https://github.com/raptor-ethz/raptor_setup",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981668,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981668,,Geometry;Tracking;Systems architecture;Grasping;Aerodynamics;Trajectory;Grippers,,17.0,,31,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/raptor-ethz/raptor_setup,https://github.com/raptor-ethz/raptor_setup
397,A Robust Sidewalk Navigation Method for Mobile Robots Based on Sparse Semantic Point Cloud,M. Wen; Y. Dai; T. Chen; C. Zhao; J. Zhang; D. Wang,"School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,7841,7846,"Last-mile delivery robots are usually required to navigate on the sidewalk through a fixed route. The current solutions heavily rely on the image-based perception and GPS localization to successfully complete delivery tasks. However, it is prone to fail and become unreliable when the robot runs in challenging conditions, such as operating in different illuminations, or under canopies of trees or buildings. To address these issues, this paper proposes a novel robust sidewalk navigation method for the last-mile delivery robots with an affordable sparse LiDAR, which consists of two main modules: Semantic Point Cloud Network (SegPCn) and Reactive Nav-igation Network (RNn), as shown in Fig. 1. More specifically, SegPCn takes the raw 3D point cloud as input and predicts the point-wise segmentation labels, presenting a robust perception capability even in the night. Then, the semantic point clouds are fed to RNn to generate an angular velocity to navigate the robot along the sidewalk, where the localization of the robot is not required. Moreover, an autolabeling mechanism is developed to reduce the labor involved in data preparation as well. And the LSTM neural network is explored to effectively leverage the historical context and derive correct decisions. Extensive experiments have been carried out to verify the efficacy of this method, and the results show that this method enables the robot to navigate on the sidewalk robustly during day and night. We open source the code and the data set on https://github.com/lukewenMX/Robust-Navigation-Method.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981281,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981281,,Point cloud compression;Location awareness;Image segmentation;Codes;Navigation;Semantics;Lighting,,9.0,,30,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/lukewenMX/Robust-Navigation-Method,https://github.com/lukewenMX/Robust-Navigation-Method
398,Scene-level Tracking and Reconstruction without Object Priors,H. Chang; A. Boularias,"Department of Computer Science, Rutgers University, NJ, USA; Department of Computer Science, Rutgers University, NJ, USA",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,3785,3792,"We present the first real-time system capable of tracking and reconstructing, individually, every visible object in a given scene, without any form of prior on the rigidness of the objects, texture existence, or object category. In contrast with previous methods such as Co-Fusion and MaskFusion that first segment the scene into individual objects and then process each object independently, the proposed method dynamically segments the non-rigid scene as part of the tracking and reconstruction process. When new measurements indicate topology change, reconstructed models are updated in real-time to reflect that change. Our proposed system can provide the live geometry and deformation of all visible objects in a novel scene in real-time, which makes it possible to be integrated seamlessly into numerous existing robotics applications that rely on object models for grasping and manipulation. The capabilities of the proposed system are demonstrated in challenging scenes that contain multiple rigid and non-rigid objects. Supplementary material, including video, can be found at https://github.com/changhaonan/STAR-no-prior.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9982159,"NSF(grant numbers:1734492,1846043,2132972); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982159,,Geometry;Deformable models;Visualization;Stars;Grasping;Streaming media;Real-time systems,,1.0,,20,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/changhaonan/STAR-no-prior,https://github.com/changhaonan/STAR-no-prior
399,Learning Object-Based State Estimators for Household Robots,Y. Du; T. Lozano-Perez; L. P. Kaelbling,"Computer Science and Artificial Intelligence Laboratory, MIT, USA; Computer Science and Artificial Intelligence Laboratory, MIT, USA; Computer Science and Artificial Intelligence Laboratory, MIT, USA",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,12558,12565,"A robot operating in a household makes observations of multiple objects as it moves around over the course of days or weeks. The objects may be moved by inhabitants, but not completely at random. The robot may be called upon later to retrieve objects and will need a long-term object-based memory in order to know how to find them. Existing work in semantic SLAM does not attempt to capture the dynamics of object movement. In this paper, we combine some aspects of classic techniques for data-association filtering with modern attention-based neural networks to construct object-based memory systems that operate on high-dimensional observations and hypotheses. We perform end-to-end learning on labeled observation trajectories to learn both the transition and observation models. We demonstrate the system's effectiveness in maintaining memory of dynamically changing objects in both simulated environment and real images, and demonstrate improvements over classical structured approaches as well as unstructured neural approaches. Additional information available at project website: https://yilundu.github.io/obm/.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981287,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981287,,Simultaneous localization and mapping;Filtering;Semantics;Neural networks;Trajectory;Intelligent robots,,1.0,,34,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://yilundu.github.io/obm,
400,Adversarial Attacks on Monocular Pose Estimation,H. Chawla; A. Varma; E. Arani; B. Zonooz,"Advanced Research Lab, NavInfo Europe, The Netherlands; Advanced Research Lab, NavInfo Europe, The Netherlands; Advanced Research Lab, NavInfo Europe, The Netherlands; Advanced Research Lab, NavInfo Europe, The Netherlands",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,12500,12505,"Advances in deep learning have resulted in steady progress in computer vision with improved accuracy on tasks such as object detection and semantic segmentation. Nevertheless, deep neural networks are vulnerable to adversarial attacks, thus presenting a challenge in reliable deployment. Two of the prominent tasks in 3D scene-understanding for robotics and advanced driver assistance systems are monocular depth and pose estimation, often learned together in an unsupervised manner. While studies evaluating the impact of adversarial attacks on monocular depth estimation exist, a systematic demonstration and analysis of adversarial perturbations against pose estimation are lacking. We show how additive imperceptible perturbations can not only change predictions to increase the trajectory drift but also catastrophically alter its geometry. We also study the relation between adversarial perturbations targeting monocular depth and pose estimation networks, as well as the transferability of perturbations to other networks with different architectures and losses. Our experiments show how the generated perturbations lead to notable errors in relative rotation and translation predictions and elucidate vulnerabilities of the networks.11Code can be found at https://github.com/NeurAI-Lab/mono-pose-attack.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9982154,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982154,,Deep learning;Training;Three-dimensional displays;Systematics;Perturbation methods;Pose estimation;Predictive models,,6.0,,23,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/NeurAI-Lab/mono-pose-attack,https://github.com/NeurAI-Lab/mono-pose-attack
401,PUTN: A Plane-fitting based Uneven Terrain Navigation Framework,Z. Jian; Z. Lu; X. Zhou; B. Lan; A. Xiao; X. Wang; B. Liang,"Center for Artificial Intelligence and Robotics, Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China; Center for Artificial Intelligence and Robotics, Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen, China; Center for Artificial Intelligence and Robotics, Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Center for Artificial Intelligence and Robotics, Shenzhen International Graduate School, Tsinghua University, Shenzhen, China",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,7160,7166,"Autonomous navigation of ground robots has been widely used in indoor structured 2D environments, but there are still many challenges in outdoor 3D unstructured environments, especially in rough, uneven terrains. This paper proposed a plane-fitting based uneven terrain navigation framework (PUTN) to solve this problem. The implementation of PUTN is divided into three steps. First, based on Rapidly-exploring Random Trees (RRT), an improved sample-based algorithm called Plane Fitting RRT*(PF- RRT*) is proposed to obtain a sparse trajectory. Each sampling point corresponds to a custom traversability index and a fitted plane on the point cloud. These planes are connected in series to form a traversable “strip”. Second, Gaussian Process Regression is used to generate traversability of the dense trajectory interpolated from the sparse trajectory, and the sampling tree is used as the training set. Finally, local planning is performed using nonlinear model predictive control (NMPC). By adding the traversability index and uncertainty to the cost function, and adding obstacles generated by the real-time point cloud to the constraint function, a safe motion planning algorithm with smooth speed and strong robustness is available. Experiments in real scenarios are conducted to verify the effectiveness of the method. The source code is released for the reference of the community11Source code: https://github.com/jianzhuozhuTHU/putn..",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981038,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981038,,Point cloud compression;Training;Strips;Uncertainty;Three-dimensional displays;Navigation;Trajectory,,40.0,,22,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/jianzhuozhuTHU/putn,https://github.com/jianzhuozhuTHU/putn
402,LF-VIO: A Visual-Inertial-Odometry Framework for Large Field-of-View Cameras with Negative Plane,Z. Wang; K. Yang; H. Shi; P. Li; F. Gao; K. Wang,"State Key Laboratory of Modern Optical Instrumentation, Zhejiang University, China; Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Germany; State Key Laboratory of Modern Optical Instrumentation, Zhejiang University, China; State Key Laboratory of Modern Optical Instrumentation, Zhejiang University, China; State Key Laboratory of Industrial Control Technology, Zhejiang University, China; State Key Laboratory of Modern Optical Instrumentation, Zhejiang University, China",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,4423,4430,"Visual-inertial-odometry has attracted extensive attention in the field of autonomous driving and robotics. The size of Field of View (FoV) plays an important role in Visual-Odometry (VO) and Visual-Inertial-Odometry (VIO), as a large FoV enables to perceive a wide range of surrounding scene elements and features. However, when the field of the camera reaches the negative half plane, one cannot simply use $[u, v, 1]^{T}$ to represent the image feature points anymore. To tackle this issue, we propose LF-VIO, a real-time VIO framework for cameras with extremely large FoV.We leverage a threedimensional vector with unit length to represent feature points, and design a series of algorithms to overcome this challenge. To address the scarcity of panoramic visual odometry datasets with ground-truth location and pose, we present the PALVIO dataset, collected with a Panoramic Annular Lens (PAL) system with an entire FoV of 36 $0^{\circ}\times(40^{\circ}\sim 120^{\circ})$ and an IMU sensor. With a comprehensive variety of experiments, the proposed LF-VIO is verified on both the established PALVIO benchmark and a public fisheye camera dataset with a FoV of $360^{\circ}\times(0^{\circ}\sim 93.5^{\circ})$. LF-VIO outperforms state-of-the-art visual-inertial-odometry methods. Our dataset and code are made publicly available at https://github.com/flysoaryun/LF-VIO",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981217,National Natural Science Foundation of China(grant numbers:12174341); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981217,,Visualization;Simultaneous localization and mapping;Robot vision systems;Pose estimation;Cameras;Real-time systems;Robustness,,8.0,,45,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/flysoaryun/LF-VIO,https://github.com/flysoaryun/LF-VIO
403,Adapting Rapid Motor Adaptation for Bipedal Robots,A. Kumar; Z. Li; J. Zeng; D. Pathak; K. Sreenath; J. Malik,"University of California, Berkeley, California, USA; University of California, Berkeley, California, USA; University of California, Berkeley, California, USA; Carnegie Mellon University; University of California, Berkeley, California, USA; University of California, Berkeley, California, USA",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,1161,1168,"Recent advances in legged locomotion have en-abled quadrupeds to walk on challenging terrains. However, bipedal robots are inherently more unstable and hence it's harder to design walking controllers for them. In this work, we leverage recent advances in rapid adaptation for locomotion control, and extend them to work on bipedal robots. Similar to existing works, we start with a base policy which produces actions while taking as input an estimated extrinsics vector from an adaptation module. This extrinsics vector contains information about the environment and enables the walking controller to rapidly adapt online. However, the extrinsics estimator could be imperfect, which might lead to poor performance of the base policy which expects a perfect estimator. In this paper, we propose A-RMA (Adapting RMA), which additionally adapts the base policy for the imperfect extrinsics estimator by finetuning it using model-free RL. We demonstrate that A-RMA outperforms a number of RL-based baseline controllers and model-based controllers in simulation, and show zero-shot deployment of a single A-RMA policy to enable a bipedal robot, Cassie, to walk in a variety of different scenarios in the real world beyond what it has seen during training. Videos and results at https: //ashish-kmr.github.io/a-rma/",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981091,National Science Foundation(grant numbers:CMMI-1944722); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981091,,Legged locomotion;Training;Adaptation models;Visualization;Robot vision systems;Surface roughness;Rough surfaces,,16.0,,48,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://ashish-kmr.github.io/a-rma,
404,Acoustic Localization and Communication Using a MEMS Microphone for Low-cost and Low-power Bio-inspired Underwater Robots,A. Hinduja; Y. Ohm; J. Liao; C. Majidi; M. Kaess,"Department of Mechanical Engineering, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Mechanical Engineering, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Mechanical Engineering, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,5470,5477,"Having accurate localization capabilities is one of the fundamental requirements of autonomous robots. For underwater vehicles, the choices for effective localization are limited due to limitations of GPS use in water and poor environ-mental visibility that makes camera-based methods ineffective. Popular inertial navigation methods for underwater localization using Doppler-velocity log sensors, sonar, high-end inertial navigation systems, or acoustic positioning systems require bulky expensive hardware which are incompatible with low-cost, bio-inspired underwater robots. In this paper, we introduce an approach for underwater robot localization inspired by GPS methods known as acoustic pseudoranging. Our method allows us to potentially localize multiple bio-inspired robots equipped with commonly available micro electro-mechanical systems microphones. This is achieved through estimating the time difference of arrival of acoustic signals sent simultaneously through four speakers with a known constellation geometry. We also leverage the same acoustic framework to perform one-way communication with the robot to execute some primitive motions. To our knowledge, this is the first application of the approach for the on-board localization of small bio-inspired robots in water. Hardware schematics and the accompanying code are released to aid further development in the field33https://github.com/rpl-cmu/underwater-acoustic-pseudoranging.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981355,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981355,,Location awareness;Autonomous underwater vehicles;Time difference of arrival;Sonar;Inertial navigation;Sonar navigation;Acoustics,,,,24,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/rpl-cmu/underwater-acoustic-pseudoranging,https://github.com/rpl-cmu/underwater-acoustic-pseudoranging
405,Markerless Suture Needle 6D Pose Tracking with Robust Uncertainty Estimation for Autonomous Minimally Invasive Robotic Surgery,Z. -Y. Chiu; A. Z. Liao; F. Richter; B. Johnson; M. C. Yip,"Department of Electrical and Computer Engineering, University of California San Diego, La Jolla, CA, USA; Department of Electrical and Computer Engineering, University of California San Diego, La Jolla, CA, USA; Department of Electrical and Computer Engineering, University of California San Diego, La Jolla, CA, USA; Department of Computer Science and Engineering, University of California San Diego, La Jolla, CA, USA; Department of Electrical and Computer Engineering, University of California San Diego, La Jolla, CA, USA",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,5286,5292,"Suture needle localization is necessary for autonomous suturing. Previous approaches in autonomous suturing often relied on fiducial markers rather than markerless detection schemes for localizing a suture needle due to the in-consistency of markerless detections. However, fiducial markers are not practical for real-world applications and can often be occluded from environmental factors in surgery (e.g., blood). Therefore in this work, we present a robust tracking approach for estimating the 6D pose of a suture needle when using inconsistent detections. We define observation models based on suture needles' geometry that captures the uncertainty of the detections and fuse them temporally in a probabilistic fashion. In our experiments, we compare different permutations of the observation models in the suture needle localization task to show their effectiveness. Our proposed method outperforms previous approaches in localizing a suture needle. We also demonstrate the proposed tracking method in an autonomous suture needle regrasping task and ex vivo environments**The code is available at https://github.com/ucsdarclab/suture-needle-tracking..",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981966,Telemedicine and Advanced Technology Research Center; NSF(grant numbers:2045803); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981966,,Location awareness;Geometry;Uncertainty;Minimally invasive surgery;Fuses;Needles;Probabilistic logic,,10.0,,29,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/ucsdarclab/suture-needle-tracking,https://github.com/ucsdarclab/suture-needle-tracking
406,Stubborn: A Strong Baseline for Indoor Object Navigation,H. Luo; A. Yue; Z. -W. Hong; P. Agrawal,"Department of Electrical Engineering and Computer Science at MIT, Improbable AI Lab; Department of Electrical Engineering and Computer Science at MIT, Improbable AI Lab; Department of Electrical Engineering and Computer Science at MIT, Improbable AI Lab; Department of Electrical Engineering and Computer Science at MIT, Improbable AI Lab",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,3287,3293,"We present a strong baseline that surpasses the performance of previously published methods on the Habitat Challenge task of navigating to a target object in indoor environments. Our method is motivated from primary failure modes of prior state-of-the-art: poor exploration, inaccurate object identification, and agent getting trapped due to imprecise map construction. We make three contributions to mitigate these issues: (i) First, we show that existing map-based methods fail to effectively use semantic clues for exploration. We present a semantic-agnostic exploration strategy (called Stubborn) without any learning that surprisingly outperforms prior work. (ii) We propose a strategy for integrating temporal information to improve object identification. (iii) Lastly, due to inaccurate depth observation the agent often gets trapped in small regions. We develop a multi-scale collision map for obstacle identification that mitigates this issue. Website: https://github.com/Improbable-AI/Stubborn",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981646,DARPA Machine Common Sense Program and Army Research Office's(grant numbers:W911NF-21-1-0328); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981646,,Navigation;Semantics;Indoor environment;Object recognition;Task analysis;Intelligent robots,,19.0,,31,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/Improbable-AI/Stubborn,https://github.com/Improbable-AI/Stubborn
407,Containerization and Orchestration of Software for Autonomous Mobile Robots: a Case Study of Mixed-Criticality Tasks across Edge-Cloud Computing Platforms,F. Lumpp; F. Fummi; H. D. Patel; N. Bombieri,"Dept. of Computer Science, Univ. of Verona, Italy; Dept. of Computer Science, Univ. of Verona, Italy; Dept. of Electrical and Computer Eng., Univ. of Waterloo, Canada; Dept. of Computer Science, Univ. of Verona, Italy",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,9708,9713,"Containerization promises to strengthen platform-independent development, better resource utilization, and secure deployment of software. As these benefits come with negligible overhead in CPU and memory utilization, containerization is increasingly being adopted in mobile robotic applications. An open challenge is supporting software tasks that have mixed-criticality requirements. Even more challenging is the combination of real-time containers with orchestration, which is an emerging paradigm to automate the deployment, networking, scaling, and availability of containerized workloads and services. This paper addresses this challenge by presenting a framework that extends the de-facto reference standard for container orchestration, Kubernetes, to schedule tasks with mixed-criticality requirements. Quantitative experimental results on the software implementing the mission of a Robotnik RB-Kairos mobile robot demonstrate the effectiveness of the proposed approach. The source code is publicly available on GitHub.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981581,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981581,,Schedules;Source coding;Containers;Software;Real-time systems;Mobile robots;Resource management,,5.0,,27,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,,
408,Scalable Model-based Policy Optimization for Decentralized Networked Systems,Y. Du; C. Ma; Y. Liu; R. Lin; H. Dong; J. Wang; Y. Yang,"King's College London, London, UK; Xiamen University, Xiamen, China; Peking University, Beijing, China; Chinese Academy of Sciences, Beijing, China; CFCS, School of CS, Peking University; University College London, London, UK; Peking University, Beijing, China",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,9019,9026,"Reinforcement learning algorithms require a large amount of samples; this often limits their real-world applications on even simple tasks. Such a challenge is more outstanding in multi-agent tasks, as each step of operation is more costly, requiring communications or shifting or resources. This work aims to improve data efficiency of multi-agent control by model-based learning. We consider networked systems where agents are cooperative and communicate only locally with their neighbors, and propose the decentralized model-based policy optimization framework (DMPO). In our method, each agent learns a dynamic model to predict future states and broadcast their predictions by communication, and then the policies are trained under the model rollouts. To alleviate the bias of model-generated data, we restrain the model usage for generating myopic rollouts, thus reducing the compounding error of model generation. To pertain the independence of policy update, we introduce extended value function and theoretically prove that the resulting policy gradient is a close approximation to true policy gradients. We evaluate our algorithm on several benchmarks for intelligent transportation systems, which are connected autonomous vehicle control tasks (Flow and CACC) and adaptive traffic signal control (ATSC). Empirical results show that our method achieves superior data efficiency and matches the performance of model-free methods using true models. The source code of our algorithm and baselines can be found at https://github.com/PKU-MARL/Model-Based-MARL.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9982253,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982253,,Adaptation models;Source coding;Reinforcement learning;Predictive models;Approximation algorithms;Data models;Task analysis,,2.0,,43,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/PKU-MARL/Model-Based-MARL,https://github.com/PKU-MARL/Model-Based-MARL
409,Reshaping Robot Trajectories Using Natural Language Commands: A Study of Multi-Modal Data Alignment Using Transformers,A. Bucker; L. Figueredo; S. Haddadinl; A. Kapoor; S. Ma; R. Bonatti,Technische Universität München; Technische Universität München; Technische Universität München; Microsoft; Microsoft; Microsoft,2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,978,984,"Natural language is the most intuitive medium for us to interact with other people when expressing commands and instructions. However, using language is seldom an easy task when humans need to express their intent towards robots, since most of the current language interfaces require rigid templates with a static set of action targets and commands. In this work, we provide a flexible language-based interface for human-robot collaboration, which allows a user to reshape existing trajectories for an autonomous agent. We take advantage of recent advancements in the field of large language models (BERT and CLIP) to encode the user command, and then combine these features with trajectory information using multi-modal attention transformers. We train the model using imitation learning over a dataset containing robot trajectories modified by language commands, and treat the trajectory generation process as a sequence prediction problem, analogously to how language generation architectures operate. We evaluate the system in multiple simulated trajectory scenarios, and show a significant performance increase of our model over baseline approaches. In addition, our real-world experiments with a robot arm show that users significantly prefer our natural language interface over traditional methods such as kinesthetic teaching or cost-function programming. Our study shows how the field of robotics can take advantage of large pre-trained language models towards creating more intuitive interfaces between robots and machines. Project webpage: https://arthurfenderbucker.github.io/NL_trajectory_reshaper/",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981810,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981810,,Natural languages;Education;Programming;Predictive models;Transformers;Manipulators;Trajectory,,24.0,,46,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://arthurfenderbucker.github.io/NL_trajectory_reshaper,https://github.com/arthurfenderbucker/NL_trajectory_reshaper
410,AssembleRL: Learning to Assemble Furniture from Their Point Clouds,O. Aslan; B. Bolat; B. Bal; T. Tumer; E. Sahin; S. Kalkan,"Dept. of Computer Engineering, KOVAN Research Lab; METU-ROMER, Center for Robotics and Artificial Intelligence, Middle East Technical University, Ankara, Turkey; Dept. of Computer Engineering, KOVAN Research Lab; METU-ROMER, Center for Robotics and Artificial Intelligence, Middle East Technical University, Ankara, Turkey; Dept. of Computer Engineering, KOVAN Research Lab; METU-ROMER, Center for Robotics and Artificial Intelligence, Middle East Technical University, Ankara, Turkey; Dept. of Computer Engineering, KOVAN Research Lab; METU-ROMER, Center for Robotics and Artificial Intelligence, Middle East Technical University, Ankara, Turkey; Dept. of Computer Engineering, KOVAN Research Lab; METU-ROMER, Center for Robotics and Artificial Intelligence, Middle East Technical University, Ankara, Turkey; Dept. of Computer Engineering, KOVAN Research Lab; METU-ROMER, Center for Robotics and Artificial Intelligence, Middle East Technical University, Ankara, Turkey",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,2748,2753,"The rise of simulation environments has enabled learning-based approaches for assembly planning, which is otherwise a labor-intensive and daunting task. Assembling furniture is especially interesting since furniture are intricate and pose challenges for learning-based approaches. Surprisingly, humans can solve furniture assembly mostly given a 2D snapshot of the assembled product. Although recent years have witnessed promising learning-based approaches for furniture assembly, they assume the availability of correct connection labels for each assembly step, which are expensive to obtain in practice. In this paper, we alleviate this assumption and aim to solve furniture assembly with as little human expertise and supervision as possible. To be specific, we assume the availability of the assembled point cloud, and comparing the point cloud of the current assembly and the point cloud of the target product, obtain a novel reward signal based on two measures: Incorrectness and incompleteness. We show that our novel reward signal can train a deep network to successfully assemble different types of furniture. Code and networks available here: https://github.com/METU-KALFA/AssembleRL.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9982112,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982112,,Point cloud compression;Convolutional codes;Solid modeling;Three-dimensional displays;Convolution;Current measurement;Manipulators,,3.0,,24,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/METU-KALFA/AssembleRL,https://github.com/METU-KALFA/AssembleRL
411,DSOL: A Fast Direct Sparse Odometry Scheme,C. Qu; S. S. Shivakumar; I. D. Miller; C. J. Taylor,"GRASP Laboratory, School of Engineering and Applied Sciences, University of Pennsylvania; GRASP Laboratory, School of Engineering and Applied Sciences, University of Pennsylvania; GRASP Laboratory, School of Engineering and Applied Sciences, University of Pennsylvania; GRASP Laboratory, School of Engineering and Applied Sciences, University of Pennsylvania",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,10587,10594,"In this paper, we describe Direct Sparse Odometry Lite (DSOL), an improved version of Direct Sparse Odometry (DSO) [1]. We propose several algorithmic and implementation enhancements which speed up computation by a significant factor (on average 5x) even on resource-constrained platforms. The increase in speed allows us to process images at higher frame rates, which in turn provides better results on rapid motions. Our open-source implementation is available at https://github.com/versatran01/dso1.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981491,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981491,,Multicore processing;Intelligent robots,,7.0,,43,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/versatran01/dso1,https://github.com/versatran01/dso1
412,TIGRIS: An Informed Sampling-based Algorithm for Informative Path Planning,B. Moon; S. Chatterjee; S. Scherer,"Robotics Institute, School of Computer Science at Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, School of Computer Science at Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, School of Computer Science at Carnegie Mellon University, Pittsburgh, PA, USA",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,5760,5766,"Informative path planning is an important and challenging problem in robotics that remains to be solved in a manner that allows for wide-spread implementation and real-world practical adoption. Among various reasons for this, one is the lack of approaches that allow for informative path planning in high-dimensional spaces and non-trivial sensor constraints. In this work we present a sampling-based approach that allows us to tackle the challenges of large and high-dimensional search spaces. This is done by performing informed sampling in the high-dimensional continuous space and incorporating potential information gain along edges in the reward estimation. This method rapidly generates a global path that maximizes information gain for the given path budget constraints. We discuss the details of our implementation for an example use case of searching for multiple objects of interest in a large search space using a fixed-wing UAV with a forward-facing camera. We compare our approach to a sampling-based planner baseline and demonstrate how our contributions allow our approach to consistently out-perform the baseline by 18.0%. With this we thus present a practical and generalizable informative path planning framework that can be used for very large environments, limited budgets, and high dimensional search spaces, such as robots with motion constraints or high-dimensional configuration spaces. [Code]aaCodebase: https://github.com/castacks/tigris [Video]bbVideo: https://youtu.be/bMw5nUGL5GQ",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981992,Office of Naval Research(grant numbers:N00014-21-1-2110); National Science Foundation(grant numbers:DGE1745016); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981992,,Target tracking;Monte Carlo methods;Estimation;Search problems;Robot sensing systems;Path planning;Entropy,,8.0,,22,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/castacks/tigris,https://github.com/castacks/tigris
413,NavDreams: Towards Camera-Only RL Navigation Among Humans,D. Dugas; O. Andersson; R. Siegwart; J. J. Chung,"Autonomous Systems Lab, ETH Zürich, Zürich, Switzerland; Autonomous Systems Lab, ETH Zürich, Zürich, Switzerland; Autonomous Systems Lab, ETH Zürich, Zürich, Switzerland; Autonomous Systems Lab, ETH Zürich, Zürich, Switzerland",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,2504,2511,"Autonomously navigating a robot in everyday crowded spaces requires solving complex perception and planning challenges. When using only monocular image sensor data as input, classical two-dimensional planning approaches cannot be used. While images present a significant challenge when it comes to perception and planning, they also allow capturing potentially important details, such as complex geometry, body movement, and other visual cues. In order to successfully solve the navigation task from only images, algorithms must be able to model the scene and its dynamics using only this channel of information. We investigate whether the world model concept, which has shown state-of-the-art results for modeling and learning policies in Atari games as well as promising results in 2D LiDAR-based crowd navigation, can also be applied to the camera-based navigation problem. To this end, we create simulated environments where a robot must navigate past static and moving humans without colliding in order to reach its goal. We find that state-of-the-art methods are able to achieve success in solving the navigation problem, and can generate dream-like predictions of future image-sequences which show consistent geometry and moving persons. We are also able to show that policy performance in our high-fidelity sim2real simulation scenario transfers to the real world by testing the policy on a real robot. We make our simulator, models and experiments available at https://github.com/danieldugas/NavDreams.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9982045,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982045,,Geometry;Visualization;Navigation;Heuristic algorithms;Semantics;Robot vision systems;Multitasking,,7.0,,31,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/danieldugas/NavDreams,https://github.com/danieldugas/NavDreams
414,FedDrive: Generalizing Federated Learning to Semantic Segmentation in Autonomous Driving,L. Fantauzzo; E. Fanì; D. Caldarola; A. Tavera; F. Cermelli; M. Ciccone; B. Caputo,"Politecnico di Torino, Turin, Italy; Politecnico di Torino, Turin, Italy; Politecnico di Torino, Turin, Italy; Politecnico di Torino, Turin, Italy; Politecnico di Torino, Turin, Italy; Politecnico di Torino, Turin, Italy; Politecnico di Torino, Turin, Italy",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,11504,11511,"Semantic Segmentation is essential to make self-driving vehicles autonomous, enabling them to understand their surroundings by assigning individual pixels to known categories. However, it operates on sensible data collected from the users' cars; thus, protecting the clients' privacy becomes a primary concern. For similar reasons, Federated Learning has been recently introduced as a new machine learning paradigm aiming to learn a global model while preserving privacy and leveraging data on millions of remote devices. Despite several efforts on this topic, no work has explicitly addressed the challenges of federated learning in semantic segmentation for driving so far. To fill this gap, we propose FedDrive, a new benchmark consisting of three settings and two datasets, incorporating the real-world challenges of statistical heterogeneity and domain generalization. We benchmark state-of-the-art algorithms from the federated learning literature through an in-depth analysis, combining them with style transfer methods to improve their generalization ability. We demonstrate that correctly handling normalization statistics is crucial to deal with the aforementioned challenges. Furthermore, style transfer improves performance when dealing with significant appearance shifts. Official website: https://feddrive.github.io.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981098,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981098,,Data privacy;Federated learning;Semantic segmentation;Semantics;Benchmark testing;Data models;Task analysis,,29.0,,56,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,,
415,GE-Grasp: Efficient Target-Oriented Grasping in Dense Clutter,Z. Liu; Z. Wang; S. Huang; J. Zhou; J. Lu,"Department of Automation, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China; Department of Automation, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China; Department of Automation, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China; Department of Automation, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China; Department of Automation, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,1388,1395,"Grasping in dense clutter is a fundamental skill for autonomous robots. However, the crowdedness and oc-clusions in the cluttered scenario cause significant difficul-ties to generate valid grasp poses without collisions, which results in low efficiency and high failure rates. To address these, we present a generic framework called GE-Grasp for robotic motion planning in dense clutter, where we leverage diverse action primitives for occluded object removal and present the generator-evaluator architecture to avoid spatial collisions. Therefore, our GE-Grasp is capable of grasping objects in dense clutter efficiently with promising success rates. Specifically, we define three action primitives: target-oriented grasping for target capturing, pushing, and nontarget-oriented grasping to reduce the crowdedness and occlusions. The gen-erators effectively provide various action candidates referring to the spatial information. Meanwhile, the evaluators assess the selected action primitive candidates, where the optimal action is implemented by the robot. Extensive experiments in simulated and real-world environments show that our approach outperforms the state-of-the-art methods of grasping in clutter with respect to motion efficiency and success rates. Moreover, we achieve comparable performance in the real world as that in the simulation environment, which indicates the strong gen-eralization ability of our GE-Grasp. Supplementary material is available at: https://github.com/CaptainWuDaoKou/GE-Grasp.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981499,"National Natural Science Foundation of China(grant numbers:U1813218,62125603); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981499,,Robot motion;Grasping;Planning;Clutter;Intelligent robots;Autonomous robots,,16.0,,28,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/CaptainWuDaoKou/GE-Grasp,https://github.com/CaptainWuDaoKou/GE-Grasp
416,InCloud: Incremental Learning for Point Cloud Place Recognition,J. Knights; P. Moghadam; M. Ramezani; S. Sridharan; C. Fookes,"Robotics and Autonomous Systems, DATA61, CSIRO, Brisbane, QLD, Australia; Robotics and Autonomous Systems, DATA61, CSIRO, Brisbane, QLD, Australia; Robotics and Autonomous Systems, DATA61, CSIRO, Brisbane, QLD, Australia; Artificial Intelligence and Vision Technologies (SAIVT) at the Queensland University of Technology (QUT), Brisbane, Australia; Artificial Intelligence and Vision Technologies (SAIVT) at the Queensland University of Technology (QUT), Brisbane, Australia",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,8559,8566,"Place recognition is a fundamental component of robotics, and has seen tremendous improvements through the use of deep learning models in recent years. Networks can experience significant drops in performance when deployed in unseen or highly dynamic environments, and require additional training on the collected data. However naively fine-tuning on new training distributions can cause severe degradation of performance on previously visited domains, a phenomenon known as catastrophic forgetting. In this paper we address the problem of incremental learning for point cloud place recognition and introduce InCloud, a structure-aware distillation-based approach which preserves the higher-order structure of the network's embedding space. We introduce several challenging new benchmarks on four popular and large-scale LiDAR datasets (Oxford, MulRan, In-house and KITTI) showing broad improvements in point cloud place recognition performance over a variety of network architectures. To the best of our knowledge, this work is the first to effectively apply incremental learning for point cloud place recognition. Data pre-processing, training and evaluation code for this paper can be found at https://github.com/csiro-robotics/InCloud.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981252,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981252,,Point cloud compression;Training;Degradation;Deep learning;Laser radar;Codes;Network architecture,,17.0,,39,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/csiro-robotics/InCloud,https://github.com/csiro-robotics/InCloud
417,PourNet: Robust Robotic Pouring Through Curriculum and Curiosity-based Reinforcement Learning,E. Babaians; T. Sharma; M. Karimi; S. Sharifzadeh; E. Steinbach,"Department of Electrical and Computer Engineering, Chair of Media Technology (LMT) and Munich Institute of Robotics and Machine Intelligence (MIRMI), Technical University of Munich (TUM); Department of Electrical and Computer Engineering, Chair of Media Technology (LMT) and Munich Institute of Robotics and Machine Intelligence (MIRMI), Technical University of Munich (TUM); Department of Electrical and Computer Engineering, Chair of Media Technology (LMT) and Munich Institute of Robotics and Machine Intelligence (MIRMI), Technical University of Munich (TUM); Ludwig Maximilian University of Munich (LMU), Germany; Department of Electrical and Computer Engineering, Chair of Media Technology (LMT) and Munich Institute of Robotics and Machine Intelligence (MIRMI), Technical University of Munich (TUM)",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,9332,9339,"Pouring liquids accurately into containers is one of the most challenging tasks for robots as they are unaware of the complex fluid dynamics and the behavior of liquids when pouring. Therefore, it is not possible to formulate a generic pouring policy for real-time applications. In this paper, we propose PourNet, as a generalized solution to pouring different liquids into containers. PourNet is a hybrid planner that uses deep reinforcement learning, for end-effector planning, and Nonlinear Model Predictive Control, for joint planning. In this work, we introduce a novel simulation environment using Unity3D and NVIDIA-Flex to train our agents. By effective choice of the state space, action space and the reward functions, we allow for a direct sim-to-real transfer of the learned skills without additional training. In the simulation, PourNet outperforms state-of-the-art by an average of 4.9g deviation for water-like, and 9.2g deviation for honey-like liquids. In the real-world scenario using Kinova Movo Platform, PourNet achieves an average pouring deviation of 2.3g for dish soap when using a novel pouring container. The average pouring deviation measured for water was 5.5g. All comprehensive experiments and the simulation environment is available at: http://cxdcxd.github.io/RRS/.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981195,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981195,,Training;Liquids;Oils;Reinforcement learning;Containers;Aerospace electronics;Real-time systems,,5.0,,30,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,http://cxdcxd.github.io/RRS,https://github.com/cxdcxd/RRS
418,Contrastive Learning for Cross-Domain Open World Recognition,F. C. Borlino; S. Bucci; T. Tommasi,"DAUIN Department at Politecnico di Torino, Italy; DAUIN Department at Politecnico di Torino, Italy; DAUIN Department at Politecnico di Torino, Italy",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,10133,10140,"The ability to evolve is fundamental for any valuable autonomous agent whose knowledge cannot remain limited to that injected by the manufacturer. Consider for example a home assistant robot: it should be able to incrementally learn new object categories when requested, but also to recognize the same objects in different environments (rooms) and poses (hand-held/on the floor/above furniture), while rejecting unknown ones. Despite its importance, this scenario has started to raise interest in the robotic community only recently and the related research is still in its infancy, with existing experimental testbeds but no tailored methods. With this work, we propose the first learning approach that deals with all the previously mentioned challenges at once by exploiting a single contrastive objective. We show how it learns a feature space perfectly suitable to incrementally include new classes and is able to capture knowledge which generalizes across a variety of visual domains. Our method is endowed with a tailored effective stopping criterion for each learning episode and exploits a self-paced thresholding strategy that provides the classifier with a reliable rejection option. Both these novel contributions are based on the observation of the data statistics and do not need manual tuning. An extensive experimental analysis confirms the effectiveness of the proposed approach in establishing the new state-of-the-art. The code is available at https://github.com/FrancescoCappio/Contrastive_Open_World.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981592,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981592,,Visualization;Codes;Manuals;Autonomous agents;Reliability;Task analysis;Tuning,,2.0,,58,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/FrancescoCappio/Contrastive_Open_World,https://github.com/FrancescoCappio/Contrastive_Open_World
419,Sequential thermal image-based adult and baby detection robust to thermal residual heat marks,D. -G. Lee; K. -S. Song; Y. -H. Nho; A. Kim; D. -S. Kwon,"Robotics Program, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea; Robotics Program, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea; Department of Neurosurgery, University of Pennsylvania, Philadelphia, PA, USA; Department of Mechanical Engineering, SNU, Seoul, Republic of Korea; Department of Mechanical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,13120,13127,"The awareness for preserving privacy in in-home monitoring robots is increasing. Although several studies have proposed privacy-preserved in-home monitoring robot systems for adults, only a limited amount of attention has been paid attention to research on privacy-preserved in-home monitoring of babies. Like previous studies, thermal infrared image-based methods could ensure a privacy-preserved monitoring of babies, yet when existing detection methods were applied to thermal images to detect babies and adults, we discovered a frequent occurrence of misdetection due to the presence of thermal residual heat marks. In this research, we propose a sequential thermal image-based detection that conjugated the characteristics of thermal residual heat marks. The proposed detection reduced misdetection caused by thermal residual heat marks by 98.7% when compared to RetinaNet. In addition, we open-source our collected thermal image-based baby and adult dataset via: https://github.com/donkeymouse/ThermalAdultandBaby.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9982115,"Ministry of Trade, Industry and Energy (MOTIE)(grant numbers:P0006702); Korea Institute for Advancement of Technology (KIAT)(grant numbers:P0006702); NRF(grant numbers:2020R1C1C1006620); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982115,,Heating systems;Pediatrics;Data privacy;Privacy;Social robots;Robustness;Mobile robots,,1.0,,36,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/donkeymouse/ThermalAdultandBaby,https://github.com/donkeymouse/ThermalAdultandBaby
420,Trajectory Prediction with Graph-based Dual-scale Context Fusion,L. Zhang; P. Li; J. Chen; S. Shen,"Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, China; DJI Technology Company, Ltd., Shenzhen, China; DJI Technology Company, Ltd., Shenzhen, China; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, China",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,11374,11381,"Motion prediction for traffic participants is essential for a safe and robust automated driving system, especially in cluttered urban environments. However, it is highly challenging due to the complex road topology as well as the uncertain intentions of the other agents. In this paper, we present a graph-based trajectory prediction network named the Dual Scale Predictor (DSP), which encodes both the static and dynamical driving context in a hierarchical manner. Different from methods based on a rasterized map or sparse lane graph, we consider the driving context as a graph with two layers, focusing on both geometrical and topological features. Graph neural networks (GNNs) are applied to extract features with different levels of granularity, and features are subsequently aggregated with attention-based inter-layer networks, realizing better local-global feature fusion. Following the recent goal-driven trajectory prediction pipeline, goal candidates with high likelihood for the target agent are extracted, and predicted trajectories are generated conditioned on these goals. Thanks to the proposed dual-scale context fusion network, our DSP is able to generate accurate and human-like multi-modal trajectories. We evaluate the proposed method on the large-scale Argoverse motion forecasting benchmark, and it achieves promising results, outperforming the recent state-of-the-art methods. We release the code on our project website. 11https://github.com/HKUST-Aerial-Robotics/DSP",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981923,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981923,,Roads;Urban areas;Pipelines;Benchmark testing;Feature extraction;Trajectory;Topology,,17.0,,40,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/HKUST-Aerial-Robotics/DSP,https://github.com/HKUST-Aerial-Robotics/DSP
421,Fisheye object detection based on standard image datasets with 24-points regression strategy,X. Xu; Y. Gao; H. Liang; Y. Yang; M. Fu,"School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,9911,9918,"Fisheye object detection is a difficult task in robotics and autonomous driving. One of the reasons is that the fisheye datasets are inferior to standard image datasets in scale and quantity, which inspires the idea of using standard image datasets for fisheye object detection. However, the models trained on standard image datasets do not perform well with fisheye data. In this work, we explore the effect of fisheye images on different stages of the YOLOX with published weights generated by standard image datasets. We also propose a new regression strategy for 24-points object representation method, which is insensitive to image distortion. The experiments show that the feature extraction part is robust to fisheye image features, while the regression part of location and category performs poorly. The strategy can achieve the position of discrete points without calculating the IOU of irregular-shaped boxes. Theoretically, the strategy can be widely adopted to regress the irregular bounding boxes composed of discrete points. Source code is at https://github.com/IN2-ViAUn/Exploration-of-Potential.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981891,"National Natural Science Foundation of China(grant numbers:NSFC 61973034,U1913203,61903034); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981891,,Source coding;Object detection;Feature extraction;Distortion;Data models;Task analysis;Standards,,2.0,,36,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/IN2-ViAUn/Exploration-of-Potential,https://github.com/IN2-ViAUn/Exploration-of-Potential
422,Efficient Spatial-Temporal Information Fusion for LiDAR-Based 3D Moving Object Segmentation,J. Sun; Y. Dai; X. Zhang; J. Xu; R. Ai; W. Gu; X. Chen,"Northwestern Polytechnical University; Northwestern Polytechnical University; HAOMO.AI Tech. Co., Ltd.; HAOMO.AI Tech. Co., Ltd.; HAOMO.AI Tech. Co., Ltd.; HAOMO.AI Tech. Co., Ltd.; HAOMO.AI Tech. Co., Ltd.",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,11456,11463,"Accurate moving object segmentation is an es-sential task for autonomous driving. It can provide effective information for many downstream tasks, such as collision avoidance, path planning, and static map construction. How to effectively exploit the spatial-temporal information is a critical question for 3D LiDAR moving object segmentation (LiDAR-MOS). In this work, we propose a novel deep neural network exploiting both spatial-temporal information and different representation modalities of LiDAR scans to improve LiDAR-MOS performance. Specifically, we first use a range image-based dual-branch structure to separately deal with spatial and temporal information that can be obtained from sequential LiDAR scans, and later combine them using motion-guided attention modules. We also use a point refinement module via 3D sparse convolution to fuse the information from both LiDAR range image and point cloud representations and reduce the artifacts on the borders of the objects. We verify the effectiveness of our proposed approach on the LiDAR-MOS benchmark of SemanticKITTI. Our method outperforms the state-of-the-art methods significantly in terms of LiDAR-MOS IoU. Benefiting from the devised coarse-to-fine architecture, our method operates online at sensor frame rate. Code is available at: https://github.com/haomo-ai/MotionSeg3D.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981210,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981210,,Point cloud compression;Laser radar;Three-dimensional displays;Fuses;Neural networks;Training data;Object segmentation,,53.0,,40,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/haomo-ai/MotionSeg3D,https://github.com/haomo-ai/MotionSeg3D
423,Parallel Monte Carlo Tree Search with Batched Rigid-body Simulations for Speeding up Long-Horizon Episodic Robot Planning,B. Huang; A. Boularias; J. Yu,"Department of Computer Science, Rutgers, the State University of New Jersey, USA; Department of Computer Science, Rutgers, the State University of New Jersey, USA; Department of Computer Science, Rutgers, the State University of New Jersey, USA",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,1153,1160,"We propose a novel Parallel Monte Carlo tree search with Batched Simulations (PMBS) algorithm for accelerating long-horizon, episodic robotic planning tasks. Monte Carlo tree search (MCTS) is an effective heuristic search algorithm for solving episodic decision-making problems whose underlying search spaces are expansive. Leveraging a GPU-based large-scale simulator, PMBS introduces massive parallelism into MCTS for solving planning tasks through the batched execution of a large number of concurrent simulations, which allows for more efficient and accurate evaluations of the expected cost-to-go over large action spaces. When applied to the challenging manipulation tasks of object retrieval from clutter, PMBS achieves a speedup of over 30× with an improved solution quality, in comparison to a serial MCTS implementation. We show that PMBS can be directly applied to real robot hardware with negligible sim-to-real differences. Supplementary material, including video, can be found at https://github.com/arc-l/pmbs.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981962,"NSF(grant numbers:1734492,1846043,1845888,2132972); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981962,,Monte Carlo methods;Computational modeling;Parallel processing;Search problems;Hardware;Real-time systems;Planning,,8.0,,57,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/arc-l/pmbs,https://github.com/arc-l/pmbs
424,Unsupervised Domain Adaptation for Point Cloud Semantic Segmentation via Graph Matching,Y. Bian; L. Hui; J. Qian; J. Xie,"PCA Lab, Key Lab of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, and Jiangsu Key Lab of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; PCA Lab, Key Lab of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, and Jiangsu Key Lab of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; PCA Lab, Key Lab of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, and Jiangsu Key Lab of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; PCA Lab, Key Lab of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, and Jiangsu Key Lab of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,9899,9904,"Unsupervised domain adaptation for point cloud semantic segmentation has attracted great attention due to its effectiveness in learning with unlabeled data. Most of existing methods use global-level feature alignment to transfer the knowledge from the source domain to the target domain, which may cause the semantic ambiguity of the feature space. In this paper, we propose a graph-based framework to explore the local-level feature alignment between the two domains, which can reserve semantic discrimination during adaptation. Specifically, in order to extract local-level features, we first dynamically construct local feature graphs on both domains and build a memory bank with the graphs from the source domain. In particular, we use optimal transport to generate the graph matching pairs. Then, based on the assignment matrix, we can align the feature distributions between the two domains with the graph-based local feature loss. Furthermore, we consider the correlation between the features of different categories and formulate a category-guided contrastive loss to guide the segmentation model to learn discriminative features on the target domain. Extensive experiments on different synthetic-to-real and real-to-real domain adaptation scenarios demonstrate that our method can achieve state-of-the-art performance. Our code is available at https://github.com/BianYikai/PointUDA.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981603,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981603,,Point cloud compression;Adaptation models;Correlation;Codes;Semantic segmentation;Semantics;Feature extraction,,6.0,,29,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/BianYikai/PointUDA,https://github.com/BianYikai/PointUDA
425,Deep Residual Reinforcement Learning based Autonomous Blimp Control,Y. T. Liu; E. Price; M. J. Black; A. Ahmad,"Max Planck Institute for Intelligent Systems., Tübingen, Germany; Institute for Flight Mechanics and Controls,The Faculty of Aerospace Engineering and Geodesy,University of Stuttgart, Stuttgart, Germany; Max Planck Institute for Intelligent Systems., Tübingen, Germany; Institute for Flight Mechanics and Controls,The Faculty of Aerospace Engineering and Geodesy,University of Stuttgart, Stuttgart, Germany",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,12566,12573,"Blimps are well suited to perform long-duration aerial tasks as they are energy efficient, relatively silent and safe. To address the blimp navigation and control task, in previous work we developed a hardware and software-in-the-loop framework and a PID-based controller for large blimps in the presence of wind disturbance. However, blimps have a deformable structure and their dynamics are inherently non-linear and time-delayed, making PID controllers difficult to tune. Thus, often resulting in large tracking errors. Moreover, the buoyancy of a blimp is constantly changing due to variations in ambient temperature and pressure. To address these issues, in this paper we present a learning-based framework based on deep residual reinforcement learning (DRRL), for the blimp control task. Within this framework, we first employ a PID controller to provide baseline performance. Subsequently, the DRRL agent learns to modify the PID decisions by interaction with the environment. We demonstrate in simulation that DRRL agent consistently improves the PID performance. Through rigorous simulation experiments, we show that the agent is robust to changes in wind speed and buoyancy. In real-world experiments, we demonstrate that the agent, trained only in simulation, is sufficiently robust to control an actual blimp in windy conditions. We openly provide the source code of our approach at https://github.com/robot-perception-group/AutonomousBlimpDRL. Video demonstration is provided at https://youtu.be/EMC4KnlH0yI.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981182,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981182,,Training;PI control;Navigation;Wind speed;Source coding;Reinforcement learning;Buoyancy,,6.0,,28,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/robot-perception-group/AutonomousBlimpDRL,https://github.com/robot-perception-group/AutonomousBlimpDRL
426,Deep Reinforcement Learning based Robot Navigation in Dynamic Environments using Occupancy Values of Motion Primitives,N. Ü. Akmandor; H. Li; G. Lvov; E. Dusel; T. Padir,"Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, USA; Khoury College of Computer Sciences, Northeastern University, Boston, MA, USA; Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, USA; Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, USA; Institute for Experiential Robotics, Boston, MA, USA",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,11687,11694,"This paper presents a Deep Reinforcement Learning based navigation approach in which we define the occu-pancy observations as heuristic evaluations of motion primitives, rather than using raw sensor data. Our method enables fast mapping of the occupancy data, generated by multi-sensor fusion, into trajectory values in 3D workspace. The computationally efficient trajectory evaluation allows dense sampling of the action space. We utilize our occupancy observations in different data structures to analyze their effects on both training process and navigation performance. We train and test our methodology on two different robots within challenging physics-based simulation environments including static and dy-namic obstacles. We benchmark our occupancy representations with other conventional data structures from state-of-the-art methods. The trained navigation policies are also validated successfully with physical robots in dynamic environments. The results show that our method not only decreases the required training time but also improves the navigation performance as compared to other occupancy representations. The open-source implementation of our work and all related info are available at https://github.com/RIVeR-Lab/tentabot.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9982133,National Science Foundation(grant numbers:1928654); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982133,,Training;Deep learning;Three-dimensional displays;Navigation;Neural networks;Reinforcement learning;Benchmark testing,,5.0,,55,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/RIVeR-Lab/tentabot,https://github.com/RIVeR-Lab/tentabot
427,Inferring Articulated Rigid Body Dynamics from RGBD Video,E. Heiden; Z. Liu; V. Vineet; E. Coumans; G. S. Sukhatme,"NVIDIA, Santa Clara, USA; Stanford University, Stanford, USA; Microsoft Research, Redmond, USA; NVIDIA, Santa Clara, USA; Google Research, Mountain View, USA",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,8383,8390,"Being able to reproduce physical phenomena ranging from light interaction to contact mechanics, simulators are becoming increasingly useful in more and more application domains where real-world interaction or labeled data are difficult to obtain. Despite recent progress, significant human effort is needed to configure simulators to accurately reproduce real-world behavior. We introduce a pipeline that combines inverse rendering with differentiable simulation to create digital twins of real-world articulated mechanisms from depth or RGB videos. Our approach automatically discovers joint types and estimates their kinematic parameters, while the dynamic properties of the overall mechanism are tuned to attain physically accurate simulations. Control policies optimized in our derived simulation transfer successfully back to the original system, as we demonstrate on a simulated system. Further, our approach accurately reconstructs the kinematic tree of an articulated mechanism being manipulated by a robot, and highly nonlinear dynamics of a real-world coupled pendulum mechanism. Website: https://eric-heiden.github.io/video2sim",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981687,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981687,,Shape;Pipelines;Kinematics;Predictive models;Rendering (computer graphics);Real-time systems;Nonlinear dynamical systems,,3.0,,41,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://eric-heiden.github.io/video2sim,
428,RARA: Zero-shot Sim2Real Visual Navigation with Following Foreground Cues,K. Kelchtermans; T. Tuytelaars,"KULeuven, ESAT-PSI, Belgium; KULeuven, ESAT-PSI, Belgium",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,1704,1710,"The gap between simulation and the real-world restrains many machine learning breakthroughs in computer vision and reinforcement learning from being applicable in the real world. In this work, we tackle this gap for the specific case of camera-based navigation, formulating it as following a visual cue in the foreground with arbitrary backgrounds. The visual cue in the foreground can often be simulated realistically, such as a line, gate or cone. The challenge then lies in coping with the unknown backgrounds and integrating both. As such, the goal is to train a visual agent on data captured in an empty simulated environment except for this foreground cue and test this model directly in a visually diverse real world. In order to bridge this big gap, we show it's crucial to combine following techniques namely: Randomized augmentation of the fore- and background, regularization with both deep supervision and triplet loss and finally abstraction of the dynamics by using waypoints rather than direct velocity commands. The various techniques are ablated in our experimental results both qualitatively and quantitatively finally demonstrating a successful transfer from simulation to the real world. Code will be made available on publication22Project page: github.com/kkelchte/tgbg.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9982066,FWO-SBO; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982066,,Training;Visualization;Target tracking;Navigation;Reinforcement learning;Predictive models;Logic gates,,2.0,,22,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/kkelchte/tgbg,https://github.com/kkelchte/tgbg
429,WFH-VR: Teleoperating a Robot Arm to set a Dining Table across the Globe via Virtual Reality,L. S. Yim; Q. T. Vo; C. -I. Huang; C. -R. Wang; W. McQueary; H. -C. Wang; H. Huang; L. -F. Yu,"Department of Electrical and Computer Engineering, National Yang Ming Chiao Tung University, Taiwan; Department of Computer Science, George Mason University, USA; Department of Electrical and Computer Engineering, National Yang Ming Chiao Tung University, Taiwan; Department of Electrical and Computer Engineering, National Yang Ming Chiao Tung University, Taiwan; Department of Computer Science, George Mason University, USA; Department of Electrical and Computer Engineering, National Yang Ming Chiao Tung University, Taiwan; Department of Computer Science, George Mason University, USA; Department of Computer Science, George Mason University, USA",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,4927,4934,"This paper presents an easy-to-deploy, virtual reality-based teleoperation system for controlling a robot arm. The proposed system is based on a consumer-grade virtual reality device (Oculus Quest 2) with a low-cost robot arm (a LoCoBot) to allow easy replication and set up. The proposed Work-from-Home Virtual Reality (WFH-VR) system allows the user to feel an intimate connection with the real remote robot arm. Virtual representations of the robot and objects to be manipulated in the real-world are presented in VR by streaming data pertaining to orientation and poses. The user studies suggest that 1) the proposed telerobotic system is effective under conditions both with and without network latency, whereas a method that simply streams video does not. This design enables the system implemented at an arbitrary distance from the actual work site. 2) The proposed system allows novices to perform manipulation tasks requiring higher dexterity than traditional keyboard controls can support, such as setting tableware. All results, hardware settings, and questionnaire feedback can be obtained at https://arg-nctu.github.io/projects/vr-robot-arm.html.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981729,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981729,,Three-dimensional displays;Robot vision systems;Pose estimation;Virtual reality;Streaming media;Manipulators;Real-time systems,,6.0,,35,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://arg-nctu.github.io/projects/vr-robot-arm,
430,Transferring Dexterous Manipulation from GPU Simulation to a Remote Real-World TriFinger,A. Allshire; M. MittaI; V. Lodaya; V. Makoviychuk; D. Makoviichuk; F. Widmaier; M. Wüthrich; S. Bauer; A. Handa; A. Garg,"University of Toronto, Vector Institute; Nvidia; University of Toronto, Vector Institute; Nvidia; Snap; MPI Tubingen; MPI Tubingen; KTH; Nvidia; University of Toronto, Vector Institute",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,11802,11809,"In-hand manipulation of objects is an important capability to enable robots to carry-out tasks which demand high levels of dexterity. This work presents a robot systems approach to learning dexterous manipulation tasks involving moving objects to arbitrary 6-DoF poses. We show empirical benefits, both in simulation and sim - to- real transfer, of using keypoint-based representations for object pose in policy observations and reward calculation to train a model-free reinforcement learning agent. By utilizing domain randomization strategies and large-scale training, we achieve a high success rate of 83 % on a real TriFinger system, with a single policy able to perform grasping, ungrasping, and finger gaiting in order to achieve arbitrary poses within the workspace. We demonstrate that our policy can generalise to unseen objects, and success rates can be further improved through finetuning. With the aim of assisting further research in learning in-hand manipulation, we provide a detailed exposition of our system and make the codebase of our system available, along with checkpoints trained on billions of steps of experience, at https://s2r2-ig.github.io",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981458,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981458,,Training;Fingers;Graphics processing units;Reinforcement learning;Grasping;Task analysis;Intelligent robots,,17.0,,29,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,,
431,Learned Depth Estimation of 3D Imaging Radar for Indoor Mapping,R. Xu; W. Dong; A. Sharma; M. Kaess,"Robotics Institute, Carnegie Mellon University (CMU), Pittsburgh, USA; Robotics Institute, Carnegie Mellon University (CMU), Pittsburgh, USA; Robotics Institute, Carnegie Mellon University (CMU), Pittsburgh, USA; Robotics Institute, Carnegie Mellon University (CMU), Pittsburgh, USA",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,13260,13267,"3D imaging radar offers robust perception capability through visually demanding environments due to the unique penetrative and reflective properties of millimeter waves (mmWave). Current approaches for 3D perception with imaging radar require knowledge of environment geometry, accumulation of data from multiple frames for perception, or access to between-frame motion. Imaging radar presents an additional difficulty due to the complexity of its data representation. To address these issues, and make imaging radar easier to use for downstream robotics tasks, we propose a learning-based method that regresses radar measurements into cylindrical depth maps using LiDAR supervision. Due to the limitation of the regression formulation, directions where the radar beam could not reach will still generate a valid depth. To address this issue, our method additionally learns a 3D filter to remove those pixels. Experiments show that our system generates visually accurate depth estimation. Furthermore, we confirm the overall ability to generalize in the indoor scene using the estimated depth for probabilistic occupancy mapping with ground truth trajectory. The code and model will be released11https://github.com/rpl-cmu/learned-depth-imaging-radar.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981572,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981572,,Three-dimensional displays;Radar measurements;Navigation;Imaging;Radar;Radar imaging;Millimeter wave radar,,6.0,,37,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/rpl-cmu/learned-depth-imaging-radar,https://github.com/rpl-cmu/learned-depth-imaging-radar
432,A Configurable Skill Oriented Architecture Based on OPC UA,J. B. Gracia; F. Leber; M. Aburaia; W. Wöber,"Department of Industrial Engineering, University of Applied Sciences Technikum, Wien, Austria; Department of Industrial Engineering, University of Applied Sciences Technikum, Wien, Austria; Department of Industrial Engineering, University of Applied Sciences Technikum, Wien, Austria; Department of Industrial Engineering, University of Applied Sciences Technikum, Wien, Austria",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,7317,7322,"Over the last years, research done in automation and industrial robotics has established the foundations for skill-oriented systems based on the OPC UA standard. Nevertheless, utilizing these advances in other areas of robotics research can be challenging and time consuming. We present a framework aiming to reduce this entry threshold. Our solution is an open source, easy to configure tool based on OPC UA, that provides with a hardware agnostic, skill-oriented, event driven interface to systems. The framework allows integrating external hardware and software by means of plugins. It also provides a mechanism for endowing skills with hardware agnostic motion control. We demonstrate how our framework can be combined with state of the art approaches to control in a simulated assembly task with multiple robots, conveyors, actuators and sensors involved.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9982164,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982164,,Service robots;Computer architecture;Robot sensing systems;Hardware;Software;Task analysis;Motion control,,1.0,,31,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,,
433,"CGiS-Net: Aggregating Colour, Geometry and Implicit Semantic Features for Indoor Place Recognition",Y. Ming; X. Yang; G. Zhang; A. Calway,"Department of Computer Science, Visual Information Laboratory, University of Bristol, Bristol, U.K.; Department of Computer Science, Visual Information Laboratory, University of Bristol, Bristol, U.K.; State Key Lab of CAD&CG, Zhejiang University, Hangzhou, China; Department of Computer Science, Visual Information Laboratory, University of Bristol, Bristol, U.K.",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,6991,6997,"We describe a novel approach to indoor place recognition from RGB point clouds based on aggregating low-level colour and geometry features with high-level implicit semantic features. It uses a 2-stage deep learning framework, in which the first stage is trained for the auxiliary task of semantic segmentation and the second stage uses features from layers in the first stage to generate discriminate descriptors for place recognition. The auxiliary task encourages the features to be semantically meaningful, hence aggregating the geometry and colour in the RGB point cloud data with implicit semantic information. We use an indoor place recognition dataset derived from the ScanNet dataset for training and evaluation, with a test set comprising 3,608 point clouds generated from 100 different rooms. Comparison with a traditional feature-based method and four state-of-the-art deep learning methods demonstrate that our approach significantly outperforms all five methods, achieving, for example, a top-3 average recall rate of 75% compared with 41% for the closest rival method. Our code is available at: https://github.com/YuhangMing/Semantic-Indoor-Place-Recognition",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981113,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981113,,Geometry;Point cloud compression;Deep learning;Training;Codes;Image color analysis;Semantic segmentation,,5.0,,35,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/YuhangMing/Semantic-Indoor-Place-Recognition,https://github.com/YuhangMing/Semantic-Indoor-Place-Recognition
434,Learning to Grasp on the Moon from 3D Octree Observations with Deep Reinforcement Learning,A. Orsula; S. Bøgh; M. Olivares-Mendez; C. Martinez,"Space Robotics Research Group (SpaceR), Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, Luxembourg; Department of Materials and Production, Robotics & Automation Group, Aalborg University, Denmark; Space Robotics Research Group (SpaceR), Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, Luxembourg; Space Robotics Research Group (SpaceR), Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, Luxembourg",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,4112,4119,"Extraterrestrial rovers with a general-purpose robotic arm have many potential applications in lunar and planetary exploration. Introducing autonomy into such systems is desirable for increasing the time that rovers can spend gathering scientific data and collecting samples. This work investigates the applicability of deep reinforcement learning for vision-based robotic grasping of objects on the Moon. A novel simulation environment with procedurally-generated datasets is created to train agents under challenging conditions in unstructured scenes with uneven terrain and harsh illumination. A model-free off-policy actor-critic algorithm is then employed for end-to-end learning of a policy that directly maps compact octree observations to continuous actions in Cartesian space. Experimental evaluation indicates that 3D data representations enable more effective learning of manipulation skills when compared to traditionally used image-based observations. Domain randomization improves the generalization of learned policies to novel scenes with previously unseen objects and different illumination conditions. To this end, we demonstrate zero-shot sim-to-real transfer by evaluating trained agents on a real robot in a Moon-analogue facility. The source code and datasets are available at https://github.com/AndrejOrsula/drl_grasping.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981661,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981661,,Deep learning;Three-dimensional displays;Source coding;Moon;Octrees;Lighting;Reinforcement learning,,8.0,,42,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/AndrejOrsula/drl_grasping,https://github.com/AndrejOrsula/drl_grasping
435,A Portable Multiscopic Camera for Novel View and Time Synthesis in Dynamic Scenes,T. Zhang; Y. -F. Lau; Q. Chen,"Individualized Interdisciplinary Program, Robotics Institute, Hong Kong University of Science and Technology, Hong Kong, China; Department of Mathematics, Hong Kong University of Science and Technology, Hong Kong, China; Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,2409,2416,"We present a portable multiscopic camera system with a dedicated model for novel view and time synthesis in dynamic scenes. Our goal is to render high-quality images for a dynamic scene from any viewpoint at any time using our portable multiscopic camera. To achieve such novel view and time synthesis, we develop a physical multiscopic camera equipped with five cameras to train a neural radiance field (NeRF) in both time and spatial domains for dynamic scenes. Our model maps a 6D coordinate (3D spatial position, 1D temporal coordinate, and 2D viewing direction) to view-dependent and time-varying emitted radiance and volume density. Volume rendering is applied to render a photo-realistic image at a specified camera pose and time. To improve the robustness of our physical camera, we propose a camera parameter optimization module and a temporal frame interpolation module to promote information propagation across time. We conduct experiments on both real-world and synthetic datasets to evaluate our system, and the results show that our approach outperforms alternative solutions qualitatively and quantitatively. Our code and dataset are available at https://yuenfuilau.github.io/.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9982040,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982040,,Solid modeling;Three-dimensional displays;Robot kinematics;Robot vision systems;Cameras;Rendering (computer graphics);Robustness,,2.0,,42,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,,
436,MPNP: Multi-Policy Neural Planner for Urban Driving,J. Cheng; R. Xin; S. Wang; M. Liu,"The Hong Kong University of Science and Technology, Hong Kong, SAR, China; The Hong Kong University of Science and Technology, Hong Kong, SAR, China; The Hong Kong University of Science and Technology, Hong Kong, SAR, China; The Hong Kong University of Science and Technology, Hong Kong, SAR, China",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,10549,10554,"Our goal is to train a neural planner that can capture diverse driving behaviors in complex urban scenarios. We observe that even state-of-the-art neural planners are struggling to perform common maneuvers such as lane change, which is rather natural for human drivers. We propose to explore the multi-modalities in the planning problem and force the neural planner to explicitly consider different policies. This is achieved by generating the future trajectories conditioned on every possible reference line, which could simply be the centerline of the surrounding lanes. We find this simple strategy yet enables the planner to perform rich and complex behaviors. We train our model using real-world driving data and demonstrate the effectiveness of our method through both open-loop and closed-loop evaluations. Project website https://jchengai.github.io/mpnp.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9982111,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982111,,Force;Data models;Behavioral sciences;Trajectory;Planning;Intelligent robots,,5.0,,40,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://jchengai.github.io/mpnp,
437,Heuristic-free Optimization of Force-Controlled Robot Search Strategies in Stochastic Environments,B. Alt; D. Katic; R. Jäkel; M. Beetz,"ArtiMinds Robotics, Karlsruhe, Germany; ArtiMinds Robotics, Karlsruhe, Germany; ArtiMinds Robotics, Karlsruhe, Germany; Institute for Artificial Intelligence, University of Bremen, Germany",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,8887,8893,"In both industrial and service domains, a central benefit of the use of robots is their ability to quickly and reliably execute repetitive tasks. However, even relatively simple peg-in-hole tasks are typically subject to stochastic variations, requiring search motions to find relevant features such as holes. While search improves robustness, it comes at the cost of increased runtime: More exhaustive search will maximize the probability of successfully executing a given task, but will significantly delay any downstream tasks. This trade-off is typically resolved by human experts according to simple heuristics, which are rarely optimal. This paper introduces an automatic, data-driven and heuristic-free approach to optimize robot search strategies. By training a neural model of the search strategy on a large set of simulated stochastic environments, conditioning it on few real-world examples and inverting the model, we can infer search strategies which adapt to the time-variant characteristics of the underlying probability distributions, while requiring very few real-world measurements. We evaluate our approach on two different industrial robots in the context of spiral and probe search for THT electronics assembly.**See github.com/benjaminalt/dpse for code and data.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9982093,German Federal Ministry of Education and Research(grant numbers:01DR19001B); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982093,,Training;Adaptation models;Spirals;Runtime;Service robots;Stochastic processes;Search problems,,,,40,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/benjaminalt/dpse,https://github.com/benjaminalt/dpse
438,Simultaneous Contact Location and Object Pose Estimation Using Proprioception and Tactile Feedback,A. Sipos; N. Fazeli,"Department of Robotics at the University of Michigan, Hayward Drive, Ann Arbor, USA; Department of Robotics at the University of Michigan, Hayward Drive, Ann Arbor, USA",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,3233,3240,"Joint estimation of grasped object pose and extrinsic contacts is central to robust and dexterous manipulation. In this paper, we propose a novel state-estimation algorithm that jointly estimates contact location and object pose in 3D using exclusively proprioception and tactile feedback. Our approach leverages two complementary particle filters: one to estimate contact location (CPFGrasp) and another to estimate object poses (SCOPE). We implement and evaluate our approach on real-world single-arm and dual-arm robotic systems. We demonstrate that by bringing two objects into contact, the robots can infer contact location and object poses simultaneously. Our proposed method can be applied to a number of downstream tasks that require accurate pose estimates, such as tool use and assembly. Code and data can be found at https://github.com/MMintLab/scope.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981762,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981762,,Geometry;Three-dimensional displays;Codes;Pose estimation;Tactile sensors;Particle filters;Task analysis,,4.0,,38,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/MMintLab/scope,https://github.com/MMintLab/scope
439,Jerk-continuous Online Trajectory Generation for Robot Manipulator with Arbitrary Initial State and Kinematic Constraints,H. Zhao; N. Abdurahiman; N. Navkar; J. Leclerc; A. T. Becker,"Department of Electrical and Computer Engineering, University of Houston, Houston, TX, USA; Department of Surgery, Hamad Medical Corporation, Doha, Qatar.; Department of Surgery, Hamad Medical Corporation, Doha, Qatar.; Department of Electrical and Computer Engineering, University of Houston, Houston, TX, USA; Department of Electrical and Computer Engineering, University of Houston, Houston, TX, USA",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,5730,5736,"This work presents an online trajectory generation algorithm using a sinusoidal jerk profile. The generator takes initial acceleration, velocity and position as input, and plans a multi-segment trajectory to a goal position under jerk, acceleration, and velocity limits. By analyzing the critical constraints and conditions, the corresponding closed-form solution for the time factors and trajectory profiles are derived. The proposed algorithm was first derived in Mathematica and then converted into a C++ implementation. Finally, the algorithm was utilized and demonstrated in ROS & Gazebo using a UR3 robot. Both the Mathematica and C++ implementations can be accessed at https://github.com/Haoran-Zhao/Jerk-continuous-online-trajectory-generator-with-constraints.git",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981086,"Qatar National Research Fund (a member of The Qatar Foundation); Alexander von Humboldt Foundation; National Science Foundation(grant numbers:IIS-1553063,1849303,2130793); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981086,,Simulation;Kinematics;C++ languages;Manipulators;Mathematics computing;Mathematical models;Generators,,1.0,,24,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/Haoran-Zhao/Jerk-continuous-online-trajectory-generator-with-constraints,https://github.com/Haoran-Zhao/Jerk-continuous-online-trajectory-generator-with-constraints
440,Implicit-Part Based Context Aggregation for Point Cloud Instance Segmentation,X. Wu; R. Wang; X. Chen,"Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,9892,9898,"Context information is important for instance segmentation on point clouds. Existing methods either only use local surroundings by stacking multiple convolution layers or use non-local methods to model long-range interactions. However, they usually directly operate on points which is an unstructured and low-level representation and is highly dependent on context. To address this issue, we propose an effective framework named Implicit-Part Context Aggregation (IPCA), which adopts implicit parts as an intermediate representation and achieves context aggregation through message passing along the implicit part graph. Specifically, we first organize unstructured points into geometrically consistent implicit parts and construct the implicit part graph according to the geometric adjacency. Then, an initial part embedding is extracted using the proposed Implicit Part Network (IPN) which can aggregate point features and capture the intrinsic geometric shape of the part. We further refine the part embedding by a graph reasoning module named Context Aggregation Network (CAN), which helps to make a more precise prediction by well exploiting the context information. Instance proposals are then generated by grouping implicit parts. Finally, we propose an additional step to attribute the entire instance proposal to a Semantic Criterion Net (SCN) to infer the semantics of the instance. The purpose is to correct the semantic prediction errors caused by not knowing the boundary and overall shape of the object in the previous steps. Extensive experiments on two large datasets, ScanNet and 3RScan, demonstrate the effectiveness of our method. To our knowledge, it yields the highest performance on the ScanNet test benchmark and its AP@50 is 9.5 points higher than the baseline. The code is available at https://github.com/xiaodongww/IPCA",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981772,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981772,,Point cloud compression;Shape;Convolution;Message passing;Semantics;Stacking;Feature extraction,,,,34,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/xiaodongww/IPCA,https://github.com/xiaodongww/IPCA
441,PoseIt: A Visual-Tactile Dataset of Holding Poses for Grasp Stability Analysis,S. Kanitkar; H. Jiang; W. Yuan,Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University,2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,71,78,"When humans grasp objects in the real world, we often move our arms to hold the object in a different pose where we can use it. In contrast, typical lab settings only study the stability of the grasp immediately after lifting, without any subsequent re-positioning of the arm. However, the grasp stability could vary widely based on the object's holding pose, as the gravitational torque and gripper contact forces could change completely. To facilitate the study of how holding poses affect grasp stability, we present PoseIt, a novel multi-modal dataset that contains visual and tactile data collected from a full cycle of grasping an object, re-positioning the arm to one of the sampled poses, and shaking the object. Using data from PoseIt, we can formulate and tackle the task of predicting whether a grasped object is stable in a particular held pose. We train an LSTM classifier that achieves 85% accuracy on the proposed task. Our experimental results show that multi-modal models trained on PoseIt achieve higher accuracy than using solely vision or tactile data and that our classifiers can also generalize to unseen objects and poses. The PoseIt dataset is publicly released here: https://github.com/CMURoboTouch/PoseIt.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981562,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981562,,Visualization;Torque;Grasping;Stability analysis;Data models;Task analysis;Grippers,,10.0,,34,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/CMURoboTouch/PoseIt,https://github.com/CMURoboTouch/PoseIt
442,LODM: Large-scale Online Dense Mapping for UAV,J. Huang; L. Li; X. Zhao; X. Lang; D. Zhu; Y. Liu,"Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,2281,2288,"This paper proposes an online large-scale dense mapping method for UAVs with a height of 150–250 meters. We first fuse the GPS with the visual odometry to estimate the scaled poses and sparse points. In order to use the depth of sparse points for depth map, we propose Sparse Confidence Cascade View-Aggregation MVSNet (SCCVA-MVSNet), which projects the depth-converged points in the sliding window on keyframes to obtain a sparse depth map. To weigh the confidence of the depth of each sparse point, we construct sparse confidence by the photometric error. The images of all keyframes, coarse depth, and confidence as the input of CVA-MVSNet to extract features and construct 3D cost volumes with adaptive view aggregation to balance the different stereo baselines between the keyframes. Our proposed network utilizes sparse features point information, the output of the network better maintains the consistency of the scale. Our experiments show that MVSNet using sparse feature point information outperforms image-only MVSNet, and our online reconstruction results are comparable to offline reconstruction methods. To benefit the research community, we open our code at https://github.com/hjxwhy/LODM.git",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981994,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981994,,Meters;Three-dimensional displays;Costs;Fuses;Reconstruction algorithms;Feature extraction;Intelligent robots,,,,34,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/hjxwhy/LODM,https://github.com/hjxwhy/LODM
443,Domain Invariant Siamese Attention Mask for Small Object Change Detection via Everyday Indoor Robot Navigation,K. Takeda; K. Tanaka; Y. Nakamura,"Tokyo Metropolitan Industrial Technology Research Institute, Tokyo, Japan; Faculty of Engineering, University of Fukui, Japan; Tokyo Metropolitan Industrial Technology Research Institute, Tokyo, Japan",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,739,745,"The problem of image change detection via every-day indoor robot navigation is explored from a novel perspective of the self-attention technique. Detecting semantically non-distinctive and visually small changes remains a key challenge in the robotics community. Intuitively, these small non-distinctive changes may be better handled by the recent paradigm of the attention mechanism, which is the basic idea of this work. However, existing self-attention models require significant retraining cost per domain, so it is not directly applicable to robotics applications. We propose a new self-attention technique with an ability of unsupervised on-the-fly domain adaptation, which introduces an attention mask into the intermediate layer of an image change detection model, without modifying the input and output layers of the model. Experiments, in which an indoor robot aims to detect visually small changes in everyday navigation, demonstrate that our attention technique significantly boosts the state-of-the-art image change detection model. Our datset is available at https://github.com/KojiTakeda00/Small_object_change_detection",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9982196,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982196,,Adaptation models;Costs;Navigation;Robots;Intelligent robots,,1.0,,36,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/KojiTakeda00/Small_object_change_detection,https://github.com/KojiTakeda00/Small_object_change_detection
444,Local Perception-Aware Transformer for Aerial Tracking,C. Fu; W. Peng; S. Li; J. Ye; Z. Cao,"School of Mechanical Engineering, Tongji University, Shanghai, Chlna; School of Mechanical Engineering, Tongji University, Shanghai, Chlna; School of Mechanical Engineering, Tongji University, Shanghai, Chlna; School of Mechanical Engineering, Tongji University, Shanghai, Chlna; School of Automotive Studies, Tongji University, Shanghai, China",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,12122,12129,"Transformer-based visual object tracking has been utilized extensively. However, the Transformer structure is lack of enough inductive bias. In addition, only focusing on encoding the global feature does harm to modeling local details, which restricts the capability of tracking in aerial robots. Specifically, with local-modeling to global-search mechanism, the proposed tracker replaces the global encoder by a novel local-recognition encoder. In the employed encoder, a local-recognition attention and a local element correction network are carefully designed for reducing the global redundant information interference and increasing local inductive bias. Meanwhile, the latter can model local object details precisely under aerial view through detail-inquiry net. The proposed method achieves competitive accuracy and robustness in several authoritative aerial benchmarks with 316 sequences in total. The proposed tracker's practicability and efficiency have been validated by the real-world tests. The source code is available at https://github.com/vision4robotics/LPAT.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981248,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981248,,Visualization;Fuses;Source coding;Pipelines;Interference;Benchmark testing;Transformers,,10.0,,50,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/vision4robotics/LPAT,https://github.com/vision4robotics/LPAT
445,FAST-LIVO: Fast and Tightly-coupled Sparse-Direct LiDAR-Inertial-Visual Odometry,C. Zheng; Q. Zhu; W. Xu; X. Liu; Q. Guo; F. Zhang,"Department of Mechanical Engineering, The University of Hong Kong, Hong Kong Special Administrative Region, People's Republic of China.; Department of Mechanical Engineering, The University of Hong Kong, Hong Kong Special Administrative Region, People's Republic of China.; Department of Mechanical Engineering, The University of Hong Kong, Hong Kong Special Administrative Region, People's Republic of China.; Department of Mechanical Engineering, The University of Hong Kong, Hong Kong Special Administrative Region, People's Republic of China.; Department of Mechanical Engineering, The University of Hong Kong, Hong Kong Special Administrative Region, People's Republic of China.; Department of Mechanical Engineering, The University of Hong Kong, Hong Kong Special Administrative Region, People's Republic of China.",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,4003,4009,"To achieve accurate and robust pose estimation in Simultaneous Localization and Mapping (SLAM) task, multisensor fusion is proven to be an effective solution and thus provides great potential in robotic applications. This paper proposes FAST-LIVO, a fast LiDAR-Inertial-Visual Odometry system, which builds on two tightly-coupled and direct odometry subsystems: a VIO subsystem and a LIO subsystem. The LIO subsystem registers raw points (instead of feature points on e.g., edges or planes) of a new scan to an incrementally-built point cloud map. The map points are additionally attached with image patches, which are then used in the VIO subsystem to align a new image by minimizing the direct photometric errors without extracting any visual features (e.g., ORB or FAST corner features). To further improve the VIO robustness and accuracy, a novel outlier rejection method is proposed to reject unstable map points that lie on edges or are occluded in the image view. Experiments on both open data sequences and our customized device data are conducted. The results show our proposed system outperforms other counterparts and can handle challenging environments at reduced computation cost. The system supports both multi-line spinning LiDARs and emerging solid-state LiDARs with completely different scanning patterns, and can run in real-time on both Intel and ARM processors. We open source our code and dataset of this work on Github22https://github.com/hku-mars/FAST-LIVO to benefit the robotics community.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981107,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981107,,Visualization;Laser radar;Simultaneous localization and mapping;Program processors;Image edge detection;Feature extraction;Systems support,,79.0,,33,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/hku-mars/FAST-LIVO,https://github.com/hku-mars/FAST-LIVO
446,OdomBeyondVision: An Indoor Multi-modal Multi-platform Odometry Dataset Beyond the Visible Spectrum,P. Li; K. Cai; M. R. U. Saputra; Z. Dai; C. X. Lu,"School of Informatics, University of Edinburgh, United Kingdom; University of Liverpool, United Kingdom; Monash University, Indonesia; University of Oxford, United Kingdom; School of Informatics, University of Edinburgh, United Kingdom",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,3845,3850,"This paper presents a multimodal indoor odometry dataset, OdomBeyondVision, featuring multiple sensors across the different spectrum and collected with different mobile platforms. Not only does OdomBeyondVision contain the traditional navigation sensors, sensors such as IMUs, mechanical LiDAR, RGBD camera, it also includes several emerging sensors such as the single-chip mmWave radar, LWIR thermal camera and solid-state LiDAR. With the above sensors on UAV, UGV and handheld platforms, we respectively recorded the multimodal odometry data and their movement trajectories in various indoor scenes and different illumination conditions. We release the exemplar radar, radar-inertial and thermal-inertial odometry implementations to demonstrate their results for future works to compare against and improve upon. The full dataset including toolkit and documentation is publicly available at: https://github.com/MAPS-Lab/OdomBeyondVision.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981865,EPSRC(grant numbers:EP/S030832/1); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981865,,Mechanical sensors;Location awareness;Laser radar;Navigation;Lighting;Thermal sensors;Sensor phenomena and characterization,,5.0,,28,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/MAPS-Lab/OdomBeyondVision,https://github.com/MAPS-Lab/OdomBeyondVision
447,Pseudo-label Guided Cross-video Pixel Contrast for Robotic Surgical Scene Segmentation with Limited Annotations,Y. Yu; Z. Zhao; Y. Jin; G. Chen; Q. Dou; P. -A. Heng,"Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Computer Science, Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS), University College London; Zhejiang Lab; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Guangdong-Hong Kong-Macao Joint Laboratory of Human-Machine Intelligence, Synergy SystemsShenzhen Institutes of Advanced Technology, Chinese Academy of Sciences",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,10857,10864,"Surgical scene segmentation is fundamentally crucial for prompting cognitive assistance in robotic surgery. However, pixel-wise annotating surgical video in a frame-by-frame manner is expensive and time consuming. To greatly reduce the labeling burden, in this work, we study semi-supervised scene segmentation from robotic surgical video, which is practically essential yet rarely explored before. We consider a clinically suitable annotation situation under the equidistant sampling. We then propose PGV-CL, a novel pseudo-label guided cross-video contrast learning method to boost scene segmentation. It effectively leverages unlabeled data for a trusty and global model regularization that produces more discriminative feature representation. Concretely, for trusty representation learning, we propose to incorporate pseudo labels to instruct the pair selection, obtaining more reliable representation pairs for pixel contrast. Moreover, we expand the representation learning space from previous image-level to cross-video, which can capture the global semantics to benefit the learning process. We extensively evaluate our method on a public robotic surgery dataset EndoVis18 and a public cataract dataset CaDIS. Experimental results demonstrate the effectiveness of our method, consistently outperforming the state-of-the-art semi-supervised methods under different labeling ratios, and even surpassing fully supervised training on EndoVis18 with 10.1% labeling. Our code is available at https://github.com/yangyu-cuhk/PGV-CL.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981798,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981798,Scene segmentation;pixel-level contrastive learning;semi-supervised learning;robotic surgical video,Representation learning;Training;Learning systems;Annotations;Semantics;Surgery;Data models,,3.0,,40,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/yangyu-cuhk/PGV-CL,https://github.com/yangyu-cuhk/PGV-CL
448,Real-time Semantic 3D Reconstruction for High- Touch Surface Recognition for Robotic Disinfection,R. -Z. Qiu; Y. Sun; J. M. Correia Marques; K. Hauser,"Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL, USA; Department of Mechanical Engineering, Stanford University, Stanford, CA, USA; Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL, USA; Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL, USA",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,9919,9925,"Disinfection robots have applications in promoting public health and reducing hospital acquired infections and have drawn considerable interest due to the COVID-19 pan-demic. To disinfect a room quickly, motion planning can be used to plan robot disinfection trajectories on a reconstructed 3D map of the room's surfaces. However, existing approaches discard semantic information of the room and, thus, take a long time to perform thorough disinfection. Human cleaners, on the other hand, disinfect rooms more efficiently by prioritizing the cleaning of high-touch surfaces. To address this gap, we present a novel GPU-based volumetric semantic TSDF (Truncated Signed Distance Function) integration system for semantic 3D reconstruction. Our system produces 3D reconstructions that distinguish high-touch surfaces from non-high-touch surfaces at approximately 50 frames per second on a consumer-grade GPU, which is approximately 5 times faster than existing CPU-based TSDF semantic reconstruction methods. In addition, we extend a UV disinfection motion planning algorithm to incorporate semantic awareness for optimizing coverage of disinfection tra-jectories. Experiments show that our semantic-aware planning outperforms geometry-only planning by disinfecting up to 20% more high-touch surfaces under the same time budget. Further, the real-time nature of our semantic reconstruction pipeline enables future work on simultaneous disinfection and mapping. Code is available at: https://github.com/uiuc-iml/RA-SLAM",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981300,NSF(grant numbers:2025782); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981300,,Surface reconstruction;Three-dimensional displays;Semantics;Pipelines;Reconstruction algorithms;Real-time systems;Planning,,2.0,,32,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/uiuc-iml/RA-SLAM,https://github.com/uiuc-iml/RA-SLAM
449,Multimodal Generation of Novel Action Appearances for Synthetic-to-Real Recognition of Activities of Daily Living,Z. Marinov; D. Schneider; A. Roitberg; R. Stiefelhagen,"Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Germany; Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Germany; Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Germany; Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Germany",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,11320,11327,"Domain shifts, such as appearance changes, are a key challenge in real-world applications of activity recognition models, which range from assistive robotics and smart homes to driver observation in intelligent vehicles. For example, while simulations are an excellent way of economical data collection, a Synthetic→Real domain shift leads to > 60% drop in accuracy when recognizing Activities of Daily Living (ADLs). We tackle this challenge and introduce an activity domain generation framework which creates novel ADL appearances (novel domains) from different existing activity modalities (source domains) inferred from video training data. Our frame-work computes human poses, heatmaps of body joints, and optical flow maps and uses them alongside the original RGB videos to learn the essence of source domains in order to generate completely new ADL domains. The model is optimized by maximizing the distance between the existing source appearances and the generated novel appearances while ensuring that the semantics of an activity is preserved through an additional classification loss. While source data multimodality is an important concept in this design, our setup does not rely on multi-sensor setups, (i.e., all source modalities are inferred from a single video only.) The newly created activity domains are then integrated in the training of the ADL classification networks, resulting in models far less susceptible to changes in data distributions. Extensive experiments on the Synthetic→Real benchmark Sims4Action demonstrate the potential of the domain generation paradigm for cross-domain ADL recognition, setting new state-of-the-art results. Our code is publicly available at https://github.com/Zrrr1997/syn2real_DG.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981946,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981946,,Training;Heating systems;Intelligent vehicles;Biological system modeling;Semantics;Training data;Smart homes,,1.0,,64,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/Zrrr1997/syn2real_DG,https://github.com/Zrrr1997/syn2real_DG
450,Patchwork++: Fast and Robust Ground Segmentation Solving Partial Under-Segmentation Using 3D Point Cloud,S. Lee; H. Lim; H. Myung,"School of Electrical Engineering at KAIST, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea; School of Electrical Engineering at KAIST, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea; School of Electrical Engineering at KAIST, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,13276,13283,"In the field of 3D perception using 3D LiDAR sensors, ground segmentation is an essential task for various purposes, such as traversable area detection and object recognition. Under these circumstances, several ground segmentation methods have been proposed. However, some limitations are still encountered. First, some ground segmentation methods require fine-tuning of parameters depending on the surroundings, which is excessively laborious and time-consuming. Moreover, even if the parameters are well adjusted, a partial under-segmentation problem can still emerge, which implies ground segmentation failures in some regions. Finally, ground segmentation methods typically fail to estimate an appropriate ground plane when the ground is above another structure, such as a retaining wall. To address these problems, we propose a robust ground segmentation method called Patchwork++, an extension of Patchwork. Patchwork++ exploits adaptive ground likelihood estimation (A-GLE) to calculate appropriate parameters adaptively based on the previous ground segmentation results. Moreover, temporal ground revert (TGR) alleviates a partial under-segmentation problem by using the temporary ground property. Also, region-wise vertical plane fitting (R-VPF) is introduced to segment the ground plane properly even if the ground is elevated with different layers. Finally, we present reflected noise removal (RNR) to eliminate virtual noise points efficiently based on the 3D LiDAR reflection model. We demonstrate the qualitative and quantitative evaluations using a SemanticKITTI dataset. Our code is available at https://github.com/url-kaist/patchwork-plusplus",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981561,"Ministry of Trade, Industry & Energy (MOTIE, Republic of Korea); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981561,,Point cloud compression;Solid modeling;Three-dimensional displays;Laser radar;Fitting;Reflection;Sensors,,67.0,,26,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/url-kaist/patchwork-plusplus,https://github.com/url-kaist/patchwork-plusplus
451,Skill-CPD: Real-time Skill Refinement for Shared Autonomy in Manipulator Teleoperation,E. Babaians; D. Yang; M. Karimi; X. Xu; S. Ayvasik; E. Steinbach,Department of Electrical and Computer Engineering; Department of Electrical and Computer Engineering; Department of Electrical and Computer Engineering; Department of Electrical and Computer Engineering; Department of Electrical and Computer Engineering; Department of Electrical and Computer Engineering,2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,6189,6196,"Advanced wireless communication networks provide lower latency and a higher transmission rate. Although this is an enabler for many new teleoperation applications, the risk of network instability or packet drop is still unavoidable. Real-time manipulator teleoperation requires data transmission with no discontinuity. Shared autonomy (SA) is a standard method to mitigate this issue. In this way, if the data from the remote side is unavailable, the controller can continue based on the previously observed models. However, due to the spatial gap between human and robot trajectories, indisputable fluctuations occur, which cause issues in teleoperation applications. This motivates us to propose a new skill refinement strategy to modify the previously trained skill and mitigate the sudden unwanted motions within the control takeover phase. To this end, our approach comprises applying the Hidden Semi-Markov Model (HSMM) and Linear Quadratic Tracker (LQT) in combination to learn and predict the user's intentions and then exploiting Coherent Point Drift (CPD) to refine the executable trajectory. We test our method both in simulation and in the real world for 2D English letter drawing and 3D robot-assisted feeding scenarios. Our experimental results using the Kinova® Movo platform show that the proposed refinement approach generates a stable trajectory and mitigates the control switching inconsistency. All comprehensive experiments and source code is available at: http://cxdcxd.github.io/SkillCPD.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9982077,Federal Ministry of Education and Research of Germany (BMBF)(grant numbers:16KISK002); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982077,,Wireless communication;Three-dimensional displays;Tracking;Source coding;Switches;Manipulators;Real-time systems,,6.0,,26,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,http://cxdcxd.github.io/SkillCPD,https://github.com/cxdcxd/SkillCPD
452,Gathering Physical Particles with a Global Magnetic Field Using Reinforcement Learning,M. Konitzny; Y. Lu; J. Leclerc; S. P. Fekete; A. T. Becker,"Department of Computer Science, TU Braunschweig, Germany; Department of Electrical and Computer Engineering, University of Houston, Houston, TX, USA; Department of Electrical and Computer Engineering, University of Houston, Houston, TX, USA; Department of Computer Science, TU Braunschweig, Germany; Department of Computer Science, TU Braunschweig, Germany",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,10126,10132,"For biomedical applications in targeted therapy delivery and interventions, a large swarm of micro-scale particles (“agents”) has to be moved through a maze-like environment (“vascular system”) to a target region (“tumor”). Due to limited on-board capabilities, these agents cannot move autonomously; instead, they are controlled by an external global force that acts uniformly on all particles. In this work, we demonstrate how to use a time-varying magnetic field to gather particles to a desired location. We use reinforcement learning to train networks to efficiently gather particles. Methods to overcome the simulation-to-reality gap are explained, and the trained networks are deployed on a set of mazes and goal locations. The hardware experiments demonstrate fast convergence, and robustness to both sensor and actuation noise. To encourage extensions and to serve as a benchmark for the reinforcement learning community, the code is available at Github.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9982256,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982256,,Training;Ultrasonic imaging;Three-dimensional displays;Reinforcement learning;Hardware;Sensors;Magnetic fields,,1.0,,20,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,,
453,Vision-Guided Quadrupedal Locomotion in the Wild with Multi-Modal Delay Randomization,C. S. Imai; M. Zhang; Y. Zhang; M. Kierebiński; R. Yang; Y. Qin; X. Wang,"University of California, San Diego, CA, USA; Tsinghua University, Beijing, China; University of California, San Diego, CA, USA; University of California, San Diego, CA, USA; University of California, San Diego, CA, USA; University of California, San Diego, CA, USA; University of California, San Diego, CA, USA",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,5556,5563,"Developing robust vision-guided controllers for quadrupedal robots in complex environments with various obstacles, dynamical surroundings and uneven terrains is very challenging. While Reinforcement Learning (RL) provides a promising paradigm for agile locomotion skills with vision inputs in simulation, it is still very challenging to deploy the vision-guided RL policy in the real world. Our key insight is that the asynchronous multi-modal observations, caused by different latencies in different components of the real robot, create a large sim2real gap for a RL policy. In this paper, we propose Multi-Modal Delay Randomization (MMDR) to address this issue when training in simulation. Specifically, we randomize the selections for both the proprioceptive states and the visual observations in time during training, aiming to simulate the asynchronous inputs when deploying to the real robot. With this technique, we are able to train a RL policy for end-to-end locomotion control in simulation, which can be directly deployed on the real A1 quadruped robot running in the wild. We evaluate our method in different outdoor environments with complex terrain and obstacles. We show that the robot can smoothly maneuver at a high speed while avoiding the obstacles, achieving significant improvement over the baselines. Our project page with videos is at https://mehooz.github.io/mmdr-wild/.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981072,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981072,,Training;Legged locomotion;Visualization;Robot control;Reinforcement learning;Delays;Quadrupedal robots,,10.0,,49,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://mehooz.github.io/mmdr-wild,https://github.com/Mehooz/vision4leg
454,Task Decoupling in Preference-based Reinforcement Learning for Personalized Human-Robot Interaction,M. Liu; C. Chen,"Department of Control and Systems Engineering, School of Management and Engineering, Nanjing University, Nanjing, China; Department of Control and Systems Engineering, School of Management and Engineering, Nanjing University, Nanjing, China",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,848,855,"Intelligent robots designed to interact with hu-mans in the real world need to adapt to the preferences of different individuals. Preference-based reinforcement learning (RL) has shown great potential for teaching robots to learn personalized behaviors from interacting with humans with-out a meticulous, hand-crafted reward function, replaced by learning reward based on a human's preferences between two robot trajectories. However, poor feedback efficiency and poor exploration in the state and reward spaces make current preference-based RL algorithms perform poorly in complex interactive tasks. To improve the performance of preference-based RL, we incorporate prior knowledge of the task into preference-based RL. Specifically, we decouple the task from preference in human-robot interaction. We utilize a sketchy task reward derived from task priori to instruct robots to conduct more effective task exploration. Then a learned reward from preference-based RL is used to optimize the robot's policy to align with human preferences. In addition, these two parts are combined organically via reward shaping. The experimental results show that our method is a practical and effective solution for personalized human-robot interaction. Code is available at https://github.com/Wenminggong/PbRL_for_PHRI.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981076,National Key Research and Development Program of China(grant numbers:2018AAA0101100); National Natural Science Foundation of China(grant numbers:62073160); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981076,,Codes;Education;Human-robot interaction;Reinforcement learning;Trajectory;Behavioral sciences;Task analysis,,2.0,,30,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/Wenminggong/PbRL_for_PHRI,https://github.com/Wenminggong/PbRL_for_PHRI
455,COMPASS: Contrastive Multimodal Pretraining for Autonomous Systems,S. Ma; S. Vemprala; W. Wang; J. K. Gupta; Y. Song; D. McDufft; A. Kapoor,"Microsoft Redmond, WA; Microsoft Redmond, WA; Carnegie Mellon University Pittsburgh, PA; Microsoft Redmond, WA; Microsoft Redmond, WA; Microsoft Redmond, WA; Microsoft Redmond, WA",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,1000,1007,"Learning representations that generalize across tasks and domains is challenging yet necessary for autonomous systems. Although task-driven approaches are appealing, de-signing models specific to each application can be difficult in the face of limited data, especially when dealing with highly variable multimodal input spaces arising from different tasks in different environments. We introduce the first general-purpose pretraining pipeline, COntrastive Multimodal Pretraining for AutonomouS Systems (COMPASS), to overcome the limitations of task-specific models and existing pretraining approaches. COMPASS constructs a multimodal graph by considering the essential information for autonomous systems and the proper-ties of different modalities. Through this graph, multimodal signals are connected and mapped into two factorized spatio-temporal latent spaces: a “motion pattern space” and a “current state space.” By learning from multimodal correspondences in each latent space, COMPASS creates state representations that models necessary information such as temporal dynamics, geometry, and semantics. We pretrain COMPASS on a large-scale multimodal simulation dataset TartanAir [1] and evaluate it on drone navigation, vehicle racing, and visual odometry tasks. The experiments indicate that COMPASS can tackle all three scenarios and can also generalize to unseen environments and real-world data.11Our code implementation can be found at https://github.com/microsoft/COMPASS",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9982241,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982241,,Autonomous systems;Navigation;Semantics;Predictive models;Data models;Compass;Task analysis,,6.0,,59,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/microsoft/COMPASS,https://github.com/microsoft/COMPASS
456,Multi-Modal Lidar Dataset for Benchmarking General-Purpose Localization and Mapping Algorithms,L. Qingqing; Y. Xianjia; J. P. Queralta; T. Westerlund,"Turku Intelligent Embedded and Robotic Systems (TIERS) Lab, University of Turku, Turku, Finland; Turku Intelligent Embedded and Robotic Systems (TIERS) Lab, University of Turku, Turku, Finland; Turku Intelligent Embedded and Robotic Systems (TIERS) Lab, University of Turku, Turku, Finland; Turku Intelligent Embedded and Robotic Systems (TIERS) Lab, University of Turku, Turku, Finland",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,3837,3844,"Lidar technology has evolved significantly over the last decade, with higher resolution, better accuracy, and lower cost devices available today. In addition, new scanning modalities and novel sensor technologies have emerged in recent years. Public datasets have enabled benchmarking of algorithms and have set standards for the cutting edge technology. However, existing datasets are not representative of the technological landscape, with only a reduced number of lidars available. This inherently limits the development and comparison of general-purpose algorithms in the evolving landscape. This paper presents a novel multi-modal lidar dataset with sensors showcasing different scanning modalities (spinning and solid-state), sensing technologies, and lidar cameras. The focus of the dataset is on low-drift odometry, with ground truth data available in both indoors and outdoors environment with sub-millimeter accuracy from a motion capture (MOCAP) system. For comparison in longer distances, we also include data recorded in larger spaces indoors and outdoors. The dataset contains point cloud data from spinning lidars and solid-state lidars. Also, it provides range images from high resolution spinning lidars, RGB and depth images from a lidar camera, and inertial data from built-in IMUs. This is, to the best of our knowledge, the lidar dataset with the most variety of sensors and environments where ground truth data is available. This dataset can be widely used in multiple research areas, such as 3D LiDAR simultaneous localization and mapping (SLAM), performance comparison between multi-modal lidars, appearance recognition and loop closure detection. The datasets are available at: https://github.com/TIERS/tiers-lidars-dataset.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981078,"Academy of Finland's AeroPolis project(grant numbers:348480); Finnish Foundation for Technology Promotion(grant numbers:7817,8089); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981078,Multi-robot systems;Autonomous driving;dataset;computer vision;solid state LiDAR;SLAM,Location awareness;Laser radar;Simultaneous localization and mapping;Three-dimensional displays;Benchmark testing;Cameras;Sensor systems,,11.0,,28,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/TIERS/tiers-lidars-dataset,https://github.com/TIERS/tiers-lidars-dataset
457,Temporal Context for Robust Maritime Obstacle Detection,L. Žust; M. Kristan,"Faculty of Computer and Information Science, University of Ljubljana, Slovenia; Faculty of Computer and Information Science, University of Ljubljana, Slovenia",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,6340,6346,"Robust maritime obstacle detection is essential for fully autonomous unmanned surface vehicles (USVs). The currently widely adopted segmentation-based obstacle detection methods are prone to misclassification of object reflections and sun glitter as obstacles, producing many false positive detections, effectively rendering the methods impractical for USV navigation. However, water-turbulence-induced temporal appearance changes on object reflections are very distinctive from the appearance dynamics of true objects. We harness this property to design WaSR-T, a novel maritime obstacle detection network, that extracts the temporal context from a sequence of recent frames to reduce ambiguity. By learning the local temporal characteristics of object reflection on the water surface, WaSR-T substantially improves obstacle detection accuracy in the presence of reflections and glitter. Compared with existing single-frame methods, WaSR-T reduces the number of false positive detections by 41% overall and by over 53% within the danger zone of the boat, while preserving a high recall, and achieving new state-of-the-art performance on the challenging MODS maritime obstacle detection benchmark. The code, pretrained models and extended datasets are available at: https://github.com/lojzezust/WaSR-T",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9982043,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982043,,Training;Water;Image segmentation;Navigation;Benchmark testing;Rendering (computer graphics);Reflection,,9.0,,18,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/lojzezust/WaSR-T,https://github.com/lojzezust/WaSR-T
458,VAST: Visual and Spectral Terrain Classification in Unstructured Multi-Class Environments,N. Hanson; M. Shaham; D. Erdoğmuş; T. Padir,"Institute for Experiential Robotics, Northeastern University, Boston, Massachusetts, USA; Institute for Experiential Robotics, Northeastern University, Boston, Massachusetts, USA; Institute for Experiential Robotics, Northeastern University, Boston, Massachusetts, USA; Institute for Experiential Robotics, Northeastern University, Boston, Massachusetts, USA",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,3956,3963,"Terrain classification is a challenging task for robots operating in unstructured environments. Existing classification methods make simplifying assumptions, such as a reduced number of classes, clearly segmentable roads, or good lighting conditions, and focus primarily on one sensor type. These assumptions do not translate well to off-road vehicles, which operate in varying terrain conditions. To provide mobile robots with the capability to identify the terrain being traversed and avoid undesirable surface types, we propose a multimodal sensor suite capable of classifying different terrains. We capture high resolution macro images of surface texture, spectral reflectance curves, and localization data from a 9 degrees of freedom (DOF) inertial measurement unit (IMU) on 11 different terrains at different times of day. Using this dataset, we train individual neural networks on each of the modalities, and then combine their outputs in a fusion network. The fused network achieved an accuracy of 99.98% percent on the test set, exceeding the results of the best individual network component by 0.98%. We conclude that a combination of visual, spectral, and IMU data provides meaningful improvement over state of the art in terrain classification approaches. The data created for this research is available at https://github.com/RIVeR-Lab/vast_data.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9982078,National Science Foundation(grant numbers:1928654); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982078,Sensor Fusion;Datasets for Robot Vision;Field Robots;Spectroscopy in Robotics,Reflectivity;Visualization;Satellites;Neural networks;Wheels;Robot sensing systems;Probabilistic logic,,11.0,,40,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/RIVeR-Lab/vast_data,https://github.com/RIVeR-Lab/vast_data
459,DRACo-SLAM: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar Equipped Underwater Robot Teams,J. McConnell; Y. Huang; P. Szenher; I. Collado-Gonzalez; B. Englot,"Stevens Institute of Technology, Hoboken, NJ, USA; Stevens Institute of Technology, Hoboken, NJ, USA; Stevens Institute of Technology, Hoboken, NJ, USA; Stevens Institute of Technology, Hoboken, NJ, USA; Stevens Institute of Technology, Hoboken, NJ, USA",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,8457,8464,"An essential task for a multi-robot system is generating a common understanding of the environment and relative poses between robots. Cooperative tasks can be executed only when a vehicle has knowledge of its own state and the states of the team members. However, this has primarily been achieved with direct rendezvous between underwater robots, via inter-robot ranging. We propose a novel distributed multi-robot simultaneous localization and mapping (SLAM) framework for underwater robots using imaging sonar-based perception. By passing only scene descriptors between robots, we do not need to pass raw sensor data unless there is a likelihood of inter-robot loop closure. We utilize pairwise consistent measurement set maximization (PCM), making our system robust to erroneous loop closures. The functionality of our system is demonstrated using two real-world datasets, one with three robots and another with two robots. We show that our system effectively estimates the trajectories of the multi-robot system and keeps the bandwidth requirements of inter-robot communication low. To our knowledge, this paper describes the first instance of multi-robot SLAM using real imaging sonar data (which we implement offline, using simulated communication). Code link: https://github.com/jake3991/DRACo-SLAM.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981822,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981822,,Point cloud compression;Autonomous underwater vehicles;Simultaneous localization and mapping;Imaging;Sonar;Real-time systems;Trajectory,,5.0,,41,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/jake3991/DRACo-SLAM,https://github.com/jake3991/DRACo-SLAM
460,Toward Global Sensing Quality Maximization: A Configuration Optimization Scheme for Camera Networks,X. Zhang; X. Ding; Y. Ren; Y. Zheng; C. Fang; J. He,"Dept. of Automation, Shanghai Jiao Tong University, Shanghai, China; Dept. of Automation, Shanghai Jiao Tong University, Shanghai, China; Tencent Robotics X Lab, Shenzhen, China; Tencent Robotics X Lab, Shenzhen, China; Dept. of Automation, Shanghai Jiao Tong University, Shanghai, China; Dept. of Automation, Shanghai Jiao Tong University, Shanghai, China",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,13386,13391,"The performance of a camera network monitoring a set of targets depends crucially on the configuration of the cameras. In this paper, we investigate the reconfiguration strategy for the parameterized camera network model, with which the sensing qualities of the multiple targets can be optimized globally and simultaneously. We first propose to use the number of pixels occupied by a unit-length object in image as a metric of the sensing quality of the object, which is determined by the parameters of the camera, such as intrinsic, extrinsic, and distortional coefficients. Then, we form a single quantity that measures the sensing quality of the targets by the camera network. This quantity further serves as the objective function of our optimization problem to obtain the optimal camera configuration. We verify the effectiveness of our approach through extensive simulations and experiments, and the results reveal its improved performance on the AprilTag detection tasks. Codes and related utilities for this work are open-sourced and available at https://github.com/sszxc/MultiCam-Simulation.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9982246,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982246,,Solid modeling;Three-dimensional displays;Target tracking;Surveillance;Cameras;Robot sensing systems;Linear programming,,,,24,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/sszxc/MultiCam-Simulation,https://github.com/sszxc/MultiCam-Simulation
461,Toolbox Release: A WiFi-Based Relative Bearing Framework for Robotics,N. Jadhav; W. Wang; D. Zhang; S. Kumar; S. Gil,"REACT Lab, Harvard; REACT Lab, Harvard; WiTech Lab, CMU; WiTech Lab, CMU; REACT Lab, Harvard",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,13714,13721,"This paper presents the WiFi-Sensor-for-Robotics (WSR) open-source toolbox111Code: https://github.com/Harvard-REACT/WSR-Toolbox Dataset: https://github.com/Harvard-REACT/WSR-Toolbox-Dataset Demo: https://github.com/Harvard-REACT/WSR-Toolbox/wiki/Demo. It enables robots in a team to obtain relative bearing to each other, even in nonline-of-sight (NLOS) settings which is a very challenging problem in robotics. It does so by analyzing the phase of their communicated WiFi signals as the robots traverse the environment. This capability, based on the theory developed in our prior works, is made available for the first time as an open-source toolbox. It is motivated by the lack of easily deployable solutions that use robots' local resources (e.g WiFi) for sensing in NLOS. This has implications for multi-robot mapping and rendezvous, ad-hoc robot networks, and security in multi-robot teams, amongst other applications. The toolbox is designed for distributed and online deployment on robot platforms using commodity hardware and on-board sensors. We also release datasets demonstrating its performance in NLOS and line-of-sight (LOS) settings and for a multi-robot localization use case. Empirical results for hardware experiments show that the bearing estimation from our toolbox achieves accuracy with mean and standard deviation of 1.13 degrees, 11.07 degrees in LOS and 6.04 degrees, 26.4 degrees for NLOS, respectively, in an indoor office environment.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981230,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981230,,Location awareness;Line-of-sight propagation;Robot sensing systems;Hardware;Sensors;Security;Robots,,5.0,,40,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/Harvard-REACT/WSR-Toolbox,https://github.com/Harvard-REACT/WSR-Toolbox
462,Robust Real-time LiDAR-inertial Initialization,F. Zhu; Y. Ren; F. Zhang,"Department of Mechanical Engineering, University of Hong Kong; Department of Mechanical Engineering, University of Hong Kong; Department of Mechanical Engineering, University of Hong Kong",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,3948,3955,"For most LiDAR-inertial odometry, accurate initial states, including temporal offset and extrinsic transfor-mation between LiDAR and 6-axis IMUs, play a significant role and are often considered as prerequisites. However, such information may not be always available in customized LiDAR-inertial systems. In this paper, we propose LI-Init: a full and real-time LiDAR-inertial system initialization process that calibrates the temporal offset and extrinsic parameter between LiDARs and IMUs, and also the gravity vector and IMU bias by aligning the state estimated from LiDAR measurements with that measured by IMU. We implement the proposed method as an initialization module, which can automatically detects the degree of excitation of the collected data and calibrate, on-the-fly, the temporal offset, extrinsic, gravity vector, and IMU bias, which are then used as high-quality initial state values for real-time LiDAR-inertial odometry systems. Experiments conducted with different types of LiDARs and LiDAR-inertial combinations show the robustness, adaptability and efficiency of our initialization method. The implementation of our LiDAR-inertial initialization procedure LI-Init and test data are open-sourced on Github11https://www.github.com/hku-mars/LiDAR IMU Init and also integrated into a state-of-the-art LiDAR-inertial odometry system FAST-LIO2.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9982225,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982225,,Laser radar;Data collection;Real-time systems;Robustness;Hardware;Gyroscopes;Calibration,,32.0,,26,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/hku-mars/LiDAR,https://github.com/hku-mars/LiDAR
463,ROLL: Long-Term Robust LiDAR-based Localization With Temporary Mapping in Changing Environments,B. Peng; H. Xie; W. Chen,"Key Laboratory of System Control and Information Processing, Institute of Medical Robotics and Department of Automation, Shanghai Jiao Tong University and Ministry of Education, Shanghai, China; Key Laboratory of System Control and Information Processing, Institute of Medical Robotics and Department of Automation, Shanghai Jiao Tong University and Ministry of Education, Shanghai, China; Key Laboratory of System Control and Information Processing, Institute of Medical Robotics and Department of Automation, Shanghai Jiao Tong University and Ministry of Education, Shanghai, China",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,2841,2847,"Long-term scene changes pose challenges to localization systems using a pre-built map. This paper presents a LiDAR-based system that provides robust localization against those challenges. Our method starts with activation of a mapping process temporarily when global matching towards the pre-built map is unreliable. The temporary map will be merged onto the pre-built map for later localization sessions once reliable matching is obtained again. We further integrate a LiDAR inertial odometry (LIO) to provide motion-compensated LiDAR scans and a reliable pose initial estimate for the global matching module. To generate a smooth real-time trajectory for navigation purposes, we fuse poses from odometry and global matching by solving a pose graph optimization problem. We evaluate our localization system with extensive experiments on the NCLT dataset including a variety of changing indoor and outdoor environments, and the results demonstrate a robust and accurate long-term localization performance. The implementations are open sourced on GitHub11https://github.com/HaisenbergPeng/ROLL.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9982153,National Natural Science Foundation of China(grant numbers:UI813206); National Key R&D Program of China(grant numbers:2020YFC2007500); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982153,,Location awareness;Laser radar;Adaptive systems;Navigation;Fuses;Real-time systems;Robustness,,8.0,,21,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/HaisenbergPeng/ROLL,https://github.com/HaisenbergPeng/ROLL
464,Grasp Stability Prediction with Sim-to-Real Transfer from Tactile Sensing,Z. Si; Z. Zhu; A. Agarwal; S. Anderson; W. Yuan,"Robotics Institute, Carnegie Mellon University; Department of Electrical Engineering, Tsinghua University; Robotics Institute, Carnegie Mellon University; Meta Reality Labs Research; Robotics Institute, Carnegie Mellon University",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,7809,7816,"Robot simulation has been an essential tool for data-driven manipulation tasks. However, most existing simulation frameworks lack either efficient and accurate models of physical interactions with tactile sensors or realistic tactile simulation. This makes the sim-to-real transfer for tactile-based manipulation tasks still challenging. In this work, we integrate simulation of robot dynamics and vision-based tactile sensors by modeling the physics of contact. This contact model uses simulated contact forces at the robot's end-effector to inform the generation of realistic tactile outputs. To eliminate the sim-to-real transfer gap, we calibrate our physics simulator of robot dynamics, contact model, and tactile optical simulator with real-world data, and then we demonstrate the effectiveness of our system on a zero-shot sim-to-real grasp stability prediction task where we achieve an average accuracy of 90.7% on various objects. Experiments reveal the potential of applying our simulation framework to more complicated manipulation tasks. We open-source our simulation framework at https://github.com/CMURoboTouch/Taxim/tree/taxim-robot.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981863,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981863,,Deformable models;Shape;Dynamics;Tactile sensors;Predictive models;Stability analysis;Data models,,20.0,,36,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/CMURoboTouch/Taxim,https://github.com/CMURoboTouch/Taxim
465,Closed-Loop Next-Best-View Planning for Target-Driven Grasping,M. Breyer; L. Ott; R. Siegwart; J. J. Chung,"Autonomous Systems Lab, ETH Zurich, Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Zurich, Switzerland",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,1411,1416,"Picking a specific object from clutter is an essential component of many manipulation tasks. Partial observations often require the robot to collect additional views of the scene before attempting a grasp. This paper proposes a closed-loop next-best-view planner that drives exploration based on occluded object parts. By continuously predicting grasps from an up-to-date scene reconstruction, our policy can decide online to finalize a grasp execution or to adapt the robot's trajectory for further exploration. We show that our reactive approach decreases execution times without loss of grasp success rates compared to common camera placements and handles situations where the fixed baselines fail. Video and code are available at https://github.com/ethz-asl/active_grasp.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981472,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981472,,Robot vision systems;Grasping;Cameras;Search problems;Robustness;Trajectory;Planning,,9.0,,38,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/ethz-asl/active_grasp,https://github.com/ethz-asl/active_grasp
466,Transporters with Visual Foresight for Solving Unseen Rearrangement Tasks,H. Wu; J. Ye; X. Meng; C. Paxton; G. S. Chirikjian,"The Johns Hopkins University, Baltimore, MD, USA; National University of Singapore, Singapore; National University of Singapore, Singapore; NVIDIA, USA; National University of Singapore, Singapore",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,10756,10763,"Rearrangement tasks have been identified as a crucial challenge for intelligent robotic manipulation, but few methods allow for precise construction of unseen structures. We propose a visual foresight model for pick-and-place rearrangement manipulation which is able to learn efficiently. In addition, we develop a multi-modal action proposal module which builds on the Goal-Conditioned Transporter Network, a state-of-the-art imitation learning method. Our image-based task planning method, Transporters with Visual Foresight, is able to learn from only a handful of data and generalize to multiple unseen tasks in a zero-shot manner. TVF is able to improve the performance of a state-of-the-art imitation learning method on unseen tasks in simulation and real robot experiments. In particular, the average success rate on unseen tasks improves from 55.4% to 78.5% in simulation experiments and from 30% to 63.3% in real robot experiments when given only tens of expert demonstrations. Video and code are available on our project website: https://chirikjianlab.github.io/tvf/",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981832,"National Research Foundation, Singapore; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981832,,Learning systems;Visualization;Interpolation;Training data;Predictive models;Data models;Planning,,8.0,,40,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://chirikjianlab.github.io/tvf,https://github.com/ChirikjianLab/tvf
467,SpeedFolding: Learning Efficient Bimanual Folding of Garments,Y. Avigal; L. Berscheid; T. Asfour; T. Kröger; K. Goldberg,AUTOLab at UC Berkeley; AUTOLab at UC Berkeley; Karlsruhe Institute of Technology (KIT); Karlsruhe Institute of Technology (KIT); AUTOLab at UC Berkeley,2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,1,8,"Folding garments reliably and efficiently is a long standing challenge in robotic manipulation due to the complex dynamics and high dimensional configuration space of garments. An intuitive approach is to initially manipulate the garment to a canonical smooth configuration before folding. In this work, we develop SpeedFolding, a reliable and efficient bimanual system, which given user-defined instructions as folding lines, manipulates an initially crumpled garment to (1) a smoothed and (2) a folded configuration. Our primary contribution is a novel neural network architecture that is able to predict pairs of gripper poses to parameterize a diverse set of bimanual action primitives. After learning from 4300 human- annotated and self-supervised actions, the robot is able to fold garments from a random initial configuration in under 120 s on average with a success rate of 93 %. Real-world experiments show that the system is able to generalize to unseen garments of different color, shape, and stiffness. While prior work achieved 3–6 Folds Per Hour (FPH), SpeedFolding achieves 30–40 FPH. See https://pantor.github.io/speedfolding for code, videos, and datasets.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981402,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981402,,Codes;Shape;Clothing;Neural networks;Color;Reliability engineering;Grippers,,39.0,,31,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://pantor.github.io/speedfolding,https://github.com/pantor/speedfolding
468,enpheeph: A Fault Injection Framework for Spiking and Compressed Deep Neural Networks,A. Colucci; A. Steininger; M. Shafique,"Institute of Computer Engineering, Technische Universität Wien, Vienna, Austria; Institute of Computer Engineering, Technische Universität Wien, Vienna, Austria; Division of Engineering, eBrain Lab, New York University, Abu Dhabi, UAE",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,5155,5162,"Research on Deep Neural Networks (DNNs) has focused on improving performance and accuracy for real-world deployments, leading to new models, such as Spiking Neural Networks (SNNs), and optimization techniques, e.g., quantization and pruning for compressed networks. However, the deployment of these innovative models and optimization techniques introduces possible reliability issues, which is a pillar for DNNs to be widely used in safety-critical applications, e.g., autonomous driving. Moreover, scaling technology nodes have the associated risk of multiple faults happening at the same time, a possibility not addressed in state-of-the-art resiliency analyses. Towards better reliability analysis for DNNs, we present enpheeph, a Fault Injection Framework for Spiking and Compressed DNNs. The enpheeph framework enables optimized execution on specialized hardware devices, e.g., GPUs, while providing complete customizability to investigate different fault models, emulating various reliability constraints and use-cases. Hence, the faults can be executed on SNNs as well as compressed networks with minimal-to-none modifications to the underlying code, a feat that is not achievable by other state-of-the-art tools. To evaluate our enpheeph framework, we analyze the resiliency of different DNN and SNN models, with different compression techniques. By injecting a random and increasing number of faults, we show that DNNs can show a reduction in accuracy with a fault rate as low as $7\times 10^{-7}$ faults per parameter, with an accuracy drop higher than 40%. Run-time overhead when executing enpheeph is less than 20% of the baseline execution time when executing 100 000 faults concurrently, at least 10× lower than state-of-the-art frameworks, making enpheeph future-proof for complex fault injection scenarios. We release the source code of our enpheeph framework under an open-source license at https://github.com/Alexei95/enpheeph.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9982181,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982181,Deep Neural Networks;Resiliency;Spiking Neural Networks;Compressed Networks;Quantized Neural Networks;Sparse Neural Networks;Fault Injection,Deep learning;Quantization (signal);Codes;Source coding;Neural networks;Licenses;Reliability,,11.0,,48,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/Alexei95/enpheeph,https://github.com/Alexei95/enpheeph
469,Model-Free Unsupervised Anomaly Detection of a General Robotic System Using a Stacked LSTM and Its Application to a Fixed-Wing Unmanned Aerial Vehicle,J. -H. Park; S. Shanbhag; D. E. Chang,"School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,4287,4293,"With the growing application of various robots in real life, the need for an automatic anomaly detection system for robots is necessary for safety. In this paper, we develop an anomaly detection method using a stacked LSTM that can be applied to any robot controlled by a feedback control. Our method does not need installation of additional sensors. Our method is model-free and unsupervised because it does not require the analytical model of the system and the training data does not require faulty operation conditions. We validate our method on real fixed-wing unmanned aerial vehicle flight data containing control surface failure scenarios. We demonstrate the superiority of the proposed algorithm over existing anomaly detection methods in the literature. Our code is available at https://github.com/superhumangod/Model-free-unsupervised-anomaly-detection.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981950,Defense Acquisition Program Administration; Agency for Defense Development(grant numbers:UD190031RD); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981950,,Training;Analytical models;Current measurement;Training data;Robot sensing systems;Autonomous aerial vehicles;Sensors,,8.0,,36,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/superhumangod/Model-free-unsupervised-anomaly-detection,https://github.com/superhumangod/Model-free-unsupervised-anomaly-detection
470,ICK-Track: A Category-Level 6-DoF Pose Tracker Using Inter-Frame Consistent Keypoints for Aerial Manipulation,J. Sun; Y. Wang; M. Feng; D. Wang; J. Zhao; C. Stachniss; X. Chen,"College of Electrical and Information Engineering, and the National Engineering Laboratory for Robot Visual Perception and Control (RVC-NATIONAL ENGNEERING LAB), Hunan University, Changsha, China; College of Electrical and Information Engineering, and the National Engineering Laboratory for Robot Visual Perception and Control (RVC-NATIONAL ENGNEERING LAB), Hunan University, Changsha, China; College of Electrical and Information Engineering, and the National Engineering Laboratory for Robot Visual Perception and Control (RVC-NATIONAL ENGNEERING LAB), Hunan University, Changsha, China; School of Electrical and Electrical Engineering, Nanyang Technological University, Singapore; College of Electrical and Information Engineering, and the National Engineering Laboratory for Robot Visual Perception and Control (RVC-NATIONAL ENGNEERING LAB), Hunan University, Changsha, China; University of Bonn, Germany; University of Bonn, Germany",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,1556,1563,"Robots that are supposed to interact with or manipulate objects in the world must be able to track the poses of objects in their sensor data. Thus, Detecting and tracking the 6-DoF poses of targeted objects is important for aerial manipulation and is still in the early stage due to the high dynamics and limited onboard capacity of such systems. In this paper, we propose ICK-Track, a novel method for onboard category-level object 6-DoF pose tracking that can be applied to aerial manipulation without using any pre-defined object CAD models. It first utilizes a semi-supervised video segmentation to detect objects in the eye-in-hand RGB-D camera stream to segment the 3D points of objects. Then, canonical keypoints are extracted using iterative farthest point sampling. We propose a novel inter-frame consistent keypoints generation network to generate the corresponding keypoint pairs, which are used together with ICP to estimate the pose changes of objects for tracking. Experimental results show that our method is more robust to viewpoint changes and runs faster than the state-of-the-art methods on category-level pose tracking. We further test our proposed method on a real aerial manipulator. A demo video showing the use of our method on a real aerial manipulator and the implementation of our method are available at: https://github.com/S-JingTao/ICK-Track.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9982183,"National Natural Science Foundation of China(grant numbers:61903135,61803089,61733004); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982183,,Solid modeling;Target tracking;Three-dimensional displays;Runtime;Streaming media;Robot sensing systems;Cameras,,3.0,,35,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/S-JingTao/ICK-Track,https://github.com/S-JingTao/ICK-Track
471,CA-SpaceNet: Counterfactual Analysis for 6D Pose Estimation in Space,S. Wang; S. Wang; B. Jiao; D. Yang; L. Su; P. Zhai; C. Chen; L. Zhang,"Academy for Engineering & Technology, Fudan University; Academy for Engineering & Technology, Fudan University; Academy for Engineering & Technology, Fudan University; Academy for Engineering & Technology, Fudan University; Academy for Engineering & Technology, Fudan University; Academy for Engineering & Technology, Fudan University; Academy for Engineering & Technology, Fudan University; Academy for Engineering & Technology, Fudan University",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,10627,10634,"Reliable and stable 6D pose estimation of un-cooperative space objects plays an essential role in on-orbit servicing and debris removal missions. Considering that the pose estimator is sensitive to background interference, this paper proposes a counterfactual analysis framework named CA-SpaceNet to complete robust 6D pose estimation of the space-borne targets under complicated background. Specifically, conventional methods are adopted to extract the features of the whole image in the factual case. In the counterfactual case, a non-existent image without the target but only the background is imagined. Side effect caused by background interference is reduced by counterfactual analysis, which leads to unbiased prediction in final results. In addition, we also carry out low-bit-width quantization for CA-SpaceNet and deploy part of the framework to a Processing-In-Memory (PIM) accelerator on FPGA. Qualitative and quantitative results demonstrate the effectiveness and efficiency of our proposed method. To our best knowledge, this paper applies causal inference and network quantization to the 6D pose estimation of space-borne targets for the first time. The code is available at https://github.com/Shunli-Wang/CA-SpaceNet.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981172,Shanghai Municipal Science and Technology Major Project(grant numbers:2021SHZDZX0103); NSFC(grant numbers:61974033); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981172,,Knowledge engineering;Quantization (signal);Space missions;Space technology;Pose estimation;Neural networks;Interference,,10.0,,42,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/Shunli-Wang/CA-SpaceNet,https://github.com/Shunli-Wang/CA-SpaceNet
472,Improving 3D Markerless Pose Estimation of Animals in the Wild using Low-Cost Cameras,N. Muramatsu; Z. da Silva; D. Joska; F. Nicolls; A. Patel,"African Robotics Unit (ARU), University of Cape Town, South Africa; African Robotics Unit (ARU), University of Cape Town, South Africa; African Robotics Unit (ARU), University of Cape Town, South Africa; African Robotics Unit (ARU), University of Cape Town, South Africa; African Robotics Unit (ARU), University of Cape Town, South Africa",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,3770,3776,"Tracking the 3D motion of agile animals in the wild will enable new insight into the design of robotic controllers. However, in-field 3D pose estimation of high-speed wildlife such as cheetahs is still a challenge [1]. In this work, we aim to solve two of these challenges: unnatural pose estimates during highly occluded sequences and synchronization error between multi-view data. We expand on our previous Full Trajectory Estimation (FTE) method with two significant additions: Pairwise FTE (PW-FTE) and Shutter-delay FTE (SD-FTE). The PW-FTE expands on image-dependent pairwise terms, produced by a convolutional neural network (CNN), to infer occluded 2D keypoints, while SD-FTE uses shutter delay estimation to correct the synchronization error. Lastly, we combine both methods into PW-SD-FTE and perform a quantitative and qualitative analysis on a subset of AcinoSet, the video dataset of rapid and agile motions of cheetahs. We found that SD-FTE has significant benefits in tracking the position of the cheetah in the world frame, while PW-FTE provided a more robust 3D pose estimate during events of high occlusion. The PW-SD-FTE was found to retain both advantages, resulting in an improved baseline for AcinoSet. Code and data can be found at https://github.com/African-Robotics-Unit/AcinoSet/tree/pw_sd_fte.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981746,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981746,,Three-dimensional displays;Codes;Tracking;Pose estimation;Wildlife;Delay estimation;Trajectory,,3.0,,43,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/African-Robotics-Unit/AcinoSet,https://github.com/African-Robotics-Unit/AcinoSet
473,Towards Defensive Autonomous Driving: Collecting and Probing Driving Demonstrations of Mixed Qualities,J. Oh; G. Lee; J. Park; W. Oh; J. Heo; H. Chung; D. H. Kim; B. Park; C. -G. Lee; S. Choi; S. Oh,"Department of Electrical and Computer Engineering, ASRI, Seoul National University, Seoul, Korea; Department of Electrical and Computer Engineering, ASRI, Seoul National University, Seoul, Korea; Department of Artificial Intelligence, Korea University, Seoul, Korea; Department of Electrical and Computer Engineering, ASRI, Seoul National University, Seoul, Korea; Department of Electrical and Computer Engineering, ASRI, Seoul National University, Seoul, Korea; Department of Electrical and Computer Engineering, ASRI, Seoul National University, Seoul, Korea; Department of Computer Science and Engineering, Seoul National University, Seoul, Korea; Department of Computer Science and Engineering, Seoul National University, Seoul, Korea; Department of Computer Science and Engineering, Seoul National University, Seoul, Korea; Department of Artificial Intelligence, Korea University, Seoul, Korea; Department of Electrical and Computer Engineering, ASRI, Seoul National University, Seoul, Korea",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,12528,12533,"Designing or learning an autonomous driving policy is undoubtedly a challenging task as the policy has to maintain its safety in all corner cases. In order to secure safety in autonomous driving, the ability to detect hazardous situations, which can be seen as an out-of-distribution (OOD) detection problem, becomes crucial. However, conventional datasets often only contain expert driving demonstrations, although some non-expert or uncommon driving behavior data are needed to implement a safety guaranteed autonomous driving platform. To this end, we present a dataset called the R3 Driving Dataset, composed of driving data with different qualities. The dataset categorizes abnormal driving behaviors into eight categories and 369 different detailed situations. The situations include dangerous lane changes and near-collision situations. To further enlighten how these abnormal driving behaviors can be detected, we utilize different uncertainty estimation and anomaly detection methods for the proposed dataset. From the results of the proposed experiment, it can be inferred that by using both uncertainty estimation and anomaly detection, most of the abnormal cases in the proposed dataset can be discriminated. https://rllab-snu.github.io/projects/R3-Driving-Dataset/doc.html",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981110,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981110,,Measurement;Uncertainty;Estimation;Data collection;Safety;Behavioral sciences;Task analysis,,,,37,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://rllab-snu.github.io/projects/R3-Driving-Dataset,
474,Robust Onboard Localization in Changing Environments Exploiting Text Spotting,N. Zimmerman; L. Wiesmann; T. Guadagnino; T. Läbe; J. Behley; C. Stachniss,"University of Bonn, Germany; University of Bonn, Germany; University of Bonn, Germany; University of Bonn, Germany; University of Bonn, Germany; University of Bonn, Germany",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,917,924,"Robust localization in a given map is a crucial component of most autonomous robots. In this paper, we address the problem of localizing in an indoor environment that changes and where prominent structures have no correspondence in the map built at a different point in time. To overcome the discrepancy between the map and the observed environment caused by such changes, we exploit human-readable localization cues to assist localization. These cues are readily available in most facilities and can be detected using RGB camera images by utilizing text spotting. We integrate these cues into a Monte Carlo localization framework using a particle filter that operates on 2D LiDAR scans and camera data. By this, we provide a robust localization solution for environments with structural changes and dynamics by humans walking. We evaluate our localization framework on multiple challenging indoor scenarios in an office environment. The experiments suggest that our approach is robust to structural changes and can run on an onboard computer. We release an open source implementation of our approach11https://github.com/PRBonn/tmcl, which uses off-the-shelf text spotting, written in C++ with a ROS wrapper.",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981049,"Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy, EXC-2070(grant numbers:390732324); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981049,,Location awareness;Legged locomotion;Monte Carlo methods;Laser radar;Navigation;Layout;Cameras,,14.0,,37,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/PRBonn/tmcl,https://github.com/PRBonn/tmcl
475,ProgressLabeller: Visual Data Stream Annotation for Training Object-Centric 3D Perception,X. Chen; H. Zhang; Z. Yu; S. Lewis; O. C. Jenkins,"Department of Electrical Engineering and Computer Science, Robotics Institute at the University of Michigan, Ann Arbor, MI, USA; Department of Electrical Engineering and Computer Science, Robotics Institute at the University of Michigan, Ann Arbor, MI, USA; Department of Electrical Engineering and Computer Science, Robotics Institute at the University of Michigan, Ann Arbor, MI, USA; Department of Electrical Engineering and Computer Science, Robotics Institute at the University of Michigan, Ann Arbor, MI, USA; Department of Electrical Engineering and Computer Science, Robotics Institute at the University of Michigan, Ann Arbor, MI, USA",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,13066,13073,"Visual perception tasks often require vast amounts of labelled data, including 3D poses and image space segmen-tation masks. The process of creating such training data sets can prove difficult or time-intensive to scale up to efficacy for general use. Consider the task of pose estimation for rigid objects. Deep neural network based approaches have shown good performance when trained on large, public datasets. However, adapting these networks for other novel objects, or fine-tuning existing models for different environments, requires significant time investment to generate newly labelled instances. Towards this end, we propose ProgressLabeller as a method for more efficiently generating large amounts of 6D pose training data from color images sequences for custom scenes in a scalable manner. ProgressLabeller is intended to also support transparent or translucent objects, for which the previous methods based on depth dense reconstruction will fail. We demonstrate the effectiveness of ProgressLabeller by rapidly create a dataset of over 1M samples with which we fine-tune a state-of-the-art pose estimation network in order to markedly improve the downstream robotic grasp success rates. Progresslabeller is open-source at https://github.com/huijieZH/ProgressLabeller",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9982076,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982076,,Deep learning;Training;Visualization;Three-dimensional displays;Annotations;Pose estimation;Neural networks,,7.0,,32,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,https://github.com/huijieZH/ProgressLabeller,https://github.com/huijieZH/ProgressLabeller
476,Robust Change Detection Based on Neural Descriptor Fields,J. Fu; Y. Du; K. Singh; J. B. Tenenbaum; J. J. Leonard,"MIT Computer Science and Artificial Intelligence Laboratory, Cambridge, MA, USA; MIT Computer Science and Artificial Intelligence Laboratory, Cambridge, MA, USA; MIT Computer Science and Artificial Intelligence Laboratory, Cambridge, MA, USA; MIT Computer Science and Artificial Intelligence Laboratory, Cambridge, MA, USA; MIT Computer Science and Artificial Intelligence Laboratory, Cambridge, MA, USA",2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),26 Dec 2022,2022,,,2817,2824,"The ability to reason about changes in the environment is crucial for robots operating over extended periods of time. Agents are expected to capture changes during operation so that actions can be followed to ensure a smooth progression of the working session. However, varying viewing angles and accumulated localization errors make it easy for robots to falsely detect changes in the surrounding world due to low observation overlap and drifted object associations. In this paper, based on the recently proposed category-level Neural Descriptor Fields (NDFs), we develop an object-level online change detection approach that is robust to partially overlapping observations and noisy localization results. Utilizing the shape completion capability and SE(3)-equivariance of NDFs, we represent objects with compact shape codes encoding full object shapes from partial observations. The objects are then organized in a spatial tree structure based on object centers recovered from NDFs for fast queries of object neighborhoods. By associating objects via shape code similarity and comparing local object-neighbor spatial layout, our proposed approach demonstrates robustness to low observation overlap and localization noises. We conduct experiments on both synthetic and real-world sequences and achieve improved change detection results compared to multiple baseline methods. Project web-page: ?http://yilundu.github.io/ndf_change",2153-0866,978-1-6654-7927-1,10.1109/IROS47612.2022.9981246,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981246,,Location awareness;Codes;Shape;Layout;Robustness;Encoding;Noise measurement,,5.0,,27,IEEE,26 Dec 2022,,,IEEE,IEEE Conferences,http://yilundu.github.io/ndf_change,
477,Some Research Questions for SLAM in Deformable Environments,S. Huang; Y. Chen; L. Zhao; Y. Zhang; M. Xu,"Centre for Autonomous Systems, University of Technology, Sydney, NSW, Australia; Centre for Autonomous Systems, University of Technology, Sydney, NSW, Australia; Centre for Autonomous Systems, University of Technology, Sydney, NSW, Australia; Centre for Autonomous Systems, University of Technology, Sydney, NSW, Australia; Centre for Autonomous Systems, University of Technology, Sydney, NSW, Australia",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,7653,7660,"SLAM in deformable environments is a very challenging research topic. Some research works have been presented by different research groups in the past few years. However, there are still some challenging research questions remaining unanswered. This paper discusses some of these research questions focusing on the case when point features are used to describe the deformable environments. The SLAM problems are formulated as extensions of point feature based SLAM in static environments, including both optimisation based offline SLAM and filter based online SLAM. To illustrate the problems and questions more clearly, some concepts and results using simple 2D examples are presented. The MATLAB source codes of the results are made publicly available (https://github.com/cyb1212/DeformableSLAM2D.git) to help the readers understand the problems more clearly.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9635883,Australian Research Council; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635883,,Simultaneous localization and mapping;Codes;Three-dimensional displays;Focusing;Kalman filters;Observability;Optimization,,2.0,,36,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/cyb1212/DeformableSLAM2D,https://github.com/cyb1212/DeformableSLAM2D
478,PTT: Point-Track-Transformer Module for 3D Single Object Tracking in Point Clouds,J. Shan; S. Zhou; Z. Fang; Y. Cui,"Faculty of Robot Science and Engineering, Northeastern University, Shenyang, China; Faculty of Robot Science and Engineering, Northeastern University, Shenyang, China; Faculty of Robot Science and Engineering, Northeastern University, Shenyang, China; Faculty of Robot Science and Engineering, Northeastern University, Shenyang, China",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,1310,1316,"3D single object tracking is a key issue for robotics. In this paper, we propose a transformer module called Point-Track-Transformer (PTT) for point cloud-based 3D single object tracking. PTT module contains three blocks for feature embedding, position encoding, and self-attention feature computation. Feature embedding aims to place features closer in the embedding space if they have similar semantic information. Position encoding is used to encode coordinates of point clouds into high dimension distinguishable features. Self-attention generates refined attention features by computing attention weights. Besides, we embed the PTT module into the open-source state-of-the-art method P2B to construct PTT-Net. Experiments on the KITTI dataset reveal that our PTT-Net surpasses the state-of-the-art by a noticeable margin $\left( {\sim 10\% } \right)$. Additionally, PTT-Net could achieve real-time performance (~40FPS) on NVIDIA 1080Ti GPU. Our code is open-sourced for the robotics community at https://github.com/shanjiayao/PTT.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636821,National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; Aeronautical Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636821,,Point cloud compression;Three-dimensional displays;Target tracking;Robot kinematics;Semantics;Transformers;Encoding,,52.0,,27,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/shanjiayao/PTT,https://github.com/shanjiayao/PTT
479,Attention Augmented ConvLSTM for Environment Prediction,B. Lange; M. Itkina; M. J. Kochenderfer,"Department of Aeronautics and Astronautics, Stanford University, Stanford, CA, USA; Department of Aeronautics and Astronautics, Stanford University, Stanford, CA, USA; Department of Aeronautics and Astronautics, Stanford University, Stanford, CA, USA",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,1346,1353,"Safe and proactive planning in robotic systems generally requires accurate predictions of the environment. Prior work on environment prediction applied video frame prediction techniques to bird’s-eye view environment representations, such as occupancy grids. ConvLSTM-based frameworks used previously often result in significant blurring of the predictions, loss of static environment structure, and vanishing of moving objects, thus hindering their applicability for use in safety-critical applications. In this work, we propose two extensions to the ConvLSTM architecture to address these issues. We present the Temporal Attention Augmented ConvLSTM (TAAConvLSTM) and Self-Attention Augmented ConvLSTM (SAAConvLSTM) frameworks for spatiotemporal occupancy grid prediction, and demonstrate improved performance over baseline architectures on the real-world KITTI and Waymo datasets. We provide our implementation at https: //github.com/sisl/AttentionAugmentedConvLSTM.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636386,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636386,,Spatiotemporal phenomena;Planning;Intelligent robots,,8.0,,37,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/sisl/AttentionAugmentedConvLSTM,https://github.com/sisl/AttentionAugmentedConvLSTM
480,ChangeSim: Towards End-to-End Online Scene Change Detection in Industrial Indoor Environments,J. -M. Park; J. -H. Jang; S. -M. Yoo; S. -K. Lee; U. -H. Kim; J. -H. Kim,"School of Electrical Engineering, KAIST, Daejeon, Republic of Korea; School of Electrical Engineering, KAIST, Daejeon, Republic of Korea; School of Electrical Engineering, KAIST, Daejeon, Republic of Korea; School of Electrical Engineering, KAIST, Daejeon, Republic of Korea; School of Electrical Engineering, KAIST, Daejeon, Republic of Korea; School of Electrical Engineering, KAIST, Daejeon, Republic of Korea",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,8578,8585,"We present a challenging dataset, ChangeSim, aimed at online scene change detection (SCD) and more. The data is collected in photo-realistic simulation environments with the presence of environmental non-targeted variations, such as air turbidity and light condition changes, as well as targeted object changes in industrial indoor environments. By collecting data in simulations, multi-modal sensor data and precise ground truth labels are obtainable such as the RGB image, depth image, semantic segmentation, change segmentation, camera poses, and 3D reconstructions. While the previous online SCD datasets evaluate models given well-aligned image pairs, ChangeSim also provides raw unpaired sequences that present an opportunity to develop an online SCD model in an end-to-end manner, considering both pairing and detection. Experiments show that even the latest pair-based SCD models suffer from the bottleneck of the pairing process, and it gets worse when the environment contains the non-targeted variations. Our dataset is available at https://sammica.github.io/ChangeSim/.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636350,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636350,,Location awareness;Image segmentation;Visualization;Solid modeling;Three-dimensional displays;Atmospheric modeling;Multimodal sensors,,14.0,,20,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://sammica.github.io/ChangeSim,https://github.com/SAMMiCA/ChangeSim
481,BundleTrack: 6D Pose Tracking for Novel Objects without Instance or Category-Level 3D Models,B. Wen; K. Bekris,"Computer Science Dept. of Rutgers, NJ, USA; Computer Science Dept. of Rutgers, NJ, USA",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,8067,8074,"Tracking the 6D pose of objects in video sequences is important for robot manipulation. Most prior efforts, however, often assume that the target object's CAD model, at least at a category-level, is available for offline training or during online template matching. This work proposes BundleTrack, a general framework for 6D pose tracking of novel objects, which does not depend upon 3D models, either at the instance or category-level. It leverages the complementary attributes of recent advances in deep learning for segmentation and robust feature extraction, as well as memory-augmented pose graph optimization for spatiotemporal consistency. This enables long-term, low-drift tracking under various challenging scenarios, including significant occlusions and object motions. Comprehensive experiments given two public benchmarks demonstrate that the proposed approach significantly outperforms state-of-art, category-level 6D tracking or dynamic SLAM methods. When compared against state-of-art methods that rely on an object instance CAD model, comparable performance is achieved, despite the proposed method’s reduced information requirements. An efficient implementation in CUDA provides a real-time performance of 10Hz for the entire framework. Code is available at: https://github.com/wenbowen123/BundleTrack",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9635991,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635991,,Training;Solid modeling;Target tracking;Three-dimensional displays;Simultaneous localization and mapping;Motion segmentation;Video sequences,,69.0,,68,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/wenbowen123/BundleTrack,https://github.com/wenbowen123/BundleTrack
482,Stereo Plane SLAM Based on Intersecting Lines,X. Zhang; W. Wang; X. Qi; Z. Liao,"Robotics Institute, Beihang University, Beijing, China; Robotics Institute, Beihang University, Beijing, China; Robotics Institute, Beihang University, Beijing, China; Robotics Institute, Beihang University, Beijing, China",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,6566,6572,"Plane features can be used to reduce drift errors in SLAM systems, especially in indoor environments. It is easy and efficient to extract planes from a dense point cloud, which is commonly generated from a RGB-D camera or a 3D lidar. But when using a stereo camera, it is hard to compute dense point clouds accurately or efficiently. In this paper, we propose a novel method to compute plane parameters using intersecting lines, which are extracted from stereo images. Plane features are commonly extracted from the surface of man-made objects or structures, which have regular shapes and straight edge lines. In three dimensions, two intersecting lines determine a unique plane. Therefore, we extract line segments from both left and right images of a stereo camera. By stereo matching, we compute lines’ endpoints and direction vectors, and then a plane from two intersecting lines is calculated. We discard inaccurate plane features in the frame tracking. Adding such plane features in the stereo SLAM system reduces drift errors and refines the performance. Finally, we build a global map consisting of both points and planes, which can reflect real scene structures. We test our proposed system on public datasets and demonstrate its accurate estimation results, compared with state-of-the-art SLAM systems. To benefit the research of plane-based SLAM, we release our codes at https://github.com/fishmarch/Stereo-Plane-SLAM.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9635961,National Key Research and Development Program of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635961,,Point cloud compression;Visualization;Simultaneous localization and mapping;Three-dimensional displays;Laser radar;Shape;Feature extraction,,10.0,,26,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/fishmarch/Stereo-Plane-SLAM,https://github.com/fishmarch/Stereo-Plane-SLAM
483,A General Framework for Lifelong Localization and Mapping in Changing Environment,M. Zhao; X. Guo; L. Song; B. Qin; X. Shi; G. H. Lee; G. Sun,"Gaussian Robotics, Shanghai, China; Gaussian Robotics, Shanghai, China; Gaussian Robotics, Shanghai, China; Gaussian Robotics, Shanghai, China; Intel Labs China, Beijing, China; Department of Computer Science, School of Computing, Computer Vision and Robotic Perception Lab, National University of Singapore, Singapore; Department of Control Science and Engineering, Harbin Institute of Technology, China",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,3305,3312,"The environment of most real-world scenarios such as malls and supermarkets changes at all times. A pre-built map that does not account for these changes becomes out-of-date easily. Therefore, it is necessary to have an up-to-date model of the environment to facilitate long-term operation of a robot. To this end, this paper presents a general lifelong simultaneous localization and mapping (SLAM) framework. Our framework uses a multiple session map representation, and exploits an efficient map updating strategy that includes map building, pose graph refinement and sparsification. To mitigate the unbounded increase of memory usage, we propose a map-trimming method based on the Chow-Liu maximum-mutual-information spanning tree. The proposed SLAM framework has been comprehensively validated by over a month of robot deployment in real supermarket environment. Furthermore, we release the dataset collected from the indoor and outdoor changing environment with the hope to accelerate lifelong SLAM research in the community. Our dataset is available at https://github.com/sanduan168/lifelong-SLAM-dataset.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9635985,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635985,,Location awareness;Simultaneous localization and mapping;Buildings;Intelligent robots,,25.0,,19,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/sanduan168/lifelong-SLAM-dataset,https://github.com/sanduan168/lifelong-SLAM-dataset
484,Object-to-Scene: Learning to Transfer Object Knowledge to Indoor Scene Recognition,B. Miao; L. Zhou; A. S. Mian; T. L. Lam; Y. Xu,"Shenzhen Institute of Artificial Intelligence and Robotics for Society, The Chinese University of Hong Kong, Shenzhen; School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen; School of Physics, Mathematics and Computing, The University of Western Australia; School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen; School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,2069,2075,"Accurate perception of the surrounding scene is helpful for robots to make reasonable judgments and behaviours. Therefore, developing effective scene representation and recognition methods are of significant importance in robotics. Currently, a large body of research focuses on developing novel auxiliary features and networks to improve indoor scene recognition ability. However, few of them focus on directly constructing object features and relations for indoor scene recognition. In this paper, we analyze the weaknesses of current methods and propose an Object-to-Scene (OTS) method, which extracts object features and learns object relations to recognize indoor scenes. The proposed OTS first extracts object features based on the segmentation network and the proposed object feature aggregation module (OFAM). Afterwards, the object relations are calculated and the scene representation is constructed based on the proposed object attention module (OAM) and global relation aggregation module (GRAM). The final results in this work show that OTS successfully extracts object features and learns object relations from the segmentation network. Moreover, OTS outperforms the state-of-the-art methods by more than 2% on indoor scene recognition without using any additional streams. Code is publicly available at: https://github.com/FreeformRobotics/OTS.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636700,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636700,,Codes;Feature extraction;Intelligent robots,,19.0,,33,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/FreeformRobotics/OTS,https://github.com/FreeformRobotics/OTS
485,Unsupervised Vehicle Re-Identification via Self-supervised Metric Learning using Feature Dictionary,J. Yu; H. Oh,"Institute for IT Convergence, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea; Institute for IT Convergence, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,3806,3813,"The key challenge of unsupervised vehicle re-identification (Re-ID) is learning discriminative features from unlabelled vehicle images. Numerous methods using domain adaptation have achieved outstanding performance, but those methods still need a labelled dataset as a source domain. This paper addresses an unsupervised vehicle Re-ID method, which no need any types of a labelled dataset, through a Self-supervised Metric Learning (SSML) based on a feature dictionary. Our method initially extracts features from vehicle images and stores them in a dictionary. Thereafter, based on the dictionary, the proposed method conducts dictionary-based positive label mining (DPLM) to search for positive labels. Pair-wise similarity, relative-rank consistency, and adjacent feature distribution similarity are jointly considered to find images that may belong to the same vehicle of a given probe image. The results of DPLM are applied to dictionary-based triplet loss (DTL) to improve the discriminativeness of learnt features and to refine the quality of the results of DPLM progressively. The iterative process with DPLM and DTL boosts the performance of unsupervised vehicle Re-ID. Experimental results demonstrate the effectiveness of the proposed method by producing promising vehicle Re-ID performance without a pre-labelled dataset. The source code for this paper is publicly available on https://github.com/andreYoo/VeRI_SSML_FD.git.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636545,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636545,,Measurement;Training;Dictionaries;Codes;Feature extraction;Probes;Intelligent robots,,15.0,,38,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/andreYoo/VeRI_SSML_FD,https://github.com/andreYoo/VeRI_SSML_FD
486,You Only Group Once: Efficient Point-Cloud Processing with Token Representation and Relation Inference Module,C. Xu; B. Zhai; B. Wu; T. Li; W. Zhan; P. Vajda; K. Keutzer; M. Tomizuka,"University of California, Berkeley; University of California, Berkeley; Facebook Reality Labs; Peking University; University of California, Berkeley; Facebook Reality Labs; University of California, Berkeley; University of California, Berkeley",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,4589,4596,"3D perception on point-cloud is a challenging and crucial computer vision task. A point-cloud consists of a sparse, unstructured, and unordered set of points. To understand a point-cloud, previous point-based methods, such as PointNet++, extract visual features through the hierarchical aggregation of local features. However, such methods have several critical limitations: 1) They require considerable sampling and grouping operations, which leads to low inference speed. 2) Despite redundancy among adjacent points, they treat all points alike with an equal amount of computation. 3) They aggregate local features together through downsampling, which causes information loss and hurts perception capability. To overcome these challenges, we propose a novel, simple, and elegant deep learning model called YOGO (You Only Group Once). YOGO divides a point-cloud into a small number of parts and extracts a high-dimensional token to represent points within each sub-region. Next, we use self-attention to capture token-to-token relations, and project the token features back to the point features. We formulate such a series of operations as a relation inference module (RIM). Compared with previous methods, YOGO is very efficient because it only needs to sample and group a point-cloud once. Instead of operating on points, YOGO operates on a small number of tokens, each of which summarizes the point features in a sub-region. This allows us to avoid redundant computation and thus boosts efficiency. Moreover, YOGO preserves pointwise features by projecting token features to point features although the RIM computes on tokens. This avoids information loss and enhances point-wise perception capability. We conduct thorough experiments to demonstrate that YOGO achieves at least 3.0x speedup over point-based baselines while delivering competitive classification and segmentation performance on a classification dataset and a segmentation dataset based on 3D Wharehouse, and S3DIS datasets. The code is available at https://github.com/chenfengxu714/YOGO.git.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636858,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636858,,Deep learning;Visualization;Computer vision;Three-dimensional displays;Computational modeling;Redundancy;Feature extraction,,14.0,,49,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/chenfengxu714/YOGO,https://github.com/chenfengxu714/YOGO
487,CLINS: Continuous-Time Trajectory Estimation for LiDAR-Inertial System,J. Lv; K. Hu; J. Xu; Y. Liu; X. Ma; X. Zuo,"April Lab, Zhejiang University, Hangzhou, China; April Lab, Zhejiang University, Hangzhou, China; April Lab, Zhejiang University, Hangzhou, China; April Lab, Zhejiang University, Hangzhou, China; NingboTech University, Ningbo, China; April Lab, Zhejiang University, Hangzhou, China",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,6657,6663,"In this paper, we propose a highly accurate continuous-time trajectory estimation framework dedicated to SLAM (Simultaneous Localization and Mapping) applications, which enables fuse high-frequency and asynchronous sensor data effectively. We apply the proposed framework in a 3D LiDAR-inertial system for evaluations. The proposed method adopts a non-rigid registration method for continuous-time trajectory estimation and simultaneously removing the motion distortion in LiDAR scans. Additionally, we propose a two-state continuous-time trajectory correction method to efficiently and efficiently tackle the computationally-intractable global optimization problem when loop closure happens. We examine the accuracy of the proposed approach on several publicly available datasets and the data we collected. The experimental results indicate that the proposed method outperforms the discrete-time methods regarding accuracy especially when aggressive motion occurs. Furthermore, we open source our code at https://github.com/APRIL-ZJU/clins to benefit research community.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636676,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636676,,Jacobian matrices;Simultaneous localization and mapping;Laser radar;Three-dimensional displays;Estimation;Trajectory;Computational efficiency,,25.0,,24,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/APRIL-ZJU/clins,https://github.com/APRIL-ZJU/clins
488,Accurate Grid Keypoint Learning for Efficient Video Prediction,X. Gao; Y. Jin; Q. Dou; C. -W. Fu; P. -A. Heng,"Department of Computer Science and Engineering, The Chinese University of Hong Kong, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, China",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,5908,5915,"Video prediction methods generally consume substantial computing resources in training and deployment, among which keypoint-based approaches show promising improvement in efficiency by simplifying dense image prediction to light keypoint prediction. However, keypoint locations are often modeled only as continuous coordinates, so noise from semantically insignificant deviations in videos easily disrupt learning stability, leading to inaccurate keypoint modeling. In this paper, we design a new grid keypoint learning framework, aiming at a robust and explainable intermediate keypoint representation for long-term efficient video prediction. We have two major technical contributions. First, we detect keypoints by jumping among candidate locations in our raised grid space and formulate a condensation loss to encourage meaningful keypoints with strong representative capability. Second, we introduce a 2D binary map to represent the detected grid keypoints and then suggest propagating keypoint locations with stochasticity by selecting entries in the discrete grid space, thus preserving the spatial structure of keypoints in the long-term horizon for better future frame generation. Extensive experiments verify that our method outperforms the state-of-the-art stochastic video prediction methods while saves more than 98% of computing resources. We also demonstrate our method on a robotic-assisted surgery dataset with promising results. Our code is available at https://github.com/xjgaocs/Grid-Keypoint-Learning.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636874,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636874,,Training;Robot kinematics;Computational modeling;Stochastic processes;Surgery;Prediction methods;Power system stability,,8.0,,31,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/xjgaocs/Grid-Keypoint-Learning,https://github.com/xjgaocs/Grid-Keypoint-Learning
489,What’s in My LiDAR Odometry Toolbox?,P. Dellenbach; J. -E. Deschaud; B. Jacquet; F. Goulette,"Centre for Robotics, MINES ParisTech, PSL University, Paris, France; Centre for Robotics, MINES ParisTech, PSL University, Paris, France; Computer Vision Team, Kitware, Villeurbanne, France; Centre for Robotics, MINES ParisTech, PSL University, Paris, France",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,4429,4436,"With the democratization of 3D LiDAR sensors, precise LiDAR odometries and SLAM are in high demand. New methods regularly appear, proposing solutions ranging from small variations in classical algorithms to radically new paradigms based on deep learning. Yet it is often difficult to compare these methods, notably due to the few datasets on which the methods can be evaluated and compared. Furthermore, their weaknesses are rarely examined, often letting the user discover the hard way whether a method would be appropriate for a use case.In this paper, we review and organize the main 3D LiDAR odometries into distinct categories. We implemented several approaches (geometric based, deep learning based, and hybrid methods) to conduct an in-depth analysis of their strengths and weaknesses on multiple datasets, guiding the reader through the different LiDAR odometries available. Implementation of the methods has been made publicly available at: https://github.com/Kitware/pyLiDAR-SLAM.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636348,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636348,,Deep learning;Laser radar;Three-dimensional displays;Simultaneous localization and mapping;Pipelines;Distance measurement;Sensors,,4.0,,21,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/Kitware/pyLiDAR-SLAM,https://github.com/Kitware/pyLiDAR-SLAM
490,Local Memory Attention for Fast Video Semantic Segmentation,M. Paul; M. Danelljan; L. V. Gool; R. Timofte,"Vision Lab, ETH Zürich, Switzerland; Vision Lab, ETH Zürich, Switzerland; Vision Lab, ETH Zürich, Switzerland; Vision Lab, ETH Zürich, Switzerland",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,1102,1109,"We propose a novel neural network module that transforms an existing single-frame semantic segmentation model into a video semantic segmentation pipeline. In contrast to prior works, we strive towards a simple, fast, and general module that can be integrated into virtually any single-frame architecture. Our approach aggregates a rich representation of the semantic information in past frames into a memory module. Information stored in the memory is then accessed through an attention mechanism. In contrast to previous memory-based approaches, we propose a fast local attention layer, providing temporal appearance cues in the local region of prior frames. We further fuse these cues with an encoding of the current frame through a second attention-based module. The segmentation decoder processes the fused representation to predict the final semantic segmentation. We integrate our approach into two popular semantic segmentation networks: ERFNet and PSPNet. We observe an improvement in segmentation performance on Cityscapes by 1.7% and 2.1% in mIoU respectively, while increasing inference time of ERFNet by only 1.5ms. Source code is available at https://github.com/mattpfr/lmanet.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636192,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636192,,Fuses;Aggregates;Semantics;Pipelines;Neural networks;Transforms;Memory modules,,22.0,,40,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/mattpfr/lmanet,https://github.com/mattpfr/lmanet
491,Score refinement for confidence-based 3D multi-object tracking,N. Benbarka; J. Schröder; A. Zell,"cognitive systems group, University of Tübingen, Tübingen, Germany; cognitive systems group, University of Tübingen, Tübingen, Germany; cognitive systems group, University of Tübingen, Tübingen, Germany",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,8083,8090,"Multi-object tracking is a critical component in autonomous navigation, as it provides valuable information for decision-making. Many researchers tackled the 3D multi-object tracking task by filtering out the frame-by-frame 3D detections; however, their focus was mainly on finding useful features or proper matching metrics. Our work focuses on a neglected part of the tracking system: score refinement and tracklet termination. We show that manipulating the scores depending on time consistency while terminating the tracklets depending on the tracklet score improves tracking results. We do this by increasing the matched tracklets’ score with score update functions and decreasing the unmatched tracklets’ score. Compared to count-based methods, our method consistently produces better AMOTA and MOTA scores when utilizing various detectors and filtering algorithms on different datasets. The improvements in AMOTA score went up to 1.83 and 2.96 in MOTA. We also used our method as a late-fusion ensembling method, and it performed better than voting-based ensemble methods by a solid margin. It achieved an AMOTA score of 67.6 on nuScenes test evaluation, which is comparable to other state-of-the-art trackers. Code is publicly available at: https://github.com/cogsys-tuebingen/CBMOT.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636032,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636032,,Measurement;Three-dimensional displays;Filtering;Pipelines;Detectors;Filtering algorithms;Solids,,41.0,,31,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/cogsys-tuebingen/CBMOT,https://github.com/cogsys-tuebingen/CBMOT
492,Soft Manipulator Fault Detection and Identification Using ANC-based LSTM,H. Gu; H. Hu; H. Wang; W. Chen,"Department of Automation, Insititue of Medical Robotics, Key Laboratory of System Control and Information Processing of Ministry of Education, Key Laboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Insititue of Medical Robotics, Key Laboratory of System Control and Information Processing of Ministry of Education, Key Laboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Insititue of Medical Robotics, Key Laboratory of System Control and Information Processing of Ministry of Education, Key Laboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Insititue of Medical Robotics, Key Laboratory of System Control and Information Processing of Ministry of Education, Key Laboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai Jiao Tong University, Shanghai, China",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,1702,1707,"Timely fault detection and identification (FDI) of soft manipulators are critical in the design of surgical systems to improve reliability. However, due to the intrinsic compliance of soft manipulators, their end effectors vibrate during the dynamic control process, which introduces noise into the measured signals and makes FDI of soft manipulators challenging. This paper proposes a novel method to accomplish these tasks based on Long Short Term Memory (LSTM) recurrent neural network. Based on LSTM network, a new Attention-based Noise Compensation (ANC) module is proposed to enable the network to filter the noise merged with signals input in a self-supervision manner. Moreover, weighted cross entropy loss is introduced to balance the normal and faulty samples in the training set. Of the 9930 samples presented to the model, 9489 are correctly diagnosed in less than 1.0 second, which implies that the method can learn the spatial and temporal dependence of the signals and distinguish the healthy modes from the faulty ones. Finally, we compare the ANC-based method with the vanilla LSTM method and the state-of-art Bruin et al. method. From the comparison, we conclude that the ANC-based method proposed in this paper not only shortens the time cost of the FDI process but also suppresses the sensitivity of diagnosis results to noise. Source code, pre-trained models and dataset are available on https://github.com/IRMVLab/ANC-LSTM-fault-detection.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636868,Shanghai Education Development Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636868,,Training;Fault diagnosis;Costs;Fault detection;Surgery;Vibration measurement;Entropy,,5.0,,18,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/IRMVLab/ANC-LSTM-fault-detection,https://github.com/IRMVLab/ANC-LSTM-fault-detection
493,Part-Aware Data Augmentation for 3D Object Detection in Point Cloud,J. Choi; Y. Song; N. Kwak,"Graduate School of Convergence Science and Technology, Seoul National University, Seoul, South Korea; Graduate School of Convergence Science and Technology, Seoul National University, Seoul, South Korea; Graduate School of Convergence Science and Technology, Seoul National University, Seoul, South Korea",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,3391,3397,"Data augmentation has greatly contributed to improving the performance in image recognition tasks, and a lot of related studies have been conducted. However, data augmentation on 3D point cloud data has not been much explored. 3D label has more sophisticated and rich structural information than the 2D label, so it enables more diverse and effective data augmentation. In this paper, we propose part-aware data augmentation (PA-AUG) that can better utilize rich information of 3D label to enhance the performance of 3D object detectors. PA-AUG divides objects into partitions and stochastically applies five augmentation methods to each local region. It is compatible with existing point cloud data augmentation methods and can be used universally regardless of the detector’s architecture. PA-AUG has improved the performance of state-of-the-art 3D object detector for all classes of the KITTI dataset and has the equivalent effect of increasing the train data by about 2.5×. We also show that PA-AUG not only increases performance for a given dataset but also is robust to corrupted data. The code is available at https://github.com/sky77764/pa-aug.pytorch",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9635887,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635887,,Point cloud compression;Three-dimensional displays;Image recognition;Semantics;Detectors;Object detection;Robustness,,25.0,,31,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/sky77764/pa-aug,https://github.com/sky77764/pa-aug
494,Graph-based Task-specific Prediction Models for Interactions between Deformable and Rigid Objects,Z. Weng; F. Paus; A. Varava; H. Yin; T. Asfour; D. Kragic,"CAS/RPL, KTH, Royal Institute of Technology, Stocholm, Sweden; Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Karlsruhe, Germany; CAS/RPL, KTH, Royal Institute of Technology, Stocholm, Sweden; CAS/RPL, KTH, Royal Institute of Technology, Stocholm, Sweden; Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Karlsruhe, Germany; CAS/RPL, KTH, Royal Institute of Technology, Stocholm, Sweden",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,5741,5748,"Capturing scene dynamics and predicting the future scene state is challenging but essential for robotic manipulation tasks, especially when the scene contains both rigid and deformable objects. In this work, we contribute a simulation environment and generate a novel dataset for task-specific manipulation, involving interactions between rigid objects and a deformable bag. The dataset incorporates a rich variety of scenarios including different object sizes, object numbers and manipulation actions. We approach dynamics learning by proposing an object-centric graph representation and two modules which are Active Prediction Module (APM) and Position Prediction Module (PPM) based on graph neural networks with an encode-process-decode architecture. At the inference stage, we build a two-stage model based on the learned modules for single time step prediction. We combine modules with different prediction horizons into a mixed-horizon model which addresses long-term prediction. In an ablation study, we show the benefits of the two-stage model for single time step prediction and the effectiveness of the mixed-horizon model for long-term prediction tasks. Supplementary material is available at https://github.com/wengzehang/deformable_rigid_interaction_prediction",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636660,Deutsche Forschungsgemeinschaft; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636660,,Deformable models;Shape;Training data;Predictive models;Data models;Graph neural networks;Task analysis,,12.0,,28,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/wengzehang/deformable_rigid_interaction_prediction,https://github.com/wengzehang/deformable_rigid_interaction_prediction
495,A Simple and Efficient Multi-task Network for 3D Object Detection and Road Understanding,D. Feng; Y. Zhou; C. Xu; M. Tomizuka; W. Zhan,"Mechanical Systems Control Lab, University of California, Berkeley, CA, USA; Mechanical Systems Control Lab, University of California, Berkeley, CA, USA; Mechanical Systems Control Lab, University of California, Berkeley, CA, USA; Mechanical Systems Control Lab, University of California, Berkeley, CA, USA; Mechanical Systems Control Lab, University of California, Berkeley, CA, USA",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,7067,7074,"Detecting dynamic objects and predicting static road information such as drivable areas and ground heights are crucial for safe autonomous driving. Previous works studied each perception task separately, and lacked a collective quantitative analysis. In this work, we show that it is possible to perform all perception tasks via a simple and efficient multi-task network. Our proposed network, LidarMTL, takes raw LiDAR point cloud as inputs, and predicts six perception outputs for 3D object detection and road understanding. The network is based on an encoder-decoder architecture with 3D sparse convolution and deconvolution operations. Extensive experiments verify the proposed method with competitive accuracies compared to state-of-the-art object detectors and other task-specific networks. LidarMTL is also leveraged for online localization. Code and pre-trained model have been made available at https://github.com/frankfengdi/LidarMTL.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9635858,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635858,,Point cloud compression;Location awareness;Three-dimensional displays;Laser radar;Statistical analysis;Roads;Object detection,,22.0,,42,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/frankfengdi/LidarMTL,https://github.com/frankfengdi/LidarMTL
496,COCOI: Contact-aware Online Context Inference for Generalizable Non-planar Pushing,Z. Xu; W. Yu; A. Herzog; W. Lu; C. Fu; M. Tomizuka; Y. Bai; C. K. Liu; D. Ho,"Everyday Robots, X The Moonshot Factory, Mountain View, CA, USA; Robotics at Google, Mountain View, CA, USA; Everyday Robots, X The Moonshot Factory, Mountain View, CA, USA; Everyday Robots, X The Moonshot Factory, Mountain View, CA, USA; Everyday Robots, X The Moonshot Factory, Mountain View, CA, USA; University of California, Berkeley, Berkeley, CA, USA; Everyday Robots, X The Moonshot Factory, Mountain View, CA, USA; Stanford University, Stanford, CA, USA; Everyday Robots, X The Moonshot Factory, Mountain View, CA, USA",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,176,182,"General contact-rich manipulation problems are long-standing challenges in robotics due to the difficulty of understanding complicated contact physics. Deep reinforcement learning (RL) has shown great potential in solving robot manipulation tasks. However, existing RL policies have limited adaptability to environments with diverse dynamics properties, which is pivotal in solving many contact-rich manipulation tasks. In this work, we propose Contact-aware Online COntext Inference (COCOI), a deep RL method that encodes a context embedding of dynamics properties online using contact-rich interactions. We sample sensor data using a novel contact-aware strategy and formulate an interpretable dynamics transition module. We study this method based on a novel and challenging non-planar pushing task, where the robot uses a monocular camera image and wrist force torque sensor reading to push an object to a goal location while keeping it upright. We run extensive experiments to demonstrate the capability of COCOI in a wide range of settings and dynamics properties in simulation, and also in a sim-to-real transfer scenario on a real robot (Webpage: https://context-inference.github.io/).",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636836,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636836,,Wrist;Torque;Dynamics;Robot vision systems;Reinforcement learning;History;Task analysis,,6.0,,32,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,,
497,Robust SLAM Systems: Are We There Yet?,M. Bujanca; X. Shi; M. Spear; P. Zhao; B. Lennox; M. Luján,"The University of Manchester, Manchester, UK; Intel Labs China, Beijing, China; The University of Manchester, Manchester, UK; Intel Labs China, Beijing, China; The University of Manchester, Manchester, UK; The University of Manchester, Manchester, UK",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,5320,5327,"Progress in the last decade has brought about significant improvements in the accuracy and speed of SLAM systems, broadening their mapping capabilities. Despite these advancements, long-term operation remains a major challenge, primarily due to the wide spectrum of perturbations robotic systems may encounter.Increasing the robustness of SLAM algorithms is an ongoing effort, however it usually addresses a specific perturbation. Generalisation of robustness across a large variety of challenging scenarios is not well-studied nor understood. This paper presents a systematic evaluation of the robustness of open-source state-of-the-art SLAM algorithms with respect to challenging conditions such as fast motion, non-uniform illumination, and dynamic scenes. The experiments are performed with perturbations present both independently of each other, as well as in combination in long-term deployment settings in unconstrained environments (lifelong operation).The detailed results (approx. 20,000 experiments) along with comprehensive documentation of the benchmarking tool for integrating new datasets and evaluating SLAM algorithms not studied in this work are available at https://robustslam.github.io/evaluation.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636814,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636814,,Simultaneous localization and mapping;Systematics;Three-dimensional displays;Heuristic algorithms;Perturbation methods;Dynamics;Lighting,,22.0,,60,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://robustslam.github.io/evaluation,https://github.com/pmarsceill/just-the-docs
498,Towards Robust Monocular Visual Odometry for Flying Robots on Planetary Missions,M. Wudenka; M. G. Müller; N. Demmel; A. Wedler; R. Triebel; D. Cremers; W. Stürzl,"Institute of Robotics and Mechatronics, German Aerospace Center (DLR); Institute of Robotics and Mechatronics, German Aerospace Center (DLR); Department of Informatics, Computer Vision Group, Technical University of Munich, Germany; Institute of Robotics and Mechatronics, German Aerospace Center (DLR); Institute of Robotics and Mechatronics, German Aerospace Center (DLR); Department of Informatics, Computer Vision Group, Technical University of Munich, Germany; Institute of Robotics and Mechatronics, German Aerospace Center (DLR)",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,8737,8744,"In the future, extraterrestrial expeditions will not only be conducted by rovers but also by flying robots. The technical demonstration drone Ingenuity, that just landed on Mars, will mark the beginning of a new era of exploration unhindered by terrain traversability. Robust self-localization is crucial for that. Cameras that are lightweight, cheap and information-rich sensors are already used to estimate the ego-motion of vehicles. However, methods proven to work in man-made environments cannot simply be deployed on other planets. The highly repetitive textures present in the wastelands of Mars pose a huge challenge to descriptor matching based approaches.In this paper, we present an advanced robust monocular odometry algorithm that uses efficient optical flow tracking to obtain feature correspondences between images and a refined keyframe selection criterion. In contrast to most other approaches, our framework can also handle rotation-only motions that are particularly challenging for monocular odometry systems. Furthermore, we present a novel approach to estimate the current risk of scale drift based on a principal component analysis of the relative translation information matrix. This way we obtain an implicit measure of uncertainty. We evaluate the validity of our approach on all sequences of a challenging real-world dataset captured in a Mars-like environment and show that it outperforms state-of-the-art approaches. The source code is publicly available at: https://github.com/DLR-RM/granit.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636844,Helmholtz Association; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636844,,Mars;Uncertainty;Tracking;Robot vision systems;Refining;Robustness;Trajectory,,6.0,,21,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/DLR-RM/granit,https://github.com/DLR-RM/granit
499,Few-leaf Learning: Weed Segmentation in Grasslands,R. Güldenring; E. Boukas; O. Ravn; L. Nalpantidis,"Department of Electrical Engineering, DTU - Technical University of Denmark, Kgs. Lyngby, Denmark; Department of Electrical Engineering, DTU - Technical University of Denmark, Kgs. Lyngby, Denmark; Department of Electrical Engineering, DTU - Technical University of Denmark, Kgs. Lyngby, Denmark; Department of Electrical Engineering, DTU - Technical University of Denmark, Kgs. Lyngby, Denmark",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,3248,3254,"Autonomous robotic weeding in grasslands requires robust weed segmentation. Deep learning models can provide solutions to this problem, but they need to be trained on large amounts of images, which in the case of grasslands are notoriously difficult to obtain and manually annotate. In this work we introduce Few-leaf Learning, a concept that facilitates the training of accurate weed segmentation models and can lead to easier generation of weed segmentation datasets with minimal human annotation effort. Our approach builds upon the fact that each plant species within the same field has relatively uniform visual characteristics due to similar environmental influences. Thus, we can train a field-and-day-specific weed segmentation model on synthetic training data stemming from just a handful of annotated weed leaves. We demonstrate the efficacy of our approach for different fields and for two common grassland weeds: Rumex obtusifolius (broad-leaved dock) and Cirsium vulgare (spear thistle). Our code is publicly available at https://github.com/RGring/WeedAnnotator.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636770,European GNSS Agency; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636770,,Training;Deep learning;Image segmentation;Visualization;Computer vision;Annotations;Pipelines,,9.0,,36,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/RGring/WeedAnnotator,https://github.com/RGring/WeedAnnotator
500,Stereo Waterdrop Removal with Row-wise Dilated Attention,Z. Shi; N. Fan; D. -Y. Yeung; Q. Chen,"Department of Computer Science and Engineering, HKUST; Department of Computer Science and Engineering, HKUST; Department of Computer Science and Engineering, HKUST; Department of Computer Science and Engineering, HKUST",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,3829,3836,"Existing vision systems for autonomous driving or robots are sensitive to waterdrops adhered to windows or camera lenses. Most recent waterdrop removal approaches take a single image as input and often fail to recover the missing content behind waterdrops faithfully. Thus, we propose a learning-based model for waterdrop removal with stereo images. To better detect and remove waterdrops from stereo images, we propose a novel row-wise dilated attention module to enlarge attention’s receptive field for effective information propagation between the two stereo images. In addition, we propose an attention consistency loss between the ground-truth disparity map and attention scores to enhance the left-right consistency in stereo images. Because of related datasets’ unavailability, we collect a real-world dataset that contains stereo images with and without waterdrops. Extensive experiments on our dataset suggest that our model outperforms state-of-the-art methods both quantitatively and qualitatively. Our source code and the stereo waterdrop dataset are available at https://github.com/VivianSZF/Stereo-Waterdrop-Removal.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636216,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636216,,Codes;Machine vision;Robot vision systems;Cameras;Propagation losses;Task analysis;Intelligent robots,,4.0,,46,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/VivianSZF/Stereo-Waterdrop-Removal,https://github.com/VivianSZF/Stereo-Waterdrop-Removal
501,DiGNet: Learning Scalable Self-Driving Policies for Generic Traffic Scenarios with Graph Neural Networks,P. Cai; H. Wang; Y. Sun; M. Liu,"ECE, the HKUST; ECE, the HKUST; ME, PolyU of HK; ECE, the HKUST",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,8979,8984,"Traditional decision and planning frameworks for self-driving vehicles (SDVs) scale poorly in new scenarios, thus they require tedious hand-tuning of rules and parameters to maintain acceptable performance in all foreseeable cases. Recently, self-driving methods based on deep learning have shown promising results with better generalization capability but less hand engineering effort. However, most of the previous learning-based methods are trained and evaluated in limited driving scenarios with scattered tasks, such as lane-following, autonomous braking, and conditional driving. In this paper, we propose a graph-based deep network to achieve scalable self-driving that can handle massive traffic scenarios. Specifically, more than 7,000 km of evaluation is conducted in a high-fidelity driving simulator, in which our method can obey the traffic rules and safely navigate the vehicle in a large variety of urban, rural, and highway environments, including unprotected left turns, narrow roads, roundabouts, and pedestrian-rich intersections. Demonstration videos are available at https: //caipeide.github.io/dignet/.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636376,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636376,,Learning systems;Deep learning;Navigation;Roads;Graph neural networks;Planning;Task analysis,,9.0,,22,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://caipeide.github.io/dignet,
502,Correlate-and-Excite: Real-Time Stereo Matching via Guided Cost Volume Excitation,A. Bangunharcana; J. W. Cho; S. Lee; I. S. Kweon; K. -S. Kim; S. Kim,"Mechatronics, Systems and Control Laboratory, KAIST, Daejeon, Republic of Korea; Robotics and Computer Vision Laboratory, KAIST, Daejeon, Republic of Korea; Robotics and Computer Vision Laboratory, KAIST, Daejeon, Republic of Korea; Robotics and Computer Vision Laboratory, KAIST, Daejeon, Republic of Korea; Mechatronics, Systems and Control Laboratory, KAIST, Daejeon, Republic of Korea; Mechatronics, Systems and Control Laboratory, KAIST, Daejeon, Republic of Korea",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,3542,3548,"Volumetric deep learning approach towards stereo matching aggregates a cost volume computed from input left and right images using 3D convolutions. Recent works showed that utilization of extracted image features and a spatially varying cost volume aggregation complements 3D convolutions. However, existing methods with spatially varying operations are complex, cost considerable computation time, and cause memory consumption to increase. In this work, we construct Guided Cost volume Excitation (GCE) and show that simple channel excitation of cost volume guided by image can improve performance considerably. Moreover, we propose a novel method of using top-k selection prior to soft-argmin disparity regression for computing the final disparity estimate. Combining our novel contributions, we present an end-to-end network that we call Correlate-and-Excite (CoEx). Extensive experiments of our model on the SceneFlow, KITTI 2012, and KITTI 2015 datasets demonstrate the effectiveness and efficiency of our model and show that our model outperforms other speed-based algorithms while also being competitive to other state-of-the-art algorithms. Codes will be made available at https://github.com/antabangun/coex.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9635909,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635909,,Convolutional codes;Deep learning;Solid modeling;Costs;Three-dimensional displays;Computational modeling;Memory management,,43.0,,39,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/antabangun/coex,https://github.com/antabangun/coex
503,RaP-Net: A Region-wise and Point-wise Weighting Network to Extract Robust Features for Indoor Localization,D. Li; J. Miao; X. Shi; Y. Tian; Q. Long; T. Cai; P. Guo; H. Yu; W. Yang; H. Yue; Q. Wei; F. Qiao,"Beijing Jiaotong University, Beijing, China; Beihang University, Beijing, China; Intel Labs China, Beijing, China; Beihang University, Beijing, China; Beijing Jiaotong University, Beijing, China; Shanghai Jiao Tong University, China; Intel Labs China, Beijing, China; Tsinghua University, Beijing, China; Beijing Jiaotong University, Beijing, China; Beihang University, Beijing, China; Tsinghua University, Beijing, China; Tsinghua University, Beijing, China",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,1331,1338,"Feature extraction plays an important role in visual localization. Unreliable features on dynamic objects or repetitive regions will interfere with feature matching and challenge indoor localization greatly. To address the problem, we propose a novel network, RaP-Net, to simultaneously predict region-wise invariability and point-wise reliability, and then extract features by considering both of them. We also introduce a new dataset, named OpenLORIS-Location, to train the proposed network. The dataset contains 1553 images from 93 indoor locations. Various appearance changes between images of the same location are included and can help the model to learn the invariability in typical indoor scenes. Experimental results show that the proposed RaP-Net trained with OpenLORIS-Location dataset achieves excellent performance in the feature matching task and significantly outperforms state-of-the-arts feature algorithms in indoor localization. The RaPNet code and dataset are available at https://github.com/ivipsourcecode/RaP-Net.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636248,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636248,,Location awareness;Training;Visualization;Pose estimation;Lighting;Interference;Feature extraction,,3.0,,42,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/ivipsourcecode/RaP-Net,https://github.com/ivipsourcecode/RaP-Net
504,A Hierarchical Framework for Quadruped Locomotion Based on Reinforcement Learning,W. Tan; X. Fang; W. Zhang; R. Song; T. Chen; Y. Zheng; Y. Li,"School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; Tencent Robotics X Lab, Shenzhen, China; School of Control Science and Engineering, Shandong University, Jinan, China",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,8462,8468,"Quadruped locomotion is a challenging task for learning-based algorithms. It requires tedious manual tuning and is difficult to deploy in reality due to the reality gap. In this paper, we propose a quadruped robot learning system for agile locomotion which does not require any pre-training and works well in various real-world terrains. We introduce a hierarchical learning framework that uses reinforcement learning as the high-level policy to adjust the low-level trajectory generator for better adaptability to the terrain. We compact the observation and action space of the reinforcement learning to deploy it on a host computer in reality. Besides, we design a trajectory generator guided by robot posture, which can generate adaptive foot trajectory to interact with the environment. Experimental results show that our system can be easily deployed in reality while only trained in simulation, and also has the advantages of fast convergence and good terrain adaptability. The supplementary video demonstration is available at https://vsislab.github.io/hfql/.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636757,Research and Development; National Natural Science Foundation of China; Natural Science Foundation of Shandong Province; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636757,,Adaptation models;Reinforcement learning;Manuals;Robot learning;Generators;Trajectory;Quadrupedal robots,,11.0,,31,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://vsislab.github.io/hfql,
505,Semantic Tracklets: An Object-Centric Representation for Visual Multi-Agent Reinforcement Learning,I. -J. Liu; Z. Ren; R. A. Yeh; A. G. Schwing,"University of Illinois at Urbana-Champaign, IL, USA; University of Illinois at Urbana-Champaign, IL, USA; University of Illinois at Urbana-Champaign, IL, USA; University of Illinois at Urbana-Champaign, IL, USA",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,5603,5610,"Solving complex real-world tasks, e.g., autonomous fleet control, often involves a coordinated team of multiple agents which learn strategies from visual inputs via reinforcement learning. Many existing multi-agent reinforcement learning (MARL) algorithms however don’t scale to environments where agents operate on visual inputs. To address this issue, algorithmically, recent works have focused on non-stationarity and exploration. In contrast, we study whether scalability can also be achieved via a disentangled representation. For this, we explicitly construct an object-centric intermediate representation to characterize the states of an environment, which we refer to as ‘semantic tracklets.’ We evaluate ‘semantic tracklets’ on the visual multi-agent particle environment (VMPE) and on the challenging visual multi-agent GFootball environment. ‘Semantic tracklets’ consistently outperform baselines on VMPE, and achieve a +2.4 higher score difference than baselines on GFootball. Notably, this method is the first to successfully learn a strategy for five players in the GFootball environment using only visual data. For more, please see our project page: https://ioujenliu.github.io/SemanticTracklets",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636592,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636592,,Visualization;Scalability;Semantics;Reinforcement learning;Task analysis;Intelligent robots,,5.0,,41,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://ioujenliu.github.io/SemanticTracklets,
506,Spatial Action Maps Augmented with Visit Frequency Maps for Exploration Tasks,Z. Wang; N. Papanikolopoulos,"Minnesota Robotics Institute (MnRI), University of Minnesota, Minneapolis, MN, USA; Faculty of the Department of Computer Science and Engineering, University of Minnesota, Minneapolis, MN, USA",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,3175,3181,"Reinforcement learning has been widely applied in exploration, navigation, manipulation, and other fields. Most of the relevant techniques generate kinematic commands (e.g., move, stop, turn) for agents based on the current state information. However, recent dense action representations based research, such as spatial action maps, pointing way-points to the agent in the same domain as its observation of the state shows great promise in mobile manipulation tasks. Inspired by that, we make the first step towards using a spatial action maps based method to effectively explore novel environmental spaces. To reduce the chance of redundant exploration, the visit frequency map (VFM) and its corresponding reward function are introduced to direct the agent to actively search previously unexplored areas. In the experimental section, our work was compared to the same method without VFM and the method based on traditional steering commands with the same input data in various environments. The results show conclusively that our method is more efficient than other methods. The project page is: https://github.com/zxwang96/sam-exploration",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636813,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636813,,Training;Navigation;Reinforcement learning;Kinematics;Frequency diversity;Space exploration;Task analysis,,3.0,,26,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/zxwang96/sam-exploration,https://github.com/zxwang96/sam-exploration
507,Semantic Segmentation-assisted Scene Completion for LiDAR Point Clouds,X. Yang; H. Zou; X. Kong; T. Huang; Y. Liu; W. Li; F. Wen; H. Zhang,"Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Huawei Noah’s Ark Lab, Beijing, China; Huawei Noah’s Ark Lab, Beijing, China; Huawei Noah’s Ark Lab, Beijing, China",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,3555,3562,"Outdoor scene completion is a challenging issue in 3D scene understanding, which plays an important role in intelligent robotics and autonomous driving. Due to the sparsity of LiDAR acquisition, it is far more complex for 3D scene completion and semantic segmentation. Since semantic features can provide constraints and semantic priors for completion tasks, the relationship between them is worth exploring. Therefore, we propose an end-to-end semantic segmentation-assisted scene completion network, including a 2D completion branch and a 3D semantic segmentation branch. Specifically, the network takes a raw point cloud as input, and merges the features from the segmentation branch into the completion branch hierarchically to provide semantic information. By adopting BEV representation and 3D sparse convolution, we can benefit from the lower operand while maintaining effective expression. Besides, the decoder of the segmentation branch is used as an auxiliary, which can be discarded in the inference stage to save computational consumption. Extensive experiments demonstrate that our method achieves competitive performance on SemanticKITTI dataset with low latency. Code and models will be released at https://github.com/jokester-zzz/SSA-SC.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636662,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636662,,Point cloud compression;Three-dimensional displays;Laser radar;Convolution;Computational modeling;Semantics;Real-time systems,,23.0,,29,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/jokester-zzz/SSA-SC,https://github.com/jokester-zzz/SSA-SC
508,TemporalFusion: Temporal Motion Reasoning with Multi-Frame Fusion for 6D Object Pose Estimation,F. Mu; R. Huang; A. Luo; X. Li; J. Qiu; H. Cheng,"Center for Robotics, School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; Center for Robotics, School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; Center for Robotics, School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; Inception Institute of Artificial Intelligence, Dubai, The United Arab Emirates; School of Mechanical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Center for Robotics, School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,5930,5936,"6D object pose estimation is an essential task in vision-based robotic grasping and manipulation. Prior works extract spatial features by fusing the RGB image and depth without considering the temporal motion information, limiting their performance in heavy occlusion robotic grasping scenarios. In this paper, we present an end-to-end model named TemporalFusion, which integrates the temporal motion information from RGB-D images for 6D object pose estimation. The core of proposed TemporalFusion model is to embed and fuse the temporal motion information from multi-frame RGB-D sequences, which could handle heavy occlusion in robotic grasping tasks. Furthermore, the proposed deep model can also obtain stable pose sequences, which is essential for real-time robotic grasping tasks. We evaluated the proposed method in the YCB-Video dataset, and experimental results show our model outperforms state-of-the-art approaches. Our code is available at https://github.com/mufengjun260/TemporalFusion21.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636583,Research and Development; National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636583,,Training;Limiting;Fuses;Pose estimation;Grasping;Feature extraction;Cognition,,1.0,,39,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/mufengjun260/TemporalFusion21,https://github.com/mufengjun260/TemporalFusion21
509,Unknown Object Segmentation from Stereo Images,M. Durner; W. Boerdijk; M. Sundermeyer; W. Friedl; Z. -C. Márton; R. Triebel,"Institute of Robotics and Mechatronics, German Aerospace Center (DLR), Wessling, Germany; Institute of Robotics and Mechatronics, German Aerospace Center (DLR), Wessling, Germany; Institute of Robotics and Mechatronics, German Aerospace Center (DLR), Wessling, Germany; Institute of Robotics and Mechatronics, German Aerospace Center (DLR), Wessling, Germany; Agile Robots AG, Munich, Germany; Institute of Robotics and Mechatronics, German Aerospace Center (DLR), Wessling, Germany",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,4823,4830,"Although instance-aware perception is a key prerequisite for many autonomous robotic applications, most of the methods only partially solve the problem by focusing solely on known object categories. However, for robots interacting in dynamic and cluttered environments, this is not realistic and severely limits the range of potential applications. Therefore, we propose a novel object instance segmentation approach that does not require any semantic or geometric information of the objects beforehand. In contrast to existing works, we do not explicitly use depth data as input, but rely on the insight that slight viewpoint changes, which for example are provided by stereo image pairs, are often sufficient to determine object boundaries and thus to segment objects. Focusing on the versatility of stereo sensors, we employ a transformer-based architecture that maps directly from the pair of input images to the object instances. This has the major advantage that instead of a noisy, and potentially incomplete depth map as an input, on which the segmentation is computed, we use the original image pair to infer the object instances and a dense depth map. In experiments in several different application domains, we show that our Instance Stereo Transformer (INSTR) algorithm outperforms current state-of-the-art methods that are based on depth maps. Training code and pretrained models are available at https://github.com/DLR-RM/instr.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636281,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636281,,Training;Image segmentation;Correlation;Semantics;Focusing;Object segmentation;Transformers,,24.0,,61,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/DLR-RM/instr,https://github.com/DLR-RM/instr
510,Model-free Vehicle Tracking and State Estimation in Point Cloud Sequences,Z. Pang; Z. Li; N. Wang,TuSimple; TuSimple; TuSimple,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,8075,8082,"Estimating the states of surrounding traffic participants stays at the core of autonomous driving. In this paper, we study a novel setting of this problem: model-free single-object tracking (SOT), which takes the object state in the first frame as input, and jointly solves state estimation and tracking in subsequent frames. The main purpose for this new setting is to break the strong limitation of the popular ""detection and tracking"" scheme in multi-object tracking. Moreover, we notice that shape completion by overlaying the point clouds, which is a by-product of our proposed task, not only improves the performance of state estimation but also has numerous applications. As no benchmark for this task is available so far, we construct a new dataset LiDAR-SOT and corresponding evaluation protocols based on the Waymo Open dataset [29]. We then propose an optimization-based algorithm called SOTracker involving point cloud registration, vehicle shapes, correspondence, and motion priors. Our quantitative and qualitative results prove the effectiveness of our SOTracker and reveal the challenging cases for SOT in point clouds, including the sparsity of LiDAR data, abrupt motion variation, etc. Finally, we also explore how the proposed task and algorithm may benefit other autonomous driving applications, including simulating LiDAR scans, generating motion data, and annotating optical flow. The code and protocols for our benchmark and algorithm are available at https://github.com/TuSimple/LiDAR_SOT/. A video demonstration is at https://www.youtube.com/watch?v=BpHixKs91i8.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636202,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636202,,Point cloud compression;Laser radar;Target tracking;Protocols;Shape;Benchmark testing;State estimation,,20.0,,41,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/TuSimple/LiDAR_SOT,https://github.com/TuSimple/LiDAR_SOT
511,Team Orienteering Coverage Planning with Uncertain Reward,B. Liu; X. Xiao; P. Stone,"Department of Computer Science, University of Texas at Austin, Austin, TX; Department of Computer Science, University of Texas at Austin, Austin, TX; Department of Computer Science, University of Texas at Austin, Austin, TX",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,9728,9733,"Many municipalities and large organizations have fleets of vehicles that need to be coordinated for tasks such as garbage collection or infrastructure inspection. Motivated by this need, this paper focuses on the common subproblem in which a team of vehicles needs to plan coordinated routes to patrol an area over iterations while minimizing temporally and spatially dependent costs. In particular, at a specific location (e.g., a vertex on a graph), we assume the cost accumulates over time and its growth rate is a random variable with a fixed but unknown mean, and the cost is reset to zero whenever any vehicle visits the vertex (representing the robot ""servicing"" the vertex). We formulate this problem in graph terminology and call it Team Orienteering Coverage Planning with Uncertain Reward (TOCPUR). We propose to solve TOCPUR by simultaneously estimating the accumulated cost at every vertex on the graph and solving a novel variant of the Team Orienteering Problem (TOP) iteratively, which we call the Team Orienteering Coverage Problem (TOCP). We provide the first mixed integer programming formulation for the TOCP, as a significant adaptation of the original TOP. We introduce a new benchmark consisting of hundreds of randomly generated graphs for comparing different methods. We show the proposed solution outperforms both the exact TOP solution and a greedy algorithm. In addition, we provide a demo of our method on a team of three physical robots in a real-world environment. The code is publicly available at https://github.com/Cranial-XIX/TOCPUR.git.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636288,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636288,,Integer programming;Maximum likelihood estimation;Costs;Terminology;Robot kinematics;Organizations;Planning,,8.0,,10,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/Cranial-XIX/TOCPUR,https://github.com/Cranial-XIX/TOCPUR
512,The Reasonable Crowd: Towards evidence-based and interpretable models of driving behavior,B. Helou; A. Dusi; A. Collin; N. Mehdipour; Z. Chen; C. Lizarazo; C. Belta; T. Wongpiromsarn; R. D. Tebbens; O. Beijbom,"Motional, Boston, MA; Motional, Boston, MA; Motional, Boston, MA; Motional, Boston, MA; Motional, Boston, MA; Motional, Boston, MA; Motional, Boston, MA; Iowa State University, IA; Motional, Boston, MA; Motional, Boston, MA",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,6708,6715,"Autonomous vehicles must balance a complex set of objectives. There is no consensus on how they should do so, nor on a model for specifying a desired driving behavior. We created a dataset to help address some of these questions in a limited operating domain. The data consists of 92 traffic scenarios, with multiple ways of traversing each scenario. Multiple annotators expressed their preference between pairs of scenario traversals. We used the data to compare an instance of a rulebook [1], carefully hand-crafted independently of the dataset, with several interpretable machine learning models such as Bayesian networks, decision trees, and logistic regression trained on the dataset. To compare driving behavior, these models use scores indicating by how much different scenario traversals violate each of 14 driving rules. The rules are interpretable and designed by subject-matter experts. First, we found that these rules were enough for these models to achieve a high classification accuracy on the dataset. Second, we found that the rulebook provides high interpretability without excessively sacrificing performance. Third, the data pointed to possible improvements in the rulebook and the rules, and to potential new rules. Fourth, we explored the interpretability vs performance trade-off by also training non-interpretable models such as a random forest. Finally, we make the dataset publicly available to encourage a discussion from the wider community on behavior specification for AVs. Please find it at github.com/bassam-motional/Reasonable-Crowd.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9635960,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635960,,Training;Data models;Bayes methods;Regression tree analysis;Random forests;Intelligent robots;Autonomous vehicles,,9.0,,37,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/bassam-motional/Reasonable-Crowd,https://github.com/bassam-motional/Reasonable-Crowd
513,A Photorealistic Terrain Simulation Pipeline for Unstructured Outdoor Environments,M. G. Müller; M. Durner; A. Gawel; W. Stürzl; R. Triebel; R. Siegwart,"Institute of Robotics and Mechatronics, German Aerospace Center (DLR), Germany; Institute of Robotics and Mechatronics, German Aerospace Center (DLR), Germany; Autonomous Systems Lab, Swiss Federal Institute of Technology (ETH Zürich), Switzerland; Institute of Robotics and Mechatronics, German Aerospace Center (DLR), Germany; Institute of Robotics and Mechatronics, German Aerospace Center (DLR), Germany; Autonomous Systems Lab, Swiss Federal Institute of Technology (ETH Zürich), Switzerland",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,9765,9772,"Suitable datasets are an integral part of robotics research, especially for training neural networks in robot perception. However, in many domains, suitable real-world data are scarce and cannot be easily obtained. This problem is especially prevalent for unstructured outdoor environments, in particular, planetary ones. Recent advances in photorealistic simulations help researchers to simulate close-to-real data in many domains. Yet, there exists no high-quality synthetic data for planetary exploration tasks. Also, existing simulators lack the fidelity required for generating planetary data, which is inherently less structured than human environments. Synthetic planetary data requires careful modeling and annotation of many different terrain aspect and details, such as textures and distributions of rocks, to become a valuable test-bed for robotics. To fill this gap, we present a novel simulator specifically designed for the needs of planetary robotics visual tasks, but also applicable for other outdoor environments. Our simulator is capable of generating large varieties of (planetary) outdoor scenes with rich generation of meta data, such as multilevel semantic and instance annotations. To demonstrate the wide applicability of this new simulator, we evaluate its performance on typical robotics applications, i.e. semantic segmentation, instance segmentation, and visual SLAM. Our simulator is accessible under https://github.com/DLR-RM/oaisys.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636644,Helmholtz Association; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636644,,Training;Visualization;Simultaneous localization and mapping;Annotations;Semantics;Neural networks;Pipelines,,22.0,,39,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/DLR-RM/oaisys,https://github.com/DLR-RM/oaisys
514,SSC: Semantic Scan Context for Large-Scale Place Recognition,L. Li; X. Kong; X. Zhao; T. Huang; W. Li; F. Wen; H. Zhang; Y. Liu,"Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, P. R. China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, P. R. China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, P. R. China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, P. R. China; Huawei Noah’s Ark Lab, Beijing, China; Huawei Noah’s Ark Lab, Beijing, China; Huawei Noah’s Ark Lab, Beijing, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, P. R. China",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,2092,2099,"Place recognition gives a SLAM system the ability to correct cumulative errors. Unlike images that contain rich texture features, point clouds are almost pure geometric information which makes place recognition based on point clouds challenging. Existing works usually encode low-level features such as coordinate, normal, reflection intensity, etc., as local or global descriptors to represent scenes. Besides, they often ignore the translation between point clouds when matching descriptors. Different from most existing methods, we explore the use of high-level features, namely semantics, to improve the descriptor’s representation ability. Also, when matching descriptors, we try to correct the translation between point clouds to improve accuracy. Concretely, we propose a novel global descriptor, Semantic Scan Context, which explores semantic information to represent scenes more effectively. We also present a two-step global semantic ICP to obtain the 3D pose (x, y, yaw) used to align the point cloud to improve matching performance. Our experiments on the KITTI dataset show that our approach outperforms the state-of-the- art methods with a large margin. Our code is available at: https://github.com/lilin-hitcrt/SSC.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9635904,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635904,,Point cloud compression;Three-dimensional displays;Simultaneous localization and mapping;Image recognition;Codes;Art;Semantics,,63.0,,35,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/lilin-hitcrt/SSC,https://github.com/lilin-hitcrt/SSC
515,DMotion: Robotic Visuomotor Control with Unsupervised Forward Model Learned from Videos,H. Yuan; R. Wu; A. Zhao; H. Zhang; Z. Ding; H. Dong,"AIIT, Peking University; CFCS, CS Dept., Peking University; CFCS, CS Dept., Peking University; CFCS, CS Dept., Peking University; Princeton University; CFCS, CS Dept., Peking University",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,7135,7142,"Learning an accurate model of the environment is essential for model-based control tasks. Existing methods in robotic visuomotor control usually learn from data with heavily labelled actions, object entities or locations, which can be demanding in many cases. To cope with this limitation, we propose a method, dubbed DMotion, that trains a forward model from video data only, via disentangling the motion of controllable agent to model the transition dynamics. An object extractor and an interaction learner are trained in an end-to-end manner without supervision. The agent’s motions are explicitly represented using spatial transformation matrices containing physical meanings. In the experiments, DMotion achieves superior performance on learning an accurate forward model in a Grid World environment, as well as a more realistic robot control environment in simulation. With the accurate learned forward models, we further demonstrate their usage in model predictive control as an effective approach for robotic manipulations. Code, video and more materials are available at: https://hyperplane-lab.github.io/dmotion.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636362,National Natural Science Foundation of China; Research and Development; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636362,,Three-dimensional displays;Supervised learning;Robot control;Training data;Predictive models;Data models;Task analysis,,1.0,,50,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://hyperplane-lab.github.io/dmotion,https://github.com/hyperplane-lab/dmotion-code
516,Simultaneous Semantic and Collision Learning for 6-DoF Grasp Pose Estimation,Y. Li; T. Kong; R. Chu; Y. Li; P. Wang; L. Li,"Institute of Automation, Chinese Academy of Sciences, Beijing, China; Bytedance AI Lab; The Chinese University of Hong Kong; Bytedance AI Lab; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Bytedance AI Lab",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,3571,3578,"Grasping in cluttered scenes has always been a great challenge for robots, due to the requirement of the ability to well understand the scene and object information. Previous works usually assume that the geometry information of the objects is available, or utilize a step-wise, multi-stage strategy to predict the feasible 6-DoF grasp poses. In this work, we propose to formalize the 6-DoF grasp pose estimation as a simultaneous multi-task learning problem. In a unified framework, we jointly predict the feasible 6-DoF grasp poses, instance semantic segmentation, and collision information. The whole framework is jointly optimized and end-to-end differentiable. Our model is evaluated on large-scale benchmarks as well as the real robot system. On the public dataset, our method outperforms prior state-of-the-art methods by a large margin (+4.08 AP). We also demonstrate the implementation of our model on a real robotic platform and show that the robot can accurately grasp target objects in cluttered scenarios with a high success rate. Project link: https://openbyterobotics.github.io/sscl.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636012,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636012,,Geometry;Semantics;Pose estimation;Grasping;Multitasking;Collision avoidance;Task analysis,,40.0,,40,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://openbyterobotics.github.io/sscl,
517,AutoPhoto: Aesthetic Photo Capture using Reinforcement Learning,H. AlZayer; H. Lin; K. Bala,"Department of Computer Science, Cornell University, Ithaca, NY, USA; Department of Computer Science, Cornell University, Ithaca, NY, USA; Department of Computer Science, Cornell University, Ithaca, NY, USA",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,944,951,"The process of capturing a well-composed photo is difficult and it takes years of experience to master. We propose a novel pipeline for an autonomous agent to automatically capture an aesthetic photograph by navigating within a local region in a scene. Instead of classical optimization over heuristics such as the rule-of-thirds, we adopt a data-driven aesthetics estimator to assess photo quality. A reinforcement learning framework is used to optimize the model with respect to the learned aesthetics metric. We train our model in simulation with indoor scenes, and we demonstrate that our system can capture aesthetic photos in both simulation and real world environments on a ground robot. To our knowledge, this is the first system that can automatically explore an environment to capture an aesthetic photo with respect to a learned aesthetic estimator. Source code is at https://github.com/HadiZayer/AutoPhoto",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636788,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636788,,Photography;Measurement;Codes;Navigation;Pipelines;Reinforcement learning;Autonomous agents,,4.0,,38,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/HadiZayer/AutoPhoto,https://github.com/HadiZayer/AutoPhoto
518,Let’s Play for Action: Recognizing Activities of Daily Living by Learning from Life Simulation Video Games,A. Roitberg; D. Schneider; A. Djamal; C. Seibold; S. Reiß; R. Stiefelhagen,"Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Germany; Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Germany; Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Germany; Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Germany; Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Germany; Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Germany",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,8563,8569,"Recognizing Activities of Daily Living (ADL) is a vital process for intelligent assistive robots, but collecting large annotated datasets requires time-consuming temporal labeling and raises privacy concerns, e.g., if the data is collected in a real household. In this work, we explore the concept of constructing training examples for ADL recognition by playing life simulation video games and introduce the SIMS4ACTION dataset created with the popular commercial game THE SIMS 4. We build SIMS4ACTION by specifically executing actions-of-interest in a ""top-down"" manner, while the gaming circumstances allow us to freely switch between environments, camera angles and subject appearances. While ADL recognition on gaming data is interesting from the theoretical perspective, the key challenge arises from transferring it to the real-world applications, such as smart-homes or assistive robotics. To meet this requirement, SIMS4ACTION is accompanied with a GAMING→REAL benchmark, where the models are evaluated on real videos derived from an existing ADL dataset. We integrate two modern algorithms for video-based activity recognition in our framework, revealing the value of life simulation video games as an inexpensive and far less intrusive source of training data. However, our results also indicate that tasks involving a mixture of gaming and real data are challenging, opening a new research direction. We will make our dataset publicly available at https://github.com/aroitberg/sims4action.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636381,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636381,,Training;Adaptation models;Robot vision systems;Training data;Games;Switches;Benchmark testing,,19.0,,33,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/aroitberg/sims4action,https://github.com/aroitberg/sims4action
519,ViNet: Pushing the limits of Visual Modality for Audio-Visual Saliency Prediction,S. Jain; P. Yarlagadda; S. Jyoti; S. Karthik; R. Subramanian; V. Gandhi,"CVIT, KCIS, International Institute for Information Technology, Hyderabad; CVIT, KCIS, International Institute for Information Technology, Hyderabad; CVIT, KCIS, International Institute for Information Technology, Hyderabad; CVIT, KCIS, International Institute for Information Technology, Hyderabad; University of Canberra; CVIT, KCIS, International Institute for Information Technology, Hyderabad",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,3520,3527,"We propose the ViNet architecture for audio-visual saliency prediction. ViNet is a fully convolutional encoder-decoder architecture. The encoder uses visual features from a network trained for action recognition, and the decoder infers a saliency map via trilinear interpolation and 3D convolutions, combining features from multiple hierarchies. The overall architecture of ViNet is conceptually simple; it is causal and runs in real-time (60 fps). ViNet does not use audio as input and still outperforms the state-of-the-art audio-visual saliency prediction models on nine different datasets (three visual-only and six audio-visual datasets). ViNet also surpasses human performance on the CC, SIM and AUC metrics for the AVE dataset, and to our knowledge, it is the first model to do so. We also explore a variation of ViNet architecture by augmenting audio features into the decoder. To our surprise, upon sufficient training, the network becomes agnostic to the input audio and provides the same output irrespective of the input. Interestingly, we also observe similar behaviour in the previous state-of-the-art models [1] for audio-visual saliency prediction. Our findings contrast with previous works on deep learning-based audio-visual saliency prediction, suggesting a clear avenue for future explorations incorporating audio in a more effective manner. The code and pre-trained models are available at https://github.com/samyak0210/ViNet.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9635989,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635989,,Convolutional codes;Training;Visualization;Three-dimensional displays;Predictive models;Real-time systems;Decoding,,46.0,,54,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/samyak0210/ViNet,https://github.com/samyak0210/ViNet
520,PackerBot: Variable-Sized Product Packing with Heuristic Deep Reinforcement Learning,Z. Yang; S. Yang; S. Song; W. Zhang; R. Song; J. Cheng; Y. Li,"School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,5002,5008,"Product packing is a typical application in ware-house automation that aims to pick objects from unstructured piles and place them into bins with optimized placing policy. However, it still remains a significant challenge to finish the product packing tasks in general logistics scenarios where the objects are variable-sized and the configurations are complex. In this work, we present the PackerBot, a complete robotic pipeline for performing variable-sized product packing in unstructured scenes. First, by leveraging the imperfect experience of human packer, we propose a heuristic DRL framework for learning optimal online 3D bin packing policy. Then we integrate it with a 6-DoF suction-based picking module and a product size estimation module, leading to a complete product packing system, namely the PackerBot. Extensive experimental results show that our method achieves the state-of-the-art performance in both simulated and real-world tests. The video demonstration is available at: https://vsislab.github.io/packerbot.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9635914,Research and Development; National Natural Science Foundation of China; Natural Science Foundation of Shandong Province; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635914,,Three-dimensional displays;Automation;Pipelines;Buildings;Estimation;Reinforcement learning;Encoding,,15.0,,35,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://vsislab.github.io/packerbot,
521,Learning to Control an Unstable System with One Minute of Data: Leveraging Gaussian Process Differentiation in Predictive Control,I. D. J. Rodriguez; U. Rosolia; A. D. Ames; Y. Yue,"California Institute of Technology, Pasadena, USA; California Institute of Technology, Pasadena, USA; California Institute of Technology, Pasadena, USA; California Institute of Technology, Pasadena, USA",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,3896,3903,"We present a straightforward and efficient way to control unstable robotic systems using an estimated dynamics model. Specifically, we show how to exploit the differentiability of Gaussian Processes to create a state-dependent linearized approximation of the true continuous dynamics that can be integrated with model predictive control. Our approach is compatible with most Gaussian process approaches for system identification, and can learn an accurate model using modest amounts of training data. We validate our approach by learning the dynamics of an unstable system such as a segway with a 7-D state space and 2-D input space (using only one minute of data), and we show that the resulting controller is robust to unmodelled dynamics and disturbances, while state-of-the-art control methods based on nominal models can fail under small perturbations. Code is open sourced at https://github.com/learning-and-control/core.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636786,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636786,,Codes;Perturbation methods;Training data;Gaussian processes;Aerospace electronics;Control systems;Data models,,1.0,,45,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/learning-and-control/core,https://github.com/learning-and-control/core
522,BORM: Bayesian Object Relation Model for Indoor Scene Recognition,L. Zhou; J. Cen; X. Wang; Z. Sun; T. L. Lam; Y. Xu,"School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen; Shenzhen Institute of Artificial Intelligence and Robotics for Society, The Chinese University of Hong Kong, Shenzhen; School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen; School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen; School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen; School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,39,46,"Scene recognition is a fundamental task in robotic perception. For human beings, scene recognition is reasonable because they have abundant object knowledge of the real world. The idea of transferring prior object knowledge from humans to scene recognition is significant but still less exploited. In this paper, we propose to utilize meaningful object representations for indoor scene representation. First, we utilize an improved object model (IOM) as a baseline that enriches the object knowledge by introducing a scene parsing algorithm pretrained on the ADE20K dataset with rich object categories related to the indoor scene. To analyze the object co-occurrences and pairwise object relations, we formulate the IOM from a Bayesian perspective as the Bayesian object relation model (BORM). Meanwhile, we incorporate the proposed BORM with the PlacesCNN model as the combined Bayesian object relation model (CBORM) for scene recognition and significantly outperforms the state-of-the-art methods on the reduced Places365 dataset, and SUN RGB-D dataset without retraining, showing the excellent generalization ability of the proposed method. Code can be found at https://github.com/FreeformRobotics/BORM.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636024,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636024,,Analytical models;Codes;Bayes methods;Task analysis;Sun;Intelligent robots,,12.0,,40,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/FreeformRobotics/BORM,https://github.com/FreeformRobotics/BORM
523,FINO-Net: A Deep Multimodal Sensor Fusion Framework for Manipulation Failure Detection,A. Inceoglu; E. E. Aksoy; A. Cihan Ak; S. Sariel,"Artificial Intelligence and Robotics Laboratory, Faculty of Computer and Informatics Engineering, Istanbul Technical University, Maslak, Turkey; School of Information Technology, Center for Applied Intelligent Systems Research, Halmstad University, Halmstad, Sweden; Artificial Intelligence and Robotics Laboratory, Faculty of Computer and Informatics Engineering, Istanbul Technical University, Maslak, Turkey; Artificial Intelligence and Robotics Laboratory, Faculty of Computer and Informatics Engineering, Istanbul Technical University, Maslak, Turkey",2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),16 Dec 2021,2021,,,6841,6847,"We need robots more aware of the unintended outcomes of their actions for ensuring safety. This can be achieved by an onboard failure detection system to monitor and detect such cases. Onboard failure detection is challenging with a limited set of onboard sensor setup due to the limitations of sensing capabilities of each sensor. To alleviate these challenges, we propose FINO-Net, a novel multimodal sensor fusion based deep neural network to detect and identify manipulation failures. We also introduce FAILURE, a multimodal dataset, containing 229 real-world manipulation data recorded with a Baxter robot. Our network combines RGB, depth and audio readings to effectively detect failures. Results indicate that fusing RGB with depth and audio modalities significantly improves the performance. FINO-Net achieves %98.60 detection accuracy on our novel dataset. Code and data are publicly available at https://github.com/ardai/fino-net.",2153-0866,978-1-6654-1714-3,10.1109/IROS51168.2021.9636455,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636455,,Deep learning;Codes;Multimodal sensors;Robot sensing systems;Safety;Monitoring;Intelligent robots,,12.0,,40,IEEE,16 Dec 2021,,,IEEE,IEEE Conferences,https://github.com/ardai/fino-net,https://github.com/ardai/fino-net
524,Targetless Calibration of LiDAR-IMU System Based on Continuous-time Batch Estimation,J. Lv; J. Xu; K. Hu; Y. Liu; X. Zuo,"Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,9968,9975,"Sensor calibration is the fundamental block for a multi-sensor fusion system. This paper presents an accurate and repeatable LiDAR-IMU calibration method (termed LI-Calib), to calibrate the 6-DOF extrinsic transformation between the 3D LiDAR and the Inertial Measurement Unit (IMU). Regarding the high data capture rate for LiDAR and IMU sensors, LI-Calib adopts a continuous-time trajectory formulation based on B-Spline, which is more suitable for fusing high-rate or asynchronous measurements than discrete-time based approaches. Additionally, LI-Calib decomposes the space into cells and identifies the planar segments for data association, which renders the calibration problem well-constrained in usual scenarios without any artificial targets. We validate the proposed calibration approach on both simulated and real-world experiments. The results demonstrate the high accuracy and good repeatability of the proposed method in common human-made scenarios. To benefit the research community, we open-source our code at https://github.com/APRIL-ZJU/lidar_IMU_calib.",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9341405,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9341405,,Laser radar;Three-dimensional displays;Robot sensing systems;Calibration;Trajectory;Splines (mathematics);Open source software,,61.0,,35,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,https://github.com/APRIL-ZJU/lidar_IMU_calib,https://github.com/APRIL-ZJU/lidar_IMU_calib
525,Uncertainty-aware Self-supervised 3D Data Association,J. Wang; S. Ancha; Y. -T. Chen; D. Held,"Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Honda Research Institute, Mountain View, CA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,8125,8132,"3D object trackers usually require training on large amounts of annotated data that is expensive and time-consuming to collect. Instead, we propose leveraging vast unlabeled datasets by self-supervised metric learning of 3D object trackers, with a focus on data association. Large scale annotations for unlabeled data are cheaply obtained by automatic object detection and association across frames. We show how these self-supervised annotations can be used in a principled manner to learn point-cloud embeddings that are effective for 3D tracking. We estimate and incorporate uncertainty in self-supervised tracking to learn more robust embeddings, without needing any labeled data. We design embeddings to differentiate objects across frames, and learn them using uncertainty-aware self-supervised training. Finally, we demonstrate their ability to perform accurate data association across frames, towards effective and accurate 3D tracking. Project videos and code are at https://jianrenw.github.io/Self-Supervised-3D-Data-Association/.",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9341251,National Science Foundation; United States Air Force; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9341251,,Training;Measurement;Three-dimensional displays;Uncertainty;Annotations;Object detection;Videos,,6.0,,31,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,https://jianrenw.github.io/Self-Supervised-3D-Data-Association,https://github.com/jianrenw/Self-Supervised-3D-Data-Association
526,Tidying Deep Saliency Prediction Architectures,N. Reddy; S. Jain; P. Yarlagadda; V. Gandhi,"CVIT, KCIS, International Institute of Information Technology, Hyderabad, India; CVIT, KCIS, International Institute of Information Technology, Hyderabad, India; CVIT, KCIS, International Institute of Information Technology, Hyderabad, India; CVIT, KCIS, International Institute of Information Technology, Hyderabad, India",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,10241,10247,"Learning computational models for visual attention (saliency estimation) is an effort to inch machines/robots closer to human visual cognitive abilities. Data-driven efforts have dominated the landscape since the introduction of deep neural network architectures. In deep learning research, the choices in architecture design are often empirical and frequently lead to more complex models than necessary. The complexity, in turn, hinders the application requirements. In this paper, we identify four key components of saliency models, i.e., input features, multi-level integration, readout architecture, and loss functions. We review the existing state of the art models on these four components and propose novel and simpler alternatives. As a result, we propose two novel end-to-end architectures called SimpleNet and MDNSal, which are neater, minimal, more interpretable and achieve state of the art performance on public saliency benchmarks. SimpleNet is an optimized encoder-decoder architecture and brings notable performance gains on the SALICON dataset (the largest saliency benchmark). MDNSal is a parametric model that directly predicts parameters of a GMM distribution and is aimed to bring more interpretability to the prediction maps. The proposed saliency models can be inferred at 25fps, making them suitable for real-time applications. Code and pre-trained models are available at https://github.com/samyak0210/saliency.",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9341574,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9341574,,Visualization;Computer architecture;Benchmark testing;Parametric statistics;Complexity theory;Faces;Saliency detection,,29.0,,39,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,https://github.com/samyak0210/saliency,https://github.com/samyak0210/saliency
527,Online Visual Place Recognition via Saliency Re-identification,H. Wang; C. Wang; L. Xie,"School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,5030,5036,"As an essential component of visual simultaneous localization and mapping (SLAM), place recognition is crucial for robot navigation and autonomous driving. Existing methods often formulate visual place recognition as feature matching, which is computationally expensive for many robotic applications with limited computing power, e.g., autonomous driving and cleaning robot. Inspired by the fact that human beings always recognize a place by remembering salient regions or landmarks that are more attractive or interesting than others, we formulate visual place recognition as saliency reidentification. In the meanwhile, we propose to perform both saliency detection and re-identification in frequency domain, in which all operations become element-wise. The experiments show that our proposed method achieves competitive accuracy and much higher speed than the state-of-the-art feature-based methods. The proposed method is open-sourced and available at https://github.com/wh200720041/SRLCD.git.",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9341703,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9341703,,Visualization;Simultaneous localization and mapping;Frequency-domain analysis;Computational efficiency;Task analysis;Autonomous vehicles;Saliency detection,,12.0,,35,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,https://github.com/wh200720041/SRLCD,https://github.com/wh200720041/SRLCD
528,Toward Hierarchical Self-Supervised Monocular Absolute Depth Estimation for Autonomous Driving Applications,F. Xue; G. Zhuo; Z. Huang; W. Fu; Z. Wu; M. H. Ang,"School of Automotive Studies, Tongji University, Shanghai, China; School of Automotive Studies, Tongji University, Shanghai, China; Department of Mechanical Engineering, National University of Singapore, Singapore; School of Automotive Studies, Tongji University, Shanghai, China; School of Automotive Studies, Tongji University, Shanghai, China; Department of Mechanical Engineering, National University of Singapore, Singapore",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,2330,2337,"In recent years, self-supervised methods for monocular depth estimation has rapidly become an significant branch of depth estimation task, especially for autonomous driving applications. Despite the high overall precision achieved, current methods still suffer from a) imprecise object-level depth inference and b) uncertain scale factor. The former problem would cause texture copy or provide inaccurate object boundary, and the latter would require current methods to have an additional sensor like LiDAR to provide depth ground-truth or stereo camera as additional training inputs, which makes them difficult to implement. In this work, we propose to address these two problems together by introducing DNet. Our contributions are twofold: a) a novel dense connected prediction (DCP) layer is proposed to provide better object-level depth estimation and b) specifically for autonomous driving scenarios, dense geometrical constrains (DGC) is introduced so that precise scale factor can be recovered without additional cost for autonomous vehicles. Extensive experiments have been conducted and, both DCP layer and DGC module are proved to be effectively solving the aforementioned problems respectively. Thanks to DCP layer, object boundary can now be better distinguished in the depth map and the depth is more continues on object level. It is also demonstrated that the performance of using DGC to perform scale recovery is comparable to that using ground-truth information, when the camera height is given and the ground point takes up more than 1.03% of the pixels. Code is available at https://github.com/TJ-IPLab/DNet.",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9340802,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9340802,,Training;Pipelines;Estimation;Cameras;Robustness;Task analysis;Autonomous vehicles,,59.0,,37,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,https://github.com/TJ-IPLab/DNet,https://github.com/TJ-IPLab/DNet
529,UAV-AdNet: Unsupervised Anomaly Detection using Deep Neural Networks for Aerial Surveillance,I. Bozcan; E. Kayacan,"Department of Engineering, Artificial Intelligence in Robotics Laboratory (Air Lab), Aarhus University, Aarhus C, Denmark; Department of Engineering, Artificial Intelligence in Robotics Laboratory (Air Lab), Aarhus University, Aarhus C, Denmark",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,1158,1164,"Anomaly detection is a key goal of autonomous surveillance systems that should be able to alert unusual observations. In this paper, we propose a holistic anomaly detection system using deep neural networks for surveillance of critical infrastructures (e.g., airports, harbors, warehouses) using an unmanned aerial vehicle (UAV). First, we present a heuristic method for the explicit representation of spatial layouts of objects in bird-view images. Then, we propose a deep neural network architecture for unsupervised anomaly detection (UAV-AdNet), which is trained on environment representations and GPS labels of bird-view images jointly. Unlike studies in the literature, we combine GPS and image data to predict abnormal observations. We evaluate our model against several baselines on our aerial surveillance dataset and show that it performs better in scene reconstruction and several anomaly detection tasks. The codes, trained models, dataset, and video will be available at https://bozcani.github.io/uavadnet.",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9341790,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9341790,,Surveillance;Neural networks;Unmanned aerial vehicles;Task analysis;Anomaly detection;Image reconstruction;Global Positioning System,,18.0,,31,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,https://bozcani.github.io/uavadnet,
530,"A decentralized framework for simultaneous calibration, localization and mapping with multiple LiDARs",J. Lin; X. Liu; F. Zhang,"Department of Mechanical Engineering, Hong Kong University, Hong Kong SAR, China; Department of Mechanical Engineering, Hong Kong University, Hong Kong SAR, China; Department of Mechanical Engineering, Hong Kong University, Hong Kong SAR, China",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,4870,4877,"LiDAR is playing a more and more essential role in autonomous driving vehicles for objection detection, self localization and mapping. A single LiDAR frequently suffers from hardware failure (e.g., temporary loss of connection) due to the harsh vehicle environment (e.g., temperature, vibration, etc.), or performance degradation due to the lack of sufficient geometry features, especially for solid-state LiDARs with small field of view (FoV). To improve the system robustness and performance in self-localization and mapping, we develop a decentralized framework for simultaneous calibration, localization and mapping with multiple LiDARs. Our proposed framework is based on an extended Kalman filter (EKF), but is specially formulated for decentralized implementation. Such an implementation could potentially distribute the intensive computation among smaller computing devices or resources dedicated for each LiDAR and remove the single point of failure problem. Then this decentralized formulation is implemented on an unmanned ground vehicle (UGV) carrying 5 low-cost LiDARs and moving at 1.3m/s in urban environments. Experiment results show that the proposed method can successfully and simultaneously estimate the vehicle state (i.e., pose and velocity) and all LiDAR extrinsic parameters. The localization accuracy is up to 0.2% on the two datasets we collected. To share our findings and to make contributions to the community, meanwhile enable the readers to verify our work, we will release all our source codes1 and hardware design blueprint2 on our Github.",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9340790,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9340790,,Location awareness;Vibrations;Laser radar;Urban areas;Hardware;Calibration;Software development management,,39.0,,26,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,,
531,LiDAR guided Small obstacle Segmentation,A. Singh; A. Kamireddypalli; V. Gandhi; K. M. Krishna,"KCIS, International Institute of Information Technology, Hyderabad, India; KCIS, International Institute of Information Technology, Hyderabad, India; KCIS, International Institute of Information Technology, Hyderabad, India; KCIS, International Institute of Information Technology, Hyderabad, India",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,8513,8520,"Detecting small obstacles on the road is critical for autonomous driving. In this paper, we present a method to reliably detect such obstacles through a multi-modal framework of sparse LiDAR(VLP-16) and Monocular vision. LiDAR is employed to provide additional context in the form of confidence maps to monocular segmentation networks. We show significant performance gains when the context is fed as an additional input to monocular semantic segmentation frameworks. We further present a new semantic segmentation dataset to the community, comprising of over 3000 image frames with corresponding LiDAR observations. The images come with pixel-wise annotations of three classes off-road, road, and small obstacle. We stress that precise calibration between LiDAR and camera is crucial for this task and thus propose a novel Hausdorff distance based calibration refinement method over extrinsic parameters. As a first benchmark over this dataset, we report our results with 73 % instance detection up to a distance of 50 meters on challenging scenarios. Qualitatively by showcasing accurate segmentation of obstacles less than 15 cms at 50m depth and quantitatively through favourable comparisons vis a vis prior art, we vindicate the method's efficacy. Our project and dataset is hosted at https://small-obstacle-dataset.github.io/.",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9341465,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9341465,,Image segmentation;Laser radar;Annotations;Roads;Semantics;Calibration;Task analysis,,13.0,,26,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,,
532,Anatomical Mesh-Based Virtual Fixtures for Surgical Robots,Z. Li; A. Gordon; T. Looi; J. Drake; C. Forrest; R. H. Taylor,"Laboratory for Computational Sensing and Robotics, Johns Hopkins University, Baltimore, Maryland, USA; Center for Image Guided Innovation and Therapeutic Intervention lab at Hospital for Sick Children, Toronto, Canada; Center for Image Guided Innovation and Therapeutic Intervention lab at Hospital for Sick Children, Toronto, Canada; Center for Image Guided Innovation and Therapeutic Intervention lab at Hospital for Sick Children, Toronto, Canada; Center for Image Guided Innovation and Therapeutic Intervention lab at Hospital for Sick Children, Toronto, Canada; Laboratory for Computational Sensing and Robotics, Johns Hopkins University, Baltimore, Maryland, USA",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,3267,3273,"This paper presents a dynamic constraint formulation to provide protective virtual fixtures of 3D anatomical structures from polygon mesh representations. The proposed approach can anisotropically limit the tool motion of surgical robots without any assumption of the local anatomical shape close to the tool. Using a bounded search strategy and Principle Directed tree, the proposed system can run efficiently at 180 Hz for a mesh object containing 989,376 triangles and 493,460 vertices. The proposed algorithm has been validated in both simulation and skull cutting experiments. The skull cutting experiment setup uses a novel piezoelectric bone cutting tool designed for the da Vinci research kit. The result shows that the virtual fixture assisted teleoperation has statistically significant improvements in the cutting path accuracy and penetration depth control. The code has been made publicly available at https://github.com/mli0603/PolygonMeshVirtualFixture.",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9341590,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9341590,,Medical robotics;Three-dimensional displays;Shape;Fixtures;Tools;Search problems;Intelligent robots,,12.0,,19,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,https://github.com/mli0603/PolygonMeshVirtualFixture,https://github.com/mli0603/PolygonMeshVirtualFixture
533,EAO-SLAM: Monocular Semi-Dense Object SLAM Based on Ensemble Data Association,Y. Wu; Y. Zhang; D. Zhu; Y. Feng; S. Coleman; D. Kerr,"Faculty of Robot Science and Engineering, Northeastern University, Shenyang, China; Faculty of Robot Science and Engineering, Northeastern University, Shenyang, China; Department of Electronic Engineering, The Chinese University of Hong Kong, China; College of Information Science and Engineering, Northeastern University, Shenyang, China; School of Computing and Intelligent Systems, Ulster University, N. Ireland, UK; School of Computing and Intelligent Systems, Ulster University, N. Ireland, UK",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,4966,4973,"Object-level data association and pose estimation play a fundamental role in semantic SLAM, which remain unsolved due to the lack of robust and accurate algorithms. In this work, we propose an ensemble data associate strategy for integrating the parametric and nonparametric statistic tests. By exploiting the nature of different statistics, our method can effectively aggregate the information of different measurements, and thus significantly improve the robustness and accuracy of data association. We then present an accurate object pose estimation framework, in which an outliers-robust centroid and scale estimation algorithm and an object pose initialization algorithm are developed to help improve the optimality of pose estimation results. Furthermore, we build a SLAM system that can generate semi-dense or lightweight object-oriented maps with a monocular camera. Extensive experiments are conducted on three publicly available datasets and a real scenario. The results show that our approach significantly outperforms state-of-the-art techniques in accuracy and robustness. The source code is available on https://github.com/yanmin-wu/EAO-SLAM.",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9341757,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9341757,,Simultaneous localization and mapping;Navigation;Pose estimation;Semantics;Robustness;Nonparametric statistics;Intelligent robots,,62.0,,27,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,https://github.com/yanmin-wu/EAO-SLAM,https://github.com/yanmin-wu/EAO-SLAM
534,Learn by Observation: Imitation Learning for Drone Patrolling from Videos of A Human Navigator,Y. Fan; S. Chu; W. Zhang; R. Song; Y. Li,"School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,5209,5216,"We present an imitation learning method for autonomous drone patrolling based only on raw videos. Different from previous methods, we propose to let the drone learn patrolling in the air by observing and imitating how a human navigator does it on the ground. The observation process enables the automatic collection and annotation of data using inter-frame geometric consistency, resulting in less manual effort and high accuracy. Then a newly designed neural network is trained based on the annotated data to predict appropriate directions and translations for the drone to patrol in a lane-keeping manner as humans. Our method allows the drone to fly at a high altitude with a broad view and low risk. It can also detect all accessible directions at crossroads and further carry out the integration of available user instructions and autonomous patrolling control commands. Extensive experiments are conducted to demonstrate the accuracy of the proposed imitating learning process as well as the reliability of the holistic system for autonomous drone navigation. The codes, datasets as well as video demonstrations are available at https://vsislab.github.io/uavpatrol.",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9340691,National Natural Science Foundation of China; Research and Development; Shandong University; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9340691,,Navigation;Roads;Real-time systems;Reliability;Drones;Videos;Testing,,6.0,,34,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,https://vsislab.github.io/uavpatrol,https://github.com/UeFan/Auto-labeling-using-Inter-Frame-Geometric-consistency
535,360° Depth Estimation from Multiple Fisheye Images with Origami Crown Representation of Icosahedron,R. Komatsu; H. Fujii; Y. Tamura; A. Yamashita; H. Asama,"Department of Precision Engineering, Graduate School of Engineering, The University of Tokyo, Tokyo, Japan; Department of Advanced Robotics, Faculty of Advanced Engineering, Chiba Institute of Technology, Narashino, Japan; Department of Robotics, Division of Mechanical Engineering, Tohoku University, Sendai, Japan; Department of Precision Engineering, Graduate School of Engineering, The University of Tokyo, Tokyo, Japan; Department of Precision Engineering, Graduate School of Engineering, The University of Tokyo, Tokyo, Japan",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,10092,10099,"In this study, we present a method for all-around depth estimation from multiple omnidirectional images for indoor environments. In particular, we focus on plane-sweeping stereo as the method for depth estimation from the images. We propose a new icosahedron-based representation and ConvNets for omnidirectional images, which we name ""CrownConv"" because the representation resembles a crown made of origami. CrownConv can be applied to both fisheye images and equirect- angular images to extract features. Furthermore, we propose icosahedron-based spherical sweeping for generating the cost volume on an icosahedron from the extracted features. The cost volume is regularized using the three-dimensional CrownConv, and the final depth is obtained by depth regression from the cost volume. Our proposed method is robust to camera alignments by using the extrinsic camera parameters; therefore, it can achieve precise depth estimation even when the camera alignment differs from that in the training dataset. We evaluate the proposed model on synthetic datasets and demonstrate its effectiveness. As our proposed method is computationally efficient, the depth is estimated from four fisheye images in less than a second using a laptop with a GPU. Therefore, it is suitable for real-world robotics applications. Our source code is available at https://github.com/matsuren/crownconv360depth.",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9340981,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9340981,,Training;Portable computers;Robot vision systems;Estimation;Graphics processing units;Cameras;Feature extraction,,19.0,,35,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,https://github.com/matsuren/crownconv360depth,https://github.com/matsuren/crownconv360depth
536,Improving Unimodal Object Recognition with Multimodal Contrastive Learning,J. Meyer; A. Eitel; T. Brox; W. Burgard,"Department of Computer Science, University of Freiburg, Germany; Department of Computer Science, University of Freiburg, Germany; Department of Computer Science, University of Freiburg, Germany; Department of Computer Science, University of Freiburg, Germany",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,5656,5663,"Robots perceive their environment using various sensor modalities, e.g., vision, depth, sound or touch. Each modality provides complementary information for perception. However, while it can be assumed that all modalities are available for training, when deploying the robot in real-world scenarios the sensor setup often varies. In order to gain flexibility with respect to the deployed sensor setup we propose a new multimodal approach within the framework of contrastive learning. In particular, we consider the case of learning from RGB-D images while testing with one modality available, i.e., exclusively RGB or depth. We leverage contrastive learning to capture high-level information between different modalities in a compact feature embedding. We extensively evaluate our multimodal contrastive learning method on the Falling Things dataset and learn representations that outperform prior methods for RGB-D object recognition on the NYU-D dataset. Our code and details on the used datasets are available at: https://github.com/meyerjo/MultiModalContrastiveLearning.",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9341029,Ministry of Education; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9341029,,Training;Learning systems;Semantics;Robot sensing systems;Object recognition;Task analysis;Testing,,9.0,,29,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,https://github.com/meyerjo/MultiModalContrastiveLearning,https://github.com/meyerjo/MultiModalContrastiveLearning
537,PERCH 2.0 : Fast and Accurate GPU-based Perception via Search for Object Pose Estimation,A. Agarwal; Y. Han; M. Likhachev,"The Robotics Institute, Carnegie Mellon University, PA, USA; The Robotics Institute, Carnegie Mellon University, PA, USA; The Robotics Institute, Carnegie Mellon University, PA, USA",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,10633,10640,"Pose estimation of known objects is fundamental to tasks such as robotic grasping and manipulation. The need for reliable grasping imposes stringent accuracy requirements on pose estimation in cluttered, occluded scenes in dynamic environments. Modern methods employ large sets of training data to learn features in order to find correspondence between 3D models and observed data. However these methods require extensive annotation of ground truth poses. An alternative is to use algorithms that search for the best explanation of the observed scene in a space of possible rendered scenes. A recently developed algorithm, PERCH (PErception Via SeaRCH) does so by using depth data to converge to a globally optimum solution using a search over a specially constructed tree. While PERCH offers strong guarantees on accuracy, the current formulation suffers from low scalability owing to its high runtime. In addition, the sole reliance on depth data for pose estimation restricts the algorithm to scenes where no two objects have the same shape. In this work, we propose PERCH 2.0, a novel perception via search strategy that takes advantage of GPU acceleration and RGB data. We show that our approach can achieve a speedup of 100x over PERCH, as well as better accuracy than the state-of-the-art data-driven approaches on 6-DoF pose estimation without the need for annotating ground truth poses in the training data. Our code and video are available at https://sbpl-cruz.github.io/perception/.",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9341257,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9341257,,Training;Runtime;Annotations;Pose estimation;Training data;Grasping;Search problems,,5.0,,43,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,https://sbpl-cruz.github.io/perception,https://github.com/SBPL-Cruz/perception
538,DXSLAM: A Robust and Efficient Visual SLAM System with Deep Features,D. Li; X. Shi; Q. Long; S. Liu; W. Yang; F. Wang; Q. Wei; F. Qiao,"Tsinghua University, Beijing, China; Intel Corporation, China; Tsinghua University, Beijing, China; Intel Corporation, China; Beijing Jiaotong University, Beijing, China; Beijing Jiaotong University, Beijing, China; Tsinghua University, Beijing, China; Tsinghua University, Beijing, China",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,4958,4965,"A robust and efficient Simultaneous Localization and Mapping (SLAM) system is essential for robot autonomy. For visual SLAM algorithms, though the theoretical framework has been well established for most aspects, feature extraction and association is still empirically designed in most cases, and can be vulnerable in complex environments. This paper shows that feature extraction with deep convolutional neural networks (CNNs) can be seamlessly incorporated into a modern SLAM framework. The proposed SLAM system utilizes a state-of-the-art CNN to detect keypoints in each image frame, and to give not only keypoint descriptors, but also a global descriptor of the whole image. These local and global features are then used by different SLAM modules, resulting in much more robustness against environmental changes and viewpoint changes compared with using hand-crafted features. We also train a visual vocabulary of local features with a Bag of Words (BoW) method. Based on the local features, global features, and the vocabulary, a highly reliable loop closure detection method is built. Experimental results show that all the proposed modules significantly outperforms the baseline, and the full system achieves much lower trajectory errors and much higher correct rates on all evaluated data. Furthermore, by optimizing the CNN with Intel OpenVINO toolkit and utilizing the Fast BoW library, the system benefits greatly from the SIMD (single-instruction-multiple-data) techniques in modern CPUs. The full system can run in real-time without any GPU or other accelerators. The code is public at https://github.com/ivipsourcecode/dxslam.",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9340907,National Natural Science Foundation of China; Beijing Innovation Center for Future Chip; Tsinghua University; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9340907,,Visualization;Vocabulary;Simultaneous localization and mapping;Feature extraction;Robustness;Real-time systems;Trajectory,,77.0,,40,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,https://github.com/ivipsourcecode/dxslam,https://github.com/ivipsourcecode/dxslam
539,One-Shot Informed Robotic Visual Search in the Wild,K. Koreitem; F. Shkurti; T. Manderson; W. -D. Chang; J. C. Gamboa Higuera; G. Dudek,"Department of Computer Science, the Robotics Institute at the University of Toronto, and Vector Institute; Department of Computer Science, the Robotics Institute at the University of Toronto, and Vector Institute; Center for Intelligent Machines, School of Computer Science, McGill University, Montreal; Center for Intelligent Machines, School of Computer Science, McGill University, Montreal; Center for Intelligent Machines, School of Computer Science, McGill University, Montreal; Center for Intelligent Machines, School of Computer Science, McGill University, Montreal",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,5800,5807,"We consider the task of underwater robot navigation for the purpose of collecting scientifically relevant video data for environmental monitoring. The majority of field robots that currently perform monitoring tasks in unstructured natural environments navigate via path-tracking a pre-specified sequence of waypoints. Although this navigation method is often necessary, it is limiting because the robot does not have a model of what the scientist deems to be relevant visual observations. Thus, the robot can neither visually search for particular types of objects, nor focus its attention on parts of the scene that might be more relevant than the pre-specified waypoints and viewpoints. In this paper we propose a method that enables informed visual navigation via a learned visual similarity operator that guides the robot's visual search towards parts of the scene that look like an exemplar image, which is given by the user as a high-level specification for data collection. We propose and evaluate a weakly supervised video representation learning method that outperforms ImageNet embeddings for similarity tasks in the underwater domain. We also demonstrate the deployment of this similarity operator during informed visual navigation in collaborative environmental monitoring scenarios, in large-scale field trials, where the robot and a human scientist collaboratively search for relevant visual content. Code: https://github.com/rvl-lab-utoronto/visual_search_in_the_wild.",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9340914,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9340914,,Visualization;Navigation;Robot vision systems;Unmanned underwater vehicles;Environmental monitoring;Task analysis;Robots,,8.0,,70,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,https://github.com/rvl-lab-utoronto/visual_search_in_the_wild,https://github.com/rvl-lab-utoronto/visual_search_in_the_wild
540,CinemAirSim: A Camera-Realistic Robotics Simulator for Cinematographic Purposes,P. Pueyo; E. Cristofalo; E. Montijano; M. Schwager,"Instituto de Investigación en Ingeniería de Aragón, Universidad de Zaragoza, Spain; Department of Aeronautics and Astronautics, Stanford University, USA; Instituto de Investigación en Ingeniería de Aragón, Universidad de Zaragoza, Spain; Department of Aeronautics and Astronautics, Stanford University, USA",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,1186,1191,"Unmanned Aerial Vehicles (UAVs) are becoming increasingly popular in the film and entertainment industries, in part because of their maneuverability and perspectives they enable. While there exists methods for controlling the position and orientation of the drones for visibility, other artistic elements of the filming process, such as focal blur, remain unexplored in the robotics community. The lack of cinematographic robotics solutions is partly due to the cost associated with the cameras and devices used in the filming industry, but also because state-of-the-art photo-realistic robotics simulators only utilize a full in-focus pinhole camera model which does not incorporate these desired artistic attributes. To overcome this, the main contribution of this work is to endow the well-known drone simulator, AirSim, with a cinematic camera as well as extend its API to control all of its parameters in real time, including various filming lenses and common cinematographic properties. In this paper, we detail the implementation of our AirSim modification, CinemAirSim, present examples that illustrate the potential of the new tool, and highlight the new research opportunities that the use of cinematic cameras can bring to research in robotics and control.",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9341066,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9341066,github.com/ppueyor/CinematicAirSim,Service robots;Robot vision systems;Process control;Tools;Cameras;Real-time systems;Drones,,6.0,,22,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,,
541,End-to-end Autonomous Driving Perception with Sequential Latent Representation Learning,J. Chen; Z. Xu; M. Tomizuka,"Department of Mechanical Engineering, University of California, Berkeley, CA, USA; Department of Mechanical Engineering, University of California, Berkeley, CA, USA; Department of Mechanical Engineering, University of California, Berkeley, CA, USA",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,1999,2006,"Current autonomous driving systems are composed of a perception system and a decision system. Both of them are divided into multiple subsystems built up with lots of human heuristics. An end-to-end approach might clean up the system and avoid huge efforts of human engineering, as well as obtain better performance with increasing data and computation resources. Compared to the decision system, the perception system is more suitable to be designed in an end-to-end framework, since it does not require online driving exploration. In this paper, we propose a novel end-to-end approach for autonomous driving perception. A latent space is introduced to capture all relevant features useful for perception, which is learned through sequential latent representation learning. The learned end-to-end perception model is able to solve the detection, tracking, localization and mapping problems altogether with only minimum human engineering efforts and without storing any maps online. The proposed method is evaluated in a realistic urban driving simulator, with both camera image and lidar point cloud as sensor inputs. The codes and videos of this work are available at our github repo† and project website‡.",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9341020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9341020,,Location awareness;Three-dimensional displays;Ergonomics;Robot vision systems;Autonomous vehicles;Videos;Software development management,,12.0,,33,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,,
542,GelTip: A Finger-shaped Optical Tactile Sensor for Robotic Manipulation,D. F. Gomes; Z. Lin; S. Luo,"smARTLab, Department of Computer Science, University of Liverpool, Liverpool, United Kingdom; School of Mechanical Engineering and Automation, Fuzhou University, Fuzhou, China; smARTLab, Department of Computer Science, University of Liverpool, Liverpool, United Kingdom",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,9903,9909,"Sensing contacts throughout the fingers is an essential capability for a robot to perform manipulation tasks in cluttered environments. However, existing tactile sensors either only have a flat sensing surface or a compliant tip with a limited sensing area. In this paper, we propose a novel optical tactile sensor, the GelTip, that is shaped as a finger and can sense contacts on any location of its surface. The sensor captures high-resolution and color-invariant tactile image that can be exploited to extract detailed information about the end-effector’s interactions against manipulated objects. Our extensive experiments show that the GelTip sensor can effectively localise the contacts on different locations its finger-shaped body, with a small localisation error of approximately 5 mm, on average, and under 1 mm in the best cases. The obtained results show the potential of the GelTip sensor in facilitating dynamic manipulation tasks with its all-round tactile sensing capability. The sensor models and further information about the GelTip sensor can be found at http://danfergo.github.io/geltip.",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9340881,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9340881,,Surface reconstruction;Fingers;Tactile sensors;Optical imaging;Adaptive optics;Optical sensors;Task analysis,,64.0,,17,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,http://danfergo.github.io/geltip,https://github.com/Kaijun/hexo-theme-huxblog
543,The Marathon 2: A Navigation System,S. Macenski; F. Martín; R. White; J. G. Clavero,"R&D Innovations, Samsung Research; Intelligent Robotics Lab, Rey Juan Carlos University; Contextual Robotics Institute, UC San Diego; Intelligent Robotics Lab, Rey Juan Carlos University",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,2718,2725,"Developments in mobile robot navigation have enabled robots to operate in warehouses, retail stores, and on sidewalks around pedestrians. Various navigation solutions have been proposed, though few as widely adopted as ROS (Robot Operating System) Navigation. 10 years on, it is still one of the most popular navigation solutions1. Yet, ROS Navigation has failed to keep up with modern trends. We propose the new navigation solution, Navigation2, which builds on the successful legacy of ROS Navigation. Navigation2 uses a behavior tree for navigator task orchestration and employs new methods designed for dynamic environments applicable to a wider variety of modern sensors. It is built on top of ROS2, a secure message passing framework suitable for safety critical applications and program lifecycle management. We present experiments in a campus setting utilizing Navigation2 to operate safely alongside students over a marathon as an extension of the experiment proposed in Eppstein et al. [1]. The Navigation2 system is freely available at https://github.com/ros-planning/navigation2 with a rich community and instructions.",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9341207,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9341207,Service Robots;Software;Middleware and Programming Environments;Behaviour-Based Systems,Navigation;Service robots;Heuristic algorithms;Sensors;Safety;Task analysis;Testing,,158.0,,25,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,https://github.com/ros-planning/navigation2,https://github.com/ros-planning/navigation2
544,Meta-Learning Deep Visual Words for Fast Video Object Segmentation,H. S. Behl; M. Naja; A. Arnab; P. H. S. Torr,University of Oxford; University of Oxford; Google Research; University of Oxford,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,8484,8491,"Personal robots and driverless cars need to be able to operate in novel environments and thus quickly and efficiently learn to recognise new object classes. We address this problem by considering the task of video object segmentation. Previous accurate methods for this task finetune a model using the first annotated frame, and/or use additional inputs such as optical flow and complex post-processing. In contrast, we develop a fast, causal algorithm that requires no finetuning, auxiliary inputs or post-processing, and segments a variable number of objects in a single forward-pass. We represent an object with clusters, or ""visual words"", in the embedding space, which correspond to object parts in the image space. This allows us to robustly match to the reference objects throughout the video, because although the global appearance of an object changes as it undergoes occlusions and deformations, the appearance of more local parts may stay consistent. We learn these visual words in an unsupervised manner, using meta-learning to ensure that our training objective matches our inference procedure. We achieve comparable accuracy to finetuning based methods (whilst being 1 to 2 orders of magnitude faster), and state-of-the-art in terms of speed/accuracy trade-offs on four video segmentation datasets. Code is available at https://github.com/harkiratbehl/MetaVOS.",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9341737,Tencent; Royal Academy of Engineering; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9341737,,Training;Visualization;Object segmentation;Task analysis;Optical flow;Intelligent robots;Strain,,7.0,,56,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,https://github.com/harkiratbehl/MetaVOS,https://github.com/harkiratbehl/MetaVOS
545,"SynChrono: A Scalable, Physics-Based Simulation Platform For Testing Groups of Autonomous Vehicles and/or Robots",J. Taves; A. Elmquist; A. Young; R. Serban; D. Negrut,"Department of Mechanical Engineering, University of Wisconsin–Madison, Madison, WI; Department of Mechanical Engineering, University of Wisconsin–Madison, Madison, WI; Department of Mechanical Engineering, University of Wisconsin–Madison, Madison, WI; Department of Mechanical Engineering, University of Wisconsin–Madison, Madison, WI; Department of Mechanical Engineering, University of Wisconsin–Madison, Madison, WI",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,2251,2256,"This contribution is concerned with the topic of using simulation to understand the behavior of groups of mutually interacting autonomous vehicles (AVs) or robots engaged in traffic/maneuvers that involve coordinated operation. We outline the structure of a multi-agent simulator called SYN-CHRONO and provide results pertaining to its scalability and ability to run real-time scenarios with humans in the loop. SYN-CHRONO is a scalable multi-agent, high-fidelity environment whose purpose is that of testing AV and robot control strategies. Four main components make up the core of the simulation platform: a physics-based dynamics engine that can simulate rigid and compliant systems, fluid-solid interactions, and deformable terrains; a module that provides sensing simulation; an agent-to-agent communication server; dynamic virtual worlds, which host the interacting agents operating in a coordinated scenario. The platform provides a virtual proving ground that can be used to answer questions such as ""what will an AV do when it skids on a patch of ice and moves one way while facing the other way?""; ""is a new agent-control strategy robust enough to handle unforeseen circumstances?""; and ""what is the effect of a loss of communication between agents engaged in a coordinated maneuver?"". Full videos based on work in the paper are available at https://tinyurl.com/ChronoIROS2020 and additional descriptions on the particular version of software used is available at https://github.com/uwsbel/publications-data/tree/master/2020/IROS.",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9341585,National Science Foundation; University Transportation Centers; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9341585,,Deformable models;Robot kinematics;Fluid dynamics;Vehicle dynamics;Autonomous vehicles;Testing;Videos,,2.0,,22,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,https://github.com/uwsbel/publications-data,https://github.com/uwsbel/publications-data
546,Learning Orientation Distributions for Object Pose Estimation,B. Okorn; M. Xu; M. Hebert; D. Held,"Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,10580,10587,"For robots to operate robustly in the real world, they should be aware of their uncertainty. However, most methods for object pose estimation return a single point estimate of the object's pose. In this work, we propose two learned methods for estimating a distribution over an object's orientation. Our methods take into account both the inaccuracies in the pose estimation as well as the object symmetries. Our first method, which regresses from deep learned features to an isotropic Bingham distribution, gives the best performance for orientation distribution estimation for non-symmetric objects. Our second method learns to compare deep features and generates a non-parameteric histogram distribution. This method gives the best performance on objects with unknown symmetries, accurately modeling both symmetric and non-symmetric objects, without any requirement of symmetry annotation. We show that both of these methods can be used to augment an existing pose estimator. Our evaluation compares our methods to a large number of baseline approaches for uncertainty estimation across a variety of different types of objects. Code available at https://bokorn.github.io/orientation-distributions/.",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9340860,United States Air Force; National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9340860,,Histograms;Uncertainty;Annotations;Pose estimation;Grasping;Information filters;Intelligent robots,,13.0,,42,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,https://bokorn.github.io/orientation-distributions,https://github.com/r-pad/se3_distributions
547,Spectral-GANs for High-Resolution 3D Point-cloud Generation,S. Ramasinghe; S. Khan; N. Barnes; S. Gould,"College of Engineering and Computer Science (CECS), Australian National University (ANU), Canberra, AU; College of Engineering and Computer Science (CECS), Australian National University (ANU), Canberra, AU; College of Engineering and Computer Science (CECS), Australian National University (ANU), Canberra, AU; College of Engineering and Computer Science (CECS), Australian National University (ANU), Canberra, AU",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,8169,8176,"Point-clouds are a popular choice for robotics and computer vision tasks due to their accurate shape description and direct acquisition from range-scanners. This demands the ability to synthesize and reconstruct high-quality point-clouds. Current deep generative models for 3D data generally work on simplified representations (e.g., voxelized objects) and cannot deal with the inherent redundancy and irregularity in point-clouds. A few recent efforts on 3D point-cloud generation offer limited resolution and their complexity grows with the increase in output resolution. In this paper, we develop a principled approach to synthesize 3D point-clouds using a spectral-domain Generative Adversarial Network (GAN). Our spectral representation is highly structured and allows us to disentangle various frequency bands such that the learning task is simplified for a GAN model. As compared to spatial-domain generative approaches, our formulation allows us to generate high-resolution point-clouds with minimal computational overhead. Furthermore, we propose a fully differentiable block to transform from the spectral to the spatial domain and back, thereby allowing us to integrate knowledge from well-established spatial models. We demonstrate that Spectral-GAN performs well for point-cloud generation task. Additionally, it can learn a highly discriminative representation in an unsupervised fashion and can be used to accurately reconstruct 3D objects. Our codes are available at https://github.com/samgregoost/Spectral-GAN/.",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9341265,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9341265,,Solid modeling;Three-dimensional displays;Computational modeling;Redundancy;Generative adversarial networks;Task analysis;Spectral analysis,,19.0,,39,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,https://github.com/samgregoost/Spectral-GAN,https://github.com/samgregoost/Spectral-GAN
548,DeepLiDARFlow: A Deep Learning Architecture For Scene Flow Estimation Using Monocular Camera and Sparse LiDAR,R. Rishav; R. Battrawy; R. Schuster; O. Wasenmüller; D. Stricker,"German Research Center for Artificial Intelligence - DFKI, Kaisers-lautern, Germany; German Research Center for Artificial Intelligence - DFKI, Kaisers-lautern, Germany; German Research Center for Artificial Intelligence - DFKI, Kaisers-lautern, Germany; German Research Center for Artificial Intelligence - DFKI, Kaisers-lautern, Germany; German Research Center for Artificial Intelligence - DFKI, Kaisers-lautern, Germany",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,10460,10467,"Scene flow is the dense 3D reconstruction of motion and geometry of a scene. Most state-of-the-art methods use a pair of stereo images as input for full scene reconstruction. These methods depend a lot on the quality of the RGB images and perform poorly in regions with reflective objects, shadows, ill-conditioned light environment and so on. LiDAR measurements are much less sensitive to the aforementioned conditions but LiDAR features are in general unsuitable for matching tasks due to their sparse nature. Hence, using both LiDAR and RGB can potentially overcome the individual disadvantages of each sensor by mutual improvement and yield robust features which can improve the matching process. In this paper, we present DeepLiDARFlow, a novel deep learning architecture which fuses high level RGB and LiDAR features at multiple scales in a monocular setup to predict dense scene flow. Its performance is much better in the critical regions where image-only and LiDAR-only methods are inaccurate. We verify our DeepLiDARFlow using the established data sets KITTI and FlyingThings3D and we show strong robustness compared to several state-of-the-art methods which used other input modalities. The code of our paper is available at https://github.com/dfki-av/DeepLiDARFlow.",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9341077,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9341077,,Deep learning;Laser radar;Three-dimensional displays;Estimation;Robot sensing systems;Robustness;Task analysis,,27.0,,32,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,https://github.com/dfki-av/DeepLiDARFlow,https://github.com/dfki-av/DeepLiDARFlow
549,Multi-Robot Coordinated Planning in Confined Environments under Kinematic Constraints,C. Mangette; P. Tokekar,"Department of Electrical and Computer Engineering, Virginia Tech, U.S.A.; Department of Computer Science, University of Maryland, U.S.A.",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,7999,8004,"We investigate the problem of multi-robot coordinated planning in environments where the robots may have to operate in close proximity to each other. We seek computationally efficient planners that ensure safe paths and adherence to kinematic constraints. We extend the central planner dRRT* with our variant, fast-dRRT (fdRRT), with the intention being to use in tight environments that lead to a high degree of coupling between robots. Our algorithm is empirically shown to achieve the trade-off between computational time and solution quality, especially in tight environments. We also demonstrate the ability of our algorithm to be adapted to the online planning problem while maintaining computational efficiency. The software implementation is available online at https://github.com/CMangette/Fast-dRRT.",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9341213,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9341213,,Robot kinematics;Software algorithms;Kinematics;Software;Planning;Computational efficiency;Intelligent robots,,2.0,,21,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,https://github.com/CMangette/Fast-dRRT,https://github.com/CMangette/Fast-dRRT
550,Crop Height and Plot Estimation for Phenotyping from Unmanned Aerial Vehicles using 3D LiDAR,H. Dhami; K. Yu; T. Xu; Q. Zhu; K. Dhakal; J. Friel; S. Li; P. Tokekar,"Department of Electrical and Computer Engineering, Virginia Tech, U.S.A; Department of Electrical and Computer Engineering, Virginia Tech, U.S.A; Department of Electrical and Computer Engineering, Virginia Tech, U.S.A; School of Plant and Environmental Sciences, Virginia Tech, U.S.A; School of Plant and Environmental Sciences, Virginia Tech, U.S.A; School of Plant and Environmental Sciences, Virginia Tech, U.S.A; School of Plant and Environmental Sciences, Virginia Tech, U.S.A; Department of Electrical and Computer Engineering, Virginia Tech, U.S.A",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,2643,2649,"We present techniques to measure crop heights using a 3D Light Detection and Ranging (LiDAR) sensor mounted on an Unmanned Aerial Vehicle (UAV). Knowing the height of plants is crucial to monitor their overall health and growth cycles, especially for high-throughput plant phenotyping. We present a methodology for extracting plant heights from 3D LiDAR point clouds, specifically focusing on plot-based phenotyping environments. We also present a toolchain that can be used to create phenotyping farms for use in Gazebo simulations. The tool creates a randomized farm with realistic 3D plant and terrain models. We conducted a series of simulations and hardware experiments in controlled and natural settings. Our algorithm was able to estimate the plant heights in a field with 112 plots with a root mean square error (RMSE) of 6.1 cm. This is the first such dataset for 3D LiDAR from an airborne robot over a wheat field. The developed simulation toolchain, algorithmic implementation, and datasets can be found on our GitHub repository.1",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9341343,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9341343,,Solid modeling;Three-dimensional displays;Laser radar;Atmospheric modeling;Tools;Unmanned aerial vehicles;Agriculture,,12.0,,17,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,,
551,Semantic Graph Based Place Recognition for 3D Point Clouds,X. Kong; X. Yang; G. Zhai; X. Zhao; X. Zeng; M. Wang; Y. Liu; W. Li; F. Wen,"Institute of Cyber-Systems and Control, Zhe-jiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhe-jiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhe-jiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhe-jiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhe-jiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhe-jiang University, Hangzhou, China; Institute of Cyber-Systems and Control, Zhe-jiang University, Hangzhou, China; Huawei Noah’s Ark Lab, Beijing, China; Huawei Noah’s Ark Lab, Beijing, China",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,8216,8223,"Due to the difficulty in generating the effective descriptors which are robust to occlusion and viewpoint changes, place recognition for 3D point cloud remains an open issue. Unlike most of the existing methods that focus on extracting local, global, and statistical features of raw point clouds, our method aims at the semantic level that can be superior in terms of robustness to environmental changes. Inspired by the perspective of humans, who recognize scenes through identifying semantic objects and capturing their relations, this paper presents a novel semantic graph based approach for place recognition. First, we propose a novel semantic graph representation for the point cloud scenes by reserving the semantic and topological information of the raw point cloud. Thus, place recognition is modeled as a graph matching problem. Then we design a fast and effective graph similarity network to compute the similarity. Exhaustive evaluations on the KITTI dataset show that our approach is robust to the occlusion as well as viewpoint changes and outperforms the state-of-the-art methods with a large margin. Our code is available at: https://github.com/kxhit/SG_PR.",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9341060,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9341060,,Three-dimensional displays;Computational modeling;Semantics;Feature extraction;Robustness;Object recognition;Intelligent robots,,83.0,,42,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,https://github.com/kxhit/SG_PR,https://github.com/kxhit/SG_PR
552,EU Long-term Dataset with Multiple Sensors for Autonomous Driving,Z. Yan; L. Sun; T. Krajník; Y. Ruichek,"CIAD UMR7533, Univ. Bourgogne Franche-Comt, UTBM, Belfort, France; Sheffield Robotics, University of Sheffield, UK; Artificial Intelligence Center, Czech Technical University, Czechia; CIAD UMR7533, Univ. Bourgogne Franche-Comt, UTBM, Belfort, France",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,10697,10704,"The field of autonomous driving has grown tremendously over the past few years, along with the rapid progress in sensor technology. One of the major purposes of using sensors is to provide environment perception for vehicle understanding, learning and reasoning, and ultimately interacting with the environment. In this paper, we first introduce a multisensor platform allowing vehicle to perceive its surroundings and locate itself in a more efficient and accurate way. The platform integrates eleven heterogeneous sensors including various cameras and lidars, a radar, an IMU (Inertial Measurement Unit), and a GPS-RTK (Global Positioning System / Real-Time Kinematic), while exploits a ROS (Robot Operating System) based software to process the sensory data. Then, we present a new dataset (https://epan-utbm.github.io/utbm_robocar_dataset/) for autonomous driving captured many new research challenges (e.g. highly dynamic environment), and especially for long-term autonomy (e.g. creating and maintaining maps), collected with our instrumented vehicle, publicly available to the community.",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9341406,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9341406,,Visualization;Laser radar;Instruments;Robot sensing systems;Cameras;General Data Protection Regulation;Autonomous vehicles,,54.0,,28,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,https://epan-utbm.github.io/utbm_robocar_dataset,https://github.com/epan-utbm/utbm_robocar_dataset/tree/baselines/camera_info
553,Faster than FAST: GPU-Accelerated Frontend for High-Speed VIO,B. Nagy; P. Foehn; D. Scaramuzza,"Dep. of Informatics, Robotics and Perception Group, University of Zurich; Dep. of Informatics, Robotics and Perception Group, University of Zurich; Dep. of Informatics, Robotics and Perception Group, University of Zurich",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,4361,4368,"The recent introduction of powerful embedded graphics processing units (GPUs) has allowed for unforeseen improvements in real-time computer vision applications. It has enabled algorithms to run onboard, well above the standard video rates, yielding not only higher information processing capability, but also reduced latency. This work focuses on the applicability of efficient low-level, GPU hardware-specific instructions to improve on existing computer vision algorithms in the field of visual-inertial odometry (VIO). While most steps of a VIO pipeline work on visual features, they rely on image data for detection and tracking, of which both steps are well suited for parallelization. Especially non-maxima suppression and the subsequent feature selection are prominent contributors to the overall image processing latency. Our work first revisits the problem of non-maxima suppression for feature detection specifically on GPUs, and proposes a solution that selects local response maxima, imposes spatial feature distribution, and extracts features simultaneously. Our second contribution introduces an enhanced FAST feature detector that applies the aforementioned non-maxima suppression method. Finally, we compare our method to other state-of-the-art CPU and GPU implementations, where we always outperform all of them in feature tracking and detection, resulting in over 1000fps throughput on an embedded Jetson TX2 platform. Additionally, we demonstrate our work integrated into a VIO pipeline achieving a metric state estimation at ~200fps.Code available at: https://github.com/uzh-rpg/vilib.",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9340851,National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9340851,,Pipelines;Graphics processing units;Detectors;Feature extraction;Throughput;Real-time systems;State estimation,,12.0,,34,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,https://github.com/uzh-rpg/vilib,https://github.com/uzh-rpg/vilib
554,Point Cloud Completion by Learning Shape Priors,X. Wang; M. H. Ang; G. Hee Lee,"Department of Mechanical Engineering, National University of Singapore; Department of Mechanical Engineering, National University of Singapore; Computer Vision and Robotic Perception (CVRP) Lab, Department of Computer Science, National University of Singapore",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),10 Feb 2021,2020,,,10719,10726,"In view of the difficulty in reconstructing object details in point cloud completion, we propose a shape prior learning method for object completion. The shape priors include geometric information in both complete and the partial point clouds. We design a feature alignment strategy to learn the shape prior from complete points, and a coarse to fine strategy to incorporate partial prior in the fine stage. To learn the complete objects prior, we first train a point cloud auto-encoder to extract the latent embeddings from complete points. Then we learn a mapping to transfer the point features from partial points to that of the complete points by optimizing feature alignment losses. The feature alignment losses consist of a L2 distance and an adversarial loss obtained by Maximum Mean Discrepancy Generative Adversarial Network (MMD-GAN). The L2 distance optimizes the partial features towards the complete ones in the feature space, and MMD-GAN decreases the statistical distance of two point features in a Reproducing Kernel Hilbert Space. We achieve state-of-the-art performances on the point cloud completion task. Our code is available at https://github.com/xiaogangw/point-cloud-completion-shape-prior.",2153-0866,978-1-7281-6212-6,10.1109/IROS45743.2020.9340862,National University of Singapore; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9340862,,Learning systems;Three-dimensional displays;Shape;Task analysis;Kernel;Optimization;Intelligent robots,,16.0,,52,IEEE,10 Feb 2021,,,IEEE,IEEE Conferences,https://github.com/xiaogangw/point-cloud-completion-shape-prior,https://github.com/xiaogangw/point-cloud-completion-shape-prior
555,EPN: Edge-Aware PointNet for Object Recognition from Multi-View 2.5D Point Clouds,S. M. Ahmed; P. Liang; C. M. Chew,"Department of Mechanical Engineering, National University of Singapore; Advanced Robotics Center, National University of Singapore; Department of Mechanical Engineering, National University of Singapore",2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),28 Jan 2020,2019,,,3445,3450,"Performance of current 3D point based detectors is limited by the number of points they can process, consequently limiting their accuracy. In this paper we propose a novel architecture coined as Edge-Aware PointNet, that incorporates geometric shape priors as binary maps, integrated in parallel with the PointNet++ framework, through convolutional neural networks (CNNs). The proposed architecture takes individual object instances as input and learns the task of object recognition for 3D shapes. To train the network, we present a dataset of 31k 2.5D synthetic point clouds rendered from ModelNet40. Through 2.5D representation, the network learns object recognition despite occlusion that enables improved performance on objects from real world, while 2D binary maps enable feature learning that is independent of number of points in the point cloud. Comprehensive experimentation shows that the proposed network is able to improve performance by 2.5% on ModelNet40 and 2.6% on ModelNet10 datasets, as compared to the baseline PointNet++. We also show improved performance as compared to state-of-the-art methods, on a real world RGBD dataset where our network improves results by 8%. Our code and dataset is publicly available at github.com/Merium88/Edge-Aware-PointNet.",2153-0866,978-1-7281-4004-9,10.1109/IROS40897.2019.8967705,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8967705,,,,6.0,,19,IEEE,28 Jan 2020,,,IEEE,IEEE Conferences,https://github.com/Merium88/Edge-Aware-PointNet,https://github.com/Merium88/Edge-Aware-PointNet
556,RangeNet ++: Fast and Accurate LiDAR Semantic Segmentation,A. Milioto; I. Vizzo; J. Behley; C. Stachniss,"University of Bonn, Germany; University of Bonn, Germany; University of Bonn, Germany; University of Bonn, Germany",2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),28 Jan 2020,2019,,,4213,4220,"Perception in autonomous vehicles is often carried out through a suite of different sensing modalities. Given the massive amount of openly available labeled RGB data and the advent of high-quality deep learning algorithms for image-based recognition, high-level semantic perception tasks are pre-dominantly solved using high-resolution cameras. As a result of that, other sensor modalities potentially useful for this task are often ignored. In this paper, we push the state of the art in LiDAR-only semantic segmentation forward in order to provide another independent source of semantic information to the vehicle. Our approach can accurately perform full semantic segmentation of LiDAR point clouds at sensor frame rate. We exploit range images as an intermediate representation in combination with a Convolutional Neural Network (CNN) exploiting the rotating LiDAR sensor model. To obtain accurate results, we propose a novel post-processing algorithm that deals with problems arising from this intermediate representation such as discretization errors and blurry CNN outputs. We implemented and thoroughly evaluated our approach including several comparisons to the state of the art. Our experiments show that our approach outperforms state-of-the-art approaches, while still running online on a single embedded GPU. The code can be accessed at https://github.com/PRBonn/lidar-bonnetal.",2153-0866,978-1-7281-4004-9,10.1109/IROS40897.2019.8967762,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8967762,,,,735.0,,23,IEEE,28 Jan 2020,,,IEEE,IEEE Conferences,https://github.com/PRBonn/lidar-bonnetal,https://github.com/PRBonn/lidar-bonnetal
557,GPU Accelerated Robust Scene Reconstruction,W. Dong; J. Park; Y. Yang; M. Kaess,"Robotics Institute, Carnegie Mellon University, PA, USA; Compute Vision Lab, POSTECH, Pohang, South Korea; Robotics Institute, Carnegie Mellon University, PA, USA; Robotics Institute, Carnegie Mellon University, PA, USA",2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),28 Jan 2020,2019,,,7863,7870,"We propose a fast and accurate 3D reconstruction system that takes a sequence of RGB-D frames and produces a globally consistent camera trajectory and a dense 3D geometry. We redesign core modules of a state-of-the-art offline reconstruction pipeline to maximally exploit the power of GPU. We introduce GPU accelerated core modules that include RGBD odometry, geometric feature extraction and matching, point cloud registration, volumetric integration, and mesh extraction. Therefore, while being able to reproduce the results of the high-fidelity offline reconstruction system, our system runs more than 10 times faster on average. Nearly 10Hz can be achieved in medium size indoor scenes, making our offline system even comparable to online Simultaneous Localization and Mapping (SLAM) systems in terms of the speed. Experimental results show that our system produces more accurate results than several state-of-the-art online systems. The system is open source at https://github.com/theNded/Open3D.",2153-0866,978-1-7281-4004-9,10.1109/IROS40897.2019.8967693,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8967693,,,,8.0,,28,IEEE,28 Jan 2020,,,IEEE,IEEE Conferences,https://github.com/theNded/Open3D,https://github.com/theNded/Open3D
558,Online Trajectory Generation of a MAV for Chasing a Moving Target in 3D Dense Environments,B. F. Jeon; H. J. Kim,"Department of mechanical and aerospace engineering, Seoul national university of South Korea; Department of mechanical and aerospace engineering, Seoul national university of South Korea",2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),28 Jan 2020,2019,,,1115,1121,"This work deals with a moving target chasing mission of an aerial vehicle equipped with a vision sensor in a cluttered environment. In contrast to obstacle-free or sparse environments, the chaser should be able to handle collision and occlusion together with flight efficiency. In order to tackle these challenges in real-time, we introduce a metric for target visibility and propose a hierarchical chasing planner. In the first phase, we generate a sequence of waypoints and chasing corridors which ensure safety and optimize visibility. In the following phase, the corridors and waypoints are utilized as constraints and objective respectively in quadratic programming from which we complete a dynamically feasible trajectory for chasing. The proposed algorithm is tested in multiple dense environments. The simulator AutoChaser with full code implementation & GUI can be found in https://github.com/icsl-Jeon/traj_gen_vis and video is available at https://youtu.be/-2d3uDlYR_M.",2153-0866,978-1-7281-4004-9,10.1109/IROS40897.2019.8967840,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8967840,,,,26.0,,28,IEEE,28 Jan 2020,,,IEEE,IEEE Conferences,https://github.com/icsl-Jeon/traj_gen_vis,https://github.com/icsl-Jeon/traj_gen_vis
559,Variable Impedance Control in End-Effector Space: An Action Space for Reinforcement Learning in Contact-Rich Tasks,R. Martín-Martín; M. A. Lee; R. Gardner; S. Savarese; J. Bohg; A. Garg,"Stanford Artificial Intelligence Lab (SAIL), Stanford University, Nvidia, USA; Stanford Artificial Intelligence Lab (SAIL), Stanford University, Nvidia, USA; Stanford Artificial Intelligence Lab (SAIL), Stanford University, Nvidia, USA; Stanford Artificial Intelligence Lab (SAIL), Stanford University, Nvidia, USA; Stanford Artificial Intelligence Lab (SAIL), Stanford University, Nvidia, USA; Stanford Artificial Intelligence Lab (SAIL), Stanford University, Nvidia, USA",2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),28 Jan 2020,2019,,,1010,1017,"Reinforcement Learning (RL) of contact-rich manipulation tasks has yielded impressive results in recent years. While many studies in RL focus on varying the observation space or reward model, few efforts focused on the choice of action space (e.g. joint or end-effector space, position, velocity, etc.). However, studies in robot motion control indicate that choosing an action space that conforms to the characteristics of the task can simplify exploration and improve robustness to disturbances. This paper studies the effect of different action spaces in deep RL and advocates for variable impedance control in end-effector space (VICES) as an advantageous action space for constrained and contact-rich tasks. We evaluate multiple action spaces on three prototypical manipulation tasks: Path Following (task with no contact), Door Opening (task with kinematic constraints), and Surface Wiping (task with continuous contact). We show that VICES improves sample efficiency, maintains low energy consumption, and ensures safety across all three experimental setups. Further, RL policies learned with VICES can transfer across different robot models in simulation, and from simulation to real for the same robot. Further information is available at https://stanfordvl.github.io/vices.",2153-0866,978-1-7281-4004-9,10.1109/IROS40897.2019.8968201,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8968201,,,,112.0,,48,IEEE,28 Jan 2020,,,IEEE,IEEE Conferences,https://stanfordvl.github.io/vices,https://github.com/StanfordVL/robosuite/tree/vices_iros19
560,Efficient and Guaranteed Planar Pose Graph optimization Using the Complex Number Representation,T. Fan; H. Wang; M. Rubenstein; T. Murphey,"McCormick School of Engineering, Northwestern University, Evanston, IL, USA; McCormick School of Engineering, Northwestern University, Evanston, IL, USA; McCormick School of Engineering, Northwestern University, Evanston, IL, USA; McCormick School of Engineering, Northwestern University, Evanston, IL, USA",2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),28 Jan 2020,2019,,,1904,1911,"In this paper, we present CPL-Sync, a certifiably correct algorithm to solve planar pose graph optimization (PGO) using the complex number representation. We formulate planar PGO as the maximum likelihood estimation (MLE) on the product of unit complex numbers, and relax this nonconvex quadratic complex optimization problem to complex semidefinite programming (SDP). Furthermore, we simplify the corresponding semidefinite programming to Riemannian staircase optimization (RSO) on complex oblique manifolds that can be solved with the Riemannian trust region (RTR) method. In addition, we prove that the SDP relaxation and RSO simplification are tight as long as the noise magnitude is below a certain threshold. The efficacy of this work is validated through comparisons with existing methods as well as applications on planar PGO in simultaneous localization and mapping (SLAM), which indicates that the proposed algorithm is capable of solving planar PGO certifiably, and is more efficient in numerical computation and more robust to measurement noises than existing state-of-the-art methods. The C++ code for CPL-Sync is available at https://github.com/fantaosha/CPL-Sync.",2153-0866,978-1-7281-4004-9,10.1109/IROS40897.2019.8968044,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8968044,,,,10.0,,23,IEEE,28 Jan 2020,,,IEEE,IEEE Conferences,https://github.com/fantaosha/CPL-Sync,https://github.com/fantaosha/CPL-Sync
561,Duckiepond: An Open Education and Research Environment for a Fleet of Autonomous Maritime Vehicles,N. -C. Lin; Y. -C. Hsiao; Y. -W. Huang; C. -T. Hung; T. -K. Chuang; P. -W. Chen; J. -T. Huang; C. -C. Hsu; A. Censi; M. Benjamin; C. -F. Chen; H. -C. Wang,"Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan; Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan; Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan; Department of Engineering Science and Ocean Engineering, National Taiwan University, Taiwan; Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan; Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan; Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan; Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan; Department of Mechanical and Process Engineering, ETH Zürich, Switzerland; Department of Mechanical Engineering, Massachusetts Institute of Technology, USA; Department of Engineering Science and Ocean Engineering, National Taiwan University, Taiwan; Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan",2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),28 Jan 2020,2019,,,7219,7226,"Duckiepond is an education and research development environment that includes software systems, educational materials, and of a fleet of autonomous surface vehicles Duckieboat. Duckieboats are designed to be easily reproducible with parts from a 3D printer and other commercially available parts, with flexible software that leverages several open source packages. The Duckiepond environment is modeled after Duckietown and AI Driving Olympics environments: Duckieboats rely only on one monocular camera, IMU, and GPS, and perform all ML processing using onboard embedded computers. Duckiepond coordinates commonly used middlewares (ROS and MOOS) and containerized software packages in Docker, making it easy to deploy. The combination of learning-based methods together with classic methods enables important maritime missions: track and trail, navigation, and coordinate among Duckieboats to avoid collisions. Duckieboats have been operating in a man-made lake, reservoir and river environments. All software, hardware, and educational materials are openly available (https://robotx-nctu.github.io/duckiepond), with the goal of supporting research and education communities across related domains.",2153-0866,978-1-7281-4004-9,10.1109/IROS40897.2019.8967798,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8967798,,,,5.0,,47,IEEE,28 Jan 2020,,,IEEE,IEEE Conferences,https://robotx-nctu.github.io/duckiepond,https://github.com/RobotX-NCTU/duckiepond_gazebo
562,Pose-Aware Placement of Objects with Semantic Labels - Brandname-based Affordance Prediction and Cooperative Dual-Arm Active Manipulation,Y. -S. Su; S. -H. Lu; P. -S. Ser; W. -T. Hsu; W. -C. Lai; B. Xie; H. -M. Huang; T. -Y. Lee; H. -W. Chen; L. -F. Yu; H. -C. Wang,"Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan; Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan; Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan; Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan; Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan; Department of Computer Science, University of Massachusetts at Boston, USA; Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan; Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; Delta Research Center, Taiwan; Department of Computer Science, George Mason University, USA; Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan",2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),28 Jan 2020,2019,,,4760,4767,"The Amazon Picking Challenge and the Amazon Robotics Challenge have shown significant progress in object picking from a cluttered scene, yet object placement remains challenging. It is useful to have pose-aware placement based on human and machine readable pieces on an object. For example, the brandname of an object placed on a shelf should be facing the human customers. The robotic vision challenges in the object placement task: a) the semantics and geometry of the object to be placed need to be analysed jointly; b) and the occlusions among objects in a cluttered scene could make it hard for proper understanding and manipulation. To overcome these challenges, we develop a pose-aware placement approach by spotting the semantic labels (e.g., brandnames) of objects in a cluttered tote and then carrying out a sequence of actions to place the objects on a shelf or on a conveyor with desired poses. Our major contributions include 1) providing an open benchmark dataset of objects and brandnames with multi-view segmentation for training and evaluations; 2) carrying out comprehensive evaluations for our brandname-based fully convolutional network (FCN) that can predict the affordance and grasp to achieve pose-aware placement, whose success rates decrease along with clutters; 3) showing that active manipulation with two cooperative manipulators and grippers can effectively handle the occlusion of brandnames. We analyzed the success rates and discussed the failure cases to provide insights for future applications. All data and benchmarks are available at https://text-pick-n-place.github.io/TextPNP/.",2153-0866,978-1-7281-4004-9,10.1109/IROS40897.2019.8967755,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8967755,,,,2.0,,28,IEEE,28 Jan 2020,,,IEEE,IEEE Conferences,https://text-pick-n-place.github.io/TextPNP,
563,"Forecasting Time-to-Collision from Monocular Video: Feasibility, Dataset, and Challenges",A. Manglik; X. Weng; E. Ohn-Bar; K. M. Kitanil,"Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Max Planck Institute for Intelligent Systems; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA",2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),28 Jan 2020,2019,,,8081,8088,"We explore the possibility of using a single monocular camera to forecast the time to collision between a suitcase-shaped robot being pushed by its user and other nearby pedestrians. We develop a purely image-based deep learning approach that directly estimates the time to collision without the need of relying on explicit geometric depth estimates or velocity information to predict future collisions. While previous work has focused on detecting immediate collision in the context of navigating Unmanned Aerial Vehicles, the detection was limited to a binary variable (i.e., collision or no collision). We propose a more fine-grained approach to collision forecasting by predicting the exact time to collision in terms of milliseconds, which is more helpful for collision avoidance in the context of dynamic path planning. To evaluate our method, we have collected a novel dataset of over 13,000 indoor video segments each showing a trajectory of at least one person ending in a close proximity (a near collision) with the camera mounted on a mobile suitcase-shaped platform. Using this dataset, we do extensive experimentation on different temporal windows as input using an exhaustive list of state-of-the-art convolutional neural networks (CNNs). Our results show that our proposed multi-stream CNN is the best model for predicting time to near-collision. The average prediction error of our time to near-collision is 0.75 seconds across the test videos. The project webpage can be found at https://aashi7.github.io/NearCollision.html.",2153-0866,978-1-7281-4004-9,10.1109/IROS40897.2019.8967730,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8967730,,,,21.0,,40,IEEE,28 Jan 2020,,,IEEE,IEEE Conferences,https://aashi7.github.io/NearCollision,https://github.com/aashi7/NearCollision
564,Frustum ConvNet: Sliding Frustums to Aggregate Local Point-Wise Features for Amodal 3D Object Detection,Z. Wang; K. Jia,"School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China",2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),28 Jan 2020,2019,,,1742,1749,"In this work, we propose a novel method termed Frustum ConvNet (F-ConvNet) for amodal 3D object detection from point clouds. Given 2D region proposals in an RGB image, our method first generates a sequence of frustums for each region proposal, and uses the obtained frustums to group local points. F-ConvNet aggregates point-wise features as frustum-level feature vectors, and arrays these feature vectors as a feature map for use of its subsequent component of fully convolutional network (FCN), which spatially fuses frustum-level features and supports an end-to-end and continuous estimation of oriented boxes in the 3D space. We also propose component variants of F-ConvNet, including an FCN variant that extracts multi-resolution frustum features, and a refined use of F-ConvNet over a reduced 3D space. Careful ablation studies verify the efficacy of these component variants. F-ConvNet assumes no prior knowledge of the working 3D environment and is thus dataset-agnostic. We present experiments on both the indoor SUN-RGBD and outdoor KITTI datasets. F-ConvNet outperforms all existing methods on SUN-RGBD, and at the time of submission it outperforms all published works on the KITTI benchmark. Code has been made available at: https://github.com/zhixinwang/frustum-convnet.",2153-0866,978-1-7281-4004-9,10.1109/IROS40897.2019.8968513,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8968513,,,,345.0,,32,IEEE,28 Jan 2020,,,IEEE,IEEE Conferences,https://github.com/zhixinwang/frustum-convnet,https://github.com/zhixinwang/frustum-convnet
565,Fast and Incremental Loop Closure Detection Using Proximity Graphs,S. An; G. Che; F. Zhou; X. Liu; X. Ma; Y. Chen,"AR/VR department, JD.com, Beijing, China; AR/VR department, JD.com, Beijing, China; AR/VR department, JD.com, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Control Science and Engineering, Shandong University, Jinan, China; AR/VR department, JD.com, Beijing, China",2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),28 Jan 2020,2019,,,378,385,"Visual loop closure detection, which can be considered as an image retrieval task, is an important problem in SLAM (Simultaneous Localization and Mapping) systems. The frequently used bag-of-words (BoW) models can achieve high precision and moderate recall. However, the requirement for lower time costs and fewer memory costs for mobile robot applications is not well satisfied. In this paper, we propose a novel loop closure detection framework titled FILD’ (Fast and Incremental Loop closure Detection), which focuses on an on-line and incremental graph vocabulary construction for fast loop closure detection. The global and local features of frames are extracted using the Convolutional Neural Networks (CNN) and SURF on the GPU, which guarantee extremely fast extraction speeds. The graph vocabulary construction is based on one type of proximity graph, named Hierarchical Navigable Small World (HNSW) graphs, which is modified to adapt to this specific application. In addition, this process is coupled with a novel strategy for real-time geometrical verification, which only keeps binary hash codes and significantly saves on memory usage. Extensive experiments on several publicly available datasets show that the proposed approach can achieve fairly good recall at 100% precision compared to other state-of-the-art methods. The source code can be downloaded at https://github.comlAnshanTJU/FILD for further studies.",2153-0866,978-1-7281-4004-9,10.1109/IROS40897.2019.8968043,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8968043,,,,29.0,,41,IEEE,28 Jan 2020,,,IEEE,IEEE Conferences,,
566,Data Flow ORB-SLAM for Real-time Performance on Embedded GPU Boards,S. Aldegheri; N. Bombieri; D. D. Bloisi; A. Farinelli,"Department of Computer Science, University of Verona, Strada le Grazie 15 - 37134 Verona, Italy; Department of Computer Science, University of Verona, Strada le Grazie 15 - 37134 Verona, Italy; Department of Mathematics, Computer Science, and Economics, University of Basilicata, Viale dell’Ateneo Lucano, 10 - 85100 Potenza, Italy; Department of Computer Science, University of Verona, Strada le Grazie 15 - 37134 Verona, Italy",2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),28 Jan 2020,2019,,,5370,5375,"The use of embedded boards on robots, including unmanned aerial and ground vehicles, is increasing thanks to the availability of GPU equipped low-cost embedded boards in the market. Porting algorithms originally designed for desktop CPUs on those boards is not straightforward due to hardware limitations. In this paper, we present how we modified and customized the open source SLAM algorithm ORB-SLAM2 to run in real-time on the NVIDIA Jetson TX2. We adopted a data flow paradigm to process the images, obtaining an efficient CPU/GPU load distribution that results in a processing speed of about 30 frames per second. Quantitative experimental results on four different sequences of the KITTI datasets demonstrate the effectiveness of the proposed approach. The source code of our data flow ORB-SLAM2 algorithm is publicly available on GitHub.",2153-0866,978-1-7281-4004-9,10.1109/IROS40897.2019.8967814,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8967814,,,,19.0,,13,IEEE,28 Jan 2020,,,IEEE,IEEE Conferences,,
567,Disparity Sliding Window: Object Proposals from Disparity Images,J. Müller; A. Fregin; K. Dietmayer,"Dietmayer are with the Department of Measurement, University of Ulm, Ulm, Germany; Daimler AG, Research and Development, Ulm, Germany; Dietmayer are with the Department of Measurement, University of Ulm, Ulm, Germany",2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),6 Jan 2019,2018,,,5777,5784,"Sliding window approaches have been widely used for object recognition tasks in recent years [19], [4], [5], [18]. They guarantee an investigation of the entire input image for the object to be detected and allow a localization of that object. Despite the current trend towards deep neural networks, sliding window methods are still used in combination with convolutional neural networks [22]. The risk of overlooking an object is clearly reduced compared to alternative detection approaches which detect objects based on shape, edges or color. Nevertheless, the sliding window technique strongly increases the computational effort as the classifier has to verify a large number of object candidates. This paper proposes a sliding window approach which also uses depth information from a stereo camera. This leads to a greatly decreased number of object candidates without significantly reducing the detection accuracy. A theoretical investigation of the conventional sliding window approach is presented first. Other publications to date only mentioned rough estimations of the computational cost. A mathematical derivation clarifies the number of object candidates with respect to parameters such as image and object size. Subsequently, the proposed disparity sliding window approach is presented in detail. The approach is evaluated on pedestrian detection with annotations and images from the KITTI [10] object detection benchmark. Furthermore, a comparison with two state-of-the-art methods is made. Code is available in C++ and Python https://github.com/julimueller/disparity-sliding-window.",2153-0866,978-1-5386-8094-0,10.1109/IROS.2018.8593390,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8593390,,Microsoft Windows;Proposals;Computational efficiency;Task analysis;Cameras;Real-time systems;Image edge detection,,2.0,,30,IEEE,6 Jan 2019,,,IEEE,IEEE Conferences,https://github.com/julimueller/disparity-sliding-window,https://github.com/julimueller/disparity-sliding-window
568,Navigation without localisation: reliable teach and repeat based on the convergence theorem,T. Krajník; F. Majer; L. Halodová; T. Vintr,"Faculty of Electrical Engineering, Czech Technical University; Faculty of Electrical Engineering, Czech Technical University; Faculty of Electrical Engineering, Czech Technical University; Faculty of Electrical Engineering, Czech Technical University",2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),6 Jan 2019,2018,,,1657,1664,"We present a novel concept for teach-and-repeat visual navigation. The proposed concept is based on a mathematical model, which indicates that in teach-and-repeat navigation scenarios, mobile robots do not need to perform explicit localisation. Rather than that, a mobile robot which repeats a previously taught path can simply “replay” the learned velocities, while using its camera information only to correct its heading relative to the intended path. To support our claim, we establish a position error model of a robot, which traverses a taught path by only correcting its heading. Then, we outline a mathematical proof which shows that this position error does not diverge over time. Based on the insights from the model, we present a simple monocular teach-and-repeat navigation method. The method is computationally efficient, it does not require camera calibration, and it can learn and autonomously traverse arbitrarily-shaped paths. In a series of experiments, we demonstrate that the method can reliably guide mobile robots in realistic indoor and outdoor conditions, and can cope with imperfect odometry, landmark deficiency, illumination variations and naturally-occurring environment changes. Furthermore, we provide the navigation system and the datasets gathered at www.github.com/gestom/stroll_bearnav.",2153-0866,978-1-5386-8094-0,10.1109/IROS.2018.8593803,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8593803,,Robot kinematics;Navigation;Cameras;Robot vision systems;Simultaneous localization and mapping;Feature extraction,,32.0,,43,IEEE,6 Jan 2019,,,IEEE,IEEE Conferences,https://github.com/gestom/stroll_bearnav,https://github.com/gestom/stroll_bearnav
569,Quadtree-Accelerated Real-Time Monocular Dense Mapping,K. Wang; W. Ding; S. Shen,"The Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, SAR China; The Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, SAR China; The Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, SAR China",2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),6 Jan 2019,2018,,,1,9,"In this paper, we propose a novel mapping method for robotic navigation. High-quality dense depth maps are estimated and fused into 3D reconstructions in real-time using a single localized moving camera. The quadtree structure of the intensity image is used to reduce the computation burden by estimating the depth map in multiple resolutions. Both the quadtree-based pixel selection and the dynamic belief propagation are proposed to speed up the mapping process: pixels are selected and optimized with the computation resource according to their levels in the quadtree. Solved depth estimations are further interpolated and fused temporally into full resolution depth maps and fused into dense 3D maps using truncated signed distance function (TSDF). We compare our method with other state-of-the-art methods using the public datasets. Onboard UAV autonomous flight is also used to further prove the usability and efficiency of our method on portable devices. For the benefit of the community, the implementation is also released as open source at https://github.com/HKUST-Aerial-Robotics/open_quadtree_mapping.",2153-0866,978-1-5386-8094-0,10.1109/IROS.2018.8594101,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8594101,,Cameras;Three-dimensional displays;Belief propagation;Estimation;Optimization;Real-time systems;Image resolution,,19.0,,23,IEEE,6 Jan 2019,,,IEEE,IEEE Conferences,https://github.com/HKUST-Aerial-Robotics/open_quadtree_mapping,https://github.com/HKUST-Aerial-Robotics/open_quadtree_mapping
570,Setting up a Reinforcement Learning Task with a Real-World Robot,A. Rupam Mahmood; D. Korenkevych; B. J. Komer; J. Bergstra,Kindred Inc.; Kindred Inc.; Kindred Inc.; Kindred Inc.,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),6 Jan 2019,2018,,,4635,4640,"Reinforcement learning is a promising approach to developing hard-to-engineer adaptive solutions for complex and diverse robotic tasks. However, learning with real-world robots is often unreliable and difficult, which resulted in their low adoption in reinforcement learning research. This difficulty is worsened by the lack of guidelines for setting up learning tasks with robots. In this work, we develop a learning task with a UR5 robotic arm to bring to light some key elements of a task setup and study their contributions to the challenges with robots11Source code of the task and the computational model behind the setup available at https://github.com/kindredresearch/SenseAct. We find that learning performance can be highly sensitive to the setup, and thus oversights and omissions in setup details can make effective learning, reproducibility, and fair comparison hard. Our study suggests some mitigating steps to help future experimenters avoid difficulties and pitfalls. We show that highly reliable and repeatable experiments can be performed in our setup, indicating the possibility of reinforcement learning research extensively based on real-world robots.",2153-0866,978-1-5386-8094-0,10.1109/IROS.2018.8593894,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8593894,,Task analysis;Robot sensing systems;Instruction sets;Delays;Reinforcement learning;Robot kinematics,,21.0,,18,IEEE,6 Jan 2019,,,IEEE,IEEE Conferences,https://github.com/kindredresearch/SenseAct,https://github.com/kindredresearch/SenseAct
571,A Maximum Likelihood Approach to Extract Polylines from 2-D Laser Range Scans,A. Schaefer; D. Büscher; L. Luft; W. Burgard,"Department of Computer Science, University of Freiburg, Germany; Department of Computer Science, University of Freiburg, Germany; Department of Computer Science, University of Freiburg, Germany; Department of Computer Science, University of Freiburg, Germany",2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),6 Jan 2019,2018,,,4766,4773,"Man-made environments such as households, offices, or factory floors are typically composed of linear structures. Accordingly, polylines are a natural way to accurately represent their geometry. In this paper, we propose a novel probabilistic method to extract polylines from raw 2-D laser range scans. The key idea of our approach is to determine a set of polylines that maximizes the likelihood of a given scan. In extensive experiments carried out on publicly available real-world datasets and on simulated laser scans, we demonstrate that our method substantially outperforms existing state-of-the-art approaches in terms of accuracy, while showing comparable computational requirements. Our implementation is available under https://github.com/acschaefer/ple.",2153-0866,978-1-5386-8094-0,10.1109/IROS.2018.8593844,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8593844,,Feature extraction;Sensors;Lasers;Probabilistic logic;Optimization;Measurement by laser beam;Laser radar,,10.0,,26,IEEE,6 Jan 2019,,,IEEE,IEEE Conferences,https://github.com/acschaefer/ple,https://github.com/acschaefer/ple
572,VarNet: Exploring Variations for Unsupervised Video Prediction,B. Jin; Y. Hu; Y. Zeng; Q. Tang; S. Liu; J. Ye,"State Key Laboratory of Computer Architecture, University of Chinese Academy of Sciences, Beijing, P.R. China; State Key Laboratory of Computer Architecture, University of Chinese Academy of Sciences, Beijing, P.R. China; State Key Laboratory of Computer Architecture, University of Chinese Academy of Sciences, Beijing, P.R. China; State Key Laboratory of Computer Architecture, University of Chinese Academy of Sciences, Beijing, P.R. China; State Key Laboratory of Computer Architecture, University of Chinese Academy of Sciences, Beijing, P.R. China; State Key Laboratory of Computer Architecture, University of Chinese Academy of Sciences, Beijing, P.R. China",2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),6 Jan 2019,2018,,,5801,5806,"Unsupervised video prediction is a very challenging task due to the complexity and diversity in natural scenes. Prior works directly predicting pixels or optical flows either have the blurring problem or require additional assumptions. We highlight that the crux for video frame prediction lies in precisely capturing the inter-frame variations which encompass the movement of objects and the evolution of the surrounding environment. We then present an unsupervised video prediction framework - Variation Network (VarNet) to directly predict the variations between adjacent frames which are then fused with current frame to generate the future frame. In addition, we propose an adaptively re-weighting mechanism for loss function to offer each pixel a fair weight according to the amplitude of its variation. Extensive experiments for both short-term and long-term video prediction are implemented on two advanced datasets - KTH and KITTI with two evaluating metrics - PSNR and SSIM. For the KTH dataset, the VarNet outperforms the state-of-the-art works up to 11.9% on PSNR and 9.5% on SSIM. As for the KITTI dataset, the performance boosts are up to 55.1% on PSNR and 15.9% on SSIM. Moreover, we verify that the generalization ability of our model excels other state-of-the-art methods by testing on the unseen CalTech Pedestrian dataset after being trained on the KITTI dataset. Source code and video are available at https://github.com/jinbeibei/VarNet.",2153-0866,978-1-5386-8094-0,10.1109/IROS.2018.8594264,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8594264,,Generators;Training;Predictive models;Decoding;Video sequences;Neural networks;Generative adversarial networks,,21.0,,27,IEEE,6 Jan 2019,,,IEEE,IEEE Conferences,https://github.com/jinbeibei/VarNet,https://github.com/jinbeibei/VarNet
573,Joint 3D Proposal Generation and Object Detection from View Aggregation,J. Ku; M. Mozifian; J. Lee; A. Harakeh; S. L. Waslander,"Department of Mechanical and Mechatronics Engineering, Faculty of Engineering, University of Waterloo, Waterloo, ON, Canada; Department of Mechanical and Mechatronics Engineering, Faculty of Engineering, University of Waterloo, Waterloo, ON, Canada; Department of Mechanical and Mechatronics Engineering, Faculty of Engineering, University of Waterloo, Waterloo, ON, Canada; Department of Mechanical and Mechatronics Engineering, Faculty of Engineering, University of Waterloo, Waterloo, ON, Canada; Department of Mechanical and Mechatronics Engineering, Faculty of Engineering, University of Waterloo, Waterloo, ON, Canada",2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),6 Jan 2019,2018,,,1,8,"We present AVOD, an Aggregate View Object Detection network for autonomous driving scenarios. The proposed neural network architecture uses LIDAR point clouds and RGB images to generate features that are shared by two subnetworks: a region proposal network (RPN) and a second stage detector network. The proposed RPN uses a novel architecture capable of performing multimodal feature fusion on high resolution feature maps to generate reliable 3D object proposals for multiple object classes in road scenes. Using these proposals, the second stage detection network performs accurate oriented 3D bounding box regression and category classification to predict the extents, orientation, and classification of objects in 3D space. Our proposed architecture is shown to produce state of the art results on the KITTI 3D object detection benchmark [1] while running in real time with a low memory footprint, making it a suitable candidate for deployment on autonomous vehicles. Code is available at: https://github.com/kujason/avod.",2153-0866,978-1-5386-8094-0,10.1109/IROS.2018.8594049,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8594049,,Three-dimensional displays;Feature extraction;Proposals;Computer architecture;Agriculture;Object detection;Two dimensional displays,,973.0,3.0,21,IEEE,6 Jan 2019,,,IEEE,IEEE Conferences,https://github.com/kujason/avod,https://github.com/kujason/avod
574,Interaction System Based on an Avatar Projected on a Pyramidal Display,D. Loza Matovelle; S. Marcos; E. Zalama; J. Gómez García Bermejo,"DISA, University of Valladolid, Valladolid, España; CARTIF Foundation, Valladolid, España; ITAP-DISA, University of Valladolid, Valladolid, España; ITAP-DISA, University of Valladolid, Valladolid, España",2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),6 Jan 2019,2018,,,3943,3948,"In this paper an interaction system based on a three dimensional virtual head projected onto a pyramidal display is proposed. The proposed system makes use of a social robot behavioral architecture already developed in our lab, which allows us to interchange developments between our robotic realizations and the 3D avatar. The overall system is divided into two parts: back projection subsystem and expression generator subsystem. The back projection subsystem projects a three-dimensional avatar onto a pyramidal structure in order to achieve a sensation of depth and realism. The expression generator subsystem carries out the avatar animations using shape keys and bones, following the Facial Action Coding System (FACS). The system consists in several nodes that are integrated in ROS middleware (Robotic Operating System), and includes a user interface that makes the avatar teleoperation easier (the package is avaible in github public respository). In order to evaluate the expressiveness of the system, two sets of experiments have been performed: one to analyze the avatar's gestural ability, that is, its capability to perform expressions that can be identified by an observer, and a second experiment to measure the emotion displaying ability in terms of valence and arousal.",2153-0866,978-1-5386-8094-0,10.1109/IROS.2018.8593740,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8593740,,Avatars;Animation;Robots;Solid modeling;Face;Shape;Bones,,1.0,,12,IEEE,6 Jan 2019,,,IEEE,IEEE Conferences,,
575,CalibNet: Geometrically Supervised Extrinsic Calibration using 3D Spatial Transformer Networks,G. Iyer; R. K. Ram; J. K. Murthy; K. M. Krishna,"Robotics Research Center, International Institute of Information Technology, Hyderabad, India; Robotics Research Center, International Institute of Information Technology, Hyderabad, India; Mila, Université de Montréal, Quebec, Canada; Robotics Research Center, International Institute of Information Technology, Hyderabad, India",2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),6 Jan 2019,2018,,,1110,1117,"3D LiDARs and 2D cameras are increasingly being used alongside each other in sensor rigs for perception tasks. Before these sensors can be used to gather meaningful data, however, their extrinsics (and intrinsics) need to be accurately calibrated, as the performance of the sensor rig is extremely sensitive to these calibration parameters. A vast majority of existing calibration techniques require significant amounts of data and/or calibration targets and human effort, severely impacting their applicability in large-scale production systems. We address this gap with CalibNet: a geometrically supervised deep network capable of automatically estimating the 6-DoF rigid body transformation between a 3D LiDAR and a 2D camera in real-time. CalibNet alleviates the need for calibration targets, thereby resulting in significant savings in calibration efforts. During training, the network only takes as input a LiDAR point cloud, the corresponding monocular image, and the camera calibration matrix K. At train time, we do not impose direct supervision (i.e., we do not directly regress to the calibration parameters, for example). Instead, we train the network to predict calibration parameters that maximize the geometric and photometric consistency of the input images and point clouds. CalibNet learns to iteratively solve the underlying geometric problem and accurately predicts extrinsic calibration parameters for a wide range of mis-calibrations, without requiring retraining or domain adaptation. The project page is hosted at https://epiception.github.io/CalibNet.",2153-0866,978-1-5386-8094-0,10.1109/IROS.2018.8593693,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8593693,,Calibration;Three-dimensional displays;Cameras;Laser radar;Robot sensing systems;Training;Two dimensional displays,,124.0,1.0,24,IEEE,6 Jan 2019,,,IEEE,IEEE Conferences,https://epiception.github.io/CalibNet,
576,Minimax Iterative Dynamic Game: Application to Nonlinear Robot Control Tasks,O. Ogunmolu; N. Gans; T. Summers,"Department of Electrical Engineering; Department of Electrical Engineering; Department of Mechanical Engineering, University of Texas at Dallas, Richardson, TX, USA",2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),6 Jan 2019,2018,,,6919,6925,"Multistage decision policies provide useful control strategies in high-dimensional state spaces, particularly in complex control tasks. However, they exhibit weak performance guarantees in the presence of disturbance, model mismatch, or model uncertainties. This brittleness limits their use in high-risk scenarios. We present how to quantify the sensitivity of such policies in order to inform of their robustness capacity. We also propose a minimax iterative dynamic game framework for designing robust policies in the presence of disturbance/uncertainties. We test the quantification hypothesis on a carefully designed deep neural network policy; we then pose a minimax iterative dynamic game (iDG) framework for improving policy robustness in the presence of adversarial disturbances. We evaluate our iDG framework on a mecanum-wheeled robot, whose goal is to find a ocally robust optimal multistage policy that achieve a given goal-reaching task. The algorithm is simple and adaptable for designing meta-learning/deep policies that are robust against disturbances, model mismatch, or model uncertainties, up to a disturbance bound. Videos of the results are on the author's website: https://goo.gl/JhshTB, while the codes for reproducing our experiments are on github: https://goo.gl/3G2VBy. A self-contained environment for reproducing our results is on docker: https://goo.gl/Bo7MBe.",2153-0866,978-1-5386-8094-0,10.1109/IROS.2018.8594037,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8594037,,Robustness;Heuristic algorithms;Trajectory;Games;Task analysis;Uncertainty;Approximation algorithms,,9.0,,28,IEEE,6 Jan 2019,,,IEEE,IEEE Conferences,,
577,Adversarially Robust Policy Learning: Active construction of physically-plausible perturbations,A. Mandlekar; Y. Zhu; A. Garg; L. Fei-Fei; S. Savarese,"AI Lab, Stanford University, CA, US; AI Lab, Stanford University, CA, US; AI Lab, Stanford University, CA, US; AI Lab, Stanford University, CA, US; AI Lab, Stanford University, CA, US",2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),14 Dec 2017,2017,,,3932,3939,"Policy search methods in reinforcement learning have demonstrated success in scaling up to larger problems beyond toy examples. However, deploying these methods on real robots remains challenging due to the large sample complexity required during learning and their vulnerability to malicious intervention. We introduce Adversarially Robust Policy Learning (ARPL), an algorithm that leverages active computation of physically-plausible adversarial examples during training to enable robust policy learning in the source domain and robust performance under both random and adversarial input perturbations. We evaluate ARPL on four continuous control tasks and show superior resilience to changes in physical environment dynamics parameters and environment state as compared to state-of-the-art robust policy learning methods. Code, data, and additional experimental results are available at: stanfordvl.github.io/ARPL.",2153-0866,978-1-5386-2682-5,10.1109/IROS.2017.8206245,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8206245,,Robustness;Perturbation methods;Training;Computational modeling;Heuristic algorithms;Learning (artificial intelligence),,60.0,1.0,39,IEEE,14 Dec 2017,,,IEEE,IEEE Conferences,https://stanfordvl.github.io/ARPL,
578,A framework for quality assessment of ROS repositories,A. Santos; A. Cunha; N. Macedo; C. Lourenço,"HASLab – High-Assurance Software Laboratory, INESC TEC & Universidade do Minho, Braga, Portugal; HASLab – High-Assurance Software Laboratory, INESC TEC & Universidade do Minho, Braga, Portugal; HASLab – High-Assurance Software Laboratory, INESC TEC & Universidade do Minho, Braga, Portugal; HASLab – High-Assurance Software Laboratory, INESC TEC & Universidade do Minho, Braga, Portugal",2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1 Dec 2016,2016,,,4491,4496,"Robots are being increasingly used in safety-critical contexts, such as transportation and health. The need for flexible behavior in these contexts, due to human interaction factors or unstructured operating environments, led to a transition from hardware- to software-based safety mechanisms in robotic systems, whose reliability and quality is imperative to guarantee. Source code static analysis is a key component in formal software verification. It consists on inspecting code, often using automated tools, to determine a set of relevant properties that are known to influence the occurrence of defects in the final product. This paper presents HAROS, a generic, plug-in-driven, framework to evaluate code quality, through static analysis, in the context of the Robot Operating System (ROS), one of the most widely used robotic middleware. This tool (equipped with plug-ins for computing metrics and conformance to coding standards) was applied to several publicly available ROS repositories, whose results are also reported in the paper, thus providing a first overview of the internal quality of the software being developed in this community.",2153-0866,978-1-5090-3762-9,10.1109/IROS.2016.7759661,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7759661,,Software;Measurement;Context;Service robots;Safety;Standards,,33.0,,20,IEEE,1 Dec 2016,,,IEEE,IEEE Conferences,,
579,Finding next best views for autonomous UAV mapping through GPU-accelerated particle simulation,B. Adler; J. Xiao; J. Zhang,"Institute of Technical Aspects of Multimodal Systems, University of Hamburg, Germany; Institute of Technical Aspects of Multimodal Systems, University of Hamburg, Germany; Institute of Technical Aspects of Multimodal Systems, University of Hamburg, Germany",2013 IEEE/RSJ International Conference on Intelligent Robots and Systems,2 Jan 2014,2013,,,1056,1061,"This paper presents a novel algorithm capable of generating multiple next best views (NBVs), sorted by achievable information gain. Although being designed for way-point generation in autonomous airborne mapping of outdoor environments, it works directly on raw point clouds and thus can be used with any sensor generating spatial occupancy information (e.g. LIDAR, kinect or Time-of-Flight cameras). To satisfy time-constraints introduced by operation on UAVs, the algorithm is implemented on a highly parallel architecture and benchmarked against the previous, CPU-based proof of concept. As the underlying hardware imposes limitations with regards to memory access and concurrency, necessary data structures and further performance considerations are explained in detail. Open-source code for this paper is available at http://www.github.com/benadler/.",2153-0866,978-1-4673-6358-7,10.1109/IROS.2013.6696481,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6696481,,Graphics processing units;Robot sensing systems;Data structures;Lasers;Indexes;Instruction sets;Vectors,,4.0,,11,IEEE,2 Jan 2014,,,IEEE,IEEE Conferences,https://github.com/benadler,https://github.com/benadler
